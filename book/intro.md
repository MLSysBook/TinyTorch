# TinyğŸ”¥Torch: Build your own Machine Learning framework from scratch. 

**Most ML education teaches you to _use_ frameworks. TinyTorch teaches you to _build_ them.**

```{admonition} ğŸ¯ What You'll Build
:class: tip
**A complete ML framework from scratch**: your own PyTorch-style toolkit that can:
- âœ… Train neural networks on CIFAR-10 (real dataset!)
- âœ… Implement automatic differentiation (the "magic" behind PyTorch)  
- âœ… Deploy production systems with 75% model compression
- âœ… Handle complete ML pipeline from data to monitoring

**Result:** You become the expert others ask about "how PyTorch actually works."
```

---

## âš–ï¸ **Science vs Engineering: A Different Approach**

Most ML education focuses on the **science**: algorithms, theory, mathematical foundations. You learn *what* neural networks do and *why* they work.

TinyTorch focuses on the **engineering**: systems, implementation, production practices. You learn *how* to build working systems.

**Both matter.** But there's a critical gap in engineering education that TinyTorch fills.

---

## ğŸ’¡ **The Core Difference**

```python
Traditional ML Course:          TinyTorch Approach:
â”œâ”€â”€ import torch               â”œâ”€â”€ class Tensor:
â”œâ”€â”€ model = nn.Linear(10, 1)   â”‚     def __add__(self, other): ...
â”œâ”€â”€ loss = nn.MSELoss()        â”‚     def backward(self): ...
â””â”€â”€ optimizer.step()           â”œâ”€â”€ class Linear:
                               â”‚     def forward(self, x):
                               â”‚       return x @ self.weight + self.bias
                               â”œâ”€â”€ def mse_loss(pred, target):
                               â”‚     return ((pred - target) ** 2).mean()
                               â”œâ”€â”€ class SGD:
                               â”‚     def step(self):
                               â””â”€â”€     param.data -= lr * param.grad

Go from "How does this work?" ğŸ¤· to "I implemented every line!" ğŸ’ª
```

This isn't just about learning. It's about developing the deep systems thinking that distinguishes ML engineers from ML users.

---

## ğŸŒŸ **What Makes TinyTorch Different**

### **ğŸ”¬ Build-First Philosophy**
You don't just learn about tensors. You implement the `Tensor` class from scratch. You don't just use ReLU. You write the activation function yourself. Every component you build becomes part of your personal ML framework that actually works on real data.

### **ğŸš€ Production-Ready Skills**
From day one, you'll use professional development practices: the `tito` CLI for project management, automated testing for quality assurance, real datasets like CIFAR-10 for training, and MLOps patterns for deployment. This isn't toy code. It's the foundation for production ML systems.

### **âš¡ Instant Results**
Your code works immediately. Implement a `ReLU` function in Module 3, and by Module 5 you're watching it power real neural networks. Visual progress indicators and comprehensive testing ensure you always know your implementations are correct.

### **ğŸ¯ Progressive Mastery**
Start simple with a `hello_world()` function, build systematically through tensors and layers, and end with production MLOps systems. Each module builds on previous work, creating a complete learning journey from foundations to advanced systems.

---

## ğŸ“ **Learning Philosophy: Build â†’ Use â†’ Master**

Every component follows the same powerful learning cycle:

### **Example: Activation Functions**

**ğŸ”§ Build:** Implement ReLU from scratch
```python
def relu(x):
    # YOU implement this function
    return np.maximum(0, x)  # Your solution
```

**ğŸš€ Use:** Immediately use your own code
```python
from tinytorch.core.activations import ReLU  # YOUR implementation!
layer = ReLU()
output = layer.forward(input_tensor)  # Your code working!
```

**ğŸ’¡ Master:** See it working in real networks
```python
# Your ReLU is now part of a real neural network
model = Sequential([
    Dense(784, 128),
    ReLU(),           # <-- Your implementation
    Dense(128, 10)
])
```

This pattern repeats for every component: tensors, layers, optimizers, even MLOps systems. You build it, use it immediately, then see how it fits into larger systems.

---

## ğŸ“š **Course Journey: 14 Modules**

```{admonition} ğŸ—ï¸ Foundation
:class: note
**1. Setup** â€¢ **2. Tensors** â€¢ **3. Activations**

Professional development workflow, multi-dimensional arrays, and the mathematical functions that enable learning.
```

```{admonition} ğŸ§± Building Blocks
:class: note
**4. Layers** â€¢ **5. Networks** â€¢ **6. CNNs**

Dense layers, sequential architecture, and convolutional operations for computer vision.
```

```{admonition} ğŸ¯ Training Systems
:class: note
**7. DataLoader** â€¢ **8. Autograd** â€¢ **9. Optimizers** â€¢ **10. Training**

CIFAR-10 loading, automatic differentiation, SGD/Adam optimizers, and complete training orchestration.
```

```{admonition} âš¡ Production & Performance
:class: note
**11. Compression** â€¢ **12. Kernels** â€¢ **13. Benchmarking** â€¢ **14. MLOps**

Model optimization, high-performance operations, systematic evaluation, and production monitoring.
```

---

## ğŸš€ **Choose Your Learning Path**

```{admonition} Three Ways to Engage with TinyTorch
:class: important

### **ğŸ”¬ [Quick Exploration](usage-paths/quick-exploration.md)** *(5 minutes)*
*"I want to see what this is about"*
- Click and run code immediately in your browser (Binder)
- No installation or setup required
- Implement ReLU, tensors, neural networks interactively
- Perfect for getting a feel for the course

### **ğŸ—ï¸ [Serious Development](usage-paths/serious-development.md)** *(8+ weeks)*
*"I want to build this myself"*
- Fork the repo and work locally with full development environment
- Build complete ML framework from scratch with `tito` CLI
- 14 progressive assignments from setup to production MLOps
- Professional development workflow with automated testing

### **ğŸ‘¨â€ğŸ« [Classroom Use](usage-paths/classroom-use.md)** *(Instructors)*
*"I want to teach this course"*
- Complete course infrastructure with NBGrader integration
- Automated grading for comprehensive testing
- Flexible pacing (8-16 weeks) with proven pedagogical structure
- Turn-key solution for ML systems education
```

---

## ğŸš€ **Ready to Start?**

### **Quick Taste: Try Module 1 Right Now**
Want to see what TinyTorch feels like? **[Launch the Setup chapter](chapters/01-setup.md)** in Binder and implement your first TinyTorch function in 2 minutes!

---

## ğŸ™ **Acknowledgments**

TinyTorch originated from CS249r: Tiny Machine Learning Systems at Harvard University. We're inspired by projects like [tinygrad](https://github.com/geohot/tinygrad) and [micrograd](https://github.com/karpathy/micrograd) that demonstrate the power of minimal implementations.

**Our unique contribution**: TinyTorch is designed specifically as a **minimalistic educational framework** with progressive scaffolding, production-ready practices, and comprehensive course infrastructure. While other projects focus on research or production, TinyTorch bridges the gap between learning and building with systematic pedagogy.

This approach transforms students from framework users into framework builders through hands-on implementation experience.
