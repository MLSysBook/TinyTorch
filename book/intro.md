# TinyğŸ”¥Torch: Build your own Machine Learning framework from scratch. 

**Most ML education teaches you to _use_ frameworks. TinyTorch teaches you to _understand_ them.**

```{admonition} ğŸ¯ What You'll Build
:class: tip
**A complete ML framework from scratch** â€” your own PyTorch-style toolkit that can:
- âœ… Train neural networks on CIFAR-10 (real dataset!)
- âœ… Implement automatic differentiation (the "magic" behind PyTorch)  
- âœ… Deploy production systems with 75% model compression
- âœ… Handle complete ML pipeline from data to monitoring

**Result:** You become the expert others ask about "how PyTorch actually works."
```

## ğŸš€ **Start Building Now**

```{admonition} Choose Your Adventure
:class: important

### **ğŸ”¬ [Quick Try (5 min)](usage-paths/quick-exploration.md)** 
*Just want to see what this is?* â†’ Click and code in your browser. No setup needed.

### **ğŸ—ï¸ [Build for Real (8+ weeks)](usage-paths/serious-development.md)** 
*Ready to build your own ML framework?* â†’ Fork the repo and start your journey.

### **ğŸ‘¨â€ğŸ« [Teach This Course](usage-paths/classroom-use.md)** 
*Want complete course infrastructure?* â†’ Turn-key solution for educators.
```

---

## ğŸ’¡ **The Core Difference**

```python
Traditional ML Course:          TinyTorch Approach:
â”œâ”€â”€ import torch               â”œâ”€â”€ class Tensor:
â”œâ”€â”€ model = nn.Linear(10, 1)   â”‚     def __add__(self, other): ...
â”œâ”€â”€ loss = nn.MSELoss()        â”‚     def backward(self): ...
â””â”€â”€ optimizer.step()           â”œâ”€â”€ class Linear:
                               â”‚     def forward(self, x):
                               â”‚       return x @ self.weight + self.bias
                               â”œâ”€â”€ def mse_loss(pred, target):
                               â”‚     return ((pred - target) ** 2).mean()
                               â”œâ”€â”€ class SGD:
                               â”‚     def step(self):
                               â””â”€â”€     param.data -= lr * param.grad

Go from "How does this work?" ğŸ¤· to "I implemented every line!" ğŸ’ª
```

---

## ğŸŒŸ **Why TinyTorch Works**

### **ğŸ”¬ Build-First Learning**
No black boxes. Implement every component from scratch. Use YOUR code in real neural networks.

### **ğŸš€ Production-Ready Skills**
Professional workflow with `tito` CLI. Real datasets like CIFAR-10. MLOps patterns from day one.

### **âš¡ Instant Results**
Code works immediately. Visual progress indicators. Watch your `ReLU` power real networks.

### **ğŸ¯ Progressive Mastery**
Start simple (`hello_world()`). Build systematically. End powerful (production MLOps).

---

## ğŸ“š **Course Journey: 14 Modules**

```{admonition} ğŸ—ï¸ Foundation (Modules 1-5)
:class: note
**Weeks 1-6: Core Infrastructure**
- **Setup**: Professional development workflow  
- **Tensors**: Multi-dimensional arrays (like NumPy, but yours!)
- **Activations**: ReLU, Sigmoid, Tanh - the math that enables learning
- **Layers**: Dense layers with matrix multiplication
- **Networks**: Sequential architecture - chain layers into complete models
```

```{admonition} ğŸ§  Deep Learning (Modules 6-10)
:class: note
**Weeks 7-12: Complete Training Systems**
- **CNNs**: Convolutional operations for computer vision
- **DataLoader**: CIFAR-10 loading, batching, preprocessing  
- **Autograd**: Automatic differentiation engine (PyTorch's "magic")
- **Optimizers**: SGD, Adam, learning rate scheduling
- **Training**: Loss functions, metrics, complete orchestration
```

```{admonition} âš¡ Production (Modules 11-14)
:class: note
**Weeks 13-16: Real-World Deployment**
- **Compression**: Model pruning and quantization (75% size reduction)
- **Kernels**: High-performance custom operations
- **Benchmarking**: Systematic evaluation and performance measurement
- **MLOps**: Production monitoring, continuous learning, complete pipeline
```

---

## ğŸ“ **Learning Philosophy: Build â†’ Use â†’ Master**

### **Example: Activation Functions**

**ğŸ”§ Build:** Implement ReLU from scratch
```python
def relu(x):
    # YOU implement this function
    return ???  # What should this be?
```

**ğŸš€ Use:** Immediately use your own code
```python
from tinytorch.core.activations import ReLU  # YOUR implementation!
layer = ReLU()
output = layer.forward(input_tensor)  # Your code working!
```

**ğŸ’¡ Master:** See it working in real networks
```python
# Your ReLU is now part of a real neural network
model = Sequential([
    Dense(784, 128),
    ReLU(),           # <-- Your implementation
    Dense(128, 10)
])
```

This pattern repeats for every component - you build it, use it immediately, then see how it fits into larger systems.

---

## ğŸ“š **Academic Foundation**

TinyTorch grew out of CS249r: Tiny Machine Learning Systems at Harvard University. While the [Machine Learning Systems book](https://mlsysbook.ai) covers broad principles, TinyTorch gives you hands-on implementation experience.

---

## ğŸš€ **Ready to Start?**

### **Quick Taste: Try Module 1 Right Now**
Want to see what TinyTorch feels like? **[Launch the Setup chapter](chapters/01-setup.md)** in Binder and implement your first TinyTorch function in 2 minutes!
