{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69604136",
   "metadata": {},
   "source": [
    "# Autograd\n",
    "\n",
    "```{admonition} Interactive Learning\n",
    ":class: tip\n",
    "ðŸš€ **Launch Binder**: Click the rocket icon above to run this chapter interactively!\n",
    "\n",
    "ðŸ’¾ **Save Your Work**: Download your completed notebook when done.\n",
    "\n",
    "ðŸ—ï¸ **Build Locally**: Ready for serious development? [Fork the repo](https://github.com/your-org/tinytorch) and work locally with the full `tito` workflow.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7b4815f",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "autograd-imports",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtinytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# For development, import from local modules\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tinytorch'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# For development, import from local modules\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18;43m__file__\u001b[39;49m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m01_tensor\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensor_dev\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "#| default_exp core.autograd\n",
    "\n",
    "#| export\n",
    "import numpy as np\n",
    "import sys\n",
    "from typing import Union, List, Tuple, Optional, Any, Callable\n",
    "from collections import defaultdict\n",
    "\n",
    "# Import our existing components\n",
    "try:\n",
    "    from tinytorch.core.tensor import Tensor\n",
    "except ImportError:\n",
    "    # For development, import from local modules\n",
    "    import os\n",
    "    sys.path.append(os.path.join(os.path.dirname(__file__), '..', '01_tensor'))\n",
    "    from tensor_dev import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f55b76c",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "autograd-setup",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"ðŸ”¥ TinyTorch Autograd Module\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Python version: {sys.version_info.major}.{sys.version_info.minor}\")\n",
    "print(\"Ready to build automatic differentiation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9dc76c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ðŸ“¦ Where This Code Lives in the Final Package\n",
    "\n",
    "**Learning Side:** You work in `modules/source/07_autograd/autograd_dev.py`  \n",
    "**Building Side:** Code exports to `tinytorch.core.autograd`\n",
    "\n",
    "```python\n",
    "# Final package structure:\n",
    "from tinytorch.core.autograd import Variable, backward  # The gradient engine!\n",
    "from tinytorch.core.tensor import Tensor\n",
    "from tinytorch.core.activations import ReLU, Sigmoid, Tanh\n",
    "```\n",
    "\n",
    "**Why this matters:**\n",
    "- **Learning:** Focused module for understanding gradients\n",
    "- **Production:** Proper organization like PyTorch's `torch.autograd`\n",
    "- **Consistency:** All gradient operations live together in `core.autograd`\n",
    "- **Foundation:** Enables training for all neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5a4ba2",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## What is Automatic Differentiation?\n",
    "\n",
    "### The Problem: Computing Gradients at Scale\n",
    "Neural networks have millions of parameters. To train them, we need gradients of the loss function with respect to every parameter:\n",
    "\n",
    "```\n",
    "âˆ‡Î¸ L = [âˆ‚L/âˆ‚wâ‚, âˆ‚L/âˆ‚wâ‚‚, ..., âˆ‚L/âˆ‚wâ‚™, âˆ‚L/âˆ‚bâ‚, âˆ‚L/âˆ‚bâ‚‚, ..., âˆ‚L/âˆ‚bâ‚˜]\n",
    "```\n",
    "\n",
    "**Manual differentiation fails** because:\n",
    "- Networks have thousands of composed functions\n",
    "- Manual computation is extremely error-prone\n",
    "- Every architecture change requires re-deriving all gradients\n",
    "\n",
    "### The Solution: Automatic Differentiation\n",
    "**Autograd** automatically computes derivatives of functions represented as computational graphs:\n",
    "\n",
    "```python\n",
    "# Instead of manually computing: âˆ‚(xÂ² + 2xy + yÂ²)/âˆ‚x = 2x + 2y\n",
    "# Autograd does it automatically:\n",
    "x = Variable(3.0, requires_grad=True)\n",
    "y = Variable(4.0, requires_grad=True)\n",
    "z = x**2 + 2*x*y + y**2\n",
    "z.backward()\n",
    "print(x.grad)  # 2*3 + 2*4 = 14 (computed automatically!)\n",
    "```\n",
    "\n",
    "### Why This is Revolutionary\n",
    "- **Efficiency**: O(1) overhead per operation\n",
    "- **Flexibility**: Works with any differentiable function\n",
    "- **Correctness**: Implements chain rule precisely\n",
    "- **Scale**: Handles millions of parameters automatically\n",
    "\n",
    "### Real-World Impact\n",
    "- **PyTorch**: `torch.autograd` enables all neural network training\n",
    "- **TensorFlow**: `tf.GradientTape` provides similar functionality\n",
    "- **JAX**: `jax.grad` for high-performance computing\n",
    "- **Deep Learning**: Made training complex models practical\n",
    "\n",
    "Let's build the engine that powers modern AI!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877670dc",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 1: The Variable Class - Gradient Tracking\n",
    "\n",
    "### What is a Variable?\n",
    "A **Variable** wraps a Tensor and tracks:\n",
    "- **Data**: The actual values (forward pass)\n",
    "- **Gradient**: The computed gradients (backward pass)\n",
    "- **Computation history**: How this Variable was created\n",
    "- **Backward function**: How to compute gradients\n",
    "\n",
    "### The Computational Graph\n",
    "Variables automatically build a computational graph:\n",
    "\n",
    "```python\n",
    "x = Variable(2.0)  # Leaf node\n",
    "y = Variable(3.0)  # Leaf node\n",
    "z = x * y          # Intermediate node: z = x * y\n",
    "w = z + 1          # Output node: w = z + 1\n",
    "\n",
    "# Graph: x â”€â”€â†’ * â”€â”€â†’ + â”€â”€â†’ w\n",
    "#        y â”€â”€â†’   â”€â”€â†’   â”€â”€â†’\n",
    "```\n",
    "\n",
    "### Design Principles\n",
    "- **Transparency**: Works seamlessly with existing operations\n",
    "- **Efficiency**: Minimal overhead for forward pass\n",
    "- **Flexibility**: Supports any differentiable operation\n",
    "- **Correctness**: Implements chain rule precisely\n",
    "\n",
    "### Real-World Context\n",
    "This is like:\n",
    "- **PyTorch**: `torch.autograd.Variable` (now integrated into tensors)\n",
    "- **TensorFlow**: `tf.Variable` with gradient tracking\n",
    "- **JAX**: Variables with `jax.grad` transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cb94f9",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "variable-class",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class Variable:\n",
    "    \"\"\"\n",
    "    Variable: Tensor wrapper with automatic differentiation capabilities.\n",
    "    \n",
    "    The fundamental class for gradient computation in TinyTorch.\n",
    "    Wraps Tensor objects and tracks computational history for backpropagation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data: Union[Tensor, np.ndarray, list, float, int], \n",
    "                 requires_grad: bool = True, grad_fn: Optional[Callable] = None):\n",
    "        \"\"\"\n",
    "        Create a Variable with gradient tracking.\n",
    "            \n",
    "        TODO: Implement Variable initialization with gradient tracking.\n",
    "        \n",
    "        STEP-BY-STEP IMPLEMENTATION:\n",
    "        1. Convert data to Tensor if it's not already a Tensor\n",
    "        2. Store the tensor data in self.data\n",
    "        3. Set gradient tracking flag (requires_grad)\n",
    "        4. Initialize gradient to None (will be computed during backward pass)\n",
    "        5. Store the gradient function for backward pass\n",
    "        6. Track if this is a leaf node (no grad_fn means it's a leaf)\n",
    "        \n",
    "        EXAMPLE USAGE:\n",
    "        ```python\n",
    "        # Create leaf variables (input data)\n",
    "        x = Variable(5.0, requires_grad=True)\n",
    "        y = Variable([1, 2, 3], requires_grad=True)\n",
    "        \n",
    "        # Create intermediate variables (results of operations)\n",
    "        z = x + y  # Has grad_fn for addition\n",
    "        ```\n",
    "        \n",
    "        IMPLEMENTATION HINTS:\n",
    "        - Use isinstance(data, Tensor) to check type\n",
    "        - Convert with Tensor(data) if needed\n",
    "        - Store requires_grad, grad_fn flags\n",
    "        - Initialize self.grad = None\n",
    "        - Leaf nodes have grad_fn = None\n",
    "        - Set self.is_leaf = (grad_fn is None)\n",
    "        \n",
    "        LEARNING CONNECTIONS:\n",
    "        - This is like torch.Tensor with requires_grad=True\n",
    "        - Forms the basis for all neural network training\n",
    "        - Each Variable is a node in the computational graph\n",
    "        - Enables automatic gradient computation\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    @property\n",
    "    def shape(self) -> Tuple[int, ...]:\n",
    "        \"\"\"Get the shape of the underlying tensor.\"\"\"\n",
    "        return self.data.shape\n",
    "    \n",
    "    @property\n",
    "    def size(self) -> int:\n",
    "        \"\"\"Get the total number of elements.\"\"\"\n",
    "        return self.data.size\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"String representation of the Variable.\"\"\"\n",
    "        grad_str = f\", grad_fn={self.grad_fn.__name__}\" if self.grad_fn else \"\"\n",
    "        return f\"Variable({self.data.data.tolist()}, requires_grad={self.requires_grad}{grad_str})\"\n",
    "    \n",
    "    def backward(self, gradient: Optional['Variable'] = None) -> None:\n",
    "        \"\"\"\n",
    "        Compute gradients using backpropagation.\n",
    "        \n",
    "        TODO: Implement backward pass for gradient computation.\n",
    "        \n",
    "        STEP-BY-STEP IMPLEMENTATION:\n",
    "        1. If gradient is None, create gradient of ones (for scalar outputs)\n",
    "        2. If this Variable requires gradients, accumulate the gradient\n",
    "        3. If this Variable has a grad_fn, call it to propagate gradients\n",
    "        4. The grad_fn will recursively call backward on input Variables\n",
    "        \n",
    "        EXAMPLE USAGE:\n",
    "        ```python\n",
    "        x = Variable(2.0, requires_grad=True)\n",
    "        y = Variable(3.0, requires_grad=True)\n",
    "        z = add(x, y)  # z = 5.0\n",
    "        z.backward()\n",
    "        print(x.grad)  # 1.0 (âˆ‚z/âˆ‚x = 1)\n",
    "        print(y.grad)  # 1.0 (âˆ‚z/âˆ‚y = 1)\n",
    "        ```\n",
    "        \n",
    "        IMPLEMENTATION HINTS:\n",
    "        - If gradient is None: gradient = Variable(np.ones_like(self.data.data))\n",
    "        - If self.requires_grad: accumulate gradient into self.grad\n",
    "        - If self.grad_fn: call self.grad_fn(gradient)\n",
    "        - Handle gradient accumulation (add to existing gradient)\n",
    "        \n",
    "        LEARNING CONNECTIONS:\n",
    "        - This implements the chain rule of calculus\n",
    "        - Gradients flow backward through the computational graph\n",
    "        - Each operation contributes its local gradient\n",
    "        - Enables training of any differentiable function\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def zero_grad(self) -> None:\n",
    "        \"\"\"Reset gradients to zero.\"\"\"\n",
    "        self.grad = None\n",
    "    \n",
    "    def __add__(self, other: Union['Variable', float, int]) -> 'Variable':\n",
    "        \"\"\"Addition operator: self + other\"\"\"\n",
    "        return add(self, other)\n",
    "    \n",
    "    def __mul__(self, other: Union['Variable', float, int]) -> 'Variable':\n",
    "        \"\"\"Multiplication operator: self * other\"\"\"\n",
    "        return multiply(self, other)\n",
    "    \n",
    "    def __sub__(self, other: Union['Variable', float, int]) -> 'Variable':\n",
    "        \"\"\"Subtraction operator: self - other\"\"\"\n",
    "        return subtract(self, other)\n",
    "    \n",
    "    def __truediv__(self, other: Union['Variable', float, int]) -> 'Variable':\n",
    "        \"\"\"Division operator: self / other\"\"\"\n",
    "        return divide(self, other) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae083ee9",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### ðŸ§ª Test Your Variable Class\n",
    "\n",
    "Once you implement the Variable class above, run this cell to test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded5fa25",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-variable-class",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_variable_class():\n",
    "    \"\"\"Test Variable class implementation\"\"\"\n",
    "    print(\"ðŸ”¬ Unit Test: Variable Class...\")\n",
    "    \n",
    "    # Test Variable creation\n",
    "    x = Variable(5.0, requires_grad=True)\n",
    "    assert x.requires_grad == True, \"Variable should require gradients\"\n",
    "    assert x.is_leaf == True, \"Variable should be a leaf node\"\n",
    "    assert x.grad is None, \"Gradient should be None initially\"\n",
    "    \n",
    "    # Test data access\n",
    "    assert x.data.data.item() == 5.0, \"Data should be accessible\"\n",
    "    assert x.shape == (), \"Scalar should have empty shape\"\n",
    "    assert x.size == 1, \"Scalar should have size 1\"\n",
    "    \n",
    "    # Test with list input\n",
    "    y = Variable([1, 2, 3], requires_grad=True)\n",
    "    assert y.shape == (3,), \"List should create 1D tensor\"\n",
    "    assert y.size == 3, \"Size should be 3\"\n",
    "    \n",
    "    # Test with requires_grad=False\n",
    "    z = Variable(10.0, requires_grad=False)\n",
    "    assert z.requires_grad == False, \"Should not require gradients\"\n",
    "    \n",
    "    # Test zero_grad\n",
    "    x.grad = Variable(1.0)\n",
    "    x.zero_grad()\n",
    "    assert x.grad is None, \"zero_grad should reset gradient to None\"\n",
    "    \n",
    "    print(\"âœ… Variable class tests passed!\")\n",
    "    print(f\"âœ… Variable creation and initialization working\")\n",
    "    print(f\"âœ… Data access and properties working\")\n",
    "    print(f\"âœ… Gradient management working\")\n",
    "\n",
    "# Run inline tests when module is executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    test_variable_class()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448c6662",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 2: Basic Operations with Gradients\n",
    "\n",
    "### The Chain Rule in Action\n",
    "Every operation must implement:\n",
    "1. **Forward pass**: Compute the result\n",
    "2. **Backward pass**: Compute gradients for inputs\n",
    "\n",
    "### Example: Addition\n",
    "For z = x + y:\n",
    "- **Forward**: z.data = x.data + y.data\n",
    "- **Backward**: âˆ‚z/âˆ‚x = 1, âˆ‚z/âˆ‚y = 1\n",
    "\n",
    "### Mathematical Foundation\n",
    "The chain rule states:\n",
    "```\n",
    "âˆ‚f/âˆ‚x = âˆ‚f/âˆ‚z Â· âˆ‚z/âˆ‚x\n",
    "```\n",
    "\n",
    "For complex expressions like f(g(h(x))):\n",
    "```\n",
    "âˆ‚f/âˆ‚x = âˆ‚f/âˆ‚g Â· âˆ‚g/âˆ‚h Â· âˆ‚h/âˆ‚x\n",
    "```\n",
    "\n",
    "### Implementation Pattern\n",
    "Each operation returns a new Variable with:\n",
    "- **Forward result**: Computed value\n",
    "- **Backward function**: Gradient computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadb7f9a",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "add-operation",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def add(a: Union[Variable, float, int], b: Union[Variable, float, int]) -> Variable:\n",
    "    \"\"\"\n",
    "    Addition operation with gradient tracking: a + b\n",
    "    \n",
    "    TODO: Implement addition with automatic differentiation.\n",
    "    \n",
    "    STEP-BY-STEP IMPLEMENTATION:\n",
    "    1. Convert inputs to Variables if they're scalars\n",
    "    2. Compute forward pass: result = a.data + b.data\n",
    "    3. Create gradient function that implements: âˆ‚(a+b)/âˆ‚a = 1, âˆ‚(a+b)/âˆ‚b = 1\n",
    "    4. Return new Variable with result and gradient function\n",
    "    \n",
    "    MATHEMATICAL FOUNDATION:\n",
    "    - Forward: z = x + y\n",
    "    - Backward: âˆ‚z/âˆ‚x = 1, âˆ‚z/âˆ‚y = 1\n",
    "    - Chain rule: âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚z Â· âˆ‚z/âˆ‚x = âˆ‚L/âˆ‚z Â· 1 = âˆ‚L/âˆ‚z\n",
    "    \n",
    "    EXAMPLE USAGE:\n",
    "    ```python\n",
    "    x = Variable(2.0, requires_grad=True)\n",
    "    y = Variable(3.0, requires_grad=True)\n",
    "    z = add(x, y)  # z = 5.0\n",
    "    z.backward()\n",
    "    print(x.grad)  # 1.0 (âˆ‚z/âˆ‚x = 1)\n",
    "    print(y.grad)  # 1.0 (âˆ‚z/âˆ‚y = 1)\n",
    "    ```\n",
    "    \n",
    "    IMPLEMENTATION HINTS:\n",
    "    - Convert scalars: if isinstance(a, (int, float)): a = Variable(a, requires_grad=False)\n",
    "    - Forward pass: result_data = a.data + b.data\n",
    "    - Backward function: def grad_fn(grad_output): if a.requires_grad: a.backward(grad_output)\n",
    "    - Return: Variable(result_data, grad_fn=grad_fn)\n",
    "    - Only propagate gradients to Variables that require them\n",
    "    \n",
    "    LEARNING CONNECTIONS:\n",
    "    - This is like torch.add() with autograd\n",
    "    - Addition distributes gradients equally to both inputs\n",
    "    - Forms the basis for bias addition in neural networks\n",
    "    - Chain rule propagates gradients through the graph\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eb9427",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### ðŸ§ª Test Your Addition Operation\n",
    "\n",
    "Once you implement the add function above, run this cell to test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6fc147",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-add-operation",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_add_operation():\n",
    "    \"\"\"Test addition operation with gradients\"\"\"\n",
    "    print(\"ðŸ”¬ Unit Test: Addition Operation...\")\n",
    "    \n",
    "    # Test basic addition\n",
    "    x = Variable(2.0, requires_grad=True)\n",
    "    y = Variable(3.0, requires_grad=True)\n",
    "    z = add(x, y)\n",
    "    \n",
    "    assert z.data.data.item() == 5.0, \"Addition result should be 5.0\"\n",
    "    assert z.requires_grad == True, \"Result should require gradients\"\n",
    "    assert z.is_leaf == False, \"Result should not be a leaf node\"\n",
    "    \n",
    "    # Test backward pass\n",
    "    z.backward()\n",
    "    \n",
    "    assert x.grad is not None, \"x should have gradient\"\n",
    "    assert y.grad is not None, \"y should have gradient\"\n",
    "    assert x.grad.data.data.item() == 1.0, \"âˆ‚z/âˆ‚x should be 1.0\"\n",
    "    assert y.grad.data.data.item() == 1.0, \"âˆ‚z/âˆ‚y should be 1.0\"\n",
    "    \n",
    "    # Test with scalar\n",
    "    a = Variable(5.0, requires_grad=True)\n",
    "    b = add(a, 3.0)  # Add scalar\n",
    "    \n",
    "    assert b.data.data.item() == 8.0, \"Addition with scalar should work\"\n",
    "    \n",
    "    b.backward()\n",
    "    assert a.grad.data.data.item() == 1.0, \"Gradient through scalar addition should be 1.0\"\n",
    "    \n",
    "    print(\"âœ… Addition operation tests passed!\")\n",
    "    print(f\"âœ… Forward pass computing correct results\")\n",
    "    print(f\"âœ… Backward pass computing correct gradients\")\n",
    "    print(f\"âœ… Scalar addition working correctly\")\n",
    "\n",
    "# Run inline tests when module is executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    test_add_operation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b5900a",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 3: Multiplication Operation\n",
    "\n",
    "### The Product Rule\n",
    "For z = x * y:\n",
    "- **Forward**: z = x * y\n",
    "- **Backward**: âˆ‚z/âˆ‚x = y, âˆ‚z/âˆ‚y = x\n",
    "\n",
    "### Why This Matters\n",
    "Multiplication is everywhere in neural networks:\n",
    "- **Weight scaling**: w * x in dense layers\n",
    "- **Attention mechanisms**: attention_weights * values\n",
    "- **Gating**: gate_signal * hidden_state\n",
    "\n",
    "### Chain Rule Application\n",
    "When gradients flow back through multiplication:\n",
    "```\n",
    "âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚z Â· âˆ‚z/âˆ‚x = âˆ‚L/âˆ‚z Â· y\n",
    "âˆ‚L/âˆ‚y = âˆ‚L/âˆ‚z Â· âˆ‚z/âˆ‚y = âˆ‚L/âˆ‚z Â· x\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d063acc8",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "multiply-operation",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def multiply(a: Union[Variable, float, int], b: Union[Variable, float, int]) -> Variable:\n",
    "    \"\"\"\n",
    "    Multiplication operation with gradient tracking: a * b\n",
    "    \n",
    "    TODO: Implement multiplication with automatic differentiation.\n",
    "    \n",
    "    STEP-BY-STEP IMPLEMENTATION:\n",
    "    1. Convert inputs to Variables if they're scalars\n",
    "    2. Compute forward pass: result = a.data * b.data\n",
    "    3. Create gradient function implementing product rule: âˆ‚(a*b)/âˆ‚a = b, âˆ‚(a*b)/âˆ‚b = a\n",
    "    4. Return new Variable with result and gradient function\n",
    "    \n",
    "    MATHEMATICAL FOUNDATION:\n",
    "    - Forward: z = x * y\n",
    "    - Backward: âˆ‚z/âˆ‚x = y, âˆ‚z/âˆ‚y = x\n",
    "    - Chain rule: âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚z Â· y, âˆ‚L/âˆ‚y = âˆ‚L/âˆ‚z Â· x\n",
    "    \n",
    "    EXAMPLE USAGE:\n",
    "    ```python\n",
    "    x = Variable(2.0, requires_grad=True)\n",
    "    y = Variable(3.0, requires_grad=True)\n",
    "    z = multiply(x, y)  # z = 6.0\n",
    "    z.backward()\n",
    "    print(x.grad)  # 3.0 (âˆ‚z/âˆ‚x = y)\n",
    "    print(y.grad)  # 2.0 (âˆ‚z/âˆ‚y = x)\n",
    "    ```\n",
    "    \n",
    "    IMPLEMENTATION HINTS:\n",
    "    - Convert scalars to Variables (same as addition)\n",
    "    - Forward pass: result_data = a.data * b.data\n",
    "    - Backward function: multiply incoming gradient by the other variable\n",
    "    - For a: a.backward(grad_output * b.data)\n",
    "    - For b: b.backward(grad_output * a.data)\n",
    "    \n",
    "    LEARNING CONNECTIONS:\n",
    "    - This is like torch.mul() with autograd\n",
    "    - Product rule is fundamental to backpropagation\n",
    "    - Used in weight updates and attention mechanisms\n",
    "    - Each input's gradient depends on the other input's value\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a780a58",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### ðŸ§ª Test Your Multiplication Operation\n",
    "\n",
    "Once you implement the multiply function above, run this cell to test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87aea4a",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": true,
     "grade_id": "test-multiply-operation",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_multiply_operation():\n",
    "    \"\"\"Test multiplication operation with gradients\"\"\"\n",
    "    print(\"ðŸ”¬ Unit Test: Multiplication Operation...\")\n",
    "    \n",
    "    # Test basic multiplication\n",
    "    x = Variable(2.0, requires_grad=True)\n",
    "    y = Variable(3.0, requires_grad=True)\n",
    "    z = multiply(x, y)\n",
    "    \n",
    "    assert z.data.data.item() == 6.0, \"Multiplication result should be 6.0\"\n",
    "    assert z.requires_grad == True, \"Result should require gradients\"\n",
    "    \n",
    "    # Test backward pass\n",
    "    z.backward()\n",
    "    \n",
    "    assert x.grad is not None, \"x should have gradient\"\n",
    "    assert y.grad is not None, \"y should have gradient\"\n",
    "    assert x.grad.data.data.item() == 3.0, \"âˆ‚z/âˆ‚x should be y = 3.0\"\n",
    "    assert y.grad.data.data.item() == 2.0, \"âˆ‚z/âˆ‚y should be x = 2.0\"\n",
    "    \n",
    "    # Test with scalar\n",
    "    a = Variable(4.0, requires_grad=True)\n",
    "    b = multiply(a, 2.0)  # Multiply by scalar\n",
    "    \n",
    "    assert b.data.data.item() == 8.0, \"Multiplication with scalar should work\"\n",
    "    \n",
    "    b.backward()\n",
    "    assert a.grad.data.data.item() == 2.0, \"Gradient through scalar multiplication should be the scalar\"\n",
    "    \n",
    "    print(\"âœ… Multiplication operation tests passed!\")\n",
    "    print(f\"âœ… Forward pass computing correct results\")\n",
    "    print(f\"âœ… Backward pass implementing product rule correctly\")\n",
    "    print(f\"âœ… Scalar multiplication working correctly\")\n",
    "\n",
    "# Run inline tests when module is executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    test_multiply_operation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035f7c1c",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "subtract-operation",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def subtract(a: Union[Variable, float, int], b: Union[Variable, float, int]) -> Variable:\n",
    "    \"\"\"\n",
    "    Subtraction operation with gradient tracking.\n",
    "    \n",
    "    Args:\n",
    "        a: First operand (minuend)\n",
    "        b: Second operand (subtrahend)\n",
    "        \n",
    "    Returns:\n",
    "        Variable with difference and gradient function\n",
    "        \n",
    "    TODO: Implement subtraction with gradient computation.\n",
    "    \n",
    "    APPROACH:\n",
    "    1. Convert inputs to Variables if needed\n",
    "    2. Compute forward pass: result = a - b\n",
    "    3. Create gradient function with correct signs\n",
    "    4. Return Variable with result and grad_fn\n",
    "    \n",
    "    MATHEMATICAL RULE:\n",
    "    If z = x - y, then dz/dx = 1, dz/dy = -1\n",
    "    \n",
    "    EXAMPLE:\n",
    "    x = Variable(5.0), y = Variable(3.0)\n",
    "    z = subtract(x, y)  # z.data = 2.0\n",
    "    z.backward()        # x.grad = 1.0, y.grad = -1.0\n",
    "    \n",
    "    HINTS:\n",
    "    - Forward pass is straightforward: a - b\n",
    "    - Gradient for a is positive, for b is negative\n",
    "    - Remember to negate the gradient for b\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f690851",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "test-subtract-operation",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_subtract_operation():\n",
    "    \"\"\"Test subtraction operation with gradients\"\"\"\n",
    "    print(\"ðŸ”¬ Unit Test: Subtraction Operation...\")\n",
    "    \n",
    "    # Test basic subtraction\n",
    "    x = Variable(5.0, requires_grad=True)\n",
    "    y = Variable(3.0, requires_grad=True)\n",
    "    z = subtract(x, y)\n",
    "    \n",
    "    assert z.data.data.item() == 2.0, \"Subtraction result should be 2.0\"\n",
    "    assert z.requires_grad == True, \"Result should require gradients\"\n",
    "    \n",
    "    # Test backward pass\n",
    "    z.backward()\n",
    "    \n",
    "    assert x.grad is not None, \"x should have gradient\"\n",
    "    assert y.grad is not None, \"y should have gradient\"\n",
    "    assert x.grad.data.data.item() == 1.0, \"âˆ‚z/âˆ‚x should be 1.0\"\n",
    "    assert y.grad.data.data.item() == -1.0, \"âˆ‚z/âˆ‚y should be -1.0\"\n",
    "    \n",
    "    # Test with scalar\n",
    "    a = Variable(4.0, requires_grad=True)\n",
    "    b = subtract(a, 2.0)  # Subtract scalar\n",
    "    \n",
    "    assert b.data.data.item() == 2.0, \"Subtraction with scalar should work\"\n",
    "    \n",
    "    b.backward()\n",
    "    assert a.grad.data.data.item() == 1.0, \"Gradient through scalar subtraction should be 1.0\"\n",
    "    \n",
    "    print(\"âœ… Subtraction operation tests passed!\")\n",
    "    print(f\"âœ… Forward pass computing correct results\")\n",
    "    print(f\"âœ… Backward pass implementing subtraction rule correctly\")\n",
    "    print(f\"âœ… Scalar subtraction working correctly\")\n",
    "\n",
    "# Run inline tests when module is executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    test_subtract_operation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26e087f",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 4: Chain Rule in Complex Expressions\n",
    "\n",
    "### Building Complex Computations\n",
    "Now let's test how multiple operations work together through the chain rule:\n",
    "\n",
    "### Example: f(x, y) = (x + y) * (x - y)\n",
    "This creates a computational graph:\n",
    "```\n",
    "x â”€â”€â†’ + â”€â”€â†’ * â”€â”€â†’ result\n",
    "y â”€â”€â†’   â”€â”€â†’   â”€â”€â†’\n",
    "â”‚            â†‘\n",
    "â””â”€â”€â†’ - â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Chain Rule Application\n",
    "- **Forward**: Compute each operation in sequence\n",
    "- **Backward**: Gradients flow back through each operation\n",
    "- **Automatic**: No manual gradient computation needed!\n",
    "\n",
    "### Real-World Significance\n",
    "Complex neural networks are just larger versions of this:\n",
    "- **Millions of operations**: Each tracked automatically\n",
    "- **Complex architectures**: ResNet, Transformer, etc.\n",
    "- **Efficient computation**: O(1) overhead per operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f8fa1f",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-chain-rule",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_chain_rule():\n",
    "    \"\"\"Test chain rule with complex expressions\"\"\"\n",
    "    print(\"ðŸ”¬ Unit Test: Chain Rule with Complex Expressions...\")\n",
    "    \n",
    "    # Test: f(x, y) = (x + y) * (x - y) = xÂ² - yÂ²\n",
    "    x = Variable(3.0, requires_grad=True)\n",
    "    y = Variable(2.0, requires_grad=True)\n",
    "    \n",
    "    # Build expression step by step\n",
    "    sum_xy = add(x, y)      # x + y = 5.0\n",
    "    diff_xy = subtract(x, y) # x - y = 1.0\n",
    "    result = multiply(sum_xy, diff_xy)  # (x + y) * (x - y) = 5.0\n",
    "    \n",
    "    # Check forward pass\n",
    "    assert result.data.data.item() == 5.0, \"Forward pass should compute 5.0\"\n",
    "    \n",
    "    # Compute gradients\n",
    "    result.backward()\n",
    "    \n",
    "    # Check gradients: âˆ‚(xÂ²-yÂ²)/âˆ‚x = 2x, âˆ‚(xÂ²-yÂ²)/âˆ‚y = -2y\n",
    "    expected_x_grad = 2 * x.data.data.item()  # 2 * 3 = 6\n",
    "    expected_y_grad = -2 * y.data.data.item()  # -2 * 2 = -4\n",
    "    \n",
    "    assert abs(x.grad.data.data.item() - expected_x_grad) < 1e-6, f\"x gradient should be {expected_x_grad}\"\n",
    "    assert abs(y.grad.data.data.item() - expected_y_grad) < 1e-6, f\"y gradient should be {expected_y_grad}\"\n",
    "    \n",
    "    # Test more complex expression: f(x) = (x + 1) * (x + 2) * (x + 3)\n",
    "    x2 = Variable(1.0, requires_grad=True)\n",
    "    \n",
    "    term1 = add(x2, 1.0)    # x + 1 = 2.0\n",
    "    term2 = add(x2, 2.0)    # x + 2 = 3.0\n",
    "    term3 = add(x2, 3.0)    # x + 3 = 4.0\n",
    "    \n",
    "    product1 = multiply(term1, term2)  # (x + 1) * (x + 2) = 6.0\n",
    "    result2 = multiply(product1, term3)  # * (x + 3) = 24.0\n",
    "    \n",
    "    assert result2.data.data.item() == 24.0, \"Complex expression should compute 24.0\"\n",
    "    \n",
    "    result2.backward()\n",
    "    \n",
    "    # For f(x) = (x+1)(x+2)(x+3), f'(x) = 3xÂ² + 12x + 11\n",
    "    # At x=1: f'(1) = 3 + 12 + 11 = 26\n",
    "    expected_grad = 3 * (1.0**2) + 12 * 1.0 + 11  # 26\n",
    "    \n",
    "    assert abs(x2.grad.data.data.item() - expected_grad) < 1e-6, f\"Complex gradient should be {expected_grad}\"\n",
    "    \n",
    "    print(\"âœ… Chain rule tests passed!\")\n",
    "    print(f\"âœ… Simple expression: (x+y)*(x-y) = xÂ²-yÂ²\")\n",
    "    print(f\"âœ… Complex expression: (x+1)*(x+2)*(x+3)\")\n",
    "    print(f\"âœ… Automatic gradient computation working correctly\")\n",
    "    print(f\"âœ… Chain rule implemented correctly\")\n",
    "\n",
    "# Run inline tests when module is executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    test_chain_rule()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502858c1",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 5: Integration with Neural Network Training\n",
    "\n",
    "### The Complete Training Loop\n",
    "Let's see how autograd enables neural network training:\n",
    "\n",
    "1. **Forward pass**: Compute predictions\n",
    "2. **Loss computation**: Compare with targets\n",
    "3. **Backward pass**: Compute gradients automatically\n",
    "4. **Parameter update**: Update weights using gradients\n",
    "\n",
    "### Example: Simple Linear Regression\n",
    "   ```python\n",
    "# Model: y = wx + b\n",
    "w = Variable(0.5, requires_grad=True)\n",
    "b = Variable(0.1, requires_grad=True)\n",
    "\n",
    "    # Forward pass\n",
    "prediction = w * x + b\n",
    "\n",
    "# Loss: mean squared error\n",
    "loss = (prediction - target)**2\n",
    "\n",
    "# Backward pass (automatic!)\n",
    "loss.backward()\n",
    "\n",
    "# Update parameters\n",
    "w.data = w.data - learning_rate * w.grad.data\n",
    "b.data = b.data - learning_rate * b.grad.data\n",
    "```\n",
    "\n",
    "### Why This is Powerful\n",
    "- **Automatic**: No manual gradient computation\n",
    "- **Flexible**: Works with any differentiable function\n",
    "- **Efficient**: Minimal computational overhead\n",
    "- **Scalable**: Handles millions of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82fda8c",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": true,
     "grade_id": "test-neural-network-training",
     "locked": true,
     "points": 25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_neural_network_training():\n",
    "    \"\"\"Test autograd in neural network training scenario\"\"\"\n",
    "    print(\"ðŸ”¬ Unit Test: Neural Network Training Comprehensive Test...\")\n",
    "    \n",
    "    # Simple linear regression: y = wx + b\n",
    "    # Training data: y = 2x + 1 + noise\n",
    "    \n",
    "    # Initialize parameters\n",
    "    w = Variable(0.1, requires_grad=True)  # Start with small random value\n",
    "    b = Variable(0.0, requires_grad=True)  # Start with zero bias\n",
    "    \n",
    "    # Training data\n",
    "    x_data = [1.0, 2.0, 3.0, 4.0]\n",
    "    y_data = [3.0, 5.0, 7.0, 9.0]  # y = 2x + 1\n",
    "    \n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(100):\n",
    "        total_loss = Variable(0.0)\n",
    "        \n",
    "        for x_val, y_val in zip(x_data, y_data):\n",
    "            # Create input variable\n",
    "            x = Variable(x_val, requires_grad=False)\n",
    "            target = Variable(y_val, requires_grad=False)\n",
    "            \n",
    "    # Forward pass\n",
    "            prediction = add(multiply(w, x), b)  # wx + b\n",
    "            \n",
    "            # Loss: squared error\n",
    "            error = subtract(prediction, target)\n",
    "            loss = multiply(error, error)  # (pred - target)Â²\n",
    "            \n",
    "            # Accumulate loss\n",
    "            total_loss = add(total_loss, loss)\n",
    "        \n",
    "        # Backward pass\n",
    "        w.zero_grad()\n",
    "        b.zero_grad()\n",
    "        total_loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        if w.grad is not None:\n",
    "            w.data = Tensor(w.data.data - learning_rate * w.grad.data.data)\n",
    "        if b.grad is not None:\n",
    "            b.data = Tensor(b.data.data - learning_rate * b.grad.data.data)\n",
    "    \n",
    "    # Check that parameters converged to correct values\n",
    "    final_w = w.data.data.item()\n",
    "    final_b = b.data.data.item()\n",
    "    \n",
    "    print(f\"Final weights: w = {final_w:.3f}, b = {final_b:.3f}\")\n",
    "    print(f\"Target weights: w = 2.000, b = 1.000\")\n",
    "    \n",
    "    # Should be close to w=2, b=1\n",
    "    assert abs(final_w - 2.0) < 0.1, f\"Weight should be close to 2.0, got {final_w}\"\n",
    "    assert abs(final_b - 1.0) < 0.1, f\"Bias should be close to 1.0, got {final_b}\"\n",
    "    \n",
    "    # Test prediction with learned parameters\n",
    "    test_x = Variable(5.0, requires_grad=False)\n",
    "    test_prediction = add(multiply(w, test_x), b)\n",
    "    expected_output = 2.0 * 5.0 + 1.0  # 11.0\n",
    "    \n",
    "    prediction_error = abs(test_prediction.data.data.item() - expected_output)\n",
    "    assert prediction_error < 0.5, f\"Prediction error should be small, got {prediction_error}\"\n",
    "    \n",
    "    print(\"âœ… Neural network training comprehensive tests passed!\")\n",
    "    print(f\"âœ… Parameters converged to correct values\")\n",
    "    print(f\"âœ… Model makes accurate predictions\")\n",
    "    print(f\"âœ… Autograd enables automatic training\")\n",
    "    print(f\"âœ… Ready for complex neural network architectures!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca53c908",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ðŸ§ª Module Testing\n",
    "\n",
    "Time to test your implementation! This section uses TinyTorch's standardized testing framework to ensure your implementation works correctly.\n",
    "\n",
    "**This testing section is locked** - it provides consistent feedback across all modules and cannot be modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444df3ff",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "standardized-testing",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STANDARDIZED MODULE TESTING - DO NOT MODIFY\n",
    "# This cell is locked to ensure consistent testing across all TinyTorch modules\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from tito.tools.testing import run_module_tests_auto\n",
    "    \n",
    "    # Automatically discover and run all tests in this module\n",
    "    success = run_module_tests_auto(\"Autograd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0013259d",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ðŸŽ¯ Module Summary: Automatic Differentiation Mastery!\n",
    "\n",
    "Congratulations! You've successfully implemented the automatic differentiation engine that powers all modern deep learning:\n",
    "\n",
    "### âœ… What You've Built\n",
    "- **Variable Class**: Tensor wrapper with gradient tracking and computational graph construction\n",
    "- **Automatic Differentiation**: Forward and backward pass implementation\n",
    "- **Basic Operations**: Addition and multiplication with proper gradient computation\n",
    "- **Chain Rule**: Automatic gradient flow through complex expressions\n",
    "- **Training Integration**: Complete neural network training with automatic gradients\n",
    "\n",
    "### âœ… Key Learning Outcomes\n",
    "- **Understanding**: How automatic differentiation works through computational graphs\n",
    "- **Implementation**: Built the gradient engine from scratch\n",
    "- **Mathematical mastery**: Chain rule, product rule, and gradient computation\n",
    "- **Real-world application**: Saw how autograd enables neural network training\n",
    "- **Systems thinking**: Understanding the foundation of modern AI systems\n",
    "\n",
    "### âœ… Mathematical Foundations Mastered\n",
    "- **Chain Rule**: âˆ‚f/âˆ‚x = âˆ‚f/âˆ‚z Â· âˆ‚z/âˆ‚x for composite functions\n",
    "- **Product Rule**: âˆ‚(xy)/âˆ‚x = y, âˆ‚(xy)/âˆ‚y = x for multiplication\n",
    "- **Gradient Accumulation**: Handling multiple paths to the same variable\n",
    "- **Computational Graphs**: Forward pass builds graph, backward pass computes gradients\n",
    "\n",
    "### âœ… Professional Skills Developed\n",
    "- **Systems architecture**: Designed a scalable gradient computation system\n",
    "- **Memory management**: Efficient gradient storage and computation\n",
    "- **API design**: Clean interfaces for automatic differentiation\n",
    "- **Testing methodology**: Comprehensive validation of gradient computation\n",
    "\n",
    "### âœ… Ready for Advanced Applications\n",
    "Your autograd engine now enables:\n",
    "- **Deep Neural Networks**: Automatic gradient computation for any architecture\n",
    "- **Optimization**: Gradient-based parameter updates\n",
    "- **Complex Models**: Transformers, ResNets, any differentiable model\n",
    "- **Research**: Foundation for experimenting with new architectures\n",
    "\n",
    "### ðŸ”— Connection to Real ML Systems\n",
    "Your implementation mirrors production systems:\n",
    "- **PyTorch**: `torch.autograd` provides identical functionality\n",
    "- **TensorFlow**: `tf.GradientTape` implements similar concepts\n",
    "- **JAX**: `jax.grad` for high-performance automatic differentiation\n",
    "- **Industry Standard**: Every major ML framework uses these exact principles\n",
    "\n",
    "### ðŸŽ¯ The Power of Automatic Differentiation\n",
    "You've unlocked the key technology that made modern AI possible:\n",
    "- **Scalability**: Handles millions of parameters automatically\n",
    "- **Flexibility**: Works with any differentiable function\n",
    "- **Efficiency**: Minimal computational overhead\n",
    "- **Universality**: Enables training of any neural network architecture\n",
    "\n",
    "### ðŸ§  Deep Learning Revolution\n",
    "You now understand the technology that revolutionized AI:\n",
    "- **Before autograd**: Manual gradient computation limited model complexity\n",
    "- **After autograd**: Automatic gradients enabled deep learning revolution\n",
    "- **Modern AI**: GPT, BERT, ResNet all rely on automatic differentiation\n",
    "- **Future**: Your understanding enables you to build next-generation AI systems\n",
    "\n",
    "### ðŸš€ What's Next\n",
    "Your autograd engine is the foundation for:\n",
    "- **Optimizers**: SGD, Adam, and other gradient-based optimizers\n",
    "- **Training Loops**: Complete neural network training systems\n",
    "- **Advanced Architectures**: Transformers, GANs, and more complex models\n",
    "- **Research**: Experimenting with new differentiable algorithms\n",
    "\n",
    "**Next Module**: Advanced training systems, optimizers, and complete neural network architectures!\n",
    "\n",
    "You've built the engine that powers modern AI. Now let's use it to train intelligent systems that can learn to solve complex problems!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "mystnb": {
   "execution_mode": "auto"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}