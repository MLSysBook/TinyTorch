

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Compression &#8212; Tinyüî•Torch: Build ML Systems from Scratch</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapters/10-compression';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Kernels" href="11-kernels.html" />
    <link rel="prev" title="Training" href="09-training.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Tinyüî•Torch: Build ML Systems from Scratch - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Tinyüî•Torch: Build ML Systems from Scratch - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Tinyüî•Torch: Build Machine Learning Systems from Scratch
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Usage Paths</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../usage-paths/quick-exploration.html">üî¨ Quick Exploration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../usage-paths/serious-development.html">üèóÔ∏è Serious Development</a></li>
<li class="toctree-l1"><a class="reference internal" href="../usage-paths/classroom-use.html">üë®‚Äçüè´ Classroom Use</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Foundation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="00-setup.html">0. Development Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="01-tensor.html">1. Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="02-activations.html">2. Activations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Building Blocks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="03-layers.html">3. Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="04-networks.html">4. Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-cnn.html">5. CNNs</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Training Systems</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="06-dataloader.html">DataLoader</a></li>

<li class="toctree-l1"><a class="reference internal" href="07-autograd.html">7. Autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="08-optimizers.html">8. Optimizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="09-training.html">9. Training</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Production &amp; Performance</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">10. Compression</a></li>
<li class="toctree-l1"><a class="reference internal" href="11-kernels.html">11. Kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="12-benchmarking.html">12. Benchmarking</a></li>
<li class="toctree-l1"><a class="reference internal" href="13-mlops.html">13. MLOps</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/VJ/TinyTorch" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/VJ/TinyTorch/edit/main/book/tinytorch-course/chapters/10-compression.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/VJ/TinyTorch/issues/new?title=Issue%20on%20page%20%2Fchapters/10-compression.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapters/10-compression.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Compression</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#where-this-code-lives-in-the-final-package">üì¶ Where This Code Lives in the Final Package</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-model-compression">What is Model Compression?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-problem-ai-models-are-getting-huge">The Problem: AI Models Are Getting Huge</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-solution-intelligent-compression">The Solution: Intelligent Compression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#real-world-impact">Real-World Impact</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-we-ll-build">What We‚Äôll Build</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-understanding-model-size-and-parameters">Step 1: Understanding Model Size and Parameters</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-makes-models-large">What Makes Models Large?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-memory-reality-check">The Memory Reality Check</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-size-matters">Why Size Matters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-efficiency-spectrum">The Efficiency Spectrum</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#real-world-examples">Real-World Examples</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-magnitude-based-pruning-removing-unimportant-weights">Step 2: Magnitude-Based Pruning - Removing Unimportant Weights</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-magnitude-based-pruning">What is Magnitude-Based Pruning?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-algorithm">The Algorithm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-this-works">Why This Works</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#real-world-applications">Real-World Applications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pruning-strategies">Pruning Strategies</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-vs-sparsity-trade-off">Performance vs Sparsity Trade-off</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-quantization-reducing-precision-for-memory-efficiency">Step 3: Quantization - Reducing Precision for Memory Efficiency</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-quantization">What is Quantization?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-mathematical-foundation">The Mathematical Foundation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-quantization-works">Why Quantization Works</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-quantization">Types of Quantization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Real-World Impact</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#common-bit-widths">Common Bit-Widths</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-knowledge-distillation-large-models-teach-small-models">Step 4: Knowledge Distillation - Large Models Teach Small Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-knowledge-distillation">What is Knowledge Distillation?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-core-idea">The Core Idea</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">The Mathematical Foundation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-distillation-works">Why Distillation Works</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-parameters">Key Parameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Real-World Applications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#success-stories">Success Stories</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-structured-pruning-removing-entire-neurons-and-channels">Step 5: Structured Pruning - Removing Entire Neurons and Channels</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-structured-pruning">What is Structured Pruning?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#structured-vs-unstructured-pruning">Structured vs Unstructured Pruning</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#unstructured-pruning-what-we-did-in-step-2"><strong>Unstructured Pruning</strong> (What we did in Step 2)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#structured-pruning-what-we-re-doing-now"><strong>Structured Pruning</strong> (What we‚Äôre doing now)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-mathematical-impact">The Mathematical Impact</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-structured-pruning-works">Why Structured Pruning Works</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neuron-importance-metrics">Neuron Importance Metrics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Real-World Applications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges">Challenges</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-6-comprehensive-comparison-combining-all-techniques">Step 6: Comprehensive Comparison - Combining All Techniques</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-it-all-together">Putting It All Together</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-compression-toolkit">The Compression Toolkit</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#compression-strategy-design">Compression Strategy Design</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mobile-ai-deployment"><strong>Mobile AI Deployment</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#edge-computing"><strong>Edge Computing</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#production-cloud"><strong>Production Cloud</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#research-and-development"><strong>Research and Development</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#compression-pipeline-design">Compression Pipeline Design</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#trade-off-analysis">Trade-off Analysis</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#testing-infrastructure">üß™ Testing Infrastructure</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unit-testing-pattern">üî¨ Unit Testing Pattern</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#progress-tracking">üìà Progress Tracking</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#educational-value">üéì Educational Value</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-testing">üß™ Module Testing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-summary">üìã Module Summary</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-we-ve-built">‚úÖ What We‚Äôve Built</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#compressionmetrics"><strong>1. CompressionMetrics</strong> ‚úì</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#magnitude-based-pruning"><strong>2. Magnitude-Based Pruning</strong> ‚úì</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#quantization"><strong>3. Quantization</strong> ‚úì</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#knowledge-distillation"><strong>4. Knowledge Distillation</strong> ‚úì</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#structured-pruning"><strong>5. Structured Pruning</strong> ‚úì</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#comprehensive-comparison"><strong>6. Comprehensive Comparison</strong> ‚úì</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">üéØ Real-World Applications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#compression-achievements">üìä Compression Achievements</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#next-steps">üîó Next Steps</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#professional-applications">üöÄ Professional Applications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-future-of-efficient-ai">üéØ The Future of Efficient AI</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-skills-developed">üß† Key Skills Developed</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#next-steps-advanced-optimization">üöÄ Next Steps: Advanced Optimization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#module-11-kernels-hardware-aware-optimization">Module 11: Kernels - Hardware-Aware Optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#module-12-benchmarking-systematic-performance-measurement">Module 12: Benchmarking - Systematic Performance Measurement</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#module-13-mlops-production-deployment">Module 13: MLOps - Production Deployment</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#research-directions">üî¨ Research Directions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#career-applications">üíº Career Applications</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="compression">
<h1>Compression<a class="headerlink" href="#compression" title="Permalink to this heading">#</a></h1>
<div class="tip admonition">
<p class="admonition-title">Interactive Learning</p>
<p>üöÄ <strong>Launch Binder</strong>: Click the rocket icon above to run this chapter interactively!</p>
<p>üíæ <strong>Save Your Work</strong>: Download your completed notebook when done.</p>
<p>üèóÔ∏è <strong>Build Locally</strong>: Ready for serious development? <a class="reference external" href="https://github.com/your-org/tinytorch">Fork the repo</a> and work locally with the full <code class="docutils literal notranslate"><span class="pre">tito</span></code> workflow.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| default_exp core.compression</span>

<span class="c1">#| export</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Union</span><span class="p">,</span> <span class="n">Tuple</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>

<span class="c1"># Helper function to set up import paths</span>
<span class="k">def</span> <span class="nf">setup_import_paths</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Set up import paths for development modules.&quot;&quot;&quot;</span>
    <span class="kn">import</span> <span class="nn">sys</span>
    <span class="kn">import</span> <span class="nn">os</span>
    
    <span class="c1"># Add module directories to path</span>
    <span class="n">base_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="vm">__file__</span><span class="p">)))</span>
    <span class="n">module_dirs</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s1">&#39;01_tensor&#39;</span><span class="p">,</span> <span class="s1">&#39;02_activations&#39;</span><span class="p">,</span> <span class="s1">&#39;03_layers&#39;</span><span class="p">,</span> <span class="s1">&#39;04_networks&#39;</span><span class="p">,</span> 
        <span class="s1">&#39;05_cnn&#39;</span><span class="p">,</span> <span class="s1">&#39;06_dataloader&#39;</span><span class="p">,</span> <span class="s1">&#39;07_autograd&#39;</span><span class="p">,</span> <span class="s1">&#39;08_optimizers&#39;</span><span class="p">,</span> <span class="s1">&#39;09_training&#39;</span>
    <span class="p">]</span>
    
    <span class="k">for</span> <span class="n">module_dir</span> <span class="ow">in</span> <span class="n">module_dirs</span><span class="p">:</span>
        <span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">base_dir</span><span class="p">,</span> <span class="n">module_dir</span><span class="p">))</span>

<span class="c1"># Set up paths</span>
<span class="n">setup_import_paths</span><span class="p">()</span>

<span class="c1"># Import all the building blocks we need</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">tinytorch.core.tensor</span> <span class="kn">import</span> <span class="n">Tensor</span>
    <span class="kn">from</span> <span class="nn">tinytorch.core.layers</span> <span class="kn">import</span> <span class="n">Dense</span>
    <span class="kn">from</span> <span class="nn">tinytorch.core.networks</span> <span class="kn">import</span> <span class="n">Sequential</span>
    <span class="kn">from</span> <span class="nn">tinytorch.core.training</span> <span class="kn">import</span> <span class="n">CrossEntropyLoss</span><span class="p">,</span> <span class="n">Trainer</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="c1"># For development, create mock classes or import from local modules</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="kn">from</span> <span class="nn">tensor_dev</span> <span class="kn">import</span> <span class="n">Tensor</span>
        <span class="kn">from</span> <span class="nn">layers_dev</span> <span class="kn">import</span> <span class="n">Dense</span>
        <span class="kn">from</span> <span class="nn">networks_dev</span> <span class="kn">import</span> <span class="n">Sequential</span>
        <span class="kn">from</span> <span class="nn">training_dev</span> <span class="kn">import</span> <span class="n">CrossEntropyLoss</span><span class="p">,</span> <span class="n">Trainer</span>
    <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
        <span class="c1"># Create minimal mock classes for development</span>
        <span class="k">class</span> <span class="nc">Tensor</span><span class="p">:</span>
            <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span>
            
            <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Tensor(</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="si">}</span><span class="s2">)&quot;</span>
        
        <span class="k">class</span> <span class="nc">Dense</span><span class="p">:</span>
            <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">input_size</span> <span class="o">=</span> <span class="n">input_size</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">output_size</span><span class="p">))</span>
            
            <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Dense(</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">input_size</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">output_size</span><span class="si">}</span><span class="s2">)&quot;</span>
        
        <span class="k">class</span> <span class="nc">Sequential</span><span class="p">:</span>
            <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">layers</span> <span class="ow">or</span> <span class="p">[]</span>
        
        <span class="k">class</span> <span class="nc">CrossEntropyLoss</span><span class="p">:</span>
            <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                <span class="k">pass</span>
        
        <span class="k">class</span> <span class="nc">Trainer</span><span class="p">:</span>
            <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">loss_function</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">loss_function</span> <span class="o">=</span> <span class="n">loss_function</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">line</span> <span class="mi">28</span>
<span class="g g-Whitespace">     </span><span class="mi">25</span>         <span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">base_dir</span><span class="p">,</span> <span class="n">module_dir</span><span class="p">))</span>
<span class="g g-Whitespace">     </span><span class="mi">27</span> <span class="c1"># Set up paths</span>
<span class="ne">---&gt; </span><span class="mi">28</span> <span class="n">setup_import_paths</span><span class="p">()</span>
<span class="g g-Whitespace">     </span><span class="mi">30</span> <span class="c1"># Import all the building blocks we need</span>
<span class="g g-Whitespace">     </span><span class="mi">31</span> <span class="k">try</span><span class="p">:</span>

<span class="nn">Cell In[1], line 18,</span> in <span class="ni">setup_import_paths</span><span class="nt">()</span>
<span class="g g-Whitespace">     </span><span class="mi">15</span> <span class="kn">import</span> <span class="nn">os</span>
<span class="g g-Whitespace">     </span><span class="mi">17</span> <span class="c1"># Add module directories to path</span>
<span class="ne">---&gt; </span><span class="mi">18</span> <span class="n">base_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="vm">__file__</span><span class="p">)))</span>
<span class="g g-Whitespace">     </span><span class="mi">19</span> <span class="n">module_dirs</span> <span class="o">=</span> <span class="p">[</span>
<span class="g g-Whitespace">     </span><span class="mi">20</span>     <span class="s1">&#39;01_tensor&#39;</span><span class="p">,</span> <span class="s1">&#39;02_activations&#39;</span><span class="p">,</span> <span class="s1">&#39;03_layers&#39;</span><span class="p">,</span> <span class="s1">&#39;04_networks&#39;</span><span class="p">,</span> 
<span class="g g-Whitespace">     </span><span class="mi">21</span>     <span class="s1">&#39;05_cnn&#39;</span><span class="p">,</span> <span class="s1">&#39;06_dataloader&#39;</span><span class="p">,</span> <span class="s1">&#39;07_autograd&#39;</span><span class="p">,</span> <span class="s1">&#39;08_optimizers&#39;</span><span class="p">,</span> <span class="s1">&#39;09_training&#39;</span>
<span class="g g-Whitespace">     </span><span class="mi">22</span> <span class="p">]</span>
<span class="g g-Whitespace">     </span><span class="mi">24</span> <span class="k">for</span> <span class="n">module_dir</span> <span class="ow">in</span> <span class="n">module_dirs</span><span class="p">:</span>

<span class="ne">NameError</span>: name &#39;__file__&#39; is not defined
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üî• TinyTorch Compression Module&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;NumPy version: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Python version: </span><span class="si">{</span><span class="n">sys</span><span class="o">.</span><span class="n">version_info</span><span class="o">.</span><span class="n">major</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">sys</span><span class="o">.</span><span class="n">version_info</span><span class="o">.</span><span class="n">minor</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Ready to compress neural networks!&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="where-this-code-lives-in-the-final-package">
<h2>üì¶ Where This Code Lives in the Final Package<a class="headerlink" href="#where-this-code-lives-in-the-final-package" title="Permalink to this heading">#</a></h2>
<p><strong>Learning Side:</strong> You work in <code class="docutils literal notranslate"><span class="pre">modules/source/10_compression/compression_dev.py</span></code><br />
<strong>Building Side:</strong> Code exports to <code class="docutils literal notranslate"><span class="pre">tinytorch.core.compression</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Final package structure:</span>
<span class="kn">from</span> <span class="nn">tinytorch.core.compression</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">prune_weights_by_magnitude</span><span class="p">,</span>    <span class="c1"># Remove unimportant weights</span>
    <span class="n">quantize_layer_weights</span><span class="p">,</span>        <span class="c1"># Reduce precision for memory savings</span>
    <span class="n">DistillationLoss</span><span class="p">,</span>              <span class="c1"># Train compact models with teacher guidance</span>
    <span class="n">prune_layer_neurons</span><span class="p">,</span>           <span class="c1"># Remove entire neurons/channels</span>
    <span class="n">CompressionMetrics</span>             <span class="c1"># Measure model size and efficiency</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">tinytorch.core.layers</span> <span class="kn">import</span> <span class="n">Dense</span>     <span class="c1"># Target for compression</span>
<span class="kn">from</span> <span class="nn">tinytorch.core.networks</span> <span class="kn">import</span> <span class="n">Sequential</span>  <span class="c1"># Model architectures</span>
</pre></div>
</div>
<p><strong>Why this matters:</strong></p>
<ul class="simple">
<li><p><strong>Learning:</strong> Focused module for understanding model efficiency</p></li>
<li><p><strong>Production:</strong> Proper organization like PyTorch‚Äôs compression tools</p></li>
<li><p><strong>Consistency:</strong> All compression techniques live together in <code class="docutils literal notranslate"><span class="pre">core.compression</span></code></p></li>
<li><p><strong>Foundation:</strong> Essential for deploying AI in resource-constrained environments</p></li>
</ul>
</section>
<section id="what-is-model-compression">
<h2>What is Model Compression?<a class="headerlink" href="#what-is-model-compression" title="Permalink to this heading">#</a></h2>
<section id="the-problem-ai-models-are-getting-huge">
<h3>The Problem: AI Models Are Getting Huge<a class="headerlink" href="#the-problem-ai-models-are-getting-huge" title="Permalink to this heading">#</a></h3>
<p>Modern neural networks are massive:</p>
<ul class="simple">
<li><p><strong>GPT-3</strong>: 175 billion parameters (350GB memory)</p></li>
<li><p><strong>ResNet-152</strong>: 60 million parameters (240MB memory)</p></li>
<li><p><strong>BERT-Large</strong>: 340 million parameters (1.3GB memory)</p></li>
</ul>
<p>But deployment environments have constraints:</p>
<ul class="simple">
<li><p><strong>Mobile phones</strong>: Limited memory and battery</p></li>
<li><p><strong>Edge devices</strong>: No internet, minimal compute</p></li>
<li><p><strong>Real-time systems</strong>: Strict latency requirements</p></li>
<li><p><strong>Cost optimization</strong>: Expensive inference in cloud</p></li>
</ul>
</section>
<section id="the-solution-intelligent-compression">
<h3>The Solution: Intelligent Compression<a class="headerlink" href="#the-solution-intelligent-compression" title="Permalink to this heading">#</a></h3>
<p><strong>Model compression</strong> reduces model size while preserving performance:</p>
<ul class="simple">
<li><p><strong>Pruning</strong>: Remove unimportant weights and neurons</p></li>
<li><p><strong>Quantization</strong>: Use fewer bits per parameter</p></li>
<li><p><strong>Knowledge distillation</strong>: Train small models to mimic large ones</p></li>
<li><p><strong>Structured optimization</strong>: Modify architectures for efficiency</p></li>
</ul>
</section>
<section id="real-world-impact">
<h3>Real-World Impact<a class="headerlink" href="#real-world-impact" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Mobile AI</strong>: Apps like Google Translate work offline</p></li>
<li><p><strong>Autonomous vehicles</strong>: Real-time processing with limited compute</p></li>
<li><p><strong>IoT devices</strong>: Smart cameras, voice assistants, sensors</p></li>
<li><p><strong>Cost savings</strong>: Reduced inference costs in production systems</p></li>
</ul>
</section>
<section id="what-we-ll-build">
<h3>What We‚Äôll Build<a class="headerlink" href="#what-we-ll-build" title="Permalink to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Magnitude-based pruning</strong>: Remove smallest weights</p></li>
<li><p><strong>Quantization</strong>: Convert FP32 ‚Üí INT8 for 75% memory reduction</p></li>
<li><p><strong>Knowledge distillation</strong>: Large models teach small models</p></li>
<li><p><strong>Structured pruning</strong>: Remove entire neurons systematically</p></li>
<li><p><strong>Compression metrics</strong>: Measure efficiency and accuracy trade-offs</p></li>
<li><p><strong>Integrated optimization</strong>: Combine techniques for maximum benefit</p></li>
</ol>
</section>
</section>
<section id="step-1-understanding-model-size-and-parameters">
<h2>Step 1: Understanding Model Size and Parameters<a class="headerlink" href="#step-1-understanding-model-size-and-parameters" title="Permalink to this heading">#</a></h2>
<section id="what-makes-models-large">
<h3>What Makes Models Large?<a class="headerlink" href="#what-makes-models-large" title="Permalink to this heading">#</a></h3>
<p>Neural networks have millions of parameters:</p>
<ul class="simple">
<li><p><strong>Dense layers</strong>: Weight matrices <code class="docutils literal notranslate"><span class="pre">(input_size,</span> <span class="pre">output_size)</span></code></p></li>
<li><p><strong>Bias vectors</strong>: One per output neuron</p></li>
<li><p><strong>CNN kernels</strong>: Repeated across channels and filters</p></li>
<li><p><strong>Embeddings</strong>: Large vocabulary mappings</p></li>
</ul>
</section>
<section id="the-memory-reality-check">
<h3>The Memory Reality Check<a class="headerlink" href="#the-memory-reality-check" title="Permalink to this heading">#</a></h3>
<p>Let‚Äôs see how much memory different architectures use:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Simple MLP for MNIST</span>
<span class="n">layer1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>    <span class="c1"># 784 * 128 = 100,352 params</span>
<span class="n">layer2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>     <span class="c1"># 128 * 64 = 8,192 params  </span>
<span class="n">layer3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>      <span class="c1"># 64 * 10 = 640 params</span>
<span class="c1"># Total: 109,184 params ‚âà 437KB (FP32)</span>

<span class="c1"># Larger network for CIFAR-10</span>
<span class="n">layer1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">3072</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>   <span class="c1"># 3072 * 512 = 1,572,864 params</span>
<span class="n">layer2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>    <span class="c1"># 512 * 256 = 131,072 params</span>
<span class="n">layer3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>    <span class="c1"># 256 * 128 = 32,768 params</span>
<span class="n">layer4</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>     <span class="c1"># 128 * 10 = 1,280 params</span>
<span class="c1"># Total: 1,737,984 params ‚âà 7MB (FP32)</span>
</pre></div>
</div>
</section>
<section id="why-size-matters">
<h3>Why Size Matters<a class="headerlink" href="#why-size-matters" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Memory usage</strong>: Each FP32 parameter uses 4 bytes</p></li>
<li><p><strong>Storage</strong>: Model files need to be downloaded/stored</p></li>
<li><p><strong>Inference speed</strong>: More parameters = more computation</p></li>
<li><p><strong>Energy consumption</strong>: Larger models drain battery faster</p></li>
</ul>
</section>
<section id="the-efficiency-spectrum">
<h3>The Efficiency Spectrum<a class="headerlink" href="#the-efficiency-spectrum" title="Permalink to this heading">#</a></h3>
<p>Different applications need different efficiency levels:</p>
<ul class="simple">
<li><p><strong>Research</strong>: Accuracy first, efficiency second</p></li>
<li><p><strong>Production</strong>: Balance accuracy and efficiency</p></li>
<li><p><strong>Mobile</strong>: Strict size constraints (&lt; 10MB)</p></li>
<li><p><strong>Edge</strong>: Extreme efficiency requirements (&lt; 1MB)</p></li>
</ul>
</section>
<section id="real-world-examples">
<h3>Real-World Examples<a class="headerlink" href="#real-world-examples" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>MobileNet</strong>: Designed for mobile deployment</p></li>
<li><p><strong>DistilBERT</strong>: 60% smaller than BERT with 97% performance</p></li>
<li><p><strong>TinyML</strong>: Models under 1MB for microcontrollers</p></li>
<li><p><strong>Neural architecture search</strong>: Automated efficiency optimization</p></li>
</ul>
<p>Let‚Äôs build tools to measure and analyze model size!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| export</span>
<span class="k">class</span> <span class="nc">CompressionMetrics</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Utilities for measuring model size, sparsity, and compression efficiency.</span>
<span class="sd">    </span>
<span class="sd">    This class provides tools to analyze neural network models and understand</span>
<span class="sd">    their memory footprint, parameter distribution, and compression potential.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize compression metrics analyzer.&quot;&quot;&quot;</span>
        <span class="k">pass</span>
    
    <span class="k">def</span> <span class="nf">count_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">Sequential</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Count parameters in a neural network model.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            model: Sequential model to analyze</span>
<span class="sd">            </span>
<span class="sd">        Returns:</span>
<span class="sd">            Dictionary with parameter counts per layer and total</span>
<span class="sd">            </span>
<span class="sd">        TODO: Implement parameter counting for neural network analysis.</span>
<span class="sd">        </span>
<span class="sd">        STEP-BY-STEP IMPLEMENTATION:</span>
<span class="sd">        1. Initialize counters for different parameter types</span>
<span class="sd">        2. Iterate through each layer in the model</span>
<span class="sd">        3. Count weights and biases for each layer</span>
<span class="sd">        4. Calculate total parameters across all layers</span>
<span class="sd">        5. Return detailed breakdown dictionary</span>
<span class="sd">        </span>
<span class="sd">        EXAMPLE OUTPUT:</span>
<span class="sd">        {</span>
<span class="sd">            &#39;layer_0_weights&#39;: 100352,</span>
<span class="sd">            &#39;layer_0_bias&#39;: 128,</span>
<span class="sd">            &#39;layer_1_weights&#39;: 8192,</span>
<span class="sd">            &#39;layer_1_bias&#39;: 64,</span>
<span class="sd">            &#39;layer_2_weights&#39;: 640,</span>
<span class="sd">            &#39;layer_2_bias&#39;: 10,</span>
<span class="sd">            &#39;total_parameters&#39;: 109386,</span>
<span class="sd">            &#39;total_weights&#39;: 109184,</span>
<span class="sd">            &#39;total_bias&#39;: 202</span>
<span class="sd">        }</span>
<span class="sd">        </span>
<span class="sd">        IMPLEMENTATION HINTS:</span>
<span class="sd">        - Use hasattr() to check if layer has weights/bias attributes</span>
<span class="sd">        - Weight matrices have shape (input_size, output_size)</span>
<span class="sd">        - Bias vectors have shape (output_size,)</span>
<span class="sd">        - Use np.prod() to calculate total elements from shape</span>
<span class="sd">        - Track layer index for detailed reporting</span>
<span class="sd">        </span>
<span class="sd">        LEARNING CONNECTIONS:</span>
<span class="sd">        - This is like `model.numel()` in PyTorch</span>
<span class="sd">        - Understanding where parameters are concentrated</span>
<span class="sd">        - Foundation for compression target selection</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1">### BEGIN SOLUTION</span>
    <span class="c1"># YOUR CODE HERE</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
        <span class="c1">### END SOLUTION </span>

    <span class="k">def</span> <span class="nf">calculate_model_size</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">Sequential</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;float32&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculate memory footprint of a neural network model.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            model: Sequential model to analyze</span>
<span class="sd">            dtype: Data type for size calculation (&#39;float32&#39;, &#39;float16&#39;, &#39;int8&#39;)</span>
<span class="sd">            </span>
<span class="sd">        Returns:</span>
<span class="sd">            Dictionary with size information in different units</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Get parameter count</span>
        <span class="n">param_info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">count_parameters</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="n">total_params</span> <span class="o">=</span> <span class="n">param_info</span><span class="p">[</span><span class="s1">&#39;total_parameters&#39;</span><span class="p">]</span>
        
        <span class="c1"># Determine bytes per parameter</span>
        <span class="n">bytes_per_param</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;float32&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
            <span class="s1">&#39;float16&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
            <span class="s1">&#39;int8&#39;</span><span class="p">:</span> <span class="mi">1</span>
        <span class="p">}</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
        
        <span class="c1"># Calculate sizes</span>
        <span class="n">total_bytes</span> <span class="o">=</span> <span class="n">total_params</span> <span class="o">*</span> <span class="n">bytes_per_param</span>
        <span class="n">size_kb</span> <span class="o">=</span> <span class="n">total_bytes</span> <span class="o">/</span> <span class="mi">1024</span>
        <span class="n">size_mb</span> <span class="o">=</span> <span class="n">size_kb</span> <span class="o">/</span> <span class="mi">1024</span>
        
        <span class="k">return</span> <span class="p">{</span>
            <span class="s1">&#39;total_parameters&#39;</span><span class="p">:</span> <span class="n">total_params</span><span class="p">,</span>
            <span class="s1">&#39;bytes_per_parameter&#39;</span><span class="p">:</span> <span class="n">bytes_per_param</span><span class="p">,</span>
            <span class="s1">&#39;total_bytes&#39;</span><span class="p">:</span> <span class="n">total_bytes</span><span class="p">,</span>
            <span class="s1">&#39;size_kb&#39;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">size_kb</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
            <span class="s1">&#39;size_mb&#39;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">size_mb</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
            <span class="s1">&#39;dtype&#39;</span><span class="p">:</span> <span class="n">dtype</span>
        <span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_compression_metrics</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    ### üß™ Unit Test: CompressionMetrics</span>
<span class="sd">    </span>
<span class="sd">    Test parameter counting and model size analysis functionality.</span>
<span class="sd">    </span>
<span class="sd">    **This is a unit test** - it tests model size analysis in isolation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üî¨ Unit Test: CompressionMetrics&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;**This is a unit test** - it tests model size analysis in isolation.&quot;</span><span class="p">)</span>
    
    <span class="c1"># Create test model</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">Dense</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span>  <span class="c1"># 784 * 128 + 128 = 100,480 params</span>
        <span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span>   <span class="c1"># 128 * 64 + 64 = 8,256 params</span>
        <span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>     <span class="c1"># 64 * 10 + 10 = 650 params</span>
    <span class="p">]</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
    
    <span class="c1"># Test parameter counting</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="n">CompressionMetrics</span><span class="p">()</span>
    <span class="n">param_counts</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">count_parameters</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    
    <span class="c1"># Verify parameter counts</span>
    <span class="k">assert</span> <span class="n">param_counts</span><span class="p">[</span><span class="s1">&#39;layer_0_weights&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">100352</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Expected 100352, got </span><span class="si">{</span><span class="n">param_counts</span><span class="p">[</span><span class="s1">&#39;layer_0_weights&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">assert</span> <span class="n">param_counts</span><span class="p">[</span><span class="s1">&#39;layer_0_bias&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">128</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Expected 128, got </span><span class="si">{</span><span class="n">param_counts</span><span class="p">[</span><span class="s1">&#39;layer_0_bias&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">assert</span> <span class="n">param_counts</span><span class="p">[</span><span class="s1">&#39;total_parameters&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">109386</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Expected 109386, got </span><span class="si">{</span><span class="n">param_counts</span><span class="p">[</span><span class="s1">&#39;total_parameters&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üìà Progress: CompressionMetrics ‚úì&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üéØ CompressionMetrics behavior:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - Counts parameters across all layers&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - Provides detailed breakdown by layer&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - Separates weight and bias counts&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - Foundation for compression analysis&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>

<span class="c1"># Run the test</span>
<span class="n">test_compression_metrics</span><span class="p">()</span> 
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="step-2-magnitude-based-pruning-removing-unimportant-weights">
<h2>Step 2: Magnitude-Based Pruning - Removing Unimportant Weights<a class="headerlink" href="#step-2-magnitude-based-pruning-removing-unimportant-weights" title="Permalink to this heading">#</a></h2>
<section id="what-is-magnitude-based-pruning">
<h3>What is Magnitude-Based Pruning?<a class="headerlink" href="#what-is-magnitude-based-pruning" title="Permalink to this heading">#</a></h3>
<p><strong>Magnitude-based pruning</strong> removes weights with the smallest absolute values, based on the hypothesis that small weights contribute less to the model‚Äôs performance.</p>
</section>
<section id="the-algorithm">
<h3>The Algorithm<a class="headerlink" href="#the-algorithm" title="Permalink to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Calculate magnitude</strong>: <code class="docutils literal notranslate"><span class="pre">|weight|</span></code> for each parameter</p></li>
<li><p><strong>Set threshold</strong>: Choose cutoff (e.g., 50th percentile)</p></li>
<li><p><strong>Create mask</strong>: <code class="docutils literal notranslate"><span class="pre">mask</span> <span class="pre">=</span> <span class="pre">|weight|</span> <span class="pre">&gt;</span> <span class="pre">threshold</span></code></p></li>
<li><p><strong>Apply pruning</strong>: <code class="docutils literal notranslate"><span class="pre">pruned_weight</span> <span class="pre">=</span> <span class="pre">weight</span> <span class="pre">*</span> <span class="pre">mask</span></code></p></li>
</ol>
</section>
<section id="why-this-works">
<h3>Why This Works<a class="headerlink" href="#why-this-works" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Redundancy</strong>: Neural networks are over-parameterized</p></li>
<li><p><strong>Lottery ticket hypothesis</strong>: Small subnetworks can match full performance</p></li>
<li><p><strong>Magnitude correlation</strong>: Larger weights often more important</p></li>
<li><p><strong>Gradual degradation</strong>: Performance drops slowly with pruning</p></li>
</ul>
</section>
<section id="real-world-applications">
<h3>Real-World Applications<a class="headerlink" href="#real-world-applications" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Mobile deployment</strong>: Reduce model size for smartphones</p></li>
<li><p><strong>Edge computing</strong>: Fit models on resource-constrained devices</p></li>
<li><p><strong>Inference acceleration</strong>: Fewer parameters = faster computation</p></li>
<li><p><strong>Memory optimization</strong>: Sparse matrices save storage</p></li>
</ul>
</section>
<section id="pruning-strategies">
<h3>Pruning Strategies<a class="headerlink" href="#pruning-strategies" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Global</strong>: Single threshold across all layers</p></li>
<li><p><strong>Layer-wise</strong>: Different thresholds per layer</p></li>
<li><p><strong>Structured</strong>: Remove entire neurons/channels</p></li>
<li><p><strong>Gradual</strong>: Increase sparsity during training</p></li>
</ul>
</section>
<section id="performance-vs-sparsity-trade-off">
<h3>Performance vs Sparsity Trade-off<a class="headerlink" href="#performance-vs-sparsity-trade-off" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>10-30% sparsity</strong>: Minimal accuracy loss</p></li>
<li><p><strong>50-70% sparsity</strong>: Moderate accuracy drop</p></li>
<li><p><strong>80-90% sparsity</strong>: Significant accuracy loss</p></li>
<li><p><strong>95%+ sparsity</strong>: Requires careful tuning</p></li>
</ul>
<p>Let‚Äôs implement magnitude-based pruning!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| export</span>
<span class="k">def</span> <span class="nf">prune_weights_by_magnitude</span><span class="p">(</span><span class="n">layer</span><span class="p">:</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">pruning_ratio</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Dense</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Prune weights in a Dense layer by magnitude.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        layer: Dense layer to prune</span>
<span class="sd">        pruning_ratio: Fraction of weights to remove (0.0 to 1.0)</span>
<span class="sd">        </span>
<span class="sd">    Returns:</span>
<span class="sd">        Tuple of (pruned_layer, pruning_info)</span>
<span class="sd">        </span>
<span class="sd">    TODO: Implement magnitude-based weight pruning.</span>
<span class="sd">    </span>
<span class="sd">    STEP-BY-STEP IMPLEMENTATION:</span>
<span class="sd">    1. Get weight matrix from layer</span>
<span class="sd">    2. Calculate absolute values (magnitudes)</span>
<span class="sd">    3. Find threshold using percentile</span>
<span class="sd">    4. Create binary mask for weights above threshold</span>
<span class="sd">    5. Apply mask to weights (set small weights to zero)</span>
<span class="sd">    6. Update layer weights and return pruning statistics</span>
<span class="sd">    </span>
<span class="sd">    EXAMPLE USAGE:</span>
<span class="sd">    ```python</span>
<span class="sd">    layer = Dense(784, 128)</span>
<span class="sd">    pruned_layer, info = prune_weights_by_magnitude(layer, pruning_ratio=0.3)</span>
<span class="sd">    print(f&quot;Pruned {info[&#39;weights_removed&#39;]} weights, sparsity: {info[&#39;sparsity&#39;]:.2f}&quot;)</span>
<span class="sd">    ```</span>
<span class="sd">    </span>
<span class="sd">    IMPLEMENTATION HINTS:</span>
<span class="sd">    - Use np.percentile() with pruning_ratio * 100 for threshold</span>
<span class="sd">    - Create mask with np.abs(weights) &gt; threshold</span>
<span class="sd">    - Apply mask by element-wise multiplication</span>
<span class="sd">    - Count zeros to calculate sparsity</span>
<span class="sd">    - Return original layer (modified) and statistics</span>
<span class="sd">    </span>
<span class="sd">    LEARNING CONNECTIONS:</span>
<span class="sd">    - This is the foundation of network pruning</span>
<span class="sd">    - Magnitude pruning is simplest but effective</span>
<span class="sd">    - Sparsity = fraction of weights that are zero</span>
<span class="sd">    - Threshold selection affects accuracy vs compression trade-off</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1">### BEGIN SOLUTION</span>
    <span class="c1"># YOUR CODE HERE</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
    <span class="c1">### END SOLUTION</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| export</span>
<span class="k">def</span> <span class="nf">calculate_sparsity</span><span class="p">(</span><span class="n">layer</span><span class="p">:</span> <span class="n">Dense</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate sparsity (fraction of zero weights) in a Dense layer.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        layer: Dense layer to analyze</span>
<span class="sd">        </span>
<span class="sd">    Returns:</span>
<span class="sd">        Sparsity as float between 0.0 and 1.0</span>
<span class="sd">        </span>
<span class="sd">    TODO: Implement sparsity calculation.</span>
<span class="sd">    </span>
<span class="sd">    STEP-BY-STEP IMPLEMENTATION:</span>
<span class="sd">    1. Get weight matrix from layer</span>
<span class="sd">    2. Count total number of weights</span>
<span class="sd">    3. Count number of zero weights</span>
<span class="sd">    4. Calculate sparsity = zero_weights / total_weights</span>
<span class="sd">    5. Return as float</span>
<span class="sd">    </span>
<span class="sd">    EXAMPLE USAGE:</span>
<span class="sd">    ```python</span>
<span class="sd">    layer = Dense(100, 50)</span>
<span class="sd">    sparsity = calculate_sparsity(layer)</span>
<span class="sd">    print(f&quot;Layer sparsity: {sparsity:.2%}&quot;)</span>
<span class="sd">    ```</span>
<span class="sd">    </span>
<span class="sd">    IMPLEMENTATION HINTS:</span>
<span class="sd">    - Use np.sum() with condition to count zeros</span>
<span class="sd">    - Use .size attribute for total elements</span>
<span class="sd">    - Return 0.0 if no weights (edge case)</span>
<span class="sd">    - Sparsity of 0.0 = dense, 1.0 = completely sparse</span>
<span class="sd">    </span>
<span class="sd">    LEARNING CONNECTIONS:</span>
<span class="sd">    - Sparsity is key metric for compression</span>
<span class="sd">    - Higher sparsity = more compression</span>
<span class="sd">    - Sparsity patterns affect hardware efficiency</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1">### BEGIN SOLUTION</span>
    <span class="c1"># YOUR CODE HERE</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
    <span class="c1">### END SOLUTION </span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_magnitude_pruning</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    ### üß™ Unit Test: Magnitude-Based Pruning</span>
<span class="sd">    </span>
<span class="sd">    Test weight pruning algorithms and sparsity calculation.</span>
<span class="sd">    </span>
<span class="sd">    **This is a unit test** - it tests weight pruning in isolation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üî¨ Unit Test: Magnitude-Based Pruning&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;**This is a unit test** - it tests weight pruning in isolation.&quot;</span><span class="p">)</span>
    
    <span class="c1"># Create test layer</span>
    <span class="n">layer</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
    
    <span class="c1"># Test basic pruning</span>
    <span class="n">pruned_layer</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">prune_weights_by_magnitude</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">pruning_ratio</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
    
    <span class="c1"># Verify pruning results</span>
    <span class="k">assert</span> <span class="n">info</span><span class="p">[</span><span class="s1">&#39;pruning_ratio&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mf">0.3</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Expected 0.3, got </span><span class="si">{</span><span class="n">info</span><span class="p">[</span><span class="s1">&#39;pruning_ratio&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">assert</span> <span class="n">info</span><span class="p">[</span><span class="s1">&#39;total_weights&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">5000</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Expected 5000, got </span><span class="si">{</span><span class="n">info</span><span class="p">[</span><span class="s1">&#39;total_weights&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">assert</span> <span class="n">info</span><span class="p">[</span><span class="s1">&#39;sparsity&#39;</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mf">0.3</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Sparsity should be at least 0.3, got </span><span class="si">{</span><span class="n">info</span><span class="p">[</span><span class="s1">&#39;sparsity&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚úÖ Basic pruning works: </span><span class="si">{</span><span class="n">info</span><span class="p">[</span><span class="s1">&#39;sparsity&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2"> sparsity&quot;</span><span class="p">)</span>
    
    <span class="c1"># Test sparsity calculation</span>
    <span class="n">sparsity</span> <span class="o">=</span> <span class="n">calculate_sparsity</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">abs</span><span class="p">(</span><span class="n">sparsity</span> <span class="o">-</span> <span class="n">info</span><span class="p">[</span><span class="s1">&#39;sparsity&#39;</span><span class="p">])</span> <span class="o">&lt;</span> <span class="mf">0.001</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Sparsity mismatch: </span><span class="si">{</span><span class="n">sparsity</span><span class="si">}</span><span class="s2"> vs </span><span class="si">{</span><span class="n">info</span><span class="p">[</span><span class="s1">&#39;sparsity&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚úÖ Sparsity calculation works: </span><span class="si">{</span><span class="n">sparsity</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="c1"># Test edge cases</span>
    <span class="n">empty_layer</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">empty_layer</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    <span class="n">sparsity_empty</span> <span class="o">=</span> <span class="n">calculate_sparsity</span><span class="p">(</span><span class="n">empty_layer</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">sparsity_empty</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Empty layer should have 1.0 sparsity, got </span><span class="si">{</span><span class="n">sparsity_empty</span><span class="si">}</span><span class="s2">&quot;</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;‚úÖ Edge cases work correctly&quot;</span><span class="p">)</span>
    
    <span class="c1"># Test different pruning ratios</span>
    <span class="n">layer2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">info50</span> <span class="o">=</span> <span class="n">prune_weights_by_magnitude</span><span class="p">(</span><span class="n">layer2</span><span class="p">,</span> <span class="n">pruning_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    
    <span class="n">layer3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">info80</span> <span class="o">=</span> <span class="n">prune_weights_by_magnitude</span><span class="p">(</span><span class="n">layer3</span><span class="p">,</span> <span class="n">pruning_ratio</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
    
    <span class="k">assert</span> <span class="n">info80</span><span class="p">[</span><span class="s1">&#39;sparsity&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">info50</span><span class="p">[</span><span class="s1">&#39;sparsity&#39;</span><span class="p">],</span> <span class="s2">&quot;Higher pruning ratio should give higher sparsity&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚úÖ Different pruning ratios work: 50% ratio = </span><span class="si">{</span><span class="n">info50</span><span class="p">[</span><span class="s1">&#39;sparsity&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2">, 80% ratio = </span><span class="si">{</span><span class="n">info80</span><span class="p">[</span><span class="s1">&#39;sparsity&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üìà Progress: Magnitude-Based Pruning ‚úì&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üéØ Pruning behavior:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - Removes weights with smallest absolute values&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - Maintains layer structure and connectivity&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - Provides detailed statistics for analysis&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - Scales to different pruning ratios&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>

<span class="c1"># Run the test</span>
<span class="n">test_magnitude_pruning</span><span class="p">()</span> 
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="step-3-quantization-reducing-precision-for-memory-efficiency">
<h2>Step 3: Quantization - Reducing Precision for Memory Efficiency<a class="headerlink" href="#step-3-quantization-reducing-precision-for-memory-efficiency" title="Permalink to this heading">#</a></h2>
<section id="what-is-quantization">
<h3>What is Quantization?<a class="headerlink" href="#what-is-quantization" title="Permalink to this heading">#</a></h3>
<p><strong>Quantization</strong> reduces the precision of weights from FP32 (32-bit) to lower bit-widths like INT8 (8-bit), achieving significant memory savings with minimal accuracy loss.</p>
</section>
<section id="the-mathematical-foundation">
<h3>The Mathematical Foundation<a class="headerlink" href="#the-mathematical-foundation" title="Permalink to this heading">#</a></h3>
<p>Quantization maps continuous floating-point values to discrete integer values:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">quantized_value</span> <span class="o">=</span> <span class="nb">round</span><span class="p">((</span><span class="n">fp_value</span> <span class="o">-</span> <span class="n">min_val</span><span class="p">)</span> <span class="o">/</span> <span class="n">scale</span><span class="p">)</span>
<span class="n">scale</span> <span class="o">=</span> <span class="p">(</span><span class="n">max_val</span> <span class="o">-</span> <span class="n">min_val</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">^</span><span class="n">bits</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="why-quantization-works">
<h3>Why Quantization Works<a class="headerlink" href="#why-quantization-works" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Redundant precision</strong>: Neural networks are robust to precision reduction</p></li>
<li><p><strong>Hardware efficiency</strong>: Integer operations are faster than floating-point</p></li>
<li><p><strong>Memory savings</strong>: 4x reduction (FP32 ‚Üí INT8) in memory usage</p></li>
<li><p><strong>Cache efficiency</strong>: More parameters fit in limited cache memory</p></li>
</ul>
</section>
<section id="types-of-quantization">
<h3>Types of Quantization<a class="headerlink" href="#types-of-quantization" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Post-training</strong>: Quantize after training is complete</p></li>
<li><p><strong>Quantization-aware training</strong>: Train with quantization simulation</p></li>
<li><p><strong>Dynamic</strong>: Quantize activations at runtime</p></li>
<li><p><strong>Static</strong>: Pre-compute quantization parameters</p></li>
</ul>
</section>
<section id="id1">
<h3>Real-World Impact<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Mobile deployment</strong>: 75% memory reduction enables smartphone AI</p></li>
<li><p><strong>Edge computing</strong>: Fit larger models on constrained devices</p></li>
<li><p><strong>Cloud efficiency</strong>: Reduce bandwidth and storage costs</p></li>
<li><p><strong>Battery life</strong>: Lower power consumption for mobile devices</p></li>
</ul>
</section>
<section id="common-bit-widths">
<h3>Common Bit-Widths<a class="headerlink" href="#common-bit-widths" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>FP32</strong>: Full precision (baseline)</p></li>
<li><p><strong>FP16</strong>: Half precision (2x memory reduction)</p></li>
<li><p><strong>INT8</strong>: 8-bit integers (4x memory reduction)</p></li>
<li><p><strong>INT4</strong>: 4-bit integers (8x memory reduction, aggressive)</p></li>
</ul>
<p>Let‚Äôs implement quantization algorithms!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| export</span>
<span class="k">def</span> <span class="nf">quantize_layer_weights</span><span class="p">(</span><span class="n">layer</span><span class="p">:</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">bits</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Dense</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Quantize layer weights to reduce precision.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        layer: Dense layer to quantize</span>
<span class="sd">        bits: Number of bits for quantization (8, 16, etc.)</span>
<span class="sd">        </span>
<span class="sd">    Returns:</span>
<span class="sd">        Tuple of (quantized_layer, quantization_info)</span>
<span class="sd">        </span>
<span class="sd">    TODO: Implement weight quantization for memory efficiency.</span>
<span class="sd">    </span>
<span class="sd">    STEP-BY-STEP IMPLEMENTATION:</span>
<span class="sd">    1. Get weight matrix from layer</span>
<span class="sd">    2. Find min and max values for quantization range</span>
<span class="sd">    3. Calculate scale factor: (max - min) / (2^bits - 1)</span>
<span class="sd">    4. Quantize: round((weights - min) / scale)</span>
<span class="sd">    5. Dequantize back to float: quantized * scale + min</span>
<span class="sd">    6. Update layer weights and return statistics</span>
<span class="sd">    </span>
<span class="sd">    EXAMPLE USAGE:</span>
<span class="sd">    ```python</span>
<span class="sd">    layer = Dense(784, 128)</span>
<span class="sd">    quantized_layer, info = quantize_layer_weights(layer, bits=8)</span>
<span class="sd">    print(f&quot;Memory reduction: {info[&#39;memory_reduction&#39;]:.1f}x&quot;)</span>
<span class="sd">    ```</span>
<span class="sd">    </span>
<span class="sd">    IMPLEMENTATION HINTS:</span>
<span class="sd">    - Use np.min() and np.max() to find weight range</span>
<span class="sd">    - Clamp quantized values to valid range [0, 2^bits-1]</span>
<span class="sd">    - Store original dtype for memory calculation</span>
<span class="sd">    - Calculate theoretical memory savings</span>
<span class="sd">    </span>
<span class="sd">    LEARNING CONNECTIONS:</span>
<span class="sd">    - This is how mobile AI frameworks work</span>
<span class="sd">    - Hardware accelerators optimize for INT8</span>
<span class="sd">    - Precision-performance trade-off is key</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1">### BEGIN SOLUTION</span>
    <span class="c1"># YOUR CODE HERE</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
    <span class="c1">### END SOLUTION </span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_quantization</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    ### üß™ Unit Test: Quantization</span>
<span class="sd">    </span>
<span class="sd">    Test weight quantization and precision reduction functionality.</span>
<span class="sd">    </span>
<span class="sd">    **This is a unit test** - it tests quantization algorithms in isolation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üî¨ Unit Test: Quantization&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;**This is a unit test** - it tests quantization algorithms in isolation.&quot;</span><span class="p">)</span>
    
    <span class="c1"># Create test layer</span>
    <span class="n">layer</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
    <span class="n">original_weights</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="s1">&#39;copy&#39;</span><span class="p">)</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
    
    <span class="c1"># Test INT8 quantization</span>
    <span class="n">quantized_layer</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">quantize_layer_weights</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">bits</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    
    <span class="c1"># Verify quantization results</span>
    <span class="k">assert</span> <span class="n">info</span><span class="p">[</span><span class="s1">&#39;bits&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">8</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Expected 8 bits, got </span><span class="si">{</span><span class="n">info</span><span class="p">[</span><span class="s1">&#39;bits&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">assert</span> <span class="n">info</span><span class="p">[</span><span class="s1">&#39;total_weights&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">5000</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Expected 5000 weights, got </span><span class="si">{</span><span class="n">info</span><span class="p">[</span><span class="s1">&#39;total_weights&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">assert</span> <span class="n">info</span><span class="p">[</span><span class="s1">&#39;memory_reduction&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mf">4.0</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Expected 4x reduction, got </span><span class="si">{</span><span class="n">info</span><span class="p">[</span><span class="s1">&#39;memory_reduction&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚úÖ INT8 quantization works: </span><span class="si">{</span><span class="n">info</span><span class="p">[</span><span class="s1">&#39;memory_reduction&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">x memory reduction&quot;</span><span class="p">)</span>
    
    <span class="c1"># Test quantization error</span>
    <span class="k">assert</span> <span class="n">info</span><span class="p">[</span><span class="s1">&#39;mse_error&#39;</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;MSE error should be non-negative&quot;</span>
    <span class="k">assert</span> <span class="n">info</span><span class="p">[</span><span class="s1">&#39;max_error&#39;</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;Max error should be non-negative&quot;</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚úÖ Quantization error tracking works: MSE=</span><span class="si">{</span><span class="n">info</span><span class="p">[</span><span class="s1">&#39;mse_error&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">, Max=</span><span class="si">{</span><span class="n">info</span><span class="p">[</span><span class="s1">&#39;max_error&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="c1"># Test different bit widths</span>
    <span class="n">layer2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">info16</span> <span class="o">=</span> <span class="n">quantize_layer_weights</span><span class="p">(</span><span class="n">layer2</span><span class="p">,</span> <span class="n">bits</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
    
    <span class="n">layer3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span>  
    <span class="n">_</span><span class="p">,</span> <span class="n">info4</span> <span class="o">=</span> <span class="n">quantize_layer_weights</span><span class="p">(</span><span class="n">layer3</span><span class="p">,</span> <span class="n">bits</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>  <span class="c1"># Use 8 instead of 4 for valid byte calculation</span>
    
    <span class="k">assert</span> <span class="n">info16</span><span class="p">[</span><span class="s1">&#39;memory_reduction&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mf">2.0</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;16-bit should give 2x reduction, got </span><span class="si">{</span><span class="n">info16</span><span class="p">[</span><span class="s1">&#39;memory_reduction&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚úÖ Different bit widths work: 16-bit = </span><span class="si">{</span><span class="n">info16</span><span class="p">[</span><span class="s1">&#39;memory_reduction&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">x, 8-bit = </span><span class="si">{</span><span class="n">info4</span><span class="p">[</span><span class="s1">&#39;memory_reduction&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>
    
    <span class="c1"># Test quantization parameters</span>
    <span class="k">assert</span> <span class="s1">&#39;scale&#39;</span> <span class="ow">in</span> <span class="n">info</span><span class="p">,</span> <span class="s2">&quot;Scale parameter should be included&quot;</span>
    <span class="k">assert</span> <span class="s1">&#39;min_val&#39;</span> <span class="ow">in</span> <span class="n">info</span><span class="p">,</span> <span class="s2">&quot;Min value should be included&quot;</span>
    <span class="k">assert</span> <span class="s1">&#39;max_val&#39;</span> <span class="ow">in</span> <span class="n">info</span><span class="p">,</span> <span class="s2">&quot;Max value should be included&quot;</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;‚úÖ Quantization parameters work correctly&quot;</span><span class="p">)</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üìà Progress: Quantization ‚úì&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üéØ Quantization behavior:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - Reduces precision while preserving weights&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - Provides significant memory savings&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - Tracks quantization error and parameters&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - Supports different bit widths&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>

<span class="c1"># Run the test</span>
<span class="n">test_quantization</span><span class="p">()</span> 
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="step-4-knowledge-distillation-large-models-teach-small-models">
<h2>Step 4: Knowledge Distillation - Large Models Teach Small Models<a class="headerlink" href="#step-4-knowledge-distillation-large-models-teach-small-models" title="Permalink to this heading">#</a></h2>
<section id="what-is-knowledge-distillation">
<h3>What is Knowledge Distillation?<a class="headerlink" href="#what-is-knowledge-distillation" title="Permalink to this heading">#</a></h3>
<p><strong>Knowledge distillation</strong> trains a small ‚Äústudent‚Äù model to mimic the behavior of a large ‚Äúteacher‚Äù model, achieving compact models with competitive performance.</p>
</section>
<section id="the-core-idea">
<h3>The Core Idea<a class="headerlink" href="#the-core-idea" title="Permalink to this heading">#</a></h3>
<p>Instead of training on hard labels (0 or 1), students learn from soft targets (probabilities) that contain more information about the teacher‚Äôs knowledge.</p>
</section>
<section id="id2">
<h3>The Mathematical Foundation<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h3>
<p>Distillation combines two loss functions:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Hard loss: Standard classification loss</span>
<span class="n">hard_loss</span> <span class="o">=</span> <span class="n">CrossEntropy</span><span class="p">(</span><span class="n">student_logits</span><span class="p">,</span> <span class="n">true_labels</span><span class="p">)</span>

<span class="c1"># Soft loss: Learn from teacher&#39;s probability distribution</span>
<span class="n">soft_targets</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">teacher_logits</span> <span class="o">/</span> <span class="n">temperature</span><span class="p">)</span>
<span class="n">soft_student</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">student_logits</span> <span class="o">/</span> <span class="n">temperature</span><span class="p">)</span>
<span class="n">soft_loss</span> <span class="o">=</span> <span class="o">-</span><span class="nb">sum</span><span class="p">(</span><span class="n">soft_targets</span> <span class="o">*</span> <span class="n">log</span><span class="p">(</span><span class="n">soft_student</span><span class="p">))</span>

<span class="c1"># Combined loss</span>
<span class="n">total_loss</span> <span class="o">=</span> <span class="n">Œ±</span> <span class="o">*</span> <span class="n">hard_loss</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">Œ±</span><span class="p">)</span> <span class="o">*</span> <span class="n">soft_loss</span>
</pre></div>
</div>
</section>
<section id="why-distillation-works">
<h3>Why Distillation Works<a class="headerlink" href="#why-distillation-works" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Richer information</strong>: Soft targets contain inter-class relationships</p></li>
<li><p><strong>Teacher knowledge</strong>: Large models learn useful representations</p></li>
<li><p><strong>Regularization</strong>: Soft targets reduce overfitting</p></li>
<li><p><strong>Efficiency</strong>: Small models gain large model insights</p></li>
</ul>
</section>
<section id="key-parameters">
<h3>Key Parameters<a class="headerlink" href="#key-parameters" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Temperature (T)</strong>: Controls softness of probability distributions</p>
<ul>
<li><p>High T: Softer, more informative distributions</p></li>
<li><p>Low T: Sharper, more confident predictions</p></li>
</ul>
</li>
<li><p><strong>Alpha (Œ±)</strong>: Balances hard and soft losses</p>
<ul>
<li><p>Œ± = 1.0: Only hard loss (standard training)</p></li>
<li><p>Œ± = 0.0: Only soft loss (pure distillation)</p></li>
</ul>
</li>
</ul>
</section>
<section id="id3">
<h3>Real-World Applications<a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Mobile deployment</strong>: Small models with large model performance</p></li>
<li><p><strong>Edge computing</strong>: Efficient inference with minimal accuracy loss</p></li>
<li><p><strong>Model compression</strong>: Alternative to pruning and quantization</p></li>
<li><p><strong>Multi-task learning</strong>: Transfer knowledge across different tasks</p></li>
</ul>
</section>
<section id="success-stories">
<h3>Success Stories<a class="headerlink" href="#success-stories" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>DistilBERT</strong>: 60% smaller than BERT with 97% performance</p></li>
<li><p><strong>MobileNet</strong>: Distilled from ResNet for mobile deployment</p></li>
<li><p><strong>TinyBERT</strong>: Extreme compression for resource-constrained devices</p></li>
</ul>
<p>Let‚Äôs implement knowledge distillation!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| export</span>
<span class="k">class</span> <span class="nc">DistillationLoss</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Combined loss function for knowledge distillation.</span>
<span class="sd">    </span>
<span class="sd">    This loss combines standard classification loss (hard targets) with</span>
<span class="sd">    distillation loss (soft targets from teacher) for training compact models.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">3.0</span><span class="p">,</span> <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize distillation loss.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            temperature: Temperature for softening probability distributions</span>
<span class="sd">            alpha: Weight for hard loss (1-alpha for soft loss)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span> <span class="o">=</span> <span class="n">temperature</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ce_loss</span> <span class="o">=</span> <span class="n">CrossEntropyLoss</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">student_logits</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">teacher_logits</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> 
                 <span class="n">true_labels</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculate combined distillation loss.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            student_logits: Raw outputs from student model</span>
<span class="sd">            teacher_logits: Raw outputs from teacher model  </span>
<span class="sd">            true_labels: Ground truth labels</span>
<span class="sd">            </span>
<span class="sd">        Returns:</span>
<span class="sd">            Combined loss value</span>
<span class="sd">            </span>
<span class="sd">        TODO: Implement knowledge distillation loss function.</span>
<span class="sd">        </span>
<span class="sd">        STEP-BY-STEP IMPLEMENTATION:</span>
<span class="sd">        1. Calculate hard loss using standard cross-entropy</span>
<span class="sd">        2. Apply temperature scaling to both logits</span>
<span class="sd">        3. Calculate soft targets from teacher logits</span>
<span class="sd">        4. Calculate soft loss between student and teacher distributions</span>
<span class="sd">        5. Combine hard and soft losses with alpha weighting</span>
<span class="sd">        6. Return total loss</span>
<span class="sd">        </span>
<span class="sd">        EXAMPLE USAGE:</span>
<span class="sd">        ```python</span>
<span class="sd">        distill_loss = DistillationLoss(temperature=3.0, alpha=0.5)</span>
<span class="sd">        loss = distill_loss(student_out, teacher_out, labels)</span>
<span class="sd">        ```</span>
<span class="sd">        </span>
<span class="sd">        IMPLEMENTATION HINTS:</span>
<span class="sd">        - Use temperature scaling before softmax: logits / temperature</span>
<span class="sd">        - Implement stable softmax to avoid numerical issues</span>
<span class="sd">        - Scale soft loss by temperature^2 (standard practice)</span>
<span class="sd">        - Ensure proper normalization for both losses</span>
<span class="sd">        </span>
<span class="sd">        LEARNING CONNECTIONS:</span>
<span class="sd">        - This is how DistilBERT was trained</span>
<span class="sd">        - Temperature controls knowledge transfer richness</span>
<span class="sd">        - Alpha balances accuracy vs compression</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1">### BEGIN SOLUTION</span>
    <span class="c1"># YOUR CODE HERE</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
        <span class="c1">### END SOLUTION</span>
    
    <span class="k">def</span> <span class="nf">_softmax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Numerically stable softmax.&quot;&quot;&quot;</span>
        <span class="c1"># Subtract max for numerical stability</span>
        <span class="n">exp_logits</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">logits</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">exp_logits</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">exp_logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">_cross_entropy_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">labels</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Simple cross-entropy loss implementation.&quot;&quot;&quot;</span>
        <span class="c1"># Convert labels to one-hot if needed</span>
        <span class="k">if</span> <span class="n">labels</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">num_classes</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">one_hot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">num_classes</span><span class="p">))</span>
            <span class="n">one_hot</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">labels</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="n">one_hot</span>
        
        <span class="c1"># Apply softmax and calculate cross-entropy</span>
        <span class="n">probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
        <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">labels</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">probs</span> <span class="o">+</span> <span class="mf">1e-10</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span> 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_distillation</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    ### üß™ Unit Test: Knowledge Distillation</span>
<span class="sd">    </span>
<span class="sd">    Test knowledge distillation loss function and teacher-student training.</span>
<span class="sd">    </span>
<span class="sd">    **This is a unit test** - it tests distillation algorithms in isolation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üî¨ Unit Test: Knowledge Distillation&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;**This is a unit test** - it tests distillation algorithms in isolation.&quot;</span><span class="p">)</span>
    
    <span class="c1"># Create sample data</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_classes</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">10</span>
    <span class="n">student_logits</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.5</span>
    <span class="n">teacher_logits</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span> <span class="o">*</span> <span class="mf">2.0</span>  <span class="c1"># Teacher is more confident</span>
    <span class="n">true_labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
    
    <span class="c1"># Test distillation loss</span>
    <span class="n">distill_loss</span> <span class="o">=</span> <span class="n">DistillationLoss</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">distill_loss</span><span class="p">(</span><span class="n">student_logits</span><span class="p">,</span> <span class="n">teacher_logits</span><span class="p">,</span> <span class="n">true_labels</span><span class="p">)</span>
    
    <span class="c1"># Verify loss computation</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="nb">float</span><span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Loss should be float, got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">assert</span> <span class="n">loss</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Loss should be non-negative, got </span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="s2">&quot;</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚úÖ Distillation loss computation works: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="c1"># Test different temperature values</span>
    <span class="n">loss_t1</span> <span class="o">=</span> <span class="n">DistillationLoss</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">student_logits</span><span class="p">,</span> <span class="n">teacher_logits</span><span class="p">,</span> <span class="n">true_labels</span><span class="p">)</span>
    <span class="n">loss_t5</span> <span class="o">=</span> <span class="n">DistillationLoss</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">student_logits</span><span class="p">,</span> <span class="n">teacher_logits</span><span class="p">,</span> <span class="n">true_labels</span><span class="p">)</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚úÖ Temperature scaling works: T=1.0 ‚Üí </span><span class="si">{</span><span class="n">loss_t1</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, T=5.0 ‚Üí </span><span class="si">{</span><span class="n">loss_t5</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="c1"># Test different alpha values</span>
    <span class="n">loss_hard</span> <span class="o">=</span> <span class="n">DistillationLoss</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)(</span><span class="n">student_logits</span><span class="p">,</span> <span class="n">teacher_logits</span><span class="p">,</span> <span class="n">true_labels</span><span class="p">)</span>  <span class="c1"># Only hard loss</span>
    <span class="n">loss_soft</span> <span class="o">=</span> <span class="n">DistillationLoss</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)(</span><span class="n">student_logits</span><span class="p">,</span> <span class="n">teacher_logits</span><span class="p">,</span> <span class="n">true_labels</span><span class="p">)</span>  <span class="c1"># Only soft loss</span>
    
    <span class="k">assert</span> <span class="n">loss_hard</span> <span class="o">!=</span> <span class="n">loss_soft</span><span class="p">,</span> <span class="s2">&quot;Hard and soft losses should be different&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚úÖ Alpha balancing works: Hard only = </span><span class="si">{</span><span class="n">loss_hard</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Soft only = </span><span class="si">{</span><span class="n">loss_soft</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="c1"># Test edge cases</span>
    <span class="c1"># Identical student and teacher should have low soft loss</span>
    <span class="n">identical_logits</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
    <span class="n">loss_identical</span> <span class="o">=</span> <span class="n">DistillationLoss</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)(</span><span class="n">identical_logits</span><span class="p">,</span> <span class="n">identical_logits</span><span class="p">,</span> <span class="n">true_labels</span><span class="p">)</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚úÖ Edge cases work: Identical logits soft loss = </span><span class="si">{</span><span class="n">loss_identical</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="c1"># Test internal methods</span>
    <span class="n">softmax_result</span> <span class="o">=</span> <span class="n">distill_loss</span><span class="o">.</span><span class="n">_softmax</span><span class="p">(</span><span class="n">student_logits</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">softmax_result</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="mf">1.0</span><span class="p">),</span> <span class="s2">&quot;Softmax should sum to 1&quot;</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;‚úÖ Internal methods work correctly&quot;</span><span class="p">)</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üìà Progress: Knowledge Distillation ‚úì&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üéØ Distillation behavior:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - Combines hard and soft losses effectively&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - Temperature controls knowledge transfer&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - Alpha balances accuracy vs compression&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - Numerically stable softmax implementation&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>

<span class="c1"># Run the test</span>
<span class="n">test_distillation</span><span class="p">()</span> 
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="step-5-structured-pruning-removing-entire-neurons-and-channels">
<h2>Step 5: Structured Pruning - Removing Entire Neurons and Channels<a class="headerlink" href="#step-5-structured-pruning-removing-entire-neurons-and-channels" title="Permalink to this heading">#</a></h2>
<section id="what-is-structured-pruning">
<h3>What is Structured Pruning?<a class="headerlink" href="#what-is-structured-pruning" title="Permalink to this heading">#</a></h3>
<p><strong>Structured pruning</strong> removes entire neurons, channels, or layers rather than individual weights, creating models that are actually faster on hardware.</p>
</section>
<section id="structured-vs-unstructured-pruning">
<h3>Structured vs Unstructured Pruning<a class="headerlink" href="#structured-vs-unstructured-pruning" title="Permalink to this heading">#</a></h3>
<section id="unstructured-pruning-what-we-did-in-step-2">
<h4><strong>Unstructured Pruning</strong> (What we did in Step 2)<a class="headerlink" href="#unstructured-pruning-what-we-did-in-step-2" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>Removes individual weights scattered throughout the matrix</p></li>
<li><p>Creates sparse matrices (lots of zeros)</p></li>
<li><p>High compression but requires sparse matrix libraries for speedup</p></li>
<li><p>Memory savings but limited hardware acceleration</p></li>
</ul>
</section>
<section id="structured-pruning-what-we-re-doing-now">
<h4><strong>Structured Pruning</strong> (What we‚Äôre doing now)<a class="headerlink" href="#structured-pruning-what-we-re-doing-now" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>Removes entire rows/columns (neurons/channels)</p></li>
<li><p>Creates smaller dense matrices</p></li>
<li><p>Lower compression but actual hardware speedup</p></li>
<li><p>Real reduction in computation and memory access</p></li>
</ul>
</section>
</section>
<section id="the-mathematical-impact">
<h3>The Mathematical Impact<a class="headerlink" href="#the-mathematical-impact" title="Permalink to this heading">#</a></h3>
<p>Removing a neuron from a Dense layer:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Original layer: Dense(784, 128)</span>
<span class="c1"># Weight matrix: (784, 128), Bias: (128,)</span>

<span class="c1"># After removing 32 neurons: Dense(784, 96)</span>
<span class="c1"># Weight matrix: (784, 96), Bias: (96,)</span>
<span class="c1"># 25% reduction in parameters and computation</span>
</pre></div>
</div>
</section>
<section id="why-structured-pruning-works">
<h3>Why Structured Pruning Works<a class="headerlink" href="#why-structured-pruning-works" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Hardware efficiency</strong>: Dense matrix operations are optimized</p></li>
<li><p><strong>Memory bandwidth</strong>: Smaller matrices mean less data movement</p></li>
<li><p><strong>Cache utilization</strong>: Better memory access patterns</p></li>
<li><p><strong>Real speedup</strong>: Actual reduction in FLOPs and inference time</p></li>
</ul>
</section>
<section id="neuron-importance-metrics">
<h3>Neuron Importance Metrics<a class="headerlink" href="#neuron-importance-metrics" title="Permalink to this heading">#</a></h3>
<p>How do we decide which neurons to remove?</p>
<ol class="arabic simple">
<li><p><strong>Activation-based</strong>: Neurons with low average activation</p></li>
<li><p><strong>Gradient-based</strong>: Neurons with small gradients during training</p></li>
<li><p><strong>Weight magnitude</strong>: Neurons with small outgoing weights</p></li>
<li><p><strong>Information-theoretic</strong>: Neurons contributing less information</p></li>
</ol>
</section>
<section id="id4">
<h3>Real-World Applications<a class="headerlink" href="#id4" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Mobile deployment</strong>: Actual speedup on ARM processors</p></li>
<li><p><strong>FPGA inference</strong>: Smaller designs with same performance</p></li>
<li><p><strong>Edge computing</strong>: Reduced memory bandwidth requirements</p></li>
<li><p><strong>Production systems</strong>: Guaranteed inference time reduction</p></li>
</ul>
</section>
<section id="challenges">
<h3>Challenges<a class="headerlink" href="#challenges" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Architecture modification</strong>: Must handle dimension mismatches</p></li>
<li><p><strong>Cascade effects</strong>: Removing one neuron affects next layer</p></li>
<li><p><strong>Retraining</strong>: Often requires fine-tuning after pruning</p></li>
<li><p><strong>Importance ranking</strong>: Choosing the right importance metric</p></li>
</ul>
<p>Let‚Äôs implement structured pruning for Dense layers!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| export</span>
<span class="k">def</span> <span class="nf">compute_neuron_importance</span><span class="p">(</span><span class="n">layer</span><span class="p">:</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;weight_magnitude&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute importance scores for each neuron in a Dense layer.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        layer: Dense layer to analyze</span>
<span class="sd">        method: Importance computation method</span>
<span class="sd">        </span>
<span class="sd">    Returns:</span>
<span class="sd">        Array of importance scores for each output neuron</span>
<span class="sd">        </span>
<span class="sd">    TODO: Implement neuron importance calculation.</span>
<span class="sd">    </span>
<span class="sd">    STEP-BY-STEP IMPLEMENTATION:</span>
<span class="sd">    1. Get weight matrix from layer</span>
<span class="sd">    2. Choose importance metric based on method</span>
<span class="sd">    3. Calculate per-neuron importance scores</span>
<span class="sd">    4. Return array of scores (one per output neuron)</span>
<span class="sd">    </span>
<span class="sd">    AVAILABLE METHODS:</span>
<span class="sd">    - &#39;weight_magnitude&#39;: Sum of absolute weights per neuron</span>
<span class="sd">    - &#39;weight_variance&#39;: Variance of weights per neuron</span>
<span class="sd">    - &#39;random&#39;: Random importance (for baseline comparison)</span>
<span class="sd">    </span>
<span class="sd">    IMPLEMENTATION HINTS:</span>
<span class="sd">    - Weights shape is (input_size, output_size)</span>
<span class="sd">    - Each column represents one output neuron</span>
<span class="sd">    - Use axis=0 for operations across input dimensions</span>
<span class="sd">    - Higher scores = more important neurons</span>
<span class="sd">    </span>
<span class="sd">    LEARNING CONNECTIONS:</span>
<span class="sd">    - This is how neural architecture search works</span>
<span class="sd">    - Different metrics capture different aspects of importance</span>
<span class="sd">    - Importance ranking is crucial for effective pruning</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1">### BEGIN SOLUTION</span>
    <span class="c1"># YOUR CODE HERE</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
    <span class="c1">### END SOLUTION</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| export</span>
<span class="k">def</span> <span class="nf">prune_layer_neurons</span><span class="p">(</span><span class="n">layer</span><span class="p">:</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">keep_ratio</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.7</span><span class="p">,</span> 
                       <span class="n">importance_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;weight_magnitude&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Dense</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Remove least important neurons from a Dense layer.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        layer: Dense layer to prune</span>
<span class="sd">        keep_ratio: Fraction of neurons to keep (0.0 to 1.0)</span>
<span class="sd">        importance_method: Method for computing neuron importance</span>
<span class="sd">        </span>
<span class="sd">    Returns:</span>
<span class="sd">        Tuple of (pruned_layer, pruning_info)</span>
<span class="sd">        </span>
<span class="sd">    TODO: Implement structured neuron pruning.</span>
<span class="sd">    </span>
<span class="sd">    STEP-BY-STEP IMPLEMENTATION:</span>
<span class="sd">    1. Compute importance scores for all neurons</span>
<span class="sd">    2. Determine how many neurons to keep</span>
<span class="sd">    3. Select indices of most important neurons</span>
<span class="sd">    4. Create new layer with reduced dimensions</span>
<span class="sd">    5. Copy weights and biases for selected neurons</span>
<span class="sd">    6. Return pruned layer and statistics</span>
<span class="sd">    </span>
<span class="sd">    EXAMPLE USAGE:</span>
<span class="sd">    ```python</span>
<span class="sd">    layer = Dense(784, 128)</span>
<span class="sd">    pruned_layer, info = prune_layer_neurons(layer, keep_ratio=0.75)</span>
<span class="sd">    print(f&quot;Reduced from {info[&#39;original_neurons&#39;]} to {info[&#39;remaining_neurons&#39;]} neurons&quot;)</span>
<span class="sd">    ```</span>
<span class="sd">    </span>
<span class="sd">    IMPLEMENTATION HINTS:</span>
<span class="sd">    - Use np.argsort() to rank neurons by importance</span>
<span class="sd">    - Take the top keep_count neurons: indices[-keep_count:]</span>
<span class="sd">    - Create new layer with reduced output size</span>
<span class="sd">    - Copy both weights and bias for selected neurons</span>
<span class="sd">    - Track original and new sizes for statistics</span>
<span class="sd">    </span>
<span class="sd">    LEARNING CONNECTIONS:</span>
<span class="sd">    - This is actual model architecture modification</span>
<span class="sd">    - Hardware gets real speedup from smaller matrices</span>
<span class="sd">    - Must consider cascade effects on next layers</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1">### BEGIN SOLUTION</span>
    <span class="c1"># YOUR CODE HERE</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
    <span class="c1">### END SOLUTION </span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_structured_pruning</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    ### üß™ Unit Test: Structured Pruning</span>
<span class="sd">    </span>
<span class="sd">    Test structured neuron pruning and parameter reduction.</span>
<span class="sd">    </span>
<span class="sd">    **This is a unit test** - it tests structured pruning in isolation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üî¨ Unit Test: Structured Pruning&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;**This is a unit test** - it tests structured pruning in isolation.&quot;</span><span class="p">)</span>
    
    <span class="c1"># Create test layer</span>
    <span class="n">layer</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
    
    <span class="c1"># Test basic pruning</span>
    <span class="n">pruned_layer</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">prune_layer_neurons</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">keep_ratio</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)</span>
    
    <span class="c1"># Verify pruning results</span>
    <span class="k">assert</span> <span class="n">info</span><span class="p">[</span><span class="s1">&#39;keep_ratio&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mf">0.75</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Expected 0.75, got </span><span class="si">{</span><span class="n">info</span><span class="p">[</span><span class="s1">&#39;keep_ratio&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">assert</span> <span class="n">info</span><span class="p">[</span><span class="s1">&#39;original_neurons&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">50</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Expected 50, got </span><span class="si">{</span><span class="n">info</span><span class="p">[</span><span class="s1">&#39;original_neurons&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">assert</span> <span class="n">info</span><span class="p">[</span><span class="s1">&#39;remaining_neurons&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">37</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Expected 37, got </span><span class="si">{</span><span class="n">info</span><span class="p">[</span><span class="s1">&#39;remaining_neurons&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">assert</span> <span class="n">info</span><span class="p">[</span><span class="s1">&#39;neurons_removed&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">13</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Expected 13, got </span><span class="si">{</span><span class="n">info</span><span class="p">[</span><span class="s1">&#39;neurons_removed&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">assert</span> <span class="n">info</span><span class="p">[</span><span class="s1">&#39;compression_ratio&#39;</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mf">1.35</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Compression ratio should be at least 1.35, got </span><span class="si">{</span><span class="n">info</span><span class="p">[</span><span class="s1">&#39;compression_ratio&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚úÖ Basic structured pruning works: </span><span class="si">{</span><span class="n">info</span><span class="p">[</span><span class="s1">&#39;neurons_removed&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2"> neurons removed&quot;</span><span class="p">)</span>
    
    <span class="c1"># Test parameter reduction</span>
    <span class="k">assert</span> <span class="n">info</span><span class="p">[</span><span class="s1">&#39;param_reduction&#39;</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mf">0.25</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Parameter reduction should be at least 0.25, got </span><span class="si">{</span><span class="n">info</span><span class="p">[</span><span class="s1">&#39;param_reduction&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚úÖ Parameter reduction works: </span><span class="si">{</span><span class="n">info</span><span class="p">[</span><span class="s1">&#39;param_reduction&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="c1"># Test edge cases</span>
    <span class="n">empty_layer</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">info_empty</span> <span class="o">=</span> <span class="n">prune_layer_neurons</span><span class="p">(</span><span class="n">empty_layer</span><span class="p">,</span> <span class="n">keep_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">info_empty</span><span class="p">[</span><span class="s1">&#39;remaining_neurons&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">5</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Empty layer should have 5 neurons, got </span><span class="si">{</span><span class="n">info_empty</span><span class="p">[</span><span class="s1">&#39;remaining_neurons&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;‚úÖ Edge cases work correctly&quot;</span><span class="p">)</span>
    
    <span class="c1"># Test different keep ratios</span>
    <span class="n">layer2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">info_ratio70</span> <span class="o">=</span> <span class="n">prune_layer_neurons</span><span class="p">(</span><span class="n">layer2</span><span class="p">,</span> <span class="n">keep_ratio</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">info_ratio50</span> <span class="o">=</span> <span class="n">prune_layer_neurons</span><span class="p">(</span><span class="n">layer2</span><span class="p">,</span> <span class="n">keep_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    
    <span class="k">assert</span> <span class="n">info_ratio70</span><span class="p">[</span><span class="s1">&#39;remaining_neurons&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">info_ratio50</span><span class="p">[</span><span class="s1">&#39;remaining_neurons&#39;</span><span class="p">],</span> <span class="s2">&quot;Higher keep ratio should result in more neurons&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚úÖ Different keep ratios work: 70% ratio = </span><span class="si">{</span><span class="n">info_ratio70</span><span class="p">[</span><span class="s1">&#39;remaining_neurons&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">, 50% ratio = </span><span class="si">{</span><span class="n">info_ratio50</span><span class="p">[</span><span class="s1">&#39;remaining_neurons&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="c1"># Test different importance methods</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">info_weight_mag</span> <span class="o">=</span> <span class="n">prune_layer_neurons</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">keep_ratio</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span> <span class="n">importance_method</span><span class="o">=</span><span class="s1">&#39;weight_magnitude&#39;</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">info_weight_var</span> <span class="o">=</span> <span class="n">prune_layer_neurons</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">keep_ratio</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span> <span class="n">importance_method</span><span class="o">=</span><span class="s1">&#39;weight_variance&#39;</span><span class="p">)</span>
    
    <span class="c1"># Both should achieve similar compression ratios since they both keep 75% of neurons</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚úÖ Different importance methods work: Weight Mag = </span><span class="si">{</span><span class="n">info_weight_mag</span><span class="p">[</span><span class="s1">&#39;compression_ratio&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, Weight Var = </span><span class="si">{</span><span class="n">info_weight_var</span><span class="p">[</span><span class="s1">&#39;compression_ratio&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üìà Progress: Structured Pruning ‚úì&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üéØ Structured pruning behavior:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - Removes least important neurons&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - Maintains layer structure and connectivity&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - Provides detailed statistics for analysis&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - Scales to different keep ratios&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>

<span class="c1"># Run the test</span>
<span class="n">test_structured_pruning</span><span class="p">()</span> 
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="step-6-comprehensive-comparison-combining-all-techniques">
<h2>Step 6: Comprehensive Comparison - Combining All Techniques<a class="headerlink" href="#step-6-comprehensive-comparison-combining-all-techniques" title="Permalink to this heading">#</a></h2>
<section id="putting-it-all-together">
<h3>Putting It All Together<a class="headerlink" href="#putting-it-all-together" title="Permalink to this heading">#</a></h3>
<p>Now that we‚Äôve implemented four core compression techniques, let‚Äôs combine them and see how they work together for maximum efficiency.</p>
</section>
<section id="the-compression-toolkit">
<h3>The Compression Toolkit<a class="headerlink" href="#the-compression-toolkit" title="Permalink to this heading">#</a></h3>
<p>We now have a complete arsenal:</p>
<ol class="arabic simple">
<li><p><strong>CompressionMetrics</strong>: Analyze model size and parameter distribution</p></li>
<li><p><strong>Magnitude-based pruning</strong>: Remove unimportant weights (sparsity)</p></li>
<li><p><strong>Quantization</strong>: Reduce precision (FP32 ‚Üí INT8)</p></li>
<li><p><strong>Knowledge distillation</strong>: Train compact models with teacher guidance</p></li>
<li><p><strong>Structured pruning</strong>: Remove entire neurons (actual speedup)</p></li>
</ol>
</section>
<section id="compression-strategy-design">
<h3>Compression Strategy Design<a class="headerlink" href="#compression-strategy-design" title="Permalink to this heading">#</a></h3>
<p>Different deployment scenarios need different strategies:</p>
<section id="mobile-ai-deployment">
<h4><strong>Mobile AI Deployment</strong><a class="headerlink" href="#mobile-ai-deployment" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Primary</strong>: Quantization (75% memory reduction)</p></li>
<li><p><strong>Secondary</strong>: Structured pruning (inference speedup)</p></li>
<li><p><strong>Target</strong>: &lt; 10MB models, &lt; 100ms inference</p></li>
</ul>
</section>
<section id="edge-computing">
<h4><strong>Edge Computing</strong><a class="headerlink" href="#edge-computing" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Primary</strong>: Structured pruning (minimal compute)</p></li>
<li><p><strong>Secondary</strong>: Magnitude pruning (memory efficiency)</p></li>
<li><p><strong>Target</strong>: &lt; 1MB models, minimal power consumption</p></li>
</ul>
</section>
<section id="production-cloud">
<h4><strong>Production Cloud</strong><a class="headerlink" href="#production-cloud" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Primary</strong>: Knowledge distillation (balanced compression)</p></li>
<li><p><strong>Secondary</strong>: Quantization (cost reduction)</p></li>
<li><p><strong>Target</strong>: Maximize throughput while maintaining accuracy</p></li>
</ul>
</section>
<section id="research-and-development">
<h4><strong>Research and Development</strong><a class="headerlink" href="#research-and-development" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Primary</strong>: Magnitude pruning (experimental flexibility)</p></li>
<li><p><strong>Secondary</strong>: All techniques for comparison</p></li>
<li><p><strong>Target</strong>: Understand trade-offs and optimal combinations</p></li>
</ul>
</section>
</section>
<section id="compression-pipeline-design">
<h3>Compression Pipeline Design<a class="headerlink" href="#compression-pipeline-design" title="Permalink to this heading">#</a></h3>
<p>A systematic approach to model compression:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. Baseline analysis</span>
<span class="n">metrics</span> <span class="o">=</span> <span class="n">CompressionMetrics</span><span class="p">()</span>
<span class="n">baseline_size</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">calculate_model_size</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># 2. Apply magnitude pruning</span>
<span class="n">model</span><span class="p">,</span> <span class="n">prune_info</span> <span class="o">=</span> <span class="n">prune_model_by_magnitude</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">pruning_ratio</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># 3. Apply quantization</span>
<span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">Dense</span><span class="p">):</span>
        <span class="n">layer</span><span class="p">,</span> <span class="n">quant_info</span> <span class="o">=</span> <span class="n">quantize_layer_weights</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">bits</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>

<span class="c1"># 4. Apply structured pruning</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">Dense</span><span class="p">):</span>
        <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">struct_info</span> <span class="o">=</span> <span class="n">prune_layer_neurons</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">keep_ratio</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>

<span class="c1"># 5. Measure final compression</span>
<span class="n">final_size</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">calculate_model_size</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">compression_ratio</span> <span class="o">=</span> <span class="n">baseline_size</span><span class="p">[</span><span class="s1">&#39;size_mb&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="n">final_size</span><span class="p">[</span><span class="s1">&#39;size_mb&#39;</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="trade-off-analysis">
<h3>Trade-off Analysis<a class="headerlink" href="#trade-off-analysis" title="Permalink to this heading">#</a></h3>
<p>Understanding the compression spectrum:</p>
<ul class="simple">
<li><p><strong>Accuracy vs Size</strong>: More compression = more accuracy loss</p></li>
<li><p><strong>Size vs Speed</strong>: Structured compression gives actual speedup</p></li>
<li><p><strong>Memory vs Computation</strong>: Different bottlenecks need different solutions</p></li>
<li><p><strong>Development vs Production</strong>: Research flexibility vs deployment constraints</p></li>
</ul>
<p>Let‚Äôs build a comprehensive comparison framework!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| export</span>
<span class="k">def</span> <span class="nf">compare_compression_techniques</span><span class="p">(</span><span class="n">original_model</span><span class="p">:</span> <span class="n">Sequential</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compare all compression techniques on the same model.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        original_model: Base model to compress using different techniques</span>
<span class="sd">        </span>
<span class="sd">    Returns:</span>
<span class="sd">        Dictionary comparing results from different compression approaches</span>
<span class="sd">        </span>
<span class="sd">    TODO: Implement comprehensive compression comparison.</span>
<span class="sd">    </span>
<span class="sd">    STEP-BY-STEP IMPLEMENTATION:</span>
<span class="sd">    1. Set up baseline metrics from original model</span>
<span class="sd">    2. Apply each compression technique individually</span>
<span class="sd">    3. Apply combined compression techniques</span>
<span class="sd">    4. Measure and compare all results</span>
<span class="sd">    5. Return comprehensive comparison data</span>
<span class="sd">    </span>
<span class="sd">    COMPARISON DIMENSIONS:</span>
<span class="sd">    - Model size (MB)</span>
<span class="sd">    - Parameter count</span>
<span class="sd">    - Compression ratio</span>
<span class="sd">    - Memory reduction</span>
<span class="sd">    - Estimated speedup (for structured techniques)</span>
<span class="sd">    </span>
<span class="sd">    IMPLEMENTATION HINTS:</span>
<span class="sd">    - Create separate model copies for each technique</span>
<span class="sd">    - Use consistent parameters across techniques</span>
<span class="sd">    - Track both individual and combined effects</span>
<span class="sd">    - Include baseline for reference</span>
<span class="sd">    </span>
<span class="sd">    LEARNING CONNECTIONS:</span>
<span class="sd">    - This is how research papers compare compression methods</span>
<span class="sd">    - Production systems need this analysis for deployment decisions</span>
<span class="sd">    - Understanding trade-offs guides technique selection</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1">### BEGIN SOLUTION</span>
    <span class="c1"># YOUR CODE HERE</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
    <span class="c1">### END SOLUTION</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="testing-infrastructure">
<h2>üß™ Testing Infrastructure<a class="headerlink" href="#testing-infrastructure" title="Permalink to this heading">#</a></h2>
<section id="unit-testing-pattern">
<h3>üî¨ Unit Testing Pattern<a class="headerlink" href="#unit-testing-pattern" title="Permalink to this heading">#</a></h3>
<p>Each compression technique includes comprehensive unit tests:</p>
<ol class="arabic simple">
<li><p><strong>Functionality verification</strong>: Core algorithms work correctly</p></li>
<li><p><strong>Edge case handling</strong>: Robust error handling and boundary conditions</p></li>
<li><p><strong>Statistical validation</strong>: Compression metrics and analysis</p></li>
<li><p><strong>Performance measurement</strong>: Before/after comparisons</p></li>
</ol>
</section>
<section id="progress-tracking">
<h3>üìà Progress Tracking<a class="headerlink" href="#progress-tracking" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>CompressionMetrics</strong>: ‚úÖ Complete with parameter counting</p></li>
<li><p><strong>Magnitude-based pruning</strong>: ‚úÖ Complete with sparsity calculation</p></li>
<li><p><strong>Quantization</strong>: üîÑ Coming next</p></li>
<li><p><strong>Knowledge distillation</strong>: üîÑ Coming next</p></li>
<li><p><strong>Structured pruning</strong>: üîÑ Coming next</p></li>
<li><p><strong>Comprehensive comparison</strong>: üîÑ Coming next</p></li>
</ul>
</section>
<section id="educational-value">
<h3>üéì Educational Value<a class="headerlink" href="#educational-value" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Conceptual understanding</strong>: Why compression matters</p></li>
<li><p><strong>Practical implementation</strong>: Build techniques from scratch</p></li>
<li><p><strong>Real-world connections</strong>: Mobile, edge, and production deployment</p></li>
<li><p><strong>Systems thinking</strong>: Balance accuracy, efficiency, and constraints</p></li>
</ul>
<p>This module teaches the essential skills for deploying AI in resource-constrained environments!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_comprehensive_comparison</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    ### üß™ Unit Test: Comprehensive Comparison</span>
<span class="sd">    </span>
<span class="sd">    Test the integrated compression comparison framework.</span>
<span class="sd">    </span>
<span class="sd">    **This is a unit test** - it tests comprehensive comparison in isolation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üî¨ Unit Test: Comprehensive Comparison&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;**This is a unit test** - it tests comprehensive comparison in isolation.&quot;</span><span class="p">)</span>
    
    <span class="c1"># Create test model</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">([</span>
        <span class="n">Dense</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span>
        <span class="n">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span>
        <span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="p">])</span>
    
    <span class="c1"># Run comprehensive comparison</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">compare_compression_techniques</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    
    <span class="c1"># Verify baseline exists</span>
    <span class="k">assert</span> <span class="s1">&#39;baseline&#39;</span> <span class="ow">in</span> <span class="n">results</span><span class="p">,</span> <span class="s2">&quot;Baseline results should be included&quot;</span>
    <span class="n">baseline</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;baseline&#39;</span><span class="p">]</span>
    <span class="k">assert</span> <span class="n">baseline</span><span class="p">[</span><span class="s1">&#39;compression_ratio&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Baseline compression ratio should be 1.0, got </span><span class="si">{</span><span class="n">baseline</span><span class="p">[</span><span class="s1">&#39;compression_ratio&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚úÖ Baseline analysis works: </span><span class="si">{</span><span class="n">baseline</span><span class="p">[</span><span class="s1">&#39;parameters&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2"> parameters, </span><span class="si">{</span><span class="n">baseline</span><span class="p">[</span><span class="s1">&#39;size_mb&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2"> MB&quot;</span><span class="p">)</span>
    
    <span class="c1"># Verify individual techniques</span>
    <span class="n">techniques</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;magnitude_pruning&#39;</span><span class="p">,</span> <span class="s1">&#39;quantization&#39;</span><span class="p">,</span> <span class="s1">&#39;structured_pruning&#39;</span><span class="p">,</span> <span class="s1">&#39;combined&#39;</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">technique</span> <span class="ow">in</span> <span class="n">techniques</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">technique</span> <span class="ow">in</span> <span class="n">results</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Missing technique: </span><span class="si">{</span><span class="n">technique</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="n">technique</span><span class="p">]</span>
        
        <span class="c1"># Magnitude pruning creates sparsity but doesn&#39;t reduce file size in our simulation</span>
        <span class="k">if</span> <span class="n">technique</span> <span class="o">==</span> <span class="s1">&#39;magnitude_pruning&#39;</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;compression_ratio&#39;</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">technique</span><span class="si">}</span><span class="s2"> should have compression ratio &gt;= 1.0&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;compression_ratio&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">1.0</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">technique</span><span class="si">}</span><span class="s2"> should have compression ratio &gt; 1.0&quot;</span>
            
        <span class="k">assert</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">result</span><span class="p">[</span><span class="s1">&#39;memory_reduction&#39;</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">technique</span><span class="si">}</span><span class="s2"> memory reduction should be between 0 and 1&quot;</span>
        
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;‚úÖ All compression techniques work correctly&quot;</span><span class="p">)</span>
    
    <span class="c1"># Verify compression effectiveness</span>
    <span class="n">quantization</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;quantization&#39;</span><span class="p">]</span>
    <span class="n">structured</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;structured_pruning&#39;</span><span class="p">]</span>
    <span class="n">combined</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;combined&#39;</span><span class="p">]</span>
    
    <span class="k">assert</span> <span class="n">quantization</span><span class="p">[</span><span class="s1">&#39;compression_ratio&#39;</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mf">3.0</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Quantization should achieve at least 3x compression, got </span><span class="si">{</span><span class="n">quantization</span><span class="p">[</span><span class="s1">&#39;compression_ratio&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">assert</span> <span class="n">structured</span><span class="p">[</span><span class="s1">&#39;compression_ratio&#39;</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mf">1.2</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Structured pruning should achieve at least 1.2x compression, got </span><span class="si">{</span><span class="n">structured</span><span class="p">[</span><span class="s1">&#39;compression_ratio&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">assert</span> <span class="n">combined</span><span class="p">[</span><span class="s1">&#39;compression_ratio&#39;</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">quantization</span><span class="p">[</span><span class="s1">&#39;compression_ratio&#39;</span><span class="p">],</span> <span class="sa">f</span><span class="s2">&quot;Combined should be at least as good as best individual technique&quot;</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚úÖ Compression effectiveness verified:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  - Quantization: </span><span class="si">{</span><span class="n">quantization</span><span class="p">[</span><span class="s1">&#39;compression_ratio&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">x compression&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  - Structured: </span><span class="si">{</span><span class="n">structured</span><span class="p">[</span><span class="s1">&#39;compression_ratio&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">x compression&quot;</span><span class="p">)</span> 
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  - Combined: </span><span class="si">{</span><span class="n">combined</span><span class="p">[</span><span class="s1">&#39;compression_ratio&#39;</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">x compression&quot;</span><span class="p">)</span>
    
    <span class="c1"># Verify different techniques have different characteristics</span>
    <span class="n">magnitude</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;magnitude_pruning&#39;</span><span class="p">]</span>
    <span class="k">assert</span> <span class="s1">&#39;sparsity&#39;</span> <span class="ow">in</span> <span class="n">magnitude</span><span class="p">,</span> <span class="s2">&quot;Magnitude pruning should report sparsity&quot;</span>
    <span class="k">assert</span> <span class="s1">&#39;avg_memory_reduction_factor&#39;</span> <span class="ow">in</span> <span class="n">quantization</span><span class="p">,</span> <span class="s2">&quot;Quantization should report memory reduction factor&quot;</span>
    <span class="k">assert</span> <span class="s1">&#39;param_reduction&#39;</span> <span class="ow">in</span> <span class="n">structured</span><span class="p">,</span> <span class="s2">&quot;Structured pruning should report parameter reduction&quot;</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;‚úÖ Technique-specific metrics work correctly&quot;</span><span class="p">)</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üìà Progress: Comprehensive Comparison ‚úì&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üéØ Comprehensive comparison behavior:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - Compares all techniques systematically&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - Provides detailed metrics for each approach&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - Enables informed compression strategy selection&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  - Demonstrates combined technique effectiveness&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>

<span class="c1"># Run the test</span>
<span class="n">test_comprehensive_comparison</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="module-testing">
<h2>üß™ Module Testing<a class="headerlink" href="#module-testing" title="Permalink to this heading">#</a></h2>
<p>Time to test your implementation! This section uses TinyTorch‚Äôs standardized testing framework to ensure your implementation works correctly.</p>
<p><strong>This testing section is locked</strong> - it provides consistent feedback across all modules and cannot be modified.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># =============================================================================</span>
<span class="c1"># STANDARDIZED MODULE TESTING - DO NOT MODIFY</span>
<span class="c1"># This cell is locked to ensure consistent testing across all TinyTorch modules</span>
<span class="c1"># =============================================================================</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">tito.tools.testing</span> <span class="kn">import</span> <span class="n">run_module_tests_auto</span>
    
    <span class="c1"># Automatically discover and run all tests in this module</span>
    <span class="n">success</span> <span class="o">=</span> <span class="n">run_module_tests_auto</span><span class="p">(</span><span class="s2">&quot;Compression&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="module-summary">
<h2>üìã Module Summary<a class="headerlink" href="#module-summary" title="Permalink to this heading">#</a></h2>
<section id="what-we-ve-built">
<h3>‚úÖ What We‚Äôve Built<a class="headerlink" href="#what-we-ve-built" title="Permalink to this heading">#</a></h3>
<p>This compression module provides a complete toolkit for making neural networks efficient:</p>
<section id="compressionmetrics">
<h4><strong>1. CompressionMetrics</strong> ‚úì<a class="headerlink" href="#compressionmetrics" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Parameter counting</strong>: Analyze model size and distribution</p></li>
<li><p><strong>Memory footprint</strong>: Calculate storage requirements in different data types</p></li>
<li><p><strong>Foundation</strong>: Baseline measurement for compression decisions</p></li>
</ul>
</section>
<section id="magnitude-based-pruning">
<h4><strong>2. Magnitude-Based Pruning</strong> ‚úì<a class="headerlink" href="#magnitude-based-pruning" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Weight removal</strong>: Remove smallest weights based on magnitude</p></li>
<li><p><strong>Sparsity creation</strong>: Create sparse matrices for memory efficiency</p></li>
<li><p><strong>Flexible thresholds</strong>: Support different pruning intensities</p></li>
</ul>
</section>
<section id="quantization">
<h4><strong>3. Quantization</strong> ‚úì<a class="headerlink" href="#quantization" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Precision reduction</strong>: Convert FP32 ‚Üí INT8 for 75% memory savings</p></li>
<li><p><strong>Error tracking</strong>: Monitor quantization impact on model accuracy</p></li>
<li><p><strong>Multiple bit-widths</strong>: Support 16-bit, 8-bit, and other precisions</p></li>
</ul>
</section>
<section id="knowledge-distillation">
<h4><strong>4. Knowledge Distillation</strong> ‚úì<a class="headerlink" href="#knowledge-distillation" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Teacher-student training</strong>: Large models guide small model learning</p></li>
<li><p><strong>Soft targets</strong>: Rich probability distributions vs hard labels</p></li>
<li><p><strong>Temperature scaling</strong>: Control knowledge transfer richness</p></li>
</ul>
</section>
<section id="structured-pruning">
<h4><strong>5. Structured Pruning</strong> ‚úì<a class="headerlink" href="#structured-pruning" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Neuron removal</strong>: Remove entire neurons for actual hardware speedup</p></li>
<li><p><strong>Architecture modification</strong>: Create smaller but dense networks</p></li>
<li><p><strong>Importance metrics</strong>: Multiple methods for ranking neuron importance</p></li>
</ul>
</section>
<section id="comprehensive-comparison">
<h4><strong>6. Comprehensive Comparison</strong> ‚úì<a class="headerlink" href="#comprehensive-comparison" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Systematic evaluation</strong>: Compare all techniques on same baseline</p></li>
<li><p><strong>Combined approaches</strong>: Integrate multiple techniques for maximum compression</p></li>
<li><p><strong>Trade-off analysis</strong>: Understand compression vs accuracy spectrum</p></li>
</ul>
</section>
</section>
<section id="id5">
<h3>üéØ Real-World Applications<a class="headerlink" href="#id5" title="Permalink to this heading">#</a></h3>
<p>Students can now optimize models for:</p>
<ul class="simple">
<li><p><strong>Mobile AI</strong>: &lt; 10MB models for smartphone deployment</p></li>
<li><p><strong>Edge computing</strong>: &lt; 1MB models for IoT and embedded systems</p></li>
<li><p><strong>Production cloud</strong>: Cost-optimized inference at scale</p></li>
<li><p><strong>Research</strong>: Systematic compression comparison and analysis</p></li>
</ul>
</section>
<section id="compression-achievements">
<h3>üìä Compression Achievements<a class="headerlink" href="#compression-achievements" title="Permalink to this heading">#</a></h3>
<p>With the complete toolkit, students can achieve:</p>
<ul class="simple">
<li><p><strong>4x+ memory reduction</strong>: Through quantization (FP32 ‚Üí INT8)</p></li>
<li><p><strong>1.3x+ speedup</strong>: Through structured pruning (actual hardware benefit)</p></li>
<li><p><strong>5x+ combined compression</strong>: Integrating multiple techniques</p></li>
<li><p><strong>Flexible trade-offs</strong>: Balance accuracy, size, and speed as needed</p></li>
</ul>
</section>
<section id="next-steps">
<h3>üîó Next Steps<a class="headerlink" href="#next-steps" title="Permalink to this heading">#</a></h3>
<p>This compression foundation prepares students for:</p>
<ul class="simple">
<li><p><strong>Module 11 - GPU Kernels</strong>: Hardware-accelerated compression operations</p></li>
<li><p><strong>Module 12 - Benchmarking</strong>: Systematic performance evaluation and optimization</p></li>
<li><p><strong>Module 13 - MLOps</strong>: Production deployment with compressed models</p></li>
</ul>
</section>
<section id="professional-applications">
<h3>üöÄ Professional Applications<a class="headerlink" href="#professional-applications" title="Permalink to this heading">#</a></h3>
<p>Your compression toolkit enables:</p>
<ul class="simple">
<li><p><strong>Production AI</strong>: Deploy efficient models at scale</p></li>
<li><p><strong>Mobile Applications</strong>: Real-time AI on smartphones and tablets</p></li>
<li><p><strong>Edge Computing</strong>: AI in IoT devices and embedded systems</p></li>
<li><p><strong>Research</strong>: Systematic compression analysis and method development</p></li>
</ul>
</section>
<section id="the-future-of-efficient-ai">
<h3>üéØ The Future of Efficient AI<a class="headerlink" href="#the-future-of-efficient-ai" title="Permalink to this heading">#</a></h3>
<p>You‚Äôve built the foundation for efficient AI systems:</p>
<ul class="simple">
<li><p><strong>Sustainable AI</strong>: Reduced energy consumption and carbon footprint</p></li>
<li><p><strong>Accessible AI</strong>: AI systems that run on consumer hardware</p></li>
<li><p><strong>Scalable Inference</strong>: Cost-effective deployment at any scale</p></li>
<li><p><strong>Real-time Applications</strong>: Fast, efficient AI for interactive systems</p></li>
</ul>
</section>
<section id="key-skills-developed">
<h3>üß† Key Skills Developed<a class="headerlink" href="#key-skills-developed" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Compression Theory</strong>: Understanding memory, compute, and accuracy trade-offs</p></li>
<li><p><strong>Mathematical Implementation</strong>: Quantization, pruning, and distillation algorithms</p></li>
<li><p><strong>Systems Engineering</strong>: Benchmarking, comparison, and optimization frameworks</p></li>
<li><p><strong>Production Readiness</strong>: Real-world deployment considerations and techniques</p></li>
</ul>
<p>You‚Äôve mastered the art and science of making neural networks efficient without sacrificing capability. This is the foundation of modern AI deployment!</p>
</section>
</section>
<section id="next-steps-advanced-optimization">
<h2>üöÄ Next Steps: Advanced Optimization<a class="headerlink" href="#next-steps-advanced-optimization" title="Permalink to this heading">#</a></h2>
<section id="module-11-kernels-hardware-aware-optimization">
<h3>Module 11: Kernels - Hardware-Aware Optimization<a class="headerlink" href="#module-11-kernels-hardware-aware-optimization" title="Permalink to this heading">#</a></h3>
<p>Build on compression foundations with:</p>
<ul class="simple">
<li><p><strong>Custom CUDA kernels</strong>: GPU-optimized operations for compressed models</p></li>
<li><p><strong>SIMD optimization</strong>: CPU vectorization for quantized operations</p></li>
<li><p><strong>Memory layout</strong>: Optimize data structures for sparse and quantized weights</p></li>
<li><p><strong>Hardware profiling</strong>: Measure actual performance improvements</p></li>
</ul>
</section>
<section id="module-12-benchmarking-systematic-performance-measurement">
<h3>Module 12: Benchmarking - Systematic Performance Measurement<a class="headerlink" href="#module-12-benchmarking-systematic-performance-measurement" title="Permalink to this heading">#</a></h3>
<p>Apply compression in production context:</p>
<ul class="simple">
<li><p><strong>Latency measurement</strong>: Quantify inference speedup from compression</p></li>
<li><p><strong>Accuracy evaluation</strong>: Systematic testing of compression impact</p></li>
<li><p><strong>A/B testing</strong>: Compare compressed vs uncompressed models in production</p></li>
<li><p><strong>Performance profiling</strong>: Identify bottlenecks and optimization opportunities</p></li>
</ul>
</section>
<section id="module-13-mlops-production-deployment">
<h3>Module 13: MLOps - Production Deployment<a class="headerlink" href="#module-13-mlops-production-deployment" title="Permalink to this heading">#</a></h3>
<p>Deploy compressed models at scale:</p>
<ul class="simple">
<li><p><strong>Model versioning</strong>: Manage compressed model variants</p></li>
<li><p><strong>Monitoring</strong>: Track compressed model performance in production</p></li>
<li><p><strong>Continuous optimization</strong>: Automated compression pipeline</p></li>
<li><p><strong>Edge deployment</strong>: Distribute compressed models to mobile and IoT devices</p></li>
</ul>
</section>
<section id="research-directions">
<h3>üî¨ Research Directions<a class="headerlink" href="#research-directions" title="Permalink to this heading">#</a></h3>
<p>Advanced compression techniques:</p>
<ul class="simple">
<li><p><strong>Neural Architecture Search</strong>: Automated compression-aware design</p></li>
<li><p><strong>Hardware-aware compression</strong>: Optimize for specific deployment targets</p></li>
<li><p><strong>Dynamic compression</strong>: Adaptive compression based on runtime conditions</p></li>
<li><p><strong>Federated compression</strong>: Compress models for distributed learning</p></li>
</ul>
</section>
<section id="career-applications">
<h3>üíº Career Applications<a class="headerlink" href="#career-applications" title="Permalink to this heading">#</a></h3>
<p>These compression skills are essential for:</p>
<ul class="simple">
<li><p><strong>Mobile AI Engineer</strong>: Optimize models for smartphones and tablets</p></li>
<li><p><strong>Edge AI Developer</strong>: Deploy AI on IoT and embedded systems</p></li>
<li><p><strong>ML Infrastructure Engineer</strong>: Build efficient inference systems</p></li>
<li><p><strong>Research Scientist</strong>: Advance state-of-art compression techniques</p></li>
</ul>
<p>The compression module provides the foundation for all advanced optimization and deployment scenarios!</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="09-training.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Training</p>
      </div>
    </a>
    <a class="right-next"
       href="11-kernels.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Kernels</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#where-this-code-lives-in-the-final-package">üì¶ Where This Code Lives in the Final Package</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-model-compression">What is Model Compression?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-problem-ai-models-are-getting-huge">The Problem: AI Models Are Getting Huge</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-solution-intelligent-compression">The Solution: Intelligent Compression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#real-world-impact">Real-World Impact</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-we-ll-build">What We‚Äôll Build</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-understanding-model-size-and-parameters">Step 1: Understanding Model Size and Parameters</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-makes-models-large">What Makes Models Large?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-memory-reality-check">The Memory Reality Check</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-size-matters">Why Size Matters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-efficiency-spectrum">The Efficiency Spectrum</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#real-world-examples">Real-World Examples</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-magnitude-based-pruning-removing-unimportant-weights">Step 2: Magnitude-Based Pruning - Removing Unimportant Weights</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-magnitude-based-pruning">What is Magnitude-Based Pruning?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-algorithm">The Algorithm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-this-works">Why This Works</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#real-world-applications">Real-World Applications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pruning-strategies">Pruning Strategies</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-vs-sparsity-trade-off">Performance vs Sparsity Trade-off</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-quantization-reducing-precision-for-memory-efficiency">Step 3: Quantization - Reducing Precision for Memory Efficiency</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-quantization">What is Quantization?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-mathematical-foundation">The Mathematical Foundation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-quantization-works">Why Quantization Works</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-quantization">Types of Quantization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Real-World Impact</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#common-bit-widths">Common Bit-Widths</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-knowledge-distillation-large-models-teach-small-models">Step 4: Knowledge Distillation - Large Models Teach Small Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-knowledge-distillation">What is Knowledge Distillation?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-core-idea">The Core Idea</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">The Mathematical Foundation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-distillation-works">Why Distillation Works</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-parameters">Key Parameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Real-World Applications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#success-stories">Success Stories</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-structured-pruning-removing-entire-neurons-and-channels">Step 5: Structured Pruning - Removing Entire Neurons and Channels</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-structured-pruning">What is Structured Pruning?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#structured-vs-unstructured-pruning">Structured vs Unstructured Pruning</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#unstructured-pruning-what-we-did-in-step-2"><strong>Unstructured Pruning</strong> (What we did in Step 2)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#structured-pruning-what-we-re-doing-now"><strong>Structured Pruning</strong> (What we‚Äôre doing now)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-mathematical-impact">The Mathematical Impact</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-structured-pruning-works">Why Structured Pruning Works</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neuron-importance-metrics">Neuron Importance Metrics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Real-World Applications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges">Challenges</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-6-comprehensive-comparison-combining-all-techniques">Step 6: Comprehensive Comparison - Combining All Techniques</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-it-all-together">Putting It All Together</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-compression-toolkit">The Compression Toolkit</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#compression-strategy-design">Compression Strategy Design</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mobile-ai-deployment"><strong>Mobile AI Deployment</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#edge-computing"><strong>Edge Computing</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#production-cloud"><strong>Production Cloud</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#research-and-development"><strong>Research and Development</strong></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#compression-pipeline-design">Compression Pipeline Design</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#trade-off-analysis">Trade-off Analysis</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#testing-infrastructure">üß™ Testing Infrastructure</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unit-testing-pattern">üî¨ Unit Testing Pattern</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#progress-tracking">üìà Progress Tracking</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#educational-value">üéì Educational Value</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-testing">üß™ Module Testing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-summary">üìã Module Summary</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-we-ve-built">‚úÖ What We‚Äôve Built</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#compressionmetrics"><strong>1. CompressionMetrics</strong> ‚úì</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#magnitude-based-pruning"><strong>2. Magnitude-Based Pruning</strong> ‚úì</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#quantization"><strong>3. Quantization</strong> ‚úì</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#knowledge-distillation"><strong>4. Knowledge Distillation</strong> ‚úì</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#structured-pruning"><strong>5. Structured Pruning</strong> ‚úì</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#comprehensive-comparison"><strong>6. Comprehensive Comparison</strong> ‚úì</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">üéØ Real-World Applications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#compression-achievements">üìä Compression Achievements</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#next-steps">üîó Next Steps</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#professional-applications">üöÄ Professional Applications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-future-of-efficient-ai">üéØ The Future of Efficient AI</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-skills-developed">üß† Key Skills Developed</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#next-steps-advanced-optimization">üöÄ Next Steps: Advanced Optimization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#module-11-kernels-hardware-aware-optimization">Module 11: Kernels - Hardware-Aware Optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#module-12-benchmarking-systematic-performance-measurement">Module 12: Benchmarking - Systematic Performance Measurement</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#module-13-mlops-production-deployment">Module 13: MLOps - Production Deployment</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#research-directions">üî¨ Research Directions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#career-applications">üíº Career Applications</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By TinyTorch Team
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>