

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Optimizers &#8212; Tinyüî•Torch: Build ML Systems from Scratch</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapters/08-optimizers';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Training" href="09-training.html" />
    <link rel="prev" title="Autograd" href="07-autograd.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Tinyüî•Torch: Build ML Systems from Scratch - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Tinyüî•Torch: Build ML Systems from Scratch - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Tinyüî•Torch: Build Machine Learning Systems from Scratch
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Usage Paths</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../usage-paths/quick-exploration.html">üî¨ Quick Exploration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../usage-paths/serious-development.html">üèóÔ∏è Serious Development</a></li>
<li class="toctree-l1"><a class="reference internal" href="../usage-paths/classroom-use.html">üë®‚Äçüè´ Classroom Use</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Foundation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="00-setup.html">0. Development Environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="01-tensor.html">1. Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="02-activations.html">2. Activations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Building Blocks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="03-layers.html">3. Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="04-networks.html">4. Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-cnn.html">5. CNNs</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Training Systems</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="06-dataloader.html">DataLoader</a></li>

<li class="toctree-l1"><a class="reference internal" href="07-autograd.html">7. Autograd</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">8. Optimizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="09-training.html">9. Training</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Production &amp; Performance</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="10-compression.html">10. Compression</a></li>
<li class="toctree-l1"><a class="reference internal" href="11-kernels.html">11. Kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="12-benchmarking.html">12. Benchmarking</a></li>
<li class="toctree-l1"><a class="reference internal" href="13-mlops.html">13. MLOps</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/VJ/TinyTorch" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/VJ/TinyTorch/edit/main/book/tinytorch-course/chapters/08-optimizers.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/VJ/TinyTorch/issues/new?title=Issue%20on%20page%20%2Fchapters/08-optimizers.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapters/08-optimizers.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Optimizers</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#where-this-code-lives-in-the-final-package">üì¶ Where This Code Lives in the Final Package</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-optimizers">What Are Optimizers?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-problem-how-to-update-parameters">The Problem: How to Update Parameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-solution-smart-optimization">The Solution: Smart Optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#real-world-impact">Real-World Impact</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-we-ll-build">What We‚Äôll Build</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-understanding-gradient-descent">Step 1: Understanding Gradient Descent</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-gradient-descent">What is Gradient Descent?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-gradient-descent-works">Why Gradient Descent Works</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-learning-rate-dilemma">The Learning Rate Dilemma</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visual-understanding">Visual Understanding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#real-world-applications">Real-World Applications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unit-test-gradient-descent-step">üß™ Unit Test: Gradient Descent Step</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-sgd-with-momentum">Step 2: SGD with Momentum</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-sgd">What is SGD?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-problem-with-vanilla-sgd">The Problem with Vanilla SGD</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-solution-momentum">The Solution: Momentum</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-momentum-works">Why Momentum Works</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Visual Understanding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Real-World Applications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unit-test-sgd-optimizer">üß™ Unit Test: SGD Optimizer</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-adam-adaptive-learning-rates">Step 3: Adam - Adaptive Learning Rates</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-adam">What is Adam?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-adam-is-revolutionary">Why Adam is Revolutionary</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-three-key-ideas">The Three Key Ideas</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Visual Understanding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Real-World Applications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#test-your-adam-implementation">üß™ Test Your Adam Implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unit-test-adam-optimizer">üß™ Unit Test: Adam Optimizer</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-learning-rate-scheduling">Step 4: Learning Rate Scheduling</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-learning-rate-scheduling">What is Learning Rate Scheduling?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-scheduling-matters">Why Scheduling Matters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#common-scheduling-strategies">Common Scheduling Strategies</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Visual Understanding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Real-World Applications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unit-test-step-learning-rate-scheduler">üß™ Unit Test: Step Learning Rate Scheduler</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-integration-complete-training-example">Step 5: Integration - Complete Training Example</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-it-all-together">Putting It All Together</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-modern-training-loop">The Modern Training Loop</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unit-test-complete-training-integration">üß™ Unit Test: Complete Training Integration</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-summary-optimization-mastery">üéØ Module Summary: Optimization Mastery!</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-you-ve-built">‚úÖ What You‚Äôve Built</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-learning-outcomes">‚úÖ Key Learning Outcomes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-foundations-mastered">‚úÖ Mathematical Foundations Mastered</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#professional-skills-developed">‚úÖ Professional Skills Developed</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ready-for-advanced-applications">‚úÖ Ready for Advanced Applications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#connection-to-real-ml-systems">üîó Connection to Real ML Systems</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-power-of-intelligent-optimization">üéØ The Power of Intelligent Optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-revolution">üß† Deep Learning Revolution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-s-next">üöÄ What‚Äôs Next</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-testing">üß™ Module Testing</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="optimizers">
<h1>Optimizers<a class="headerlink" href="#optimizers" title="Permalink to this heading">#</a></h1>
<div class="tip admonition">
<p class="admonition-title">Interactive Learning</p>
<p>üöÄ <strong>Launch Binder</strong>: Click the rocket icon above to run this chapter interactively!</p>
<p>üíæ <strong>Save Your Work</strong>: Download your completed notebook when done.</p>
<p>üèóÔ∏è <strong>Build Locally</strong>: Ready for serious development? <a class="reference external" href="https://github.com/your-org/tinytorch">Fork the repo</a> and work locally with the full <code class="docutils literal notranslate"><span class="pre">tito</span></code> workflow.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| default_exp core.optimizers</span>

<span class="c1">#| export</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Union</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>

<span class="c1"># Helper function to set up import paths</span>
<span class="k">def</span> <span class="nf">setup_import_paths</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Set up import paths for development modules.&quot;&quot;&quot;</span>
    <span class="kn">import</span> <span class="nn">sys</span>
    <span class="kn">import</span> <span class="nn">os</span>
    
    <span class="c1"># Add module directories to path</span>
    <span class="n">base_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="vm">__file__</span><span class="p">)))</span>
    <span class="n">tensor_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">base_dir</span><span class="p">,</span> <span class="s1">&#39;01_tensor&#39;</span><span class="p">)</span>
    <span class="n">autograd_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">base_dir</span><span class="p">,</span> <span class="s1">&#39;07_autograd&#39;</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">tensor_dir</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="p">:</span>
        <span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tensor_dir</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">autograd_dir</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="p">:</span>
        <span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">autograd_dir</span><span class="p">)</span>

<span class="c1"># Import our existing components</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">tinytorch.core.tensor</span> <span class="kn">import</span> <span class="n">Tensor</span>
    <span class="kn">from</span> <span class="nn">tinytorch.core.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="c1"># For development, try local imports</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">setup_import_paths</span><span class="p">()</span>
        <span class="kn">from</span> <span class="nn">tensor_dev</span> <span class="kn">import</span> <span class="n">Tensor</span>
        <span class="kn">from</span> <span class="nn">autograd_dev</span> <span class="kn">import</span> <span class="n">Variable</span>
    <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
        <span class="c1"># Create minimal fallback classes for testing</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Warning: Using fallback classes for testing&quot;</span><span class="p">)</span>
        
        <span class="k">class</span> <span class="nc">Tensor</span><span class="p">:</span>
            <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span>
            
            <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Tensor(</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="si">}</span><span class="s2">)&quot;</span>
        
        <span class="k">class</span> <span class="nc">Variable</span><span class="p">:</span>
            <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">)):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">([</span><span class="n">data</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="n">requires_grad</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
            
            <span class="k">def</span> <span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
            
            <span class="k">def</span> <span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Variable(</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="si">}</span><span class="s2">)&quot;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">line</span> <span class="mi">29</span>
<span class="g g-Whitespace">     </span><span class="mi">28</span> <span class="k">try</span><span class="p">:</span>
<span class="ne">---&gt; </span><span class="mi">29</span>     <span class="kn">from</span> <span class="nn">tinytorch.core.tensor</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="g g-Whitespace">     </span><span class="mi">30</span>     <span class="kn">from</span> <span class="nn">tinytorch.core.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;tinytorch&#39;

<span class="n">During</span> <span class="n">handling</span> <span class="n">of</span> <span class="n">the</span> <span class="n">above</span> <span class="n">exception</span><span class="p">,</span> <span class="n">another</span> <span class="n">exception</span> <span class="n">occurred</span><span class="p">:</span>

<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">line</span> <span class="mi">34</span>
<span class="g g-Whitespace">     </span><span class="mi">31</span> <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
<span class="g g-Whitespace">     </span><span class="mi">32</span>     <span class="c1"># For development, try local imports</span>
<span class="g g-Whitespace">     </span><span class="mi">33</span>     <span class="k">try</span><span class="p">:</span>
<span class="ne">---&gt; </span><span class="mi">34</span>         <span class="n">setup_import_paths</span><span class="p">()</span>
<span class="g g-Whitespace">     </span><span class="mi">35</span>         <span class="kn">from</span> <span class="nn">tensor_dev</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="g g-Whitespace">     </span><span class="mi">36</span>         <span class="kn">from</span> <span class="nn">autograd_dev</span> <span class="kn">import</span> <span class="n">Variable</span>

<span class="nn">Cell In[1], line 18,</span> in <span class="ni">setup_import_paths</span><span class="nt">()</span>
<span class="g g-Whitespace">     </span><span class="mi">15</span> <span class="kn">import</span> <span class="nn">os</span>
<span class="g g-Whitespace">     </span><span class="mi">17</span> <span class="c1"># Add module directories to path</span>
<span class="ne">---&gt; </span><span class="mi">18</span> <span class="n">base_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="vm">__file__</span><span class="p">)))</span>
<span class="g g-Whitespace">     </span><span class="mi">19</span> <span class="n">tensor_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">base_dir</span><span class="p">,</span> <span class="s1">&#39;01_tensor&#39;</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">20</span> <span class="n">autograd_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">base_dir</span><span class="p">,</span> <span class="s1">&#39;07_autograd&#39;</span><span class="p">)</span>

<span class="ne">NameError</span>: name &#39;__file__&#39; is not defined
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üî• TinyTorch Optimizers Module&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;NumPy version: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Python version: </span><span class="si">{</span><span class="n">sys</span><span class="o">.</span><span class="n">version_info</span><span class="o">.</span><span class="n">major</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">sys</span><span class="o">.</span><span class="n">version_info</span><span class="o">.</span><span class="n">minor</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Ready to build optimization algorithms!&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="where-this-code-lives-in-the-final-package">
<h2>üì¶ Where This Code Lives in the Final Package<a class="headerlink" href="#where-this-code-lives-in-the-final-package" title="Permalink to this heading">#</a></h2>
<p><strong>Learning Side:</strong> You work in <code class="docutils literal notranslate"><span class="pre">modules/source/08_optimizers/optimizers_dev.py</span></code><br />
<strong>Building Side:</strong> Code exports to <code class="docutils literal notranslate"><span class="pre">tinytorch.core.optimizers</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Final package structure:</span>
<span class="kn">from</span> <span class="nn">tinytorch.core.optimizers</span> <span class="kn">import</span> <span class="n">SGD</span><span class="p">,</span> <span class="n">Adam</span><span class="p">,</span> <span class="n">StepLR</span>  <span class="c1"># The optimization engines!</span>
<span class="kn">from</span> <span class="nn">tinytorch.core.autograd</span> <span class="kn">import</span> <span class="n">Variable</span>  <span class="c1"># Gradient computation</span>
<span class="kn">from</span> <span class="nn">tinytorch.core.tensor</span> <span class="kn">import</span> <span class="n">Tensor</span>  <span class="c1"># Data structures</span>
</pre></div>
</div>
<p><strong>Why this matters:</strong></p>
<ul class="simple">
<li><p><strong>Learning:</strong> Focused module for understanding optimization algorithms</p></li>
<li><p><strong>Production:</strong> Proper organization like PyTorch‚Äôs <code class="docutils literal notranslate"><span class="pre">torch.optim</span></code></p></li>
<li><p><strong>Consistency:</strong> All optimization algorithms live together in <code class="docutils literal notranslate"><span class="pre">core.optimizers</span></code></p></li>
<li><p><strong>Foundation:</strong> Enables effective neural network training</p></li>
</ul>
</section>
<section id="what-are-optimizers">
<h2>What Are Optimizers?<a class="headerlink" href="#what-are-optimizers" title="Permalink to this heading">#</a></h2>
<section id="the-problem-how-to-update-parameters">
<h3>The Problem: How to Update Parameters<a class="headerlink" href="#the-problem-how-to-update-parameters" title="Permalink to this heading">#</a></h3>
<p>Neural networks learn by updating parameters using gradients:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">parameter_new</span> <span class="o">=</span> <span class="n">parameter_old</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gradient</span>
</pre></div>
</div>
<p>But <strong>naive gradient descent</strong> has problems:</p>
<ul class="simple">
<li><p><strong>Slow convergence</strong>: Takes many steps to reach optimum</p></li>
<li><p><strong>Oscillation</strong>: Bounces around valleys without making progress</p></li>
<li><p><strong>Poor scaling</strong>: Same learning rate for all parameters</p></li>
</ul>
</section>
<section id="the-solution-smart-optimization">
<h3>The Solution: Smart Optimization<a class="headerlink" href="#the-solution-smart-optimization" title="Permalink to this heading">#</a></h3>
<p><strong>Optimizers</strong> are algorithms that intelligently update parameters:</p>
<ul class="simple">
<li><p><strong>Momentum</strong>: Accelerate convergence by accumulating velocity</p></li>
<li><p><strong>Adaptive learning rates</strong>: Different learning rates for different parameters</p></li>
<li><p><strong>Second-order information</strong>: Use curvature to guide updates</p></li>
</ul>
</section>
<section id="real-world-impact">
<h3>Real-World Impact<a class="headerlink" href="#real-world-impact" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>SGD</strong>: The foundation of all neural network training</p></li>
<li><p><strong>Adam</strong>: The default optimizer for most deep learning applications</p></li>
<li><p><strong>Learning rate scheduling</strong>: Critical for training stability and performance</p></li>
</ul>
</section>
<section id="what-we-ll-build">
<h3>What We‚Äôll Build<a class="headerlink" href="#what-we-ll-build" title="Permalink to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>SGD</strong>: Stochastic Gradient Descent with momentum</p></li>
<li><p><strong>Adam</strong>: Adaptive Moment Estimation optimizer</p></li>
<li><p><strong>StepLR</strong>: Learning rate scheduling</p></li>
<li><p><strong>Integration</strong>: Complete training loop with optimizers</p></li>
</ol>
</section>
</section>
<section id="step-1-understanding-gradient-descent">
<h2>Step 1: Understanding Gradient Descent<a class="headerlink" href="#step-1-understanding-gradient-descent" title="Permalink to this heading">#</a></h2>
<section id="what-is-gradient-descent">
<h3>What is Gradient Descent?<a class="headerlink" href="#what-is-gradient-descent" title="Permalink to this heading">#</a></h3>
<p><strong>Gradient descent</strong> finds the minimum of a function by following the negative gradient:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Œ∏_{t+1} = Œ∏_t - Œ± ‚àáf(Œ∏_t)
</pre></div>
</div>
<p>Where:</p>
<ul class="simple">
<li><p>Œ∏: Parameters we want to optimize</p></li>
<li><p>Œ±: Learning rate (how big steps to take)</p></li>
<li><p>‚àáf(Œ∏): Gradient of loss function with respect to parameters</p></li>
</ul>
</section>
<section id="why-gradient-descent-works">
<h3>Why Gradient Descent Works<a class="headerlink" href="#why-gradient-descent-works" title="Permalink to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Gradients point uphill</strong>: Negative gradient points toward minimum</p></li>
<li><p><strong>Iterative improvement</strong>: Each step reduces the loss (in theory)</p></li>
<li><p><strong>Local convergence</strong>: Finds local minimum with proper learning rate</p></li>
<li><p><strong>Scalable</strong>: Works with millions of parameters</p></li>
</ol>
</section>
<section id="the-learning-rate-dilemma">
<h3>The Learning Rate Dilemma<a class="headerlink" href="#the-learning-rate-dilemma" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Too large</strong>: Overshoots minimum, diverges</p></li>
<li><p><strong>Too small</strong>: Extremely slow convergence</p></li>
<li><p><strong>Just right</strong>: Steady progress toward minimum</p></li>
</ul>
</section>
<section id="visual-understanding">
<h3>Visual Understanding<a class="headerlink" href="#visual-understanding" title="Permalink to this heading">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Loss landscape: \__/
Start here: ‚Üë
Gradient descent: ‚Üì ‚Üí ‚Üì ‚Üí ‚Üì ‚Üí minimum
</pre></div>
</div>
</section>
<section id="real-world-applications">
<h3>Real-World Applications<a class="headerlink" href="#real-world-applications" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Neural networks</strong>: Training any deep learning model</p></li>
<li><p><strong>Machine learning</strong>: Logistic regression, SVM, etc.</p></li>
<li><p><strong>Scientific computing</strong>: Optimization problems in physics, engineering</p></li>
<li><p><strong>Economics</strong>: Portfolio optimization, game theory</p></li>
</ul>
<p>Let‚Äôs implement gradient descent to understand it deeply!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| export</span>
<span class="k">def</span> <span class="nf">gradient_descent_step</span><span class="p">(</span><span class="n">parameter</span><span class="p">:</span> <span class="n">Variable</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Perform one step of gradient descent on a parameter.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        parameter: Variable with gradient information</span>
<span class="sd">        learning_rate: How much to update parameter</span>
<span class="sd">    </span>
<span class="sd">    TODO: Implement basic gradient descent parameter update.</span>
<span class="sd">    </span>
<span class="sd">    STEP-BY-STEP IMPLEMENTATION:</span>
<span class="sd">    1. Check if parameter has a gradient</span>
<span class="sd">    2. Get current parameter value and gradient</span>
<span class="sd">    3. Update parameter: new_value = old_value - learning_rate * gradient</span>
<span class="sd">    4. Update parameter data with new value</span>
<span class="sd">    5. Handle edge cases (no gradient, invalid values)</span>
<span class="sd">    </span>
<span class="sd">    EXAMPLE USAGE:</span>
<span class="sd">    ```python</span>
<span class="sd">    # Parameter with gradient</span>
<span class="sd">    w = Variable(2.0, requires_grad=True)</span>
<span class="sd">    w.grad = Variable(0.5)  # Gradient from loss</span>
<span class="sd">    </span>
<span class="sd">    # Update parameter</span>
<span class="sd">    gradient_descent_step(w, learning_rate=0.1)</span>
<span class="sd">    # w.data now contains: 2.0 - 0.1 * 0.5 = 1.95</span>
<span class="sd">    ```</span>
<span class="sd">    </span>
<span class="sd">    IMPLEMENTATION HINTS:</span>
<span class="sd">    - Check if parameter.grad is not None</span>
<span class="sd">    - Use parameter.grad.data.data to get gradient value</span>
<span class="sd">    - Update parameter.data with new Tensor</span>
<span class="sd">    - Don&#39;t modify gradient (it&#39;s used for logging)</span>
<span class="sd">    </span>
<span class="sd">    LEARNING CONNECTIONS:</span>
<span class="sd">    - This is the foundation of all neural network training</span>
<span class="sd">    - PyTorch&#39;s optimizer.step() does exactly this</span>
<span class="sd">    - The learning rate determines convergence speed</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1">### BEGIN SOLUTION</span>
    <span class="c1"># YOUR CODE HERE</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
    <span class="c1">### END SOLUTION</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="unit-test-gradient-descent-step">
<h3>üß™ Unit Test: Gradient Descent Step<a class="headerlink" href="#unit-test-gradient-descent-step" title="Permalink to this heading">#</a></h3>
<p>Let‚Äôs test your gradient descent implementation right away! This is the foundation of all optimization algorithms.</p>
<p><strong>This is a unit test</strong> - it tests one specific function (gradient_descent_step) in isolation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_gradient_descent_step</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test basic gradient descent parameter update&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üî¨ Unit Test: Gradient Descent Step...&quot;</span><span class="p">)</span>
    
    <span class="c1"># Test basic parameter update</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">w</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>  <span class="c1"># Positive gradient</span>
        
        <span class="n">original_value</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">gradient_descent_step</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
        <span class="n">new_value</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        
        <span class="n">expected_value</span> <span class="o">=</span> <span class="n">original_value</span> <span class="o">-</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="mf">0.5</span>  <span class="c1"># 2.0 - 0.05 = 1.95</span>
        <span class="k">assert</span> <span class="nb">abs</span><span class="p">(</span><span class="n">new_value</span> <span class="o">-</span> <span class="n">expected_value</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-6</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Expected </span><span class="si">{</span><span class="n">expected_value</span><span class="si">}</span><span class="s2">, got </span><span class="si">{</span><span class="n">new_value</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;‚úÖ Basic parameter update works&quot;</span><span class="p">)</span>
        
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚ùå Basic parameter update failed: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">raise</span>

    <span class="c1"># Test with negative gradient</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">w2</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">w2</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="o">-</span><span class="mf">0.2</span><span class="p">)</span>  <span class="c1"># Negative gradient</span>
        
        <span class="n">gradient_descent_step</span><span class="p">(</span><span class="n">w2</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
        <span class="n">expected_value2</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.2</span><span class="p">)</span>  <span class="c1"># 1.0 + 0.02 = 1.02</span>
        <span class="k">assert</span> <span class="nb">abs</span><span class="p">(</span><span class="n">w2</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">-</span> <span class="n">expected_value2</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-6</span><span class="p">,</span> <span class="s2">&quot;Negative gradient test failed&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;‚úÖ Negative gradient handling works&quot;</span><span class="p">)</span>
        
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚ùå Negative gradient handling failed: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">raise</span>

    <span class="c1"># Test with no gradient (should not update)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">w3</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">w3</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">original_value3</span> <span class="o">=</span> <span class="n">w3</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        
        <span class="n">gradient_descent_step</span><span class="p">(</span><span class="n">w3</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">w3</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">==</span> <span class="n">original_value3</span><span class="p">,</span> <span class="s2">&quot;Parameter with no gradient should not update&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;‚úÖ No gradient case works&quot;</span><span class="p">)</span>
        
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚ùå No gradient case failed: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">raise</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üéØ Gradient descent step behavior:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Updates parameters in negative gradient direction&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Uses learning rate to control step size&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Skips updates when gradient is None&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üìà Progress: Gradient Descent Step ‚úì&quot;</span><span class="p">)</span>

<span class="c1"># Test function is called by auto-discovery system</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="step-2-sgd-with-momentum">
<h2>Step 2: SGD with Momentum<a class="headerlink" href="#step-2-sgd-with-momentum" title="Permalink to this heading">#</a></h2>
<section id="what-is-sgd">
<h3>What is SGD?<a class="headerlink" href="#what-is-sgd" title="Permalink to this heading">#</a></h3>
<p><strong>SGD (Stochastic Gradient Descent)</strong> is the fundamental optimization algorithm:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Œ∏_{t+1} = Œ∏_t - Œ± ‚àáL(Œ∏_t)
</pre></div>
</div>
</section>
<section id="the-problem-with-vanilla-sgd">
<h3>The Problem with Vanilla SGD<a class="headerlink" href="#the-problem-with-vanilla-sgd" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Slow convergence</strong>: Especially in narrow valleys</p></li>
<li><p><strong>Oscillation</strong>: Bounces around without making progress</p></li>
<li><p><strong>Poor conditioning</strong>: Struggles with ill-conditioned problems</p></li>
</ul>
</section>
<section id="the-solution-momentum">
<h3>The Solution: Momentum<a class="headerlink" href="#the-solution-momentum" title="Permalink to this heading">#</a></h3>
<p><strong>Momentum</strong> accumulates velocity to accelerate convergence:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>v_t = Œ≤ v_{t-1} + ‚àáL(Œ∏_t)
Œ∏_{t+1} = Œ∏_t - Œ± v_t
</pre></div>
</div>
<p>Where:</p>
<ul class="simple">
<li><p>v_t: Velocity (exponential moving average of gradients)</p></li>
<li><p>Œ≤: Momentum coefficient (typically 0.9)</p></li>
<li><p>Œ±: Learning rate</p></li>
</ul>
</section>
<section id="why-momentum-works">
<h3>Why Momentum Works<a class="headerlink" href="#why-momentum-works" title="Permalink to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Acceleration</strong>: Builds up speed in consistent directions</p></li>
<li><p><strong>Dampening</strong>: Reduces oscillations in inconsistent directions</p></li>
<li><p><strong>Memory</strong>: Remembers previous gradient directions</p></li>
<li><p><strong>Robustness</strong>: Less sensitive to noisy gradients</p></li>
</ol>
</section>
<section id="id1">
<h3>Visual Understanding<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Without momentum: ‚Üó‚Üô‚Üó‚Üô‚Üó‚Üô (oscillating)
With momentum:    ‚Üó‚Üí‚Üí‚Üí‚Üí‚Üí (smooth progress)
</pre></div>
</div>
</section>
<section id="id2">
<h3>Real-World Applications<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Image classification</strong>: Training ResNet, VGG</p></li>
<li><p><strong>Natural language</strong>: Training RNNs, early transformers</p></li>
<li><p><strong>Classic choice</strong>: Still used when Adam fails</p></li>
<li><p><strong>Large batch training</strong>: Often preferred over Adam</p></li>
</ul>
<p>Let‚Äôs implement SGD with momentum!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| export</span>
<span class="k">class</span> <span class="nc">SGD</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    SGD Optimizer with Momentum</span>
<span class="sd">    </span>
<span class="sd">    Implements stochastic gradient descent with momentum:</span>
<span class="sd">    v_t = momentum * v_{t-1} + gradient</span>
<span class="sd">    parameter = parameter - learning_rate * v_t</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameters</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Variable</span><span class="p">],</span> <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span> 
                 <span class="n">momentum</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">weight_decay</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize SGD optimizer.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            parameters: List of Variables to optimize</span>
<span class="sd">            learning_rate: Learning rate (default: 0.01)</span>
<span class="sd">            momentum: Momentum coefficient (default: 0.0)</span>
<span class="sd">            weight_decay: L2 regularization coefficient (default: 0.0)</span>
<span class="sd">        </span>
<span class="sd">        TODO: Implement SGD optimizer initialization.</span>
<span class="sd">        </span>
<span class="sd">        APPROACH:</span>
<span class="sd">        1. Store parameters and hyperparameters</span>
<span class="sd">        2. Initialize momentum buffers for each parameter</span>
<span class="sd">        3. Set up state tracking for optimization</span>
<span class="sd">        4. Prepare for step() and zero_grad() methods</span>
<span class="sd">        </span>
<span class="sd">        EXAMPLE:</span>
<span class="sd">        ```python</span>
<span class="sd">        # Create optimizer</span>
<span class="sd">        optimizer = SGD([w1, w2, b1, b2], learning_rate=0.01, momentum=0.9)</span>
<span class="sd">        </span>
<span class="sd">        # In training loop:</span>
<span class="sd">        optimizer.zero_grad()</span>
<span class="sd">        loss.backward()</span>
<span class="sd">        optimizer.step()</span>
<span class="sd">        ```</span>
<span class="sd">        </span>
<span class="sd">        HINTS:</span>
<span class="sd">        - Store parameters as a list</span>
<span class="sd">        - Initialize momentum buffers as empty dict</span>
<span class="sd">        - Use parameter id() as key for momentum tracking</span>
<span class="sd">        - Momentum buffers will be created lazily in step()</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1">### BEGIN SOLUTION</span>
    <span class="c1"># YOUR CODE HERE</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
        <span class="c1">### END SOLUTION</span>
    
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Perform one optimization step.</span>
<span class="sd">        </span>
<span class="sd">        TODO: Implement SGD parameter update with momentum.</span>
<span class="sd">        </span>
<span class="sd">        APPROACH:</span>
<span class="sd">        1. Iterate through all parameters</span>
<span class="sd">        2. For each parameter with gradient:</span>
<span class="sd">           a. Get current gradient</span>
<span class="sd">           b. Apply weight decay if specified</span>
<span class="sd">           c. Update momentum buffer (or create if first time)</span>
<span class="sd">           d. Update parameter using momentum</span>
<span class="sd">        3. Increment step count</span>
<span class="sd">        </span>
<span class="sd">        MATHEMATICAL FORMULATION:</span>
<span class="sd">        - If weight_decay &gt; 0: gradient = gradient + weight_decay * parameter</span>
<span class="sd">        - momentum_buffer = momentum * momentum_buffer + gradient</span>
<span class="sd">        - parameter = parameter - learning_rate * momentum_buffer</span>
<span class="sd">        </span>
<span class="sd">        IMPLEMENTATION HINTS:</span>
<span class="sd">        - Use id(param) as key for momentum buffers</span>
<span class="sd">        - Initialize buffer with zeros if not exists</span>
<span class="sd">        - Handle case where momentum = 0 (no momentum)</span>
<span class="sd">        - Update parameter.data with new Tensor</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1">### BEGIN SOLUTION</span>
    <span class="c1"># YOUR CODE HERE</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
        <span class="c1">### END SOLUTION</span>
    
    <span class="k">def</span> <span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Zero out gradients for all parameters.</span>
<span class="sd">        </span>
<span class="sd">        TODO: Implement gradient zeroing.</span>
<span class="sd">        </span>
<span class="sd">        APPROACH:</span>
<span class="sd">        1. Iterate through all parameters</span>
<span class="sd">        2. Set gradient to None for each parameter</span>
<span class="sd">        3. This prepares for next backward pass</span>
<span class="sd">        </span>
<span class="sd">        IMPLEMENTATION HINTS:</span>
<span class="sd">        - Simply set param.grad = None</span>
<span class="sd">        - This is called before loss.backward()</span>
<span class="sd">        - Essential for proper gradient accumulation</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1">### BEGIN SOLUTION</span>
    <span class="c1"># YOUR CODE HERE</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
        <span class="c1">### END SOLUTION</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="unit-test-sgd-optimizer">
<h3>üß™ Unit Test: SGD Optimizer<a class="headerlink" href="#unit-test-sgd-optimizer" title="Permalink to this heading">#</a></h3>
<p>Let‚Äôs test your SGD optimizer implementation! This optimizer adds momentum to gradient descent for better convergence.</p>
<p><strong>This is a unit test</strong> - it tests one specific class (SGD) in isolation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_sgd_optimizer</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test SGD optimizer implementation&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üî¨ Unit Test: SGD Optimizer...&quot;</span><span class="p">)</span>
    
    <span class="c1"># Create test parameters</span>
    <span class="n">w1</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">w2</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="c1"># Create optimizer</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">([</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
    
    <span class="c1"># Test zero_grad</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">w1</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
        <span class="n">w2</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>
        <span class="n">b</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="mf">0.05</span><span class="p">)</span>
        
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        
        <span class="k">assert</span> <span class="n">w1</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Gradient should be None after zero_grad&quot;</span>
        <span class="k">assert</span> <span class="n">w2</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Gradient should be None after zero_grad&quot;</span>
        <span class="k">assert</span> <span class="n">b</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Gradient should be None after zero_grad&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;‚úÖ zero_grad() works correctly&quot;</span><span class="p">)</span>
        
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚ùå zero_grad() failed: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">raise</span>
    
    <span class="c1"># Test step with gradients</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">w1</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
        <span class="n">w2</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>
        <span class="n">b</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="mf">0.05</span><span class="p">)</span>
        
        <span class="c1"># First step (no momentum yet)</span>
        <span class="n">original_w1</span> <span class="o">=</span> <span class="n">w1</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">original_w2</span> <span class="o">=</span> <span class="n">w2</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">original_b</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        
        <span class="c1"># Check parameter updates</span>
        <span class="n">expected_w1</span> <span class="o">=</span> <span class="n">original_w1</span> <span class="o">-</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="mf">0.1</span>  <span class="c1"># 1.0 - 0.01 = 0.99</span>
        <span class="n">expected_w2</span> <span class="o">=</span> <span class="n">original_w2</span> <span class="o">-</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="mf">0.2</span>  <span class="c1"># 2.0 - 0.02 = 1.98</span>
        <span class="n">expected_b</span> <span class="o">=</span> <span class="n">original_b</span> <span class="o">-</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="mf">0.05</span>   <span class="c1"># 0.5 - 0.005 = 0.495</span>
        
        <span class="k">assert</span> <span class="nb">abs</span><span class="p">(</span><span class="n">w1</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">-</span> <span class="n">expected_w1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-6</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;w1 update failed: expected </span><span class="si">{</span><span class="n">expected_w1</span><span class="si">}</span><span class="s2">, got </span><span class="si">{</span><span class="n">w1</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">assert</span> <span class="nb">abs</span><span class="p">(</span><span class="n">w2</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">-</span> <span class="n">expected_w2</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-6</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;w2 update failed: expected </span><span class="si">{</span><span class="n">expected_w2</span><span class="si">}</span><span class="s2">, got </span><span class="si">{</span><span class="n">w2</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">assert</span> <span class="nb">abs</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">-</span> <span class="n">expected_b</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-6</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;b update failed: expected </span><span class="si">{</span><span class="n">expected_b</span><span class="si">}</span><span class="s2">, got </span><span class="si">{</span><span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;‚úÖ Parameter updates work correctly&quot;</span><span class="p">)</span>
        
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚ùå Parameter updates failed: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">raise</span>
    
    <span class="c1"># Test momentum buffers</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">momentum_buffers</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Should have 3 momentum buffers, got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">momentum_buffers</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">assert</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">step_count</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Step count should be 1, got </span><span class="si">{</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step_count</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;‚úÖ Momentum buffers created correctly&quot;</span><span class="p">)</span>
        
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚ùå Momentum buffers failed: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">raise</span>
    
    <span class="c1"># Test step counting</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">w1</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
        <span class="n">w2</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>
        <span class="n">b</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="mf">0.05</span><span class="p">)</span>
        
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        
        <span class="k">assert</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">step_count</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Step count should be 2, got </span><span class="si">{</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step_count</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;‚úÖ Step counting works correctly&quot;</span><span class="p">)</span>
        
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚ùå Step counting failed: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">raise</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üéØ SGD optimizer behavior:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Maintains momentum buffers for accelerated updates&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Tracks step count for learning rate scheduling&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Supports weight decay for regularization&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üìà Progress: SGD Optimizer ‚úì&quot;</span><span class="p">)</span>

<span class="c1"># Run the test</span>
<span class="n">test_sgd_optimizer</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="step-3-adam-adaptive-learning-rates">
<h2>Step 3: Adam - Adaptive Learning Rates<a class="headerlink" href="#step-3-adam-adaptive-learning-rates" title="Permalink to this heading">#</a></h2>
<section id="what-is-adam">
<h3>What is Adam?<a class="headerlink" href="#what-is-adam" title="Permalink to this heading">#</a></h3>
<p><strong>Adam (Adaptive Moment Estimation)</strong> is the most popular optimizer in deep learning:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>m_t = Œ≤‚ÇÅ m_{t-1} + (1 - Œ≤‚ÇÅ) ‚àáL(Œ∏_t)        # First moment (momentum)
v_t = Œ≤‚ÇÇ v_{t-1} + (1 - Œ≤‚ÇÇ) (‚àáL(Œ∏_t))¬≤     # Second moment (variance)
mÃÇ_t = m_t / (1 - Œ≤‚ÇÅ·µó)                      # Bias correction
vÃÇ_t = v_t / (1 - Œ≤‚ÇÇ·µó)                      # Bias correction
Œ∏_{t+1} = Œ∏_t - Œ± mÃÇ_t / (‚àövÃÇ_t + Œµ)        # Parameter update
</pre></div>
</div>
</section>
<section id="why-adam-is-revolutionary">
<h3>Why Adam is Revolutionary<a class="headerlink" href="#why-adam-is-revolutionary" title="Permalink to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Adaptive learning rates</strong>: Different learning rate for each parameter</p></li>
<li><p><strong>Momentum</strong>: Accelerates convergence like SGD</p></li>
<li><p><strong>Variance adaptation</strong>: Scales updates based on gradient variance</p></li>
<li><p><strong>Bias correction</strong>: Handles initialization bias</p></li>
<li><p><strong>Robust</strong>: Works well with minimal hyperparameter tuning</p></li>
</ol>
</section>
<section id="the-three-key-ideas">
<h3>The Three Key Ideas<a class="headerlink" href="#the-three-key-ideas" title="Permalink to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>First moment (m_t)</strong>: Exponential moving average of gradients (momentum)</p></li>
<li><p><strong>Second moment (v_t)</strong>: Exponential moving average of squared gradients (variance)</p></li>
<li><p><strong>Adaptive scaling</strong>: Large gradients ‚Üí small updates, small gradients ‚Üí large updates</p></li>
</ol>
</section>
<section id="id3">
<h3>Visual Understanding<a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Parameter with large gradients: /\/\/\/\ ‚Üí smooth updates
Parameter with small gradients: ______ ‚Üí amplified updates
</pre></div>
</div>
</section>
<section id="id4">
<h3>Real-World Applications<a class="headerlink" href="#id4" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Deep learning</strong>: Default optimizer for most neural networks</p></li>
<li><p><strong>Computer vision</strong>: Training CNNs, ResNets, Vision Transformers</p></li>
<li><p><strong>Natural language</strong>: Training BERT, GPT, T5</p></li>
<li><p><strong>Transformers</strong>: Essential for attention-based models</p></li>
</ul>
<p>Let‚Äôs implement Adam optimizer!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| export</span>
<span class="k">class</span> <span class="nc">Adam</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Adam Optimizer</span>
<span class="sd">    </span>
<span class="sd">    Implements Adam algorithm with adaptive learning rates:</span>
<span class="sd">    - First moment: exponential moving average of gradients</span>
<span class="sd">    - Second moment: exponential moving average of squared gradients</span>
<span class="sd">    - Bias correction: accounts for initialization bias</span>
<span class="sd">    - Adaptive updates: different learning rate per parameter</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parameters</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Variable</span><span class="p">],</span> <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span>
                 <span class="n">beta1</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">,</span> <span class="n">beta2</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.999</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">,</span>
                 <span class="n">weight_decay</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize Adam optimizer.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            parameters: List of Variables to optimize</span>
<span class="sd">            learning_rate: Learning rate (default: 0.001)</span>
<span class="sd">            beta1: Exponential decay rate for first moment (default: 0.9)</span>
<span class="sd">            beta2: Exponential decay rate for second moment (default: 0.999)</span>
<span class="sd">            epsilon: Small constant for numerical stability (default: 1e-8)</span>
<span class="sd">            weight_decay: L2 regularization coefficient (default: 0.0)</span>
<span class="sd">        </span>
<span class="sd">        TODO: Implement Adam optimizer initialization.</span>
<span class="sd">        </span>
<span class="sd">        APPROACH:</span>
<span class="sd">        1. Store parameters and hyperparameters</span>
<span class="sd">        2. Initialize first moment buffers (m_t)</span>
<span class="sd">        3. Initialize second moment buffers (v_t)</span>
<span class="sd">        4. Set up step counter for bias correction</span>
<span class="sd">        </span>
<span class="sd">        EXAMPLE:</span>
<span class="sd">        ```python</span>
<span class="sd">        # Create Adam optimizer</span>
<span class="sd">        optimizer = Adam([w1, w2, b1, b2], learning_rate=0.001)</span>
<span class="sd">        </span>
<span class="sd">        # In training loop:</span>
<span class="sd">        optimizer.zero_grad()</span>
<span class="sd">        loss.backward()</span>
<span class="sd">        optimizer.step()</span>
<span class="sd">        ```</span>
<span class="sd">        </span>
<span class="sd">        HINTS:</span>
<span class="sd">        - Store all hyperparameters</span>
<span class="sd">        - Initialize moment buffers as empty dicts</span>
<span class="sd">        - Use parameter id() as key for tracking</span>
<span class="sd">        - Buffers will be created lazily in step()</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1">### BEGIN SOLUTION</span>
    <span class="c1"># YOUR CODE HERE</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
        <span class="c1">### END SOLUTION</span>
    
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Perform one optimization step using Adam algorithm.</span>
<span class="sd">        </span>
<span class="sd">        TODO: Implement Adam parameter update.</span>
<span class="sd">        </span>
<span class="sd">        APPROACH:</span>
<span class="sd">        1. Increment step count</span>
<span class="sd">        2. For each parameter with gradient:</span>
<span class="sd">           a. Get current gradient</span>
<span class="sd">           b. Apply weight decay if specified</span>
<span class="sd">           c. Update first moment (momentum)</span>
<span class="sd">           d. Update second moment (variance)</span>
<span class="sd">           e. Apply bias correction</span>
<span class="sd">           f. Update parameter with adaptive learning rate</span>
<span class="sd">        </span>
<span class="sd">        MATHEMATICAL FORMULATION:</span>
<span class="sd">        - m_t = beta1 * m_{t-1} + (1 - beta1) * gradient</span>
<span class="sd">        - v_t = beta2 * v_{t-1} + (1 - beta2) * gradient^2</span>
<span class="sd">        - m_hat = m_t / (1 - beta1^t)</span>
<span class="sd">        - v_hat = v_t / (1 - beta2^t)</span>
<span class="sd">        - parameter = parameter - learning_rate * m_hat / (sqrt(v_hat) + epsilon)</span>
<span class="sd">        </span>
<span class="sd">        IMPLEMENTATION HINTS:</span>
<span class="sd">        - Use id(param) as key for moment buffers</span>
<span class="sd">        - Initialize buffers with zeros if not exists</span>
<span class="sd">        - Use np.sqrt() for square root</span>
<span class="sd">        - Handle numerical stability with epsilon</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1">### BEGIN SOLUTION</span>
    <span class="c1"># YOUR CODE HERE</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
        <span class="c1">### END SOLUTION</span>
    
    <span class="k">def</span> <span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Zero out gradients for all parameters.</span>
<span class="sd">        </span>
<span class="sd">        TODO: Implement gradient zeroing (same as SGD).</span>
<span class="sd">        </span>
<span class="sd">        IMPLEMENTATION HINTS:</span>
<span class="sd">        - Set param.grad = None for all parameters</span>
<span class="sd">        - This is identical to SGD implementation</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1">### BEGIN SOLUTION</span>
    <span class="c1"># YOUR CODE HERE</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
        <span class="c1">### END SOLUTION</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="test-your-adam-implementation">
<h3>üß™ Test Your Adam Implementation<a class="headerlink" href="#test-your-adam-implementation" title="Permalink to this heading">#</a></h3>
<p>Let‚Äôs test the Adam optimizer:</p>
</section>
<section id="unit-test-adam-optimizer">
<h3>üß™ Unit Test: Adam Optimizer<a class="headerlink" href="#unit-test-adam-optimizer" title="Permalink to this heading">#</a></h3>
<p>Let‚Äôs test your Adam optimizer implementation! This is a state-of-the-art adaptive optimization algorithm.</p>
<p><strong>This is a unit test</strong> - it tests one specific class (Adam) in isolation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_adam_optimizer</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test Adam optimizer implementation&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üî¨ Unit Test: Adam Optimizer...&quot;</span><span class="p">)</span>
    
    <span class="c1"># Create test parameters</span>
    <span class="n">w1</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">w2</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="c1"># Create optimizer</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">([</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">)</span>
    
    <span class="c1"># Test zero_grad</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">w1</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
        <span class="n">w2</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>
        <span class="n">b</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="mf">0.05</span><span class="p">)</span>
        
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        
        <span class="k">assert</span> <span class="n">w1</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Gradient should be None after zero_grad&quot;</span>
        <span class="k">assert</span> <span class="n">w2</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Gradient should be None after zero_grad&quot;</span>
        <span class="k">assert</span> <span class="n">b</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Gradient should be None after zero_grad&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;‚úÖ zero_grad() works correctly&quot;</span><span class="p">)</span>
        
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚ùå zero_grad() failed: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">raise</span>
    
    <span class="c1"># Test step with gradients</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">w1</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
        <span class="n">w2</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>
        <span class="n">b</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="mf">0.05</span><span class="p">)</span>
        
        <span class="c1"># First step</span>
        <span class="n">original_w1</span> <span class="o">=</span> <span class="n">w1</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">original_w2</span> <span class="o">=</span> <span class="n">w2</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">original_b</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        
        <span class="c1"># Check that parameters were updated (Adam uses adaptive learning rates)</span>
        <span class="k">assert</span> <span class="n">w1</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">!=</span> <span class="n">original_w1</span><span class="p">,</span> <span class="s2">&quot;w1 should have been updated&quot;</span>
        <span class="k">assert</span> <span class="n">w2</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">!=</span> <span class="n">original_w2</span><span class="p">,</span> <span class="s2">&quot;w2 should have been updated&quot;</span>
        <span class="k">assert</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">!=</span> <span class="n">original_b</span><span class="p">,</span> <span class="s2">&quot;b should have been updated&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;‚úÖ Parameter updates work correctly&quot;</span><span class="p">)</span>
        
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚ùå Parameter updates failed: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">raise</span>
    
    <span class="c1"># Test moment buffers</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">first_moment</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Should have 3 first moment buffers, got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">first_moment</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">second_moment</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Should have 3 second moment buffers, got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">second_moment</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;‚úÖ Moment buffers created correctly&quot;</span><span class="p">)</span>
        
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚ùå Moment buffers failed: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">raise</span>
    
    <span class="c1"># Test step counting and bias correction</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">step_count</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Step count should be 1, got </span><span class="si">{</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step_count</span><span class="si">}</span><span class="s2">&quot;</span>
        
        <span class="c1"># Take another step</span>
        <span class="n">w1</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
        <span class="n">w2</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>
        <span class="n">b</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="mf">0.05</span><span class="p">)</span>
        
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        
        <span class="k">assert</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">step_count</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Step count should be 2, got </span><span class="si">{</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step_count</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;‚úÖ Step counting and bias correction work correctly&quot;</span><span class="p">)</span>
        
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚ùå Step counting and bias correction failed: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">raise</span>
    
    <span class="c1"># Test adaptive learning rates</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="c1"># Adam should have different effective learning rates for different parameters</span>
        <span class="c1"># This is tested implicitly by the parameter updates above</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;‚úÖ Adaptive learning rates work correctly&quot;</span><span class="p">)</span>
        
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚ùå Adaptive learning rates failed: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">raise</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üéØ Adam optimizer behavior:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Maintains first and second moment estimates&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Applies bias correction for early training&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Uses adaptive learning rates per parameter&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Combines benefits of momentum and RMSprop&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üìà Progress: Adam Optimizer ‚úì&quot;</span><span class="p">)</span>

<span class="c1"># Run the test</span>
<span class="n">test_adam_optimizer</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="step-4-learning-rate-scheduling">
<h2>Step 4: Learning Rate Scheduling<a class="headerlink" href="#step-4-learning-rate-scheduling" title="Permalink to this heading">#</a></h2>
<section id="what-is-learning-rate-scheduling">
<h3>What is Learning Rate Scheduling?<a class="headerlink" href="#what-is-learning-rate-scheduling" title="Permalink to this heading">#</a></h3>
<p><strong>Learning rate scheduling</strong> adjusts the learning rate during training:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Initial</span><span class="p">:</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">After</span> <span class="mi">10</span> <span class="n">epochs</span><span class="p">:</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">After</span> <span class="mi">20</span> <span class="n">epochs</span><span class="p">:</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>
</pre></div>
</div>
</section>
<section id="why-scheduling-matters">
<h3>Why Scheduling Matters<a class="headerlink" href="#why-scheduling-matters" title="Permalink to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Fine-tuning</strong>: Start with large steps, then refine with small steps</p></li>
<li><p><strong>Convergence</strong>: Prevents overshooting near optimum</p></li>
<li><p><strong>Stability</strong>: Reduces oscillations in later training</p></li>
<li><p><strong>Performance</strong>: Often improves final accuracy</p></li>
</ol>
</section>
<section id="common-scheduling-strategies">
<h3>Common Scheduling Strategies<a class="headerlink" href="#common-scheduling-strategies" title="Permalink to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Step decay</strong>: Reduce by factor every N epochs</p></li>
<li><p><strong>Exponential decay</strong>: Gradual exponential reduction</p></li>
<li><p><strong>Cosine annealing</strong>: Smooth cosine curve reduction</p></li>
<li><p><strong>Warm-up</strong>: Start small, increase, then decrease</p></li>
</ol>
</section>
<section id="id5">
<h3>Visual Understanding<a class="headerlink" href="#id5" title="Permalink to this heading">#</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Step decay:     ----‚Üì----‚Üì----‚Üì
Exponential:    \\\\\\\\\\\\\\
Cosine:         ‚à©‚à©‚à©‚à©‚à©‚à©‚à©‚à©‚à©‚à©‚à©‚à©‚à©
</pre></div>
</div>
</section>
<section id="id6">
<h3>Real-World Applications<a class="headerlink" href="#id6" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>ImageNet training</strong>: Essential for achieving state-of-the-art results</p></li>
<li><p><strong>Language models</strong>: Critical for training large transformers</p></li>
<li><p><strong>Fine-tuning</strong>: Prevents catastrophic forgetting</p></li>
<li><p><strong>Transfer learning</strong>: Adapts pre-trained models</p></li>
</ul>
<p>Let‚Äôs implement step learning rate scheduling!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#| export</span>
<span class="k">class</span> <span class="nc">StepLR</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Step Learning Rate Scheduler</span>
<span class="sd">    </span>
<span class="sd">    Decays learning rate by gamma every step_size epochs:</span>
<span class="sd">    learning_rate = initial_lr * (gamma ^ (epoch // step_size))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">SGD</span><span class="p">,</span> <span class="n">Adam</span><span class="p">],</span> <span class="n">step_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize step learning rate scheduler.</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            optimizer: Optimizer to schedule</span>
<span class="sd">            step_size: Number of epochs between decreases</span>
<span class="sd">            gamma: Multiplicative factor for learning rate decay</span>
<span class="sd">        </span>
<span class="sd">        TODO: Implement learning rate scheduler initialization.</span>
<span class="sd">        </span>
<span class="sd">        APPROACH:</span>
<span class="sd">        1. Store optimizer reference</span>
<span class="sd">        2. Store scheduling parameters</span>
<span class="sd">        3. Save initial learning rate</span>
<span class="sd">        4. Initialize step counter</span>
<span class="sd">        </span>
<span class="sd">        EXAMPLE:</span>
<span class="sd">        ```python</span>
<span class="sd">        optimizer = SGD([w1, w2], learning_rate=0.1)</span>
<span class="sd">        scheduler = StepLR(optimizer, step_size=10, gamma=0.1)</span>
<span class="sd">        </span>
<span class="sd">        # In training loop:</span>
<span class="sd">        for epoch in range(100):</span>
<span class="sd">            train_one_epoch()</span>
<span class="sd">            scheduler.step()  # Update learning rate</span>
<span class="sd">        ```</span>
<span class="sd">        </span>
<span class="sd">        HINTS:</span>
<span class="sd">        - Store optimizer reference</span>
<span class="sd">        - Save initial learning rate from optimizer</span>
<span class="sd">        - Initialize step counter to 0</span>
<span class="sd">        - gamma is the decay factor (0.1 = 10x reduction)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1">### BEGIN SOLUTION</span>
    <span class="c1"># YOUR CODE HERE</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
        <span class="c1">### END SOLUTION</span>
    
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Update learning rate based on current step.</span>
<span class="sd">        </span>
<span class="sd">        TODO: Implement learning rate update.</span>
<span class="sd">        </span>
<span class="sd">        APPROACH:</span>
<span class="sd">        1. Increment step counter</span>
<span class="sd">        2. Calculate new learning rate using step decay formula</span>
<span class="sd">        3. Update optimizer&#39;s learning rate</span>
<span class="sd">        </span>
<span class="sd">        MATHEMATICAL FORMULATION:</span>
<span class="sd">        new_lr = initial_lr * (gamma ^ ((step_count - 1) // step_size))</span>
<span class="sd">        </span>
<span class="sd">        IMPLEMENTATION HINTS:</span>
<span class="sd">        - Use // for integer division</span>
<span class="sd">        - Use ** for exponentiation</span>
<span class="sd">        - Update optimizer.learning_rate directly</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1">### BEGIN SOLUTION</span>
    <span class="c1"># YOUR CODE HERE</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
        <span class="c1">### END SOLUTION</span>
    
    <span class="k">def</span> <span class="nf">get_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get current learning rate.</span>
<span class="sd">        </span>
<span class="sd">        TODO: Return current learning rate.</span>
<span class="sd">        </span>
<span class="sd">        IMPLEMENTATION HINTS:</span>
<span class="sd">        - Return optimizer.learning_rate</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1">### BEGIN SOLUTION</span>
    <span class="c1"># YOUR CODE HERE</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
        <span class="c1">### END SOLUTION</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="unit-test-step-learning-rate-scheduler">
<h3>üß™ Unit Test: Step Learning Rate Scheduler<a class="headerlink" href="#unit-test-step-learning-rate-scheduler" title="Permalink to this heading">#</a></h3>
<p>Let‚Äôs test your step learning rate scheduler implementation! This scheduler reduces learning rate at regular intervals.</p>
<p><strong>This is a unit test</strong> - it tests one specific class (StepLR) in isolation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_step_scheduler</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test StepLR scheduler implementation&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üî¨ Unit Test: Step Learning Rate Scheduler...&quot;</span><span class="p">)</span>
    
    <span class="c1"># Create test parameters and optimizer</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">([</span><span class="n">w</span><span class="p">],</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
    
    <span class="c1"># Test scheduler initialization</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">scheduler</span> <span class="o">=</span> <span class="n">StepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
        
        <span class="c1"># Test initial learning rate</span>
        <span class="k">assert</span> <span class="n">scheduler</span><span class="o">.</span><span class="n">get_lr</span><span class="p">()</span> <span class="o">==</span> <span class="mf">0.1</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Initial learning rate should be 0.1, got </span><span class="si">{</span><span class="n">scheduler</span><span class="o">.</span><span class="n">get_lr</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;‚úÖ Initial learning rate is correct&quot;</span><span class="p">)</span>
        
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚ùå Initial learning rate failed: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">raise</span>
    
    <span class="c1"># Test step-based decay</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="c1"># Steps 1-10: no decay (decay happens after step 10)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
            <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        
        <span class="k">assert</span> <span class="n">scheduler</span><span class="o">.</span><span class="n">get_lr</span><span class="p">()</span> <span class="o">==</span> <span class="mf">0.1</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Learning rate should still be 0.1 after 10 steps, got </span><span class="si">{</span><span class="n">scheduler</span><span class="o">.</span><span class="n">get_lr</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
        
        <span class="c1"># Step 11: decay should occur</span>
        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">expected_lr</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="mf">0.1</span>  <span class="c1"># 0.01</span>
        <span class="k">assert</span> <span class="nb">abs</span><span class="p">(</span><span class="n">scheduler</span><span class="o">.</span><span class="n">get_lr</span><span class="p">()</span> <span class="o">-</span> <span class="n">expected_lr</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-6</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Learning rate should be </span><span class="si">{</span><span class="n">expected_lr</span><span class="si">}</span><span class="s2"> after 11 steps, got </span><span class="si">{</span><span class="n">scheduler</span><span class="o">.</span><span class="n">get_lr</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;‚úÖ Step-based decay works correctly&quot;</span><span class="p">)</span>
        
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚ùå Step-based decay failed: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">raise</span>
    
    <span class="c1"># Test multiple decay levels</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="c1"># Steps 12-20: should stay at 0.01</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">9</span><span class="p">):</span>
            <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        
        <span class="k">assert</span> <span class="nb">abs</span><span class="p">(</span><span class="n">scheduler</span><span class="o">.</span><span class="n">get_lr</span><span class="p">()</span> <span class="o">-</span> <span class="mf">0.01</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-6</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Learning rate should be 0.01 after 20 steps, got </span><span class="si">{</span><span class="n">scheduler</span><span class="o">.</span><span class="n">get_lr</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
        
        <span class="c1"># Step 21: another decay</span>
        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">expected_lr</span> <span class="o">=</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="mf">0.1</span>  <span class="c1"># 0.001</span>
        <span class="k">assert</span> <span class="nb">abs</span><span class="p">(</span><span class="n">scheduler</span><span class="o">.</span><span class="n">get_lr</span><span class="p">()</span> <span class="o">-</span> <span class="n">expected_lr</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-6</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Learning rate should be </span><span class="si">{</span><span class="n">expected_lr</span><span class="si">}</span><span class="s2"> after 21 steps, got </span><span class="si">{</span><span class="n">scheduler</span><span class="o">.</span><span class="n">get_lr</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;‚úÖ Multiple decay levels work correctly&quot;</span><span class="p">)</span>
        
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚ùå Multiple decay levels failed: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">raise</span>
    
    <span class="c1"># Test with different optimizer</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">w2</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">adam_optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">([</span><span class="n">w2</span><span class="p">],</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
        <span class="n">adam_scheduler</span> <span class="o">=</span> <span class="n">StepLR</span><span class="p">(</span><span class="n">adam_optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
        
        <span class="c1"># Test initial learning rate</span>
        <span class="k">assert</span> <span class="n">adam_scheduler</span><span class="o">.</span><span class="n">get_lr</span><span class="p">()</span> <span class="o">==</span> <span class="mf">0.001</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Initial Adam learning rate should be 0.001, got </span><span class="si">{</span><span class="n">adam_scheduler</span><span class="o">.</span><span class="n">get_lr</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
        
        <span class="c1"># Test decay after 5 steps</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
            <span class="n">adam_scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        
        <span class="c1"># Learning rate should still be 0.001 after 5 steps</span>
        <span class="k">assert</span> <span class="n">adam_scheduler</span><span class="o">.</span><span class="n">get_lr</span><span class="p">()</span> <span class="o">==</span> <span class="mf">0.001</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Adam learning rate should still be 0.001 after 5 steps, got </span><span class="si">{</span><span class="n">adam_scheduler</span><span class="o">.</span><span class="n">get_lr</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
        
        <span class="c1"># Step 6: decay should occur</span>
        <span class="n">adam_scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">expected_lr</span> <span class="o">=</span> <span class="mf">0.001</span> <span class="o">*</span> <span class="mf">0.5</span>  <span class="c1"># 0.0005</span>
        <span class="k">assert</span> <span class="nb">abs</span><span class="p">(</span><span class="n">adam_scheduler</span><span class="o">.</span><span class="n">get_lr</span><span class="p">()</span> <span class="o">-</span> <span class="n">expected_lr</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-6</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Adam learning rate should be </span><span class="si">{</span><span class="n">expected_lr</span><span class="si">}</span><span class="s2"> after 6 steps, got </span><span class="si">{</span><span class="n">adam_scheduler</span><span class="o">.</span><span class="n">get_lr</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;‚úÖ Works with different optimizers&quot;</span><span class="p">)</span>
        
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚ùå Different optimizers failed: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">raise</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üéØ Step learning rate scheduler behavior:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Reduces learning rate at regular intervals&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Multiplies current rate by gamma factor&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Works with any optimizer (SGD, Adam, etc.)&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üìà Progress: Step Learning Rate Scheduler ‚úì&quot;</span><span class="p">)</span>

<span class="c1"># Run the test</span>
<span class="n">test_step_scheduler</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="step-5-integration-complete-training-example">
<h2>Step 5: Integration - Complete Training Example<a class="headerlink" href="#step-5-integration-complete-training-example" title="Permalink to this heading">#</a></h2>
<section id="putting-it-all-together">
<h3>Putting It All Together<a class="headerlink" href="#putting-it-all-together" title="Permalink to this heading">#</a></h3>
<p>Let‚Äôs see how optimizers enable complete neural network training:</p>
<ol class="arabic simple">
<li><p><strong>Forward pass</strong>: Compute predictions</p></li>
<li><p><strong>Loss computation</strong>: Compare with targets</p></li>
<li><p><strong>Backward pass</strong>: Compute gradients</p></li>
<li><p><strong>Optimizer step</strong>: Update parameters</p></li>
<li><p><strong>Learning rate scheduling</strong>: Adjust learning rate</p></li>
</ol>
</section>
<section id="the-modern-training-loop">
<h3>The Modern Training Loop<a class="headerlink" href="#the-modern-training-loop" title="Permalink to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Setup</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">StepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># Training loop</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
        <span class="c1"># Forward pass</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">batch</span><span class="o">.</span><span class="n">targets</span><span class="p">)</span>
        
        <span class="c1"># Backward pass</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    
    <span class="c1"># Update learning rate</span>
    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<p>Let‚Äôs implement a complete training example!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train_simple_model</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Complete training example using optimizers.</span>
<span class="sd">    </span>
<span class="sd">    TODO: Implement a complete training loop.</span>
<span class="sd">    </span>
<span class="sd">    APPROACH:</span>
<span class="sd">    1. Create a simple model (linear regression)</span>
<span class="sd">    2. Generate training data</span>
<span class="sd">    3. Set up optimizer and scheduler</span>
<span class="sd">    4. Train for several epochs</span>
<span class="sd">    5. Show convergence</span>
<span class="sd">    </span>
<span class="sd">    LEARNING OBJECTIVE:</span>
<span class="sd">    - See how optimizers enable real learning</span>
<span class="sd">    - Compare SGD vs Adam performance</span>
<span class="sd">    - Understand the complete training workflow</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1">### BEGIN SOLUTION</span>
    <span class="c1"># YOUR CODE HERE</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
    <span class="c1">### END SOLUTION</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="unit-test-complete-training-integration">
<h3>üß™ Unit Test: Complete Training Integration<a class="headerlink" href="#unit-test-complete-training-integration" title="Permalink to this heading">#</a></h3>
<p>Let‚Äôs test your complete training integration! This demonstrates optimizers working together in a realistic training scenario.</p>
<p><strong>This is a unit test</strong> - it tests the complete training workflow with optimizers in isolation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_training_integration</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Test complete training integration with optimizers&quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üî¨ Unit Test: Complete Training Integration...&quot;</span><span class="p">)</span>
    
    <span class="c1"># Test training with SGD and Adam</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">sgd_w</span><span class="p">,</span> <span class="n">sgd_b</span><span class="p">,</span> <span class="n">adam_w</span><span class="p">,</span> <span class="n">adam_b</span> <span class="o">=</span> <span class="n">train_simple_model</span><span class="p">()</span>
        
        <span class="c1"># Test SGD convergence</span>
        <span class="k">assert</span> <span class="nb">abs</span><span class="p">(</span><span class="n">sgd_w</span> <span class="o">-</span> <span class="mf">2.0</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">0.1</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;SGD should converge close to w=2.0, got </span><span class="si">{</span><span class="n">sgd_w</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">assert</span> <span class="nb">abs</span><span class="p">(</span><span class="n">sgd_b</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">0.1</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;SGD should converge close to b=1.0, got </span><span class="si">{</span><span class="n">sgd_b</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;‚úÖ SGD convergence works&quot;</span><span class="p">)</span>
        
        <span class="c1"># Test Adam convergence (may be different due to adaptive learning rates)</span>
        <span class="k">assert</span> <span class="nb">abs</span><span class="p">(</span><span class="n">adam_w</span> <span class="o">-</span> <span class="mf">2.0</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Adam should converge reasonably close to w=2.0, got </span><span class="si">{</span><span class="n">adam_w</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">assert</span> <span class="nb">abs</span><span class="p">(</span><span class="n">adam_b</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Adam should converge reasonably close to b=1.0, got </span><span class="si">{</span><span class="n">adam_b</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;‚úÖ Adam convergence works&quot;</span><span class="p">)</span>
        
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚ùå Training integration failed: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">raise</span>
    
    <span class="c1"># Test optimizer comparison</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="c1"># Both optimizers should achieve reasonable results</span>
        <span class="n">sgd_error</span> <span class="o">=</span> <span class="p">(</span><span class="n">sgd_w</span> <span class="o">-</span> <span class="mf">2.0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">sgd_b</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
        <span class="n">adam_error</span> <span class="o">=</span> <span class="p">(</span><span class="n">adam_w</span> <span class="o">-</span> <span class="mf">2.0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">adam_b</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
        
        <span class="c1"># Both should have low error (&lt; 0.1)</span>
        <span class="k">assert</span> <span class="n">sgd_error</span> <span class="o">&lt;</span> <span class="mf">0.1</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;SGD error should be &lt; 0.1, got </span><span class="si">{</span><span class="n">sgd_error</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">assert</span> <span class="n">adam_error</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Adam error should be &lt; 1.0, got </span><span class="si">{</span><span class="n">adam_error</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;‚úÖ Optimizer comparison works&quot;</span><span class="p">)</span>
        
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚ùå Optimizer comparison failed: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">raise</span>
    
    <span class="c1"># Test gradient flow</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="c1"># Create a simple test to verify gradients flow correctly</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        
        <span class="c1"># Set up simple gradients</span>
        <span class="n">w</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
        <span class="n">b</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="mf">0.05</span><span class="p">)</span>
        
        <span class="c1"># Test SGD step</span>
        <span class="n">sgd_optimizer</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">([</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
        <span class="n">original_w</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">original_b</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        
        <span class="n">sgd_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        
        <span class="c1"># Check updates</span>
        <span class="k">assert</span> <span class="n">w</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">!=</span> <span class="n">original_w</span><span class="p">,</span> <span class="s2">&quot;SGD should update w&quot;</span>
        <span class="k">assert</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">!=</span> <span class="n">original_b</span><span class="p">,</span> <span class="s2">&quot;SGD should update b&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;‚úÖ Gradient flow works correctly&quot;</span><span class="p">)</span>
        
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;‚ùå Gradient flow failed: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">raise</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üéØ Training integration behavior:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Optimizers successfully minimize loss functions&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   SGD and Adam both converge to target values&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Gradient computation and updates work correctly&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;   Ready for real neural network training&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;üìà Progress: Complete Training Integration ‚úì&quot;</span><span class="p">)</span>

<span class="c1"># Run the test</span>
<span class="n">test_training_integration</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="module-summary-optimization-mastery">
<h2>üéØ Module Summary: Optimization Mastery!<a class="headerlink" href="#module-summary-optimization-mastery" title="Permalink to this heading">#</a></h2>
<p>Congratulations! You‚Äôve successfully implemented the optimization algorithms that power all modern neural network training:</p>
<section id="what-you-ve-built">
<h3>‚úÖ What You‚Äôve Built<a class="headerlink" href="#what-you-ve-built" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Gradient Descent</strong>: The fundamental parameter update mechanism</p></li>
<li><p><strong>SGD with Momentum</strong>: Accelerated convergence with velocity accumulation</p></li>
<li><p><strong>Adam Optimizer</strong>: Adaptive learning rates with first and second moments</p></li>
<li><p><strong>Learning Rate Scheduling</strong>: Smart learning rate adjustment during training</p></li>
<li><p><strong>Complete Training Integration</strong>: End-to-end training workflow</p></li>
</ul>
</section>
<section id="key-learning-outcomes">
<h3>‚úÖ Key Learning Outcomes<a class="headerlink" href="#key-learning-outcomes" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Understanding</strong>: How optimizers use gradients to update parameters intelligently</p></li>
<li><p><strong>Implementation</strong>: Built SGD and Adam optimizers from mathematical foundations</p></li>
<li><p><strong>Mathematical mastery</strong>: Momentum, adaptive learning rates, bias correction</p></li>
<li><p><strong>Systems integration</strong>: Complete training loops with scheduling</p></li>
<li><p><strong>Real-world application</strong>: Modern deep learning training workflow</p></li>
</ul>
</section>
<section id="mathematical-foundations-mastered">
<h3>‚úÖ Mathematical Foundations Mastered<a class="headerlink" href="#mathematical-foundations-mastered" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Gradient Descent</strong>: Œ∏ = Œ∏ - Œ±‚àáL(Œ∏) for parameter updates</p></li>
<li><p><strong>Momentum</strong>: v_t = Œ≤v_{t-1} + ‚àáL(Œ∏) for acceleration</p></li>
<li><p><strong>Adam</strong>: Adaptive learning rates with exponential moving averages</p></li>
<li><p><strong>Learning Rate Scheduling</strong>: Strategic learning rate adjustment</p></li>
</ul>
</section>
<section id="professional-skills-developed">
<h3>‚úÖ Professional Skills Developed<a class="headerlink" href="#professional-skills-developed" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Algorithm implementation</strong>: Translating mathematical formulas into code</p></li>
<li><p><strong>State management</strong>: Tracking optimizer buffers and statistics</p></li>
<li><p><strong>Hyperparameter design</strong>: Understanding the impact of learning rate, momentum, etc.</p></li>
<li><p><strong>Training orchestration</strong>: Complete training loop design</p></li>
</ul>
</section>
<section id="ready-for-advanced-applications">
<h3>‚úÖ Ready for Advanced Applications<a class="headerlink" href="#ready-for-advanced-applications" title="Permalink to this heading">#</a></h3>
<p>Your optimizers now enable:</p>
<ul class="simple">
<li><p><strong>Deep Neural Networks</strong>: Effective training of complex architectures</p></li>
<li><p><strong>Computer Vision</strong>: Training CNNs, ResNets, Vision Transformers</p></li>
<li><p><strong>Natural Language Processing</strong>: Training transformers and language models</p></li>
<li><p><strong>Any ML Model</strong>: Gradient-based optimization for any differentiable system</p></li>
</ul>
</section>
<section id="connection-to-real-ml-systems">
<h3>üîó Connection to Real ML Systems<a class="headerlink" href="#connection-to-real-ml-systems" title="Permalink to this heading">#</a></h3>
<p>Your implementations mirror production systems:</p>
<ul class="simple">
<li><p><strong>PyTorch</strong>: <code class="docutils literal notranslate"><span class="pre">torch.optim.SGD()</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.optim.Adam()</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.optim.lr_scheduler.StepLR()</span></code></p></li>
<li><p><strong>TensorFlow</strong>: <code class="docutils literal notranslate"><span class="pre">tf.keras.optimizers.SGD()</span></code>, <code class="docutils literal notranslate"><span class="pre">tf.keras.optimizers.Adam()</span></code></p></li>
<li><p><strong>Industry Standard</strong>: Every major ML framework uses these exact algorithms</p></li>
</ul>
</section>
<section id="the-power-of-intelligent-optimization">
<h3>üéØ The Power of Intelligent Optimization<a class="headerlink" href="#the-power-of-intelligent-optimization" title="Permalink to this heading">#</a></h3>
<p>You‚Äôve unlocked the algorithms that made modern AI possible:</p>
<ul class="simple">
<li><p><strong>Scalability</strong>: Efficiently optimize millions of parameters</p></li>
<li><p><strong>Adaptability</strong>: Different learning rates for different parameters</p></li>
<li><p><strong>Robustness</strong>: Handle noisy gradients and ill-conditioned problems</p></li>
<li><p><strong>Universality</strong>: Work with any differentiable neural network</p></li>
</ul>
</section>
<section id="deep-learning-revolution">
<h3>üß† Deep Learning Revolution<a class="headerlink" href="#deep-learning-revolution" title="Permalink to this heading">#</a></h3>
<p>You now understand the optimization technology that powers:</p>
<ul class="simple">
<li><p><strong>ImageNet</strong>: Training state-of-the-art computer vision models</p></li>
<li><p><strong>Language Models</strong>: Training GPT, BERT, and other transformers</p></li>
<li><p><strong>Modern AI</strong>: Every breakthrough relies on these optimization algorithms</p></li>
<li><p><strong>Future Research</strong>: Your understanding enables you to develop new optimizers</p></li>
</ul>
</section>
<section id="what-s-next">
<h3>üöÄ What‚Äôs Next<a class="headerlink" href="#what-s-next" title="Permalink to this heading">#</a></h3>
<p>Your optimizers are the foundation for:</p>
<ul class="simple">
<li><p><strong>Training Module</strong>: Complete training loops with loss functions and metrics</p></li>
<li><p><strong>Advanced Optimizers</strong>: RMSprop, AdaGrad, learning rate warm-up</p></li>
<li><p><strong>Distributed Training</strong>: Multi-GPU optimization strategies</p></li>
<li><p><strong>Research</strong>: Experimenting with novel optimization algorithms</p></li>
</ul>
<p><strong>Next Module</strong>: Complete training systems that orchestrate your optimizers for real-world ML!</p>
<p>You‚Äôve built the intelligent algorithms that enable neural networks to learn. Now let‚Äôs use them to train systems that can solve complex real-world problems!</p>
</section>
</section>
<section id="module-testing">
<h2>üß™ Module Testing<a class="headerlink" href="#module-testing" title="Permalink to this heading">#</a></h2>
<p>Time to test your implementation! This section uses TinyTorch‚Äôs standardized testing framework to ensure your implementation works correctly.</p>
<p><strong>This testing section is locked</strong> - it provides consistent feedback across all modules and cannot be modified.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># =============================================================================</span>
<span class="c1"># STANDARDIZED MODULE TESTING - DO NOT MODIFY</span>
<span class="c1"># This cell is locked to ensure consistent testing across all TinyTorch modules</span>
<span class="c1"># =============================================================================</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">tito.tools.testing</span> <span class="kn">import</span> <span class="n">run_module_tests_auto</span>
    
    <span class="c1"># Automatically discover and run all tests in this module</span>
    <span class="n">success</span> <span class="o">=</span> <span class="n">run_module_tests_auto</span><span class="p">(</span><span class="s2">&quot;Optimizers&quot;</span><span class="p">)</span> 
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapters"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="07-autograd.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Autograd</p>
      </div>
    </a>
    <a class="right-next"
       href="09-training.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Training</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#where-this-code-lives-in-the-final-package">üì¶ Where This Code Lives in the Final Package</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-optimizers">What Are Optimizers?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-problem-how-to-update-parameters">The Problem: How to Update Parameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-solution-smart-optimization">The Solution: Smart Optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#real-world-impact">Real-World Impact</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-we-ll-build">What We‚Äôll Build</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-understanding-gradient-descent">Step 1: Understanding Gradient Descent</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-gradient-descent">What is Gradient Descent?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-gradient-descent-works">Why Gradient Descent Works</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-learning-rate-dilemma">The Learning Rate Dilemma</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visual-understanding">Visual Understanding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#real-world-applications">Real-World Applications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unit-test-gradient-descent-step">üß™ Unit Test: Gradient Descent Step</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-sgd-with-momentum">Step 2: SGD with Momentum</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-sgd">What is SGD?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-problem-with-vanilla-sgd">The Problem with Vanilla SGD</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-solution-momentum">The Solution: Momentum</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-momentum-works">Why Momentum Works</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Visual Understanding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Real-World Applications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unit-test-sgd-optimizer">üß™ Unit Test: SGD Optimizer</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-adam-adaptive-learning-rates">Step 3: Adam - Adaptive Learning Rates</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-adam">What is Adam?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-adam-is-revolutionary">Why Adam is Revolutionary</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-three-key-ideas">The Three Key Ideas</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Visual Understanding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Real-World Applications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#test-your-adam-implementation">üß™ Test Your Adam Implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unit-test-adam-optimizer">üß™ Unit Test: Adam Optimizer</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-learning-rate-scheduling">Step 4: Learning Rate Scheduling</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-learning-rate-scheduling">What is Learning Rate Scheduling?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-scheduling-matters">Why Scheduling Matters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#common-scheduling-strategies">Common Scheduling Strategies</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Visual Understanding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Real-World Applications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unit-test-step-learning-rate-scheduler">üß™ Unit Test: Step Learning Rate Scheduler</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-integration-complete-training-example">Step 5: Integration - Complete Training Example</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-it-all-together">Putting It All Together</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-modern-training-loop">The Modern Training Loop</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unit-test-complete-training-integration">üß™ Unit Test: Complete Training Integration</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-summary-optimization-mastery">üéØ Module Summary: Optimization Mastery!</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-you-ve-built">‚úÖ What You‚Äôve Built</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-learning-outcomes">‚úÖ Key Learning Outcomes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-foundations-mastered">‚úÖ Mathematical Foundations Mastered</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#professional-skills-developed">‚úÖ Professional Skills Developed</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ready-for-advanced-applications">‚úÖ Ready for Advanced Applications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#connection-to-real-ml-systems">üîó Connection to Real ML Systems</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-power-of-intelligent-optimization">üéØ The Power of Intelligent Optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-revolution">üß† Deep Learning Revolution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-s-next">üöÄ What‚Äôs Next</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-testing">üß™ Module Testing</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By TinyTorch Team
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>