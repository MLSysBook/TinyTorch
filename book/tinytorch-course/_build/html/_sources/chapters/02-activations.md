# Activation Functions

Mathematical functions that give neural networks the power to learn complex patterns!

## 🎯 What You'll Learn
- Understand why activation functions are essential for neural networks
- Implement ReLU, Sigmoid, and Tanh activation functions
- Explore the mathematical properties that make each function useful
- See how nonlinearity enables complex pattern learning

## ⚡ Key Concept
**Without activation functions, neural networks are just linear transformations!**

`Linear → Linear → Linear = Still just Linear`  
`Linear → Activation → Linear = Can learn complex patterns!`

## 📊 Chapter Status  
🚧 **Under Development** - Converting from `modules/source/02_activations/activations_dev.py`

This interactive chapter will include:
- **🔥 ReLU**: `f(x) = max(0, x)` - Simple, sparse, unbounded
- **📈 Sigmoid**: `f(x) = 1 / (1 + e^(-x))` - Bounded, smooth, probabilistic  
- **〰️ Tanh**: `f(x) = tanh(x)` - Zero-centered, bounded, smooth

## 🚀 Coming Soon
Full interactive notebook with:
- Mathematical derivations and visualizations
- Interactive code for experimenting with different functions
- Real examples showing how each activation affects learning

*Check back soon for the complete interactive experience!*
