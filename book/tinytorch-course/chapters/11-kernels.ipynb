{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Kernels\n",
        "\n",
        "```{admonition} Interactive Learning\n",
        ":class: tip\n",
        "\ud83d\ude80 **Launch Binder**: Click the rocket icon above to run this chapter interactively!\n",
        "\n",
        "\ud83d\udcbe **Save Your Work**: Download your completed notebook when done.\n",
        "\n",
        "\ud83c\udfd7\ufe0f **Build Locally**: Ready for serious development? [Fork the repo](https://github.com/your-org/tinytorch) and work locally with the full `tito` workflow.\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1155524",
      "metadata": {
        "lines_to_next_cell": 1,
        "nbgrader": {
          "grade": false,
          "grade_id": "kernels-imports",
          "locked": false,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "#| default_exp core.kernels\n",
        "\n",
        "#| export\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "import tracemalloc\n",
        "import psutil\n",
        "from typing import Callable, Dict, Any, Optional, Tuple, List\n",
        "from functools import wraps\n",
        "from pathlib import Path\n",
        "\n",
        "# Import our existing components\n",
        "try:\n",
        "    from tinytorch.core.tensor import Tensor\n",
        "    from tinytorch.core.layers import matmul_naive as matmul\n",
        "    from tinytorch.core.activations import ReLU, Sigmoid, Tanh\n",
        "    from tinytorch.core.cnn import Conv2D\n",
        "except ImportError:\n",
        "    # For development, import from local modules\n",
        "    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
        "    sys.path.extend([\n",
        "        os.path.join(base_dir, '01_tensor'),\n",
        "        os.path.join(base_dir, '02_activations'),\n",
        "        os.path.join(base_dir, '03_layers'),\n",
        "        os.path.join(base_dir, '05_cnn'),\n",
        "        os.path.join(base_dir, 'utils')\n",
        "    ])\n",
        "    \n",
        "    try:\n",
        "        from tensor_dev import Tensor\n",
        "        from layers_dev import matmul_naive as matmul\n",
        "        from activations_dev import ReLU, Sigmoid, Tanh\n",
        "        from cnn_dev import Conv2D\n",
        "    except ImportError:\n",
        "        # Create minimal mock for development\n",
        "        class Tensor:\n",
        "            def __init__(self, data):\n",
        "                self.data = np.array(data)\n",
        "                self.shape = self.data.shape\n",
        "            def __str__(self):\n",
        "                return f\"Tensor({self.data})\"\n",
        "\n",
        "# Simple timing utility for kernel performance measurement\n",
        "def time_kernel(func, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    Simple timing function for measuring kernel performance.\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (result, time_in_microseconds)\n",
        "    \"\"\"\n",
        "    start = time.perf_counter()\n",
        "    result = func(*args, **kwargs)\n",
        "    end = time.perf_counter()\n",
        "    microseconds = (end - start) * 1_000_000\n",
        "    return result, microseconds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2192572",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "kernels-setup",
          "locked": false,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "print(\"\ud83d\udd25 TinyTorch Kernels Module\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"Python version: {sys.version_info.major}.{sys.version_info.minor}\")\n",
        "print(f\"System: {psutil.cpu_count()} CPU cores, {psutil.virtual_memory().total // (1024**3):.1f}GB RAM\")\n",
        "print(\"Ready to optimize ML operations!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7bb10da",
      "metadata": {
        "cell_marker": "\"\"\""
      },
      "source": [
        "## \ud83d\udce6 Where This Code Lives in the Final Package\n",
        "\n",
        "**Learning Side:** You work in `modules/source/11_kernels/kernels_dev.py`  \n",
        "**Building Side:** Code exports to `tinytorch.core.kernels`\n",
        "\n",
        "```python\n",
        "# Final package structure:\n",
        "from tinytorch.core.kernels import vectorized_matmul, parallel_relu, cached_conv2d\n",
        "from tinytorch.core.tensor import Tensor\n",
        "from tinytorch.core.layers import Dense\n",
        "```\n",
        "\n",
        "**Why this matters:**\n",
        "- **Performance:** Custom kernels can be 2-10x faster than naive implementations\n",
        "- **Understanding:** Learn how PyTorch, TensorFlow achieve their speed\n",
        "- **Real-world:** Modern ML frameworks rely heavily on optimized kernels\n",
        "- **Hardware:** Bridge the gap between algorithms and computer architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "398554bb",
      "metadata": {
        "cell_marker": "\"\"\""
      },
      "source": [
        "## What are ML Kernels?\n",
        "\n",
        "### The Performance Gap\n",
        "Your neural network training is slow. A simple matrix multiplication that should take milliseconds takes seconds. Why?\n",
        "\n",
        "**The problem:** NumPy operations, while convenient, aren't optimized for your specific hardware or use case.\n",
        "\n",
        "**The solution:** Custom kernels - specialized functions written to extract maximum performance from your hardware.\n",
        "\n",
        "### What is a Kernel?\n",
        "A **kernel** is a highly optimized function that performs a specific computation:\n",
        "\n",
        "```python\n",
        "# Standard approach - easy but slow\n",
        "def slow_matmul(A, B):\n",
        "    return np.dot(A, B)\n",
        "\n",
        "# Kernel approach - harder but fast\n",
        "def fast_matmul(A, B):\n",
        "    # Optimized for your CPU's cache hierarchy\n",
        "    # Uses SIMD instructions for parallel operations\n",
        "    # Minimizes memory allocations\n",
        "    return optimized_result\n",
        "```\n",
        "\n",
        "### Why Kernels Matter for ML\n",
        "Modern ML frameworks achieve their speed through thousands of optimized kernels:\n",
        "\n",
        "- **PyTorch**: 2000+ CUDA kernels, 500+ CPU kernels\n",
        "- **TensorFlow**: XLA compiler generates optimized kernels\n",
        "- **JAX**: JIT compilation creates specialized kernels\n",
        "- **Hardware**: GPUs have 1000s of cores, TPUs have specialized ML units\n",
        "\n",
        "### The Performance Hierarchy\n",
        "```\n",
        "Python loops:        1x speed    (baseline)\n",
        "NumPy operations:    10x speed   (vectorized)\n",
        "Optimized kernels:   100x speed  (hardware-aware)\n",
        "GPU kernels:         1000x speed (massive parallelism)\n",
        "```\n",
        "\n",
        "### Real-World Impact\n",
        "- **Training time**: 10-hour training \u2192 1-hour training\n",
        "- **Inference cost**: $1000/month \u2192 $100/month\n",
        "- **Model size**: Enable larger models through efficiency\n",
        "- **Energy**: 90% reduction in power consumption\n",
        "\n",
        "### What You'll Learn\n",
        "1. **Custom operations** - Moving beyond NumPy limitations\n",
        "2. **Vectorization** - Using SIMD for parallel computation\n",
        "3. **Memory optimization** - Cache-friendly algorithms\n",
        "4. **Parallel processing** - CPU and GPU-style parallelism\n",
        "5. **Performance measurement** - Professional profiling tools\n",
        "6. **Compressed kernels** - Optimizations for quantized models\n",
        "\n",
        "Let's build the optimizations that power modern AI!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "279c417a",
      "metadata": {
        "cell_marker": "\"\"\"",
        "lines_to_next_cell": 1
      },
      "source": [
        "## Step 1: Custom Operations - Beyond NumPy\n",
        "\n",
        "### Why Custom Operations?\n",
        "NumPy is great for prototyping, but has limitations:\n",
        "- **Generic**: Optimized for general use, not your specific case\n",
        "- **Memory**: Creates temporary arrays, wastes memory\n",
        "- **Control**: Can't control memory layout, algorithm choice\n",
        "- **Specialization**: Can't optimize for your data patterns\n",
        "\n",
        "### The Philosophy\n",
        "Instead of using general-purpose functions, we write **specialized** functions:\n",
        "\n",
        "```python\n",
        "# Generic NumPy approach\n",
        "def generic_activation(x):\n",
        "    return np.maximum(0, x)  # ReLU\n",
        "\n",
        "# Specialized kernel approach  \n",
        "def fast_relu_kernel(x):\n",
        "    # Optimized for your specific use case\n",
        "    # No unnecessary memory allocations\n",
        "    # Optimized for your data sizes\n",
        "    return result\n",
        "```\n",
        "\n",
        "### Design Principles\n",
        "- **Specialization**: Optimize for specific input patterns\n",
        "- **Memory efficiency**: Minimize allocations and copies\n",
        "- **Algorithmic choice**: Pick the best algorithm for your data\n",
        "- **Measurement**: Always profile before and after\n",
        "\n",
        "### Real-World Context\n",
        "This is how:\n",
        "- **PyTorch**: Custom autograd functions override standard operations\n",
        "- **TensorFlow**: tf.function compiles optimized graphs\n",
        "- **JAX**: jax.jit creates specialized kernels\n",
        "- **CUDA**: Every GPU operation is a custom kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c2060b4",
      "metadata": {
        "lines_to_next_cell": 1,
        "nbgrader": {
          "grade": false,
          "grade_id": "custom-matmul",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "#| export\n",
        "def matmul_baseline(A: Tensor, B: Tensor) -> Tensor:\n",
        "    \"\"\"\n",
        "    Baseline matrix multiplication using TinyTorch's proven implementation.\n",
        "    \n",
        "    This function demonstrates how to build on existing TinyTorch components\n",
        "    rather than reinventing the wheel. We use the standard matmul from Module 03\n",
        "    as our baseline for comparison with optimized kernels.\n",
        "    \n",
        "    This is NOT a custom implementation - it's the standard TinyTorch matmul\n",
        "    wrapped for use in kernel comparisons and benchmarking.\n",
        "    \n",
        "    TODO: Use TinyTorch's standard matmul implementation as a baseline.\n",
        "    \n",
        "    STEP-BY-STEP IMPLEMENTATION:\n",
        "    1. Import the standard matmul function from tinytorch.core.layers\n",
        "    2. Extract numpy arrays from input Tensors\n",
        "    3. Use the proven implementation from TinyTorch\n",
        "    4. Wrap result back in Tensor format\n",
        "    5. Return the result\n",
        "    \n",
        "    CODE REUSE PRINCIPLES:\n",
        "    1. Always use the packaged version for reliability\n",
        "    2. Don't duplicate working code - reference the source\n",
        "    3. Use descriptive names that indicate what the function actually does\n",
        "    4. Keep dependencies simple and reliable\n",
        "    \n",
        "    EXAMPLE USAGE:\n",
        "    ```python\n",
        "    A = Tensor([[1, 2], [3, 4]])\n",
        "    B = Tensor([[5, 6], [7, 8]])\n",
        "    C = matmul_baseline(A, B)\n",
        "    # Expected: [[19, 22], [43, 50]]\n",
        "    ```\n",
        "    \n",
        "    LEARNING CONNECTIONS:\n",
        "    - This shows how to use TinyTorch as a library\n",
        "    - Demonstrates reliable dependency management\n",
        "    - Serves as baseline for kernel performance comparisons\n",
        "    - Shows proper software engineering practices\n",
        "    \"\"\"\n",
        "    ### BEGIN SOLUTION\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "    ### END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e574ab1b",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "test-custom-matmul",
          "locked": false,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "### \ud83e\uddea Unit Test: Baseline Matrix Multiplication\n",
        "\n",
        "def test_matmul_baseline():\n",
        "    \"\"\"Test baseline matrix multiplication implementation.\"\"\"\n",
        "    print(\"\ud83d\udd2c Unit Test: Baseline Matrix Multiplication...\")\n",
        "    \n",
        "    # Test case 1: Small matrices (2x2)\n",
        "    A = Tensor([[1, 2], [3, 4]])\n",
        "    B = Tensor([[5, 6], [7, 8]])\n",
        "    C = matmul_baseline(A, B)\n",
        "    expected = Tensor([[19, 22], [43, 50]])  # Hand-computed\n",
        "    \n",
        "    assert np.allclose(C.data, expected.data), f\"Expected {expected.data}, got {C.data}\"\n",
        "    print(\"\u2705 Small matrix multiplication works\")\n",
        "    \n",
        "    # Test case 2: Rectangular matrices\n",
        "    A = Tensor([[1, 2, 3], [4, 5, 6]])  # 2x3\n",
        "    B = Tensor([[7, 8], [9, 10], [11, 12]])  # 3x2\n",
        "    C = matmul_baseline(A, B)\n",
        "    expected = Tensor([[58, 64], [139, 154]])\n",
        "    \n",
        "    assert np.allclose(C.data, expected.data), f\"Expected {expected.data}, got {C.data}\"\n",
        "    print(\"\u2705 Rectangular matrix multiplication works\")\n",
        "    \n",
        "    # Test case 3: Compare with NumPy (medium size - should use TinyTorch implementation)\n",
        "    np.random.seed(42)\n",
        "    A = Tensor(np.random.randn(32, 32))\n",
        "    B = Tensor(np.random.randn(32, 32))\n",
        "    \n",
        "    C_baseline = matmul_baseline(A, B)\n",
        "    C_numpy = Tensor(np.dot(A.data, B.data))\n",
        "    \n",
        "    assert np.allclose(C_baseline.data, C_numpy.data, rtol=1e-10), \"Baseline implementation differs from NumPy\"\n",
        "    print(\"\u2705 Baseline implementation matches NumPy\")\n",
        "    \n",
        "    # Test case 4: Large matrix\n",
        "    A = Tensor(np.random.randn(100, 100))\n",
        "    B = Tensor(np.random.randn(100, 100))\n",
        "    C = matmul_baseline(A, B)\n",
        "    \n",
        "    assert C.shape == (100, 100), f\"Expected shape (100, 100), got {C.shape}\"\n",
        "    print(\"\u2705 Large matrix multiplication works\")\n",
        "    \n",
        "    print(\"\ud83d\udcc8 Progress: Baseline Matrix Multiplication \u2713\")\n",
        "\n",
        "# Run the test\n",
        "test_matmul_baseline()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24588483",
      "metadata": {
        "cell_marker": "\"\"\"",
        "lines_to_next_cell": 1
      },
      "source": [
        "## Step 2: Vectorized Operations - SIMD Principles\n",
        "\n",
        "### What is Vectorization?\n",
        "**Vectorization** means processing multiple data elements in parallel using SIMD (Single Instruction, Multiple Data) operations.\n",
        "\n",
        "### The Problem with Loops\n",
        "```python\n",
        "# Scalar processing - one element at a time\n",
        "def slow_relu(x):\n",
        "    result = np.zeros_like(x)\n",
        "    for i in range(len(x)):\n",
        "        result[i] = max(0, x[i])  # One operation per cycle\n",
        "    return result\n",
        "```\n",
        "\n",
        "### The Vectorization Solution\n",
        "```python\n",
        "# Vector processing - multiple elements at once\n",
        "def fast_relu(x):\n",
        "    return np.maximum(0, x)  # Many operations per cycle\n",
        "```\n",
        "\n",
        "### Why Vectorization Matters\n",
        "- **CPU SIMD**: Modern CPUs can process 4-8 floats simultaneously\n",
        "- **GPU parallelism**: GPUs have thousands of cores for parallel processing\n",
        "- **Memory bandwidth**: Better utilization of memory transfers\n",
        "- **Compiler optimization**: Enables automatic vectorization\n",
        "\n",
        "### SIMD Principles\n",
        "1. **Data parallelism**: Same operation on multiple data elements\n",
        "2. **Memory alignment**: Aligned data enables faster SIMD instructions\n",
        "3. **Batch processing**: Process data in chunks that fit SIMD registers\n",
        "4. **Avoid branches**: Conditional operations break SIMD efficiency\n",
        "\n",
        "### Real-World Context\n",
        "- **NumPy**: All operations are vectorized using BLAS/LAPACK\n",
        "- **PyTorch**: Vectorized operations compile to SIMD instructions\n",
        "- **GPU kernels**: Thousands of parallel threads process data\n",
        "- **AVX-512**: Intel's latest SIMD can process 16 floats at once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51b486ae",
      "metadata": {
        "lines_to_next_cell": 1,
        "nbgrader": {
          "grade": false,
          "grade_id": "vectorized-relu",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "#| export\n",
        "def vectorized_relu(x: Tensor) -> Tensor:\n",
        "    \"\"\"\n",
        "    Vectorized ReLU implementation demonstrating SIMD principles.\n",
        "    \n",
        "    This function shows how to write operations that take advantage of\n",
        "    CPU vectorization capabilities for better performance.\n",
        "    \n",
        "    TODO: Implement a vectorized ReLU that's optimized for performance.\n",
        "    \n",
        "    STEP-BY-STEP IMPLEMENTATION:\n",
        "    1. Extract numpy array from Tensor\n",
        "    2. Use NumPy's vectorized operations (these compile to SIMD instructions)\n",
        "    3. Apply ReLU: f(x) = max(0, x) for all elements simultaneously\n",
        "    4. Return result as Tensor\n",
        "    \n",
        "    VECTORIZATION TECHNIQUES:\n",
        "    1. Use np.maximum instead of loops - this is vectorized\n",
        "    2. Ensure input is contiguous in memory for better SIMD performance\n",
        "    3. Consider using specific dtypes (float32 vs float64) for SIMD alignment\n",
        "    4. Avoid conditional operations that break vectorization\n",
        "    \n",
        "    EXAMPLE USAGE:\n",
        "    ```python\n",
        "    x = Tensor([-2, -1, 0, 1, 2])\n",
        "    y = vectorized_relu(x)\n",
        "    # Expected: [0, 0, 0, 1, 2]\n",
        "    ```\n",
        "    \n",
        "    PERFORMANCE CONSIDERATIONS:\n",
        "    - np.maximum is vectorized and uses SIMD instructions\n",
        "    - Memory layout matters: contiguous arrays are faster\n",
        "    - Data type matters: float32 allows more SIMD parallelism than float64\n",
        "    - Avoid Python loops - they can't be vectorized\n",
        "    \n",
        "    LEARNING CONNECTIONS:\n",
        "    - This is how PyTorch's ReLU is implemented under the hood\n",
        "    - GPU kernels use similar principles with thousands of parallel threads\n",
        "    - Modern CPUs can process 4-16 floats simultaneously with SIMD\n",
        "    \"\"\"\n",
        "    ### BEGIN SOLUTION\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "    ### END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04c19360",
      "metadata": {
        "lines_to_next_cell": 1,
        "nbgrader": {
          "grade": false,
          "grade_id": "vectorized-operations",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "#| export\n",
        "def vectorized_operations(x: Tensor, y: Tensor) -> Dict[str, Tensor]:\n",
        "    \"\"\"\n",
        "    Demonstration of various vectorized operations.\n",
        "    \n",
        "    Shows how multiple operations can be vectorized for better performance.\n",
        "    \n",
        "    TODO: Implement a collection of vectorized operations.\n",
        "    \n",
        "    STEP-BY-STEP IMPLEMENTATION:\n",
        "    1. Extract numpy arrays from input Tensors\n",
        "    2. Implement vectorized versions of common operations\n",
        "    3. Use NumPy's built-in vectorized functions\n",
        "    4. Return dictionary of results\n",
        "    \n",
        "    OPERATIONS TO IMPLEMENT:\n",
        "    - element_wise_multiply: x * y (element-wise)\n",
        "    - element_wise_add: x + y (element-wise)\n",
        "    - squared_difference: (x - y)^2\n",
        "    - euclidean_distance: sqrt(sum((x - y)^2))\n",
        "    - dot_product: sum(x * y)\n",
        "    \n",
        "    VECTORIZATION PRINCIPLES:\n",
        "    - Use NumPy operations instead of Python loops\n",
        "    - Combine operations when possible: (x - y)**2 instead of subtract then square\n",
        "    - Consider memory layout and data types\n",
        "    - Measure performance improvements\n",
        "    \n",
        "    EXAMPLE USAGE:\n",
        "    ```python\n",
        "    x = Tensor([1, 2, 3, 4])\n",
        "    y = Tensor([2, 3, 4, 5])\n",
        "    results = vectorized_operations(x, y)\n",
        "    # Returns dict with all vectorized operation results\n",
        "    ```\n",
        "    \"\"\"\n",
        "    ### BEGIN SOLUTION\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "    ### END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0979985b",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "test-vectorized-operations",
          "locked": false,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "### \ud83e\uddea Unit Test: Vectorized Operations\n",
        "\n",
        "def test_vectorized_operations():\n",
        "    \"\"\"Test vectorized operations implementation.\"\"\"\n",
        "    print(\"\ud83d\udd2c Unit Test: Vectorized Operations...\")\n",
        "    \n",
        "    # Test vectorized ReLU\n",
        "    x = Tensor([-2, -1, 0, 1, 2])\n",
        "    y = vectorized_relu(x)\n",
        "    expected = [0, 0, 0, 1, 2]\n",
        "    \n",
        "    assert np.allclose(y.data, expected), f\"Expected {expected}, got {y.data}\"\n",
        "    print(\"\u2705 Vectorized ReLU works\")\n",
        "    \n",
        "    # Test vectorized operations\n",
        "    x = Tensor([1, 2, 3, 4])\n",
        "    y = Tensor([2, 3, 4, 5])\n",
        "    results = vectorized_operations(x, y)\n",
        "    \n",
        "    # Check element-wise multiply\n",
        "    expected_mul = [2, 6, 12, 20]\n",
        "    assert np.allclose(results['element_wise_multiply'].data, expected_mul), \\\n",
        "        f\"Expected {expected_mul}, got {results['element_wise_multiply'].data}\"\n",
        "    print(\"\u2705 Element-wise multiply works\")\n",
        "    \n",
        "    # Check element-wise add\n",
        "    expected_add = [3, 5, 7, 9]\n",
        "    assert np.allclose(results['element_wise_add'].data, expected_add), \\\n",
        "        f\"Expected {expected_add}, got {results['element_wise_add'].data}\"\n",
        "    print(\"\u2705 Element-wise add works\")\n",
        "    \n",
        "    # Check squared difference\n",
        "    expected_sq_diff = [1, 1, 1, 1]  # (1-2)^2, (2-3)^2, etc.\n",
        "    assert np.allclose(results['squared_difference'].data, expected_sq_diff), \\\n",
        "        f\"Expected {expected_sq_diff}, got {results['squared_difference'].data}\"\n",
        "    print(\"\u2705 Squared difference works\")\n",
        "    \n",
        "    # Check dot product\n",
        "    expected_dot = 40  # 1*2 + 2*3 + 3*4 + 4*5 = 2 + 6 + 12 + 20 = 40\n",
        "    assert np.allclose(results['dot_product'].data, expected_dot), \\\n",
        "        f\"Expected {expected_dot}, got {results['dot_product'].data}\"\n",
        "    print(\"\u2705 Dot product works\")\n",
        "    \n",
        "    print(\"\ud83d\udcc8 Progress: Vectorized Operations \u2713\")\n",
        "\n",
        "# Run the test\n",
        "test_vectorized_operations()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d9802f7",
      "metadata": {
        "cell_marker": "\"\"\"",
        "lines_to_next_cell": 1
      },
      "source": [
        "## Step 3: Memory Layout Optimization - Cache-Friendly Algorithms\n",
        "\n",
        "### Why Memory Layout Matters\n",
        "Modern CPUs are **memory-bound**, not compute-bound. The bottleneck isn't how fast you can multiply numbers\u2014it's how fast you can get data from memory.\n",
        "\n",
        "### The Memory Hierarchy\n",
        "```\n",
        "CPU Registers:    1 cycle     (fastest, tiny)\n",
        "L1 Cache:         3 cycles    (fast, small)\n",
        "L2 Cache:         10 cycles   (medium, medium)\n",
        "L3 Cache:         40 cycles   (slow, large)\n",
        "Main Memory:      200+ cycles (slowest, huge)\n",
        "```\n",
        "\n",
        "### Cache-Friendly Principles\n",
        "1. **Spatial locality**: Access nearby memory locations\n",
        "2. **Temporal locality**: Reuse recently accessed data\n",
        "3. **Cache lines**: Memory is loaded in 64-byte chunks\n",
        "4. **Cache blocking**: Process data in cache-sized chunks\n",
        "\n",
        "### Real-World Impact\n",
        "- **Matrix multiplication**: Cache-friendly algorithms are 10x faster\n",
        "- **Image processing**: Row-major vs column-major access patterns\n",
        "- **Neural networks**: Memory layout affects training speed significantly\n",
        "\n",
        "### The Problem with Naive Algorithms\n",
        "```python\n",
        "# Cache-unfriendly: jumps around memory\n",
        "def slow_transpose(A):\n",
        "    for i in range(rows):\n",
        "        for j in range(cols):\n",
        "            B[j, i] = A[i, j]  # Poor cache locality\n",
        "```\n",
        "\n",
        "### Cache-Friendly Solution\n",
        "```python\n",
        "# Cache-friendly: processes data in blocks\n",
        "def fast_transpose(A):\n",
        "    # Process in cache-sized blocks\n",
        "    for block_i in range(0, rows, BLOCK_SIZE):\n",
        "        for block_j in range(0, cols, BLOCK_SIZE):\n",
        "            # Process block - good cache locality\n",
        "            for i in range(block_i, min(block_i + BLOCK_SIZE, rows)):\n",
        "                for j in range(block_j, min(block_j + BLOCK_SIZE, cols)):\n",
        "                    B[j, i] = A[i, j]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0859a9c",
      "metadata": {
        "lines_to_next_cell": 1,
        "nbgrader": {
          "grade": false,
          "grade_id": "cache-friendly-matmul",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "#| export\n",
        "def cache_friendly_matmul(A: Tensor, B: Tensor, block_size: int = 32) -> Tensor:\n",
        "    \"\"\"\n",
        "    Cache-friendly matrix multiplication using blocking technique.\n",
        "    \n",
        "    This implementation uses cache blocking to improve memory access patterns\n",
        "    and achieve better performance on modern CPUs.\n",
        "    \n",
        "    TODO: Implement cache-friendly matrix multiplication using blocking.\n",
        "    \n",
        "    STEP-BY-STEP IMPLEMENTATION:\n",
        "    1. Extract numpy arrays and get dimensions\n",
        "    2. Pre-allocate output matrix\n",
        "    3. Use three nested loops for blocks: block_i, block_j, block_k\n",
        "    4. Within each block, use three nested loops for elements: i, j, k\n",
        "    5. Process data in cache-sized blocks for better locality\n",
        "    \n",
        "    BLOCKING ALGORITHM:\n",
        "    1. Divide matrices into blocks of size block_size x block_size\n",
        "    2. For each block of C, compute contribution from corresponding A and B blocks\n",
        "    3. This keeps data in cache longer, reducing memory access time\n",
        "    \n",
        "    CACHE OPTIMIZATION PRINCIPLES:\n",
        "    - Process data in small blocks that fit in cache\n",
        "    - Reuse data as much as possible while it's in cache\n",
        "    - Access memory in predictable patterns\n",
        "    - Minimize cache misses\n",
        "    \n",
        "    EXAMPLE USAGE:\n",
        "    ```python\n",
        "    A = Tensor([[1, 2], [3, 4]])\n",
        "    B = Tensor([[5, 6], [7, 8]])\n",
        "    C = cache_friendly_matmul(A, B, block_size=2)\n",
        "    # Expected: [[19, 22], [43, 50]]\n",
        "    ```\n",
        "    \n",
        "    PERFORMANCE HINTS:\n",
        "    - block_size should be chosen based on cache size\n",
        "    - Typical L1 cache: 32KB, so block_size=32 for float32 matrices\n",
        "    - Experiment with different block sizes for your hardware\n",
        "    - This algorithm is O(n^3) but with much better constants\n",
        "    \n",
        "    LEARNING CONNECTIONS:\n",
        "    - This is how BLAS libraries achieve high performance\n",
        "    - GPUs use similar tiling strategies for shared memory\n",
        "    - Modern compilers can sometimes do this automatically\n",
        "    \"\"\"\n",
        "    ### BEGIN SOLUTION\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "    ### END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04eee613",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "test-cache-friendly",
          "locked": false,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "### \ud83e\uddea Unit Test: Cache-Friendly Matrix Multiplication\n",
        "\n",
        "def test_cache_friendly_matmul():\n",
        "    \"\"\"Test cache-friendly matrix multiplication implementation.\"\"\"\n",
        "    print(\"\ud83d\udd2c Unit Test: Cache-Friendly Matrix Multiplication...\")\n",
        "    \n",
        "    # Test case 1: Small matrices\n",
        "    A = Tensor([[1, 2], [3, 4]])\n",
        "    B = Tensor([[5, 6], [7, 8]])\n",
        "    C = cache_friendly_matmul(A, B, block_size=2)\n",
        "    expected = [[19, 22], [43, 50]]\n",
        "    \n",
        "    assert np.allclose(C.data, expected), f\"Expected {expected}, got {C.data}\"\n",
        "    print(\"\u2705 Small matrix cache-friendly multiplication works\")\n",
        "    \n",
        "    # Test case 2: Larger matrices with different block sizes\n",
        "    np.random.seed(42)\n",
        "    A = Tensor(np.random.randn(64, 64))\n",
        "    B = Tensor(np.random.randn(64, 64))\n",
        "    \n",
        "    C_blocked = cache_friendly_matmul(A, B, block_size=16)\n",
        "    C_numpy = Tensor(np.dot(A.data, B.data))\n",
        "    \n",
        "    assert np.allclose(C_blocked.data, C_numpy.data, rtol=1e-4), \\\n",
        "        \"Cache-friendly implementation differs from NumPy\"\n",
        "    print(\"\u2705 Cache-friendly implementation matches NumPy\")\n",
        "    \n",
        "    # Test case 3: Non-square matrices\n",
        "    A = Tensor(np.random.randn(48, 32))\n",
        "    B = Tensor(np.random.randn(32, 48))\n",
        "    \n",
        "    C_blocked = cache_friendly_matmul(A, B, block_size=8)\n",
        "    C_numpy = Tensor(np.dot(A.data, B.data))\n",
        "    \n",
        "    assert np.allclose(C_blocked.data, C_numpy.data, rtol=1e-4), \\\n",
        "        \"Non-square cache-friendly implementation differs from NumPy\"\n",
        "    print(\"\u2705 Non-square matrix cache-friendly multiplication works\")\n",
        "    \n",
        "    print(\"\ud83d\udcc8 Progress: Cache-Friendly Algorithms \u2713\")\n",
        "\n",
        "# Run the test\n",
        "test_cache_friendly_matmul()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04066d2b",
      "metadata": {
        "cell_marker": "\"\"\"",
        "lines_to_next_cell": 1
      },
      "source": [
        "## Step 4: Parallel Processing - CPU and GPU-Style Computing\n",
        "\n",
        "### Why Parallel Processing?\n",
        "Modern hardware has multiple cores, and ML workloads are inherently parallel. We need to use all available compute resources.\n",
        "\n",
        "### Types of Parallelism\n",
        "1. **Data parallelism**: Split data across processors\n",
        "2. **Task parallelism**: Split operations across processors\n",
        "3. **Pipeline parallelism**: Different stages on different processors\n",
        "4. **Model parallelism**: Split model across processors\n",
        "\n",
        "### CPU vs GPU Parallelism\n",
        "- **CPU**: Few cores (4-64), complex operations, low latency\n",
        "- **GPU**: Many cores (1000s), simple operations, high throughput\n",
        "\n",
        "### Parallel Processing Patterns\n",
        "```python\n",
        "# Sequential processing\n",
        "for i in range(n):\n",
        "    result[i] = expensive_operation(data[i])\n",
        "\n",
        "# Parallel processing\n",
        "with ThreadPoolExecutor() as executor:\n",
        "    futures = [executor.submit(expensive_operation, data[i]) for i in range(n)]\n",
        "    results = [f.result() for f in futures]\n",
        "```\n",
        "\n",
        "### Real-World Context\n",
        "- **PyTorch**: Parallel data loading, distributed training\n",
        "- **TensorFlow**: tf.data for parallel preprocessing\n",
        "- **NumPy**: Multithreaded BLAS operations\n",
        "- **GPU kernels**: Thousands of parallel threads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd139e16",
      "metadata": {
        "lines_to_next_cell": 1,
        "nbgrader": {
          "grade": false,
          "grade_id": "parallel-relu",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "#| export\n",
        "def parallel_relu(x: Tensor, num_workers: int = 4) -> Tensor:\n",
        "    \"\"\"\n",
        "    Parallel ReLU implementation using multiple CPU cores.\n",
        "    \n",
        "    This function demonstrates data parallelism by splitting the input\n",
        "    across multiple worker processes.\n",
        "    \n",
        "    TODO: Implement parallel ReLU using multiprocessing or threading.\n",
        "    \n",
        "    STEP-BY-STEP IMPLEMENTATION:\n",
        "    1. Extract numpy array from Tensor\n",
        "    2. Split array into chunks for parallel processing\n",
        "    3. Define worker function that applies ReLU to a chunk\n",
        "    4. Use ThreadPoolExecutor to process chunks in parallel\n",
        "    5. Combine results from all workers\n",
        "    6. Return result as Tensor\n",
        "    \n",
        "    PARALLELIZATION STRATEGY:\n",
        "    1. Split input into num_workers chunks\n",
        "    2. Each worker processes its chunk independently\n",
        "    3. Apply ReLU: max(0, x) to each chunk\n",
        "    4. Combine results preserving original order\n",
        "    \n",
        "    EXAMPLE USAGE:\n",
        "    ```python\n",
        "    x = Tensor(np.random.randn(1000))\n",
        "    y = parallel_relu(x, num_workers=4)\n",
        "    # Processes data using 4 parallel workers\n",
        "    ```\n",
        "    \n",
        "    PERFORMANCE CONSIDERATIONS:\n",
        "    - Overhead of parallel processing may not be worth it for small arrays\n",
        "    - Threading vs multiprocessing trade-offs\n",
        "    - Chunk size should be large enough to amortize overhead\n",
        "    - Consider memory bandwidth limitations\n",
        "    \n",
        "    LEARNING CONNECTIONS:\n",
        "    - This is how PyTorch processes batches in parallel\n",
        "    - GPUs naturally do this with thousands of parallel threads\n",
        "    - Modern deep learning frameworks heavily use parallelism\n",
        "    \"\"\"\n",
        "    ### BEGIN SOLUTION\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "    ### END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79865f53",
      "metadata": {
        "lines_to_next_cell": 1,
        "nbgrader": {
          "grade": false,
          "grade_id": "parallel-batch-processing",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "#| export\n",
        "def parallel_batch_processing(batch_data: List[Tensor], operation: Callable, num_workers: int = 4) -> List[Tensor]:\n",
        "    \"\"\"\n",
        "    Process a batch of tensors in parallel using multiple workers.\n",
        "    \n",
        "    This function demonstrates how to parallelize operations across\n",
        "    multiple data samples, similar to how modern ML frameworks work.\n",
        "    \n",
        "    TODO: Implement parallel batch processing.\n",
        "    \n",
        "    STEP-BY-STEP IMPLEMENTATION:\n",
        "    1. Take a list of Tensors and an operation function\n",
        "    2. Use ThreadPoolExecutor to process multiple tensors simultaneously\n",
        "    3. Apply the operation to each tensor in parallel\n",
        "    4. Return list of results in original order\n",
        "    \n",
        "    PARALLELIZATION STRATEGY:\n",
        "    1. Each worker processes one tensor at a time\n",
        "    2. Multiple workers can process different tensors simultaneously\n",
        "    3. Preserve order of results to match input order\n",
        "    \n",
        "    EXAMPLE USAGE:\n",
        "    ```python\n",
        "    batch = [Tensor(np.random.randn(100, 100)) for _ in range(8)]\n",
        "    relu_op = lambda x: vectorized_relu(x)\n",
        "    results = parallel_batch_processing(batch, relu_op, num_workers=4)\n",
        "    # Processes 8 tensors using 4 parallel workers\n",
        "    ```\n",
        "    \n",
        "    PERFORMANCE CONSIDERATIONS:\n",
        "    - Each tensor should be large enough to justify parallel overhead\n",
        "    - Balance number of workers with available CPU cores\n",
        "    - Consider memory usage with multiple workers\n",
        "    - Thread vs process pool trade-offs\n",
        "    \n",
        "    LEARNING CONNECTIONS:\n",
        "    - This is how PyTorch's DataLoader processes batches\n",
        "    - Similar to how GPUs process multiple samples simultaneously\n",
        "    - Foundation for distributed training across multiple nodes\n",
        "    \"\"\"\n",
        "    ### BEGIN SOLUTION\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "    ### END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1bfd137",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "test-parallel-processing",
          "locked": false,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "### \ud83e\uddea Unit Test: Parallel Processing\n",
        "\n",
        "def test_parallel_processing():\n",
        "    \"\"\"Test parallel processing implementations.\"\"\"\n",
        "    print(\"\ud83d\udd2c Unit Test: Parallel Processing...\")\n",
        "    \n",
        "    # Test parallel ReLU\n",
        "    x = Tensor(np.array([-2, -1, 0, 1, 2]))\n",
        "    y = parallel_relu(x, num_workers=2)\n",
        "    expected = [0, 0, 0, 1, 2]\n",
        "    \n",
        "    assert np.allclose(y.data, expected), f\"Expected {expected}, got {y.data}\"\n",
        "    print(\"\u2705 Parallel ReLU works\")\n",
        "    \n",
        "    # Test parallel ReLU with larger data\n",
        "    x_large = Tensor(np.random.randn(2000))\n",
        "    y_large = parallel_relu(x_large, num_workers=4)\n",
        "    y_sequential = vectorized_relu(x_large)\n",
        "    \n",
        "    assert np.allclose(y_large.data, y_sequential.data), \\\n",
        "        \"Parallel ReLU differs from sequential version\"\n",
        "    print(\"\u2705 Parallel ReLU matches sequential version\")\n",
        "    \n",
        "    # Test parallel batch processing\n",
        "    batch = [Tensor(np.random.randn(100)) for _ in range(8)]\n",
        "    relu_op = lambda x: vectorized_relu(x)\n",
        "    \n",
        "    results_parallel = parallel_batch_processing(batch, relu_op, num_workers=4)\n",
        "    results_sequential = [relu_op(tensor) for tensor in batch]\n",
        "    \n",
        "    assert len(results_parallel) == len(results_sequential), \\\n",
        "        f\"Expected {len(results_sequential)} results, got {len(results_parallel)}\"\n",
        "    \n",
        "    for i, (parallel, sequential) in enumerate(zip(results_parallel, results_sequential)):\n",
        "        assert np.allclose(parallel.data, sequential.data), \\\n",
        "            f\"Batch item {i}: parallel differs from sequential\"\n",
        "    \n",
        "    print(\"\u2705 Parallel batch processing works\")\n",
        "    print(\"\ud83d\udcc8 Progress: Parallel Processing \u2713\")\n",
        "\n",
        "# Run the test\n",
        "test_parallel_processing()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a2c9024",
      "metadata": {
        "cell_marker": "\"\"\"",
        "lines_to_next_cell": 1
      },
      "source": [
        "## Step 5: Simple Performance Measurement - Timing Your Kernels\n",
        "\n",
        "### Why Timing Matters\n",
        "> \"Premature optimization is the root of all evil\" - Donald Knuth\n",
        "\n",
        "But **measured optimization** based on simple timing is essential for understanding kernel performance.\n",
        "\n",
        "### What We'll Measure\n",
        "1. **Execution time**: How long does each kernel take?\n",
        "2. **Relative performance**: Which implementation is faster?\n",
        "3. **Scale effects**: How does performance change with data size?\n",
        "4. **Optimization impact**: Did our changes actually help?\n",
        "\n",
        "### The Simple Timing Process\n",
        "1. **Measure baseline**: Time the standard implementation\n",
        "2. **Time optimizations**: Measure your improved versions\n",
        "3. **Compare results**: See which is faster\n",
        "4. **Verify correctness**: Ensure optimized code produces correct results\n",
        "\n",
        "### Our Simple Timing Tool\n",
        "We use `time.perf_counter()` for microsecond-precision timing:\n",
        "- **Precise**: Measures actual execution time\n",
        "- **Simple**: Easy to understand and use\n",
        "- **Realistic**: Shows kernel performance at the right scale\n",
        "- **Educational**: Immediate feedback on optimization impact\n",
        "\n",
        "### Real-World Context\n",
        "- **Kernel operations**: Typically take 10-1000 microseconds\n",
        "- **Optimization impact**: Good kernels are 2-10x faster\n",
        "- **Professional tools**: Production systems use sophisticated profilers\n",
        "- **Foundation**: Simple timing teaches measurement principles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21d2acf4",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "test-profiling",
          "locked": false,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "### \ud83e\uddea Unit Test: Simple Kernel Timing\n",
        "\n",
        "def test_simple_kernel_timing():\n",
        "    \"\"\"Test simple kernel timing capabilities.\"\"\"\n",
        "    print(\"\ud83d\udd2c Unit Test: Simple Kernel Timing...\")\n",
        "    \n",
        "    # Test timing different matrix multiplication methods\n",
        "    np.random.seed(42)\n",
        "    A = Tensor(np.random.randn(100, 100))\n",
        "    B = Tensor(np.random.randn(100, 100))\n",
        "    \n",
        "    # Time NumPy matmul\n",
        "    result_numpy, time_numpy = time_kernel(lambda: Tensor(np.dot(A.data, B.data)))\n",
        "    print(f\"\ud83d\udd0d NumPy matmul: {time_numpy:.1f} \u03bcs\")\n",
        "    \n",
        "    # Time baseline matmul  \n",
        "    result_baseline, time_baseline = time_kernel(matmul_baseline, A, B)\n",
        "    print(f\"\ud83d\udd0d Baseline matmul: {time_baseline:.1f} \u03bcs\")\n",
        "    \n",
        "    # Time cache-friendly matmul\n",
        "    result_cache, time_cache = time_kernel(cache_friendly_matmul, A, B, 16)\n",
        "    print(f\"\ud83d\udd0d Cache-friendly matmul: {time_cache:.1f} \u03bcs\")\n",
        "    \n",
        "    # Verify results are similar\n",
        "    assert np.allclose(result_numpy.data, result_baseline.data, rtol=1e-4), \\\n",
        "        \"NumPy and baseline results differ\"\n",
        "    assert np.allclose(result_numpy.data, result_cache.data, rtol=1e-2), \\\n",
        "        \"NumPy and cache-friendly results differ\"\n",
        "    \n",
        "    print(\"\u2705 All matrix multiplication methods produce correct results\")\n",
        "    \n",
        "    # Test timing parallel vs sequential ReLU\n",
        "    x_large = Tensor(np.random.randn(10000))\n",
        "    \n",
        "    result_seq, time_seq = time_kernel(vectorized_relu, x_large)\n",
        "    result_par, time_par = time_kernel(parallel_relu, x_large, 4)\n",
        "    \n",
        "    print(f\"\ud83d\udd0d Sequential ReLU: {time_seq:.1f} \u03bcs\")\n",
        "    print(f\"\ud83d\udd0d Parallel ReLU: {time_par:.1f} \u03bcs\")\n",
        "    \n",
        "    # Verify results are the same\n",
        "    assert np.allclose(result_seq.data, result_par.data), \\\n",
        "        \"Sequential and parallel ReLU results differ\"\n",
        "    \n",
        "    print(\"\u2705 Simple timing works correctly\")\n",
        "    print(\"\ud83d\udcc8 Progress: Simple Kernel Timing \u2713\")\n",
        "\n",
        "# Run the test\n",
        "test_simple_kernel_timing()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80ab8f93",
      "metadata": {
        "cell_marker": "\"\"\"",
        "lines_to_next_cell": 1
      },
      "source": [
        "## Step 6: Compressed Model Kernels - Optimizing Quantized Operations\n",
        "\n",
        "### Why Compressed Model Kernels?\n",
        "Modern deployment requires smaller, faster models:\n",
        "- **Mobile devices**: Limited compute and memory\n",
        "- **Edge computing**: Real-time inference requirements\n",
        "- **Cloud costs**: Reduce computational expenses\n",
        "- **Energy efficiency**: Lower power consumption\n",
        "\n",
        "### Types of Model Compression\n",
        "1. **Quantization**: Reduce precision (float32 \u2192 int8)\n",
        "2. **Pruning**: Remove unimportant weights\n",
        "3. **Knowledge distillation**: Train smaller models\n",
        "4. **Low-rank approximation**: Factorize weight matrices\n",
        "\n",
        "### Quantization Fundamentals\n",
        "```python\n",
        "# Original: 32-bit floating point\n",
        "weights_fp32 = np.array([1.234, -0.567, 2.891])\n",
        "\n",
        "# Quantized: 8-bit integer\n",
        "scale = max(weights_fp32) / 127\n",
        "weights_int8 = np.round(weights_fp32 / scale).astype(np.int8)\n",
        "\n",
        "# Dequantized for computation\n",
        "weights_dequant = weights_int8 * scale\n",
        "```\n",
        "\n",
        "### Why Custom Kernels for Compression?\n",
        "- **Integer arithmetic**: Faster than floating-point on many devices\n",
        "- **Memory bandwidth**: 4x less data to transfer\n",
        "- **Specialized instructions**: CPUs have optimized int8 operations\n",
        "- **Accumulation**: Need to handle precision carefully\n",
        "\n",
        "### Real-World Context\n",
        "- **TensorFlow Lite**: Quantized inference kernels\n",
        "- **PyTorch Mobile**: Optimized int8 operations\n",
        "- **ONNX Runtime**: Hardware-specific quantized kernels\n",
        "- **Hardware accelerators**: TPUs, Neural Processing Units"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c2d4e23",
      "metadata": {
        "lines_to_next_cell": 1,
        "nbgrader": {
          "grade": false,
          "grade_id": "quantized-matmul",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "#| export\n",
        "def quantized_matmul(A: Tensor, B: Tensor, scale_A: float = 1.0, scale_B: float = 1.0) -> Tensor:\n",
        "    \"\"\"\n",
        "    Quantized matrix multiplication kernel for compressed models.\n",
        "    \n",
        "    This function demonstrates how to perform matrix multiplication\n",
        "    with quantized (int8) weights while maintaining numerical accuracy.\n",
        "    \n",
        "    TODO: Implement quantized matrix multiplication.\n",
        "    \n",
        "    STEP-BY-STEP IMPLEMENTATION:\n",
        "    1. Extract numpy arrays from Tensors\n",
        "    2. Quantize inputs to int8 using provided scales\n",
        "    3. Perform integer matrix multiplication\n",
        "    4. Rescale result back to appropriate range\n",
        "    5. Return result as Tensor\n",
        "    \n",
        "    QUANTIZATION PROCESS:\n",
        "    1. Quantize: int8_value = round(float_value / scale)\n",
        "    2. Compute: int8_result = int8_A @ int8_B\n",
        "    3. Rescale: float_result = int8_result * scale_A * scale_B\n",
        "    \n",
        "    EXAMPLE USAGE:\n",
        "    ```python\n",
        "    A = Tensor([[1.0, 2.0], [3.0, 4.0]])\n",
        "    B = Tensor([[0.5, 1.5], [2.5, 3.5]])\n",
        "    C = quantized_matmul(A, B, scale_A=1.0/127, scale_B=1.0/127)\n",
        "    # Should approximate regular matrix multiplication\n",
        "    ```\n",
        "    \n",
        "    PERFORMANCE CONSIDERATIONS:\n",
        "    - int8 operations are often faster than float32\n",
        "    - Memory usage is 4x lower\n",
        "    - Accumulation in int32 to prevent overflow\n",
        "    - Careful handling of scales to maintain precision\n",
        "    \n",
        "    LEARNING CONNECTIONS:\n",
        "    - This is how TensorFlow Lite performs quantized inference\n",
        "    - Similar to how mobile ML accelerators work\n",
        "    - Foundation for edge deployment of neural networks\n",
        "    \"\"\"\n",
        "    ### BEGIN SOLUTION\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "    ### END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f0138d7",
      "metadata": {
        "lines_to_next_cell": 1,
        "nbgrader": {
          "grade": false,
          "grade_id": "quantized-relu",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "#| export\n",
        "def quantized_relu(x: Tensor, scale: float = 1.0) -> Tensor:\n",
        "    \"\"\"\n",
        "    Quantized ReLU implementation for compressed models.\n",
        "    \n",
        "    This function shows how to apply ReLU activation to quantized values\n",
        "    while maintaining the quantization format.\n",
        "    \n",
        "    TODO: Implement quantized ReLU activation.\n",
        "    \n",
        "    STEP-BY-STEP IMPLEMENTATION:\n",
        "    1. Extract numpy array from Tensor\n",
        "    2. Quantize input to int8 using provided scale\n",
        "    3. Apply ReLU in integer domain: max(0, x)\n",
        "    4. Keep result in int8 format (no rescaling needed for ReLU)\n",
        "    5. Convert back to float using scale\n",
        "    6. Return result as Tensor\n",
        "    \n",
        "    QUANTIZED RELU PROCESS:\n",
        "    1. Quantize: int8_value = round(float_value / scale)\n",
        "    2. Apply ReLU: int8_result = max(0, int8_value)\n",
        "    3. Dequantize: float_result = int8_result * scale\n",
        "    \n",
        "    EXAMPLE USAGE:\n",
        "    ```python\n",
        "    x = Tensor([-1.0, 0.0, 1.0, 2.0])\n",
        "    y = quantized_relu(x, scale=1.0/127)\n",
        "    # Should produce [0.0, 0.0, 1.0, 2.0] (approximately)\n",
        "    ```\n",
        "    \n",
        "    OPTIMIZATION NOTES:\n",
        "    - ReLU in int8 is just max(0, x) - very fast\n",
        "    - No floating-point operations needed during activation\n",
        "    - Maintains quantization format throughout\n",
        "    - Can be vectorized efficiently\n",
        "    \n",
        "    LEARNING CONNECTIONS:\n",
        "    - This is how quantized neural networks maintain speed\n",
        "    - Similar to how mobile processors optimize ML inference\n",
        "    - Foundation for real-time edge computing applications\n",
        "    \"\"\"\n",
        "    ### BEGIN SOLUTION\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "    ### END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b6866a9",
      "metadata": {
        "lines_to_next_cell": 1,
        "nbgrader": {
          "grade": false,
          "grade_id": "test-compressed-kernels",
          "locked": false,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "### \ud83e\uddea Unit Test: Compressed Model Kernels\n",
        "\n",
        "def test_compressed_kernels():\n",
        "    \"\"\"Test compressed model kernel implementations.\"\"\"\n",
        "    print(\"\ud83d\udd2c Unit Test: Compressed Model Kernels...\")\n",
        "    \n",
        "    # Test quantized matrix multiplication\n",
        "    A = Tensor([[1.0, 2.0], [3.0, 4.0]])\n",
        "    B = Tensor([[0.5, 1.5], [2.5, 3.5]])\n",
        "    \n",
        "    # Regular matrix multiplication\n",
        "    C_regular = matmul_baseline(A, B)\n",
        "    \n",
        "    # Quantized matrix multiplication\n",
        "    # Use larger scales to prevent int8 overflow\n",
        "    scale_A = 1.0 / 20  # Max value 4.0 / (1/20) = 80, fits in int8\n",
        "    scale_B = 1.0 / 20  # Max value 3.5 / (1/20) = 70, fits in int8\n",
        "    C_quantized = quantized_matmul(A, B, scale_A, scale_B)\n",
        "    \n",
        "    # Should be approximately equal (some quantization error expected)\n",
        "    assert np.allclose(C_regular.data, C_quantized.data, rtol=0.1), \\\n",
        "        f\"Regular: {C_regular.data}, Quantized: {C_quantized.data}\"\n",
        "    print(\"\u2705 Quantized matrix multiplication works\")\n",
        "    \n",
        "    # Test quantized ReLU\n",
        "    x = Tensor([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
        "    \n",
        "    # Regular ReLU\n",
        "    y_regular = vectorized_relu(x)\n",
        "    \n",
        "    # Quantized ReLU\n",
        "    # Use larger scale to prevent int8 overflow\n",
        "    scale = 1.0 / 50  # Max value 2.0 / (1/50) = 100, fits in int8\n",
        "    y_quantized = quantized_relu(x, scale)\n",
        "    \n",
        "    # Should be approximately equal\n",
        "    assert np.allclose(y_regular.data, y_quantized.data, rtol=0.1), \\\n",
        "        f\"Regular: {y_regular.data}, Quantized: {y_quantized.data}\"\n",
        "    print(\"\u2705 Quantized ReLU works\")\n",
        "    \n",
        "    # Test that quantized operations can be timed\n",
        "    # This shows the performance characteristics of quantized vs regular operations\n",
        "    x_large = Tensor(np.random.randn(1000))\n",
        "    \n",
        "    # Time regular ReLU\n",
        "    _, time_regular = time_kernel(vectorized_relu, x_large)\n",
        "    \n",
        "    # Time quantized ReLU\n",
        "    _, time_quantized = time_kernel(quantized_relu, x_large, 1.0/127)\n",
        "    \n",
        "    print(f\"\ud83d\udd0d Regular ReLU: {time_regular:.1f} \u03bcs\")\n",
        "    print(f\"\ud83d\udd0d Quantized ReLU: {time_quantized:.1f} \u03bcs\")\n",
        "    \n",
        "    print(\"\u2705 Quantized operations timing works\")\n",
        "    print(\"\ud83d\udcc8 Progress: Compressed Model Kernels \u2713\")\n",
        "\n",
        "# Run the test\n",
        "test_compressed_kernels()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0709e1f3",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "final-performance-test",
          "locked": false,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "### \ud83e\uddea Unit Test: Comprehensive Kernel Performance Comparison\n",
        "\n",
        "def final_performance_test():\n",
        "    \"\"\"Comprehensive performance test of all implemented kernels.\"\"\"\n",
        "    print(\"\ud83d\udd2c Final Performance Test: Comprehensive Kernel Comparison\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Create test data\n",
        "    np.random.seed(42)\n",
        "    A = Tensor(np.random.randn(256, 256))\n",
        "    B = Tensor(np.random.randn(256, 256))\n",
        "    x = Tensor(np.random.randn(10000))\n",
        "    \n",
        "    print(\"\\n\ud83d\udcca Matrix Multiplication Performance:\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    # Test different matrix multiplication methods\n",
        "    methods = [\n",
        "        (\"NumPy\", lambda: Tensor(np.dot(A.data, B.data))),\n",
        "        (\"Baseline\", lambda: matmul_baseline(A, B)),\n",
        "        (\"Cache-friendly\", lambda: cache_friendly_matmul(A, B, 32)),\n",
        "        (\"Quantized\", lambda: quantized_matmul(A, B, 1.0/127, 1.0/127))\n",
        "    ]\n",
        "    \n",
        "    results = {}\n",
        "    for name, method in methods:\n",
        "        result, time_us = time_kernel(method)\n",
        "        results[name] = (result, time_us)\n",
        "        print(f\"{name:15}: {time_us:.1f} \u03bcs\")\n",
        "    \n",
        "    print(\"\\n\ud83d\udcca ReLU Activation Performance:\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    # Test different ReLU methods\n",
        "    relu_methods = [\n",
        "        (\"Vectorized\", lambda: vectorized_relu(x)),\n",
        "        (\"Parallel\", lambda: parallel_relu(x, 4)),\n",
        "        (\"Quantized\", lambda: quantized_relu(x, 1.0/127))\n",
        "    ]\n",
        "    \n",
        "    relu_results = {}\n",
        "    for name, method in relu_methods:\n",
        "        result, time_us = time_kernel(method)\n",
        "        relu_results[name] = (result, time_us)\n",
        "        print(f\"{name:15}: {time_us:.1f} \u03bcs\")\n",
        "    \n",
        "    print(\"\\n\u2705 All kernels implemented successfully!\")\n",
        "    print(\"\ud83d\udcc8 Progress: Complete Kernels Module \u2713\")\n",
        "    \n",
        "    # Verify correctness\n",
        "    print(\"\\n\ud83d\udd0d Correctness Verification:\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    # Check that all matrix multiplication methods produce similar results\n",
        "    base_result = results[\"NumPy\"][0]\n",
        "    for name, (result, _) in results.items():\n",
        "        if name != \"NumPy\":\n",
        "            if name == \"Quantized\":\n",
        "                # Skip quantized comparison in final test - already validated individually\n",
        "                print(f\"\u26a0\ufe0f  Skipping {name} comparison (quantization errors expected)\")\n",
        "            else:\n",
        "                assert np.allclose(base_result.data, result.data, rtol=1e-2), \\\n",
        "                    f\"{name} differs from NumPy\"\n",
        "    \n",
        "    # Check that all ReLU methods produce similar results\n",
        "    base_relu = relu_results[\"Vectorized\"][0]\n",
        "    for name, (result, _) in relu_results.items():\n",
        "        if name != \"Vectorized\":\n",
        "            if name == \"Quantized\":\n",
        "                # Skip quantized ReLU comparison - already validated individually\n",
        "                print(f\"\u26a0\ufe0f  Skipping {name} ReLU comparison (quantization errors expected)\")\n",
        "            else:\n",
        "                assert np.allclose(base_relu.data, result.data, rtol=1e-4), \\\n",
        "                    f\"{name} ReLU differs from vectorized\"\n",
        "    \n",
        "    print(\"\u2705 All implementations produce correct results!\")\n",
        "    \n",
        "    print(\"\\n\ud83c\udf89 CONGRATULATIONS! \ud83c\udf89\")\n",
        "    print(\"You've successfully implemented hardware-optimized ML kernels!\")\n",
        "    print(\"You now understand the performance optimizations that power modern AI frameworks.\")\n",
        "\n",
        "# Run the final test\n",
        "final_performance_test()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e056d4f3",
      "metadata": {
        "cell_marker": "\"\"\""
      },
      "source": [
        "## \ud83e\uddea Module Testing\n",
        "\n",
        "Time to test your implementation! This section uses TinyTorch's standardized testing framework to ensure your implementation works correctly.\n",
        "\n",
        "**This testing section is locked** - it provides consistent feedback across all modules and cannot be modified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a567b75e",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "standardized-testing",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# STANDARDIZED MODULE TESTING - DO NOT MODIFY\n",
        "# This cell is locked to ensure consistent testing across all TinyTorch modules\n",
        "# =============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    from tito.tools.testing import run_module_tests_auto\n",
        "    \n",
        "    # Automatically discover and run all tests in this module\n",
        "    success = run_module_tests_auto(\"Kernels\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd8aa378",
      "metadata": {
        "cell_marker": "\"\"\""
      },
      "source": [
        "## \ud83c\udfaf Module Summary: Hardware-Optimized ML Operations\n",
        "\n",
        "### What You've Built\n",
        "You've implemented a complete set of hardware-optimized ML kernels:\n",
        "\n",
        "1. **Custom Operations**: Specialized matrix multiplication beyond NumPy\n",
        "2. **Vectorized Operations**: SIMD-optimized ReLU and element-wise operations\n",
        "3. **Cache-Friendly Algorithms**: Blocked matrix multiplication for better memory access\n",
        "4. **Parallel Processing**: Multi-core CPU utilization for large operations\n",
        "5. **Performance Profiling**: Tools to measure and optimize kernel performance\n",
        "6. **Compressed Kernels**: Quantized operations for mobile deployment\n",
        "\n",
        "### Key Insights\n",
        "- **Specialization beats generalization**: Custom kernels outperform generic libraries\n",
        "- **Memory is the bottleneck**: Cache-friendly algorithms are crucial\n",
        "- **Parallelism is everywhere**: From SIMD to multi-core to GPU-style processing\n",
        "- **Measurement drives optimization**: Profile first, optimize second\n",
        "- **Compression enables deployment**: Quantized models run faster with less memory\n",
        "\n",
        "### Real-World Connections\n",
        "- **PyTorch**: Uses thousands of optimized kernels for speed\n",
        "- **TensorFlow**: XLA compiler generates specialized kernels\n",
        "- **Mobile ML**: Quantized kernels enable edge deployment\n",
        "- **Cloud computing**: Kernel optimization reduces server costs\n",
        "- **Research**: Custom kernels enable larger models and faster experimentation\n",
        "\n",
        "### Next Steps\n",
        "In real ML systems, you'd:\n",
        "1. **GPU kernels**: Implement CUDA/OpenCL versions\n",
        "2. **Auto-tuning**: Automatically find optimal parameters\n",
        "3. **Hardware specialization**: Optimize for specific processors\n",
        "4. **Kernel fusion**: Combine multiple operations into single kernels\n",
        "5. **Distributed computing**: Scale kernels across multiple machines\n",
        "\n",
        "### \ud83c\udfc6 Achievement Unlocked\n",
        "You've mastered the performance optimization techniques that power modern ML frameworks. You understand how to move beyond high-level libraries to extract maximum performance from hardware!\n",
        "\n",
        "**You've completed the TinyTorch Kernels module!** \ud83c\udf89"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "main_language": "python"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8+"
    },
    "mystnb": {
      "execution_mode": "auto"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}