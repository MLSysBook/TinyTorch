{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MLOps\n",
        "\n",
        "```{admonition} Interactive Learning\n",
        ":class: tip\n",
        "\ud83d\ude80 **Launch Binder**: Click the rocket icon above to run this chapter interactively!\n",
        "\n",
        "\ud83d\udcbe **Save Your Work**: Download your completed notebook when done.\n",
        "\n",
        "\ud83c\udfd7\ufe0f **Build Locally**: Ready for serious development? [Fork the repo](https://github.com/your-org/tinytorch) and work locally with the full `tito` workflow.\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7a8252d",
      "metadata": {
        "lines_to_next_cell": 1,
        "nbgrader": {
          "grade": false,
          "grade_id": "mlops-imports",
          "locked": false,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "#| default_exp core.mlops\n",
        "\n",
        "#| export\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "from typing import Dict, List, Tuple, Optional, Any, Callable\n",
        "from dataclasses import dataclass, field\n",
        "from datetime import datetime, timedelta\n",
        "from collections import defaultdict\n",
        "\n",
        "# Import our dependencies - try from package first, then local modules\n",
        "try:\n",
        "    from tinytorch.core.tensor import Tensor\n",
        "    from tinytorch.core.training import Trainer, MeanSquaredError, CrossEntropyLoss, Accuracy\n",
        "    from tinytorch.core.benchmarking import TinyTorchPerf, StatisticalValidator\n",
        "    from tinytorch.core.compression import quantize_layer_weights, prune_weights_by_magnitude\n",
        "    from tinytorch.core.networks import Sequential\n",
        "    from tinytorch.core.layers import Dense\n",
        "    from tinytorch.core.activations import ReLU, Sigmoid, Softmax\n",
        "except ImportError:\n",
        "    # For development, import from local modules\n",
        "    sys.path.append(os.path.join(os.path.dirname(__file__), '..', '01_tensor'))\n",
        "    sys.path.append(os.path.join(os.path.dirname(__file__), '..', '09_training'))\n",
        "    sys.path.append(os.path.join(os.path.dirname(__file__), '..', '12_benchmarking'))\n",
        "    sys.path.append(os.path.join(os.path.dirname(__file__), '..', '10_compression'))\n",
        "    sys.path.append(os.path.join(os.path.dirname(__file__), '..', '04_networks'))\n",
        "    sys.path.append(os.path.join(os.path.dirname(__file__), '..', '03_layers'))\n",
        "    sys.path.append(os.path.join(os.path.dirname(__file__), '..', '02_activations'))\n",
        "    try:\n",
        "        from tensor_dev import Tensor\n",
        "        from training_dev import Trainer, MeanSquaredError, CrossEntropyLoss, Accuracy\n",
        "        from benchmarking_dev import TinyTorchPerf, StatisticalValidator\n",
        "        from compression_dev import quantize_layer_weights, prune_weights_by_magnitude\n",
        "        from networks_dev import Sequential\n",
        "        from layers_dev import Dense\n",
        "        from activations_dev import ReLU, Sigmoid, Softmax\n",
        "    except ImportError:\n",
        "        print(\"\u26a0\ufe0f  Development imports failed - some functionality may be limited\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af7550a3",
      "metadata": {
        "lines_to_next_cell": 1,
        "nbgrader": {
          "grade": false,
          "grade_id": "mlops-setup",
          "locked": false,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "#| hide\n",
        "#| export\n",
        "def _should_show_plots():\n",
        "    \"\"\"Check if we should show plots (disable during testing)\"\"\"\n",
        "    # Check multiple conditions that indicate we're in test mode\n",
        "    is_pytest = (\n",
        "        'pytest' in sys.modules or\n",
        "        'test' in sys.argv or\n",
        "        os.environ.get('PYTEST_CURRENT_TEST') is not None or\n",
        "        any('test' in arg for arg in sys.argv) or\n",
        "        any('pytest' in arg for arg in sys.argv)\n",
        "    )\n",
        "    \n",
        "    # Show plots in development mode (when not in test mode)\n",
        "    return not is_pytest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c25e95b",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "mlops-welcome",
          "locked": false,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "print(\"\ud83d\ude80 TinyTorch MLOps Module\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"Python version: {sys.version_info.major}.{sys.version_info.minor}\")\n",
        "print(\"Ready to build production ML systems!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58f4b58e",
      "metadata": {
        "cell_marker": "\"\"\""
      },
      "source": [
        "## \ud83d\udce6 Where This Code Lives in the Final Package\n",
        "\n",
        "**Learning Side:** You work in `modules/source/13_mlops/mlops_dev.py`  \n",
        "**Building Side:** Code exports to `tinytorch.core.mlops`\n",
        "\n",
        "```python\n",
        "# Final package structure:\n",
        "from tinytorch.core.mlops import ModelMonitor, DriftDetector, MLOpsPipeline\n",
        "from tinytorch.core.training import Trainer  # Reuse your training system\n",
        "from tinytorch.core.benchmarking import TinyTorchPerf  # Reuse your benchmarking\n",
        "from tinytorch.core.compression import quantize_layer_weights  # Reuse compression\n",
        "```\n",
        "\n",
        "**Why this matters:**\n",
        "- **Integration:** MLOps orchestrates all TinyTorch components\n",
        "- **Reusability:** Uses everything you've built in previous modules\n",
        "- **Production:** Real-world ML system lifecycle management\n",
        "- **Maintainability:** Systems that keep working over time"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99f32ad0",
      "metadata": {
        "cell_marker": "\"\"\""
      },
      "source": [
        "## What is MLOps?\n",
        "\n",
        "### The Production Reality: Models Degrade Over Time\n",
        "You've built an amazing ML system:\n",
        "- **Training pipeline**: Produces high-quality models\n",
        "- **Compression**: Optimizes models for deployment\n",
        "- **Kernels**: Accelerates inference\n",
        "- **Benchmarking**: Measures performance\n",
        "\n",
        "But there's a critical problem: **Models degrade over time without maintenance.**\n",
        "\n",
        "### Why Models Fail in Production\n",
        "1. **Data drift**: Input data distribution changes\n",
        "2. **Concept drift**: Relationship between inputs and outputs changes\n",
        "3. **Performance degradation**: Accuracy drops over time\n",
        "4. **System changes**: Infrastructure updates break assumptions\n",
        "\n",
        "### The MLOps Solution\n",
        "**MLOps** (Machine Learning Operations) is the practice of maintaining ML systems in production:\n",
        "- **Monitor**: Track model performance continuously\n",
        "- **Detect**: Identify when models are failing\n",
        "- **Respond**: Automatically retrain and redeploy\n",
        "- **Validate**: Ensure new models are actually better\n",
        "\n",
        "### Real-World Examples\n",
        "- **Netflix**: Recommendation models retrain when viewing patterns change\n",
        "- **Uber**: Demand prediction models adapt to new cities and events\n",
        "- **Google**: Search ranking models update as web content evolves\n",
        "- **Tesla**: Autonomous driving models improve with new driving data\n",
        "\n",
        "### The Complete TinyTorch Lifecycle\n",
        "```\n",
        "Data \u2192 Training \u2192 Compression \u2192 Kernels \u2192 Benchmarking \u2192 Monitor \u2192 Detect \u2192 Retrain \u2192 Deploy\n",
        "                                                             \u2191__________________________|\n",
        "```\n",
        "\n",
        "MLOps closes this loop, creating **self-maintaining systems**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3107143",
      "metadata": {
        "cell_marker": "\"\"\"",
        "lines_to_next_cell": 1
      },
      "source": [
        "## Step 1: Performance Drift Monitor - Tracking Model Health\n",
        "\n",
        "### The Problem: Silent Model Degradation\n",
        "Without monitoring, you won't know when your model stops working:\n",
        "- **Accuracy drops** from 95% to 85% over 3 months\n",
        "- **Latency increases** as data patterns change\n",
        "- **System failures** go unnoticed until user complaints\n",
        "\n",
        "### The Solution: Continuous Performance Monitoring\n",
        "Track key metrics over time:\n",
        "- **Accuracy/Error rates**: Primary model performance\n",
        "- **Latency/Throughput**: System performance\n",
        "- **Data statistics**: Input distribution changes\n",
        "- **System health**: Infrastructure metrics\n",
        "\n",
        "### What We'll Build\n",
        "A `ModelMonitor` that:\n",
        "1. **Tracks performance** over time\n",
        "2. **Stores metric history** for trend analysis\n",
        "3. **Detects degradation** when metrics drop\n",
        "4. **Alerts** when thresholds are crossed\n",
        "\n",
        "### Real-World Applications\n",
        "- **E-commerce**: Monitor recommendation click-through rates\n",
        "- **Finance**: Track fraud detection false positive rates\n",
        "- **Healthcare**: Monitor diagnostic accuracy over time\n",
        "- **Autonomous vehicles**: Track object detection confidence scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81c4abe9",
      "metadata": {
        "lines_to_next_cell": 1,
        "nbgrader": {
          "grade": false,
          "grade_id": "model-monitor",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "#| export\n",
        "@dataclass\n",
        "class ModelMonitor:\n",
        "    \"\"\"\n",
        "    Monitors ML model performance over time and detects degradation.\n",
        "    \n",
        "    Tracks key metrics, stores history, and alerts when performance drops.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model_name: str, baseline_accuracy: float = 0.95):\n",
        "        \"\"\"\n",
        "        TODO: Initialize the ModelMonitor for tracking model performance.\n",
        "        \n",
        "        STEP-BY-STEP IMPLEMENTATION:\n",
        "        1. Store the model_name and baseline_accuracy\n",
        "        2. Create empty lists to store metric history:\n",
        "           - accuracy_history: List[float] \n",
        "           - latency_history: List[float]\n",
        "           - timestamp_history: List[datetime]\n",
        "        3. Set performance thresholds:\n",
        "           - accuracy_threshold: baseline_accuracy * 0.9 (10% drop triggers alert)\n",
        "           - latency_threshold: 200.0 (milliseconds)\n",
        "        4. Initialize alert flags:\n",
        "           - accuracy_alert: False\n",
        "           - latency_alert: False\n",
        "        \n",
        "        EXAMPLE USAGE:\n",
        "        ```python\n",
        "        monitor = ModelMonitor(\"image_classifier\", baseline_accuracy=0.93)\n",
        "        monitor.record_performance(accuracy=0.92, latency=150.0)\n",
        "        alerts = monitor.check_alerts()\n",
        "        ```\n",
        "        \n",
        "        IMPLEMENTATION HINTS:\n",
        "        - Use self.model_name = model_name\n",
        "        - Initialize lists with self.accuracy_history = []\n",
        "        - Use datetime.now() for timestamps\n",
        "        - Set thresholds relative to baseline (e.g., 90% of baseline)\n",
        "        \n",
        "        LEARNING CONNECTIONS:\n",
        "        - This builds on benchmarking concepts from Module 12\n",
        "        - Performance tracking is essential for production systems\n",
        "        - Thresholds prevent false alarms while catching real issues\n",
        "        \"\"\"\n",
        "        ### BEGIN SOLUTION\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "        ### END SOLUTION\n",
        "    \n",
        "    def record_performance(self, accuracy: float, latency: float):\n",
        "        \"\"\"\n",
        "        TODO: Record a new performance measurement.\n",
        "        \n",
        "        STEP-BY-STEP IMPLEMENTATION:\n",
        "        1. Get current timestamp with datetime.now()\n",
        "        2. Append accuracy to self.accuracy_history\n",
        "        3. Append latency to self.latency_history\n",
        "        4. Append timestamp to self.timestamp_history\n",
        "        5. Check if accuracy is below threshold:\n",
        "           - If accuracy < self.accuracy_threshold: set self.accuracy_alert = True\n",
        "           - Else: set self.accuracy_alert = False\n",
        "        6. Check if latency is above threshold:\n",
        "           - If latency > self.latency_threshold: set self.latency_alert = True\n",
        "           - Else: set self.latency_alert = False\n",
        "        \n",
        "        EXAMPLE BEHAVIOR:\n",
        "        ```python\n",
        "        monitor.record_performance(0.94, 120.0)  # Good performance\n",
        "        monitor.record_performance(0.84, 250.0)  # Triggers both alerts\n",
        "        ```\n",
        "        \n",
        "        IMPLEMENTATION HINTS:\n",
        "        - Use datetime.now() for timestamps\n",
        "        - Update alert flags based on current measurement\n",
        "        - Don't forget to store all three values (accuracy, latency, timestamp)\n",
        "        \"\"\"\n",
        "        ### BEGIN SOLUTION\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "        ### END SOLUTION\n",
        "    \n",
        "    def check_alerts(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        TODO: Check current alert status and return alert information.\n",
        "        \n",
        "        STEP-BY-STEP IMPLEMENTATION:\n",
        "        1. Create result dictionary with basic info:\n",
        "           - \"model_name\": self.model_name\n",
        "           - \"accuracy_alert\": self.accuracy_alert\n",
        "           - \"latency_alert\": self.latency_alert\n",
        "        2. If accuracy_alert is True, add:\n",
        "           - \"accuracy_message\": f\"Accuracy below threshold: {current_accuracy:.3f} < {self.accuracy_threshold:.3f}\"\n",
        "           - \"current_accuracy\": most recent accuracy from history\n",
        "        3. If latency_alert is True, add:\n",
        "           - \"latency_message\": f\"Latency above threshold: {current_latency:.1f}ms > {self.latency_threshold:.1f}ms\"\n",
        "           - \"current_latency\": most recent latency from history\n",
        "        4. Add overall alert status:\n",
        "           - \"any_alerts\": True if any alert is active\n",
        "        \n",
        "        EXAMPLE RETURN:\n",
        "        ```python\n",
        "        {\n",
        "            \"model_name\": \"image_classifier\",\n",
        "            \"accuracy_alert\": True,\n",
        "            \"latency_alert\": False,\n",
        "            \"accuracy_message\": \"Accuracy below threshold: 0.840 < 0.855\",\n",
        "            \"current_accuracy\": 0.840,\n",
        "            \"any_alerts\": True\n",
        "        }\n",
        "        ```\n",
        "        \n",
        "        IMPLEMENTATION HINTS:\n",
        "        - Use self.accuracy_history[-1] for most recent values\n",
        "        - Format numbers with f-strings for readability\n",
        "        - Include both alert flags and descriptive messages\n",
        "        \"\"\"\n",
        "        ### BEGIN SOLUTION\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "        ### END SOLUTION\n",
        "    \n",
        "    def get_performance_trend(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        TODO: Analyze performance trends over time.\n",
        "        \n",
        "        STEP-BY-STEP IMPLEMENTATION:\n",
        "        1. Check if we have enough data (at least 2 measurements)\n",
        "        2. Calculate accuracy trend:\n",
        "           - If accuracy_history has < 2 points: trend = \"insufficient_data\"\n",
        "           - Else: compare recent avg (last 3) vs older avg (first 3)\n",
        "           - If recent > older: trend = \"improving\"\n",
        "           - If recent < older: trend = \"degrading\"\n",
        "           - Else: trend = \"stable\"\n",
        "        3. Calculate similar trend for latency\n",
        "        4. Return dictionary with:\n",
        "           - \"measurements_count\": len(self.accuracy_history)\n",
        "           - \"accuracy_trend\": trend analysis\n",
        "           - \"latency_trend\": trend analysis\n",
        "           - \"baseline_accuracy\": self.baseline_accuracy\n",
        "           - \"current_accuracy\": most recent accuracy (if available)\n",
        "        \n",
        "        EXAMPLE RETURN:\n",
        "        ```python\n",
        "        {\n",
        "            \"measurements_count\": 10,\n",
        "            \"accuracy_trend\": \"degrading\",\n",
        "            \"latency_trend\": \"stable\",\n",
        "            \"baseline_accuracy\": 0.95,\n",
        "            \"current_accuracy\": 0.87\n",
        "        }\n",
        "        ```\n",
        "        \n",
        "        IMPLEMENTATION HINTS:\n",
        "        - Use len(self.accuracy_history) for data count\n",
        "        - Use np.mean() for calculating averages\n",
        "        - Handle edge cases (empty history, insufficient data)\n",
        "        \"\"\"\n",
        "        ### BEGIN SOLUTION\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "        ### END SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d42890ec",
      "metadata": {
        "cell_marker": "\"\"\"",
        "lines_to_next_cell": 1
      },
      "source": [
        "### \ud83e\uddea Test Your Performance Monitor\n",
        "\n",
        "Once you implement the `ModelMonitor` class above, run this cell to test it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78aacc89",
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "test-model-monitor",
          "locked": true,
          "points": 20,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def test_model_monitor():\n",
        "    \"\"\"Test ModelMonitor implementation\"\"\"\n",
        "    print(\"\ud83d\udd2c Unit Test: Performance Drift Monitor...\")\n",
        "    \n",
        "    # Test initialization\n",
        "    monitor = ModelMonitor(\"test_model\", baseline_accuracy=0.90)\n",
        "    \n",
        "    assert monitor.model_name == \"test_model\"\n",
        "    assert monitor.baseline_accuracy == 0.90\n",
        "    assert monitor.accuracy_threshold == 0.81  # 90% of 0.90\n",
        "    assert monitor.latency_threshold == 200.0\n",
        "    assert not monitor.accuracy_alert\n",
        "    assert not monitor.latency_alert\n",
        "    \n",
        "    # Test good performance (no alerts)\n",
        "    monitor.record_performance(accuracy=0.92, latency=150.0)\n",
        "    \n",
        "    alerts = monitor.check_alerts()\n",
        "    assert not alerts[\"accuracy_alert\"]\n",
        "    assert not alerts[\"latency_alert\"]\n",
        "    assert not alerts[\"any_alerts\"]\n",
        "    \n",
        "    # Test accuracy degradation\n",
        "    monitor.record_performance(accuracy=0.80, latency=150.0)  # Below threshold\n",
        "    \n",
        "    alerts = monitor.check_alerts()\n",
        "    assert alerts[\"accuracy_alert\"]\n",
        "    assert not alerts[\"latency_alert\"]\n",
        "    assert alerts[\"any_alerts\"]\n",
        "    assert \"Accuracy below threshold\" in alerts[\"accuracy_message\"]\n",
        "    \n",
        "    # Test latency degradation\n",
        "    monitor.record_performance(accuracy=0.85, latency=250.0)  # Above threshold\n",
        "    \n",
        "    alerts = monitor.check_alerts()\n",
        "    assert not alerts[\"accuracy_alert\"]  # Back above threshold\n",
        "    assert alerts[\"latency_alert\"]\n",
        "    assert alerts[\"any_alerts\"]\n",
        "    assert \"Latency above threshold\" in alerts[\"latency_message\"]\n",
        "    \n",
        "    # Test trend analysis\n",
        "    # Add more measurements to test trends\n",
        "    for i in range(5):\n",
        "        monitor.record_performance(accuracy=0.90 - i*0.02, latency=120.0 + i*10)\n",
        "    \n",
        "    trend = monitor.get_performance_trend()\n",
        "    assert trend[\"measurements_count\"] >= 5\n",
        "    assert trend[\"accuracy_trend\"] in [\"improving\", \"degrading\", \"stable\"]\n",
        "    assert trend[\"latency_trend\"] in [\"improving\", \"degrading\", \"stable\"]\n",
        "    assert trend[\"baseline_accuracy\"] == 0.90\n",
        "    \n",
        "    print(\"\u2705 ModelMonitor initialization works correctly\")\n",
        "    print(\"\u2705 Performance recording and alert detection work\")\n",
        "    print(\"\u2705 Alert checking returns proper format\")\n",
        "    print(\"\u2705 Trend analysis provides meaningful insights\")\n",
        "    print(\"\ud83d\udcc8 Progress: Performance Drift Monitor \u2713\")\n",
        "\n",
        "# Run the test\n",
        "test_model_monitor()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1041115",
      "metadata": {
        "cell_marker": "\"\"\"",
        "lines_to_next_cell": 1
      },
      "source": [
        "## Step 2: Simple Drift Detection - Detecting Data Changes\n",
        "\n",
        "### The Problem: Silent Data Distribution Changes\n",
        "Your model was trained on specific data patterns, but production data evolves:\n",
        "- **Seasonal changes**: E-commerce traffic patterns change during holidays\n",
        "- **User behavior shifts**: App usage patterns evolve over time\n",
        "- **External factors**: Economic conditions affect financial predictions\n",
        "- **System changes**: New data sources introduce different distributions\n",
        "\n",
        "### The Solution: Statistical Drift Detection\n",
        "Compare current data to baseline data using statistical tests:\n",
        "- **Kolmogorov-Smirnov test**: Detects distribution changes\n",
        "- **Mean/Standard deviation shifts**: Simple but effective\n",
        "- **Population stability index**: Common in industry\n",
        "- **Chi-square test**: For categorical features\n",
        "\n",
        "### What We'll Build\n",
        "A `DriftDetector` that:\n",
        "1. **Stores baseline data** from training time\n",
        "2. **Compares new data** to baseline using statistical tests\n",
        "3. **Detects significant changes** in distribution\n",
        "4. **Provides interpretable results** for debugging\n",
        "\n",
        "### Real-World Applications\n",
        "- **Fraud detection**: New fraud patterns emerge constantly\n",
        "- **Recommendation systems**: User preferences shift over time\n",
        "- **Medical diagnosis**: Patient demographics change\n",
        "- **Computer vision**: Camera quality, lighting conditions evolve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77aaa3e0",
      "metadata": {
        "lines_to_next_cell": 1,
        "nbgrader": {
          "grade": false,
          "grade_id": "drift-detector",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "#| export\n",
        "class DriftDetector:\n",
        "    \"\"\"\n",
        "    Detects data drift by comparing current data distributions to baseline.\n",
        "    \n",
        "    Uses statistical tests to identify significant changes in data patterns.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, baseline_data: np.ndarray, feature_names: Optional[List[str]] = None):\n",
        "        \"\"\"\n",
        "        TODO: Initialize the DriftDetector with baseline data.\n",
        "        \n",
        "        STEP-BY-STEP IMPLEMENTATION:\n",
        "        1. Store baseline_data and feature_names\n",
        "        2. Calculate baseline statistics:\n",
        "           - baseline_mean: np.mean(baseline_data, axis=0)\n",
        "           - baseline_std: np.std(baseline_data, axis=0)\n",
        "           - baseline_min: np.min(baseline_data, axis=0)\n",
        "           - baseline_max: np.max(baseline_data, axis=0)\n",
        "        3. Set drift detection threshold (default: 0.05 for 95% confidence)\n",
        "        4. Initialize drift history storage:\n",
        "           - drift_history: List[Dict] to store drift test results\n",
        "        \n",
        "        EXAMPLE USAGE:\n",
        "        ```python\n",
        "        baseline = np.random.normal(0, 1, (1000, 3))\n",
        "        detector = DriftDetector(baseline, [\"feature1\", \"feature2\", \"feature3\"])\n",
        "        drift_result = detector.detect_drift(new_data)\n",
        "        ```\n",
        "        \n",
        "        IMPLEMENTATION HINTS:\n",
        "        - Use axis=0 for column-wise statistics\n",
        "        - Handle case when feature_names is None\n",
        "        - Store original baseline_data for KS test\n",
        "        - Set significance level (alpha) to 0.05\n",
        "        \"\"\"\n",
        "        ### BEGIN SOLUTION\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "        ### END SOLUTION\n",
        "    \n",
        "    def detect_drift(self, new_data: np.ndarray) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        TODO: Detect drift by comparing new data to baseline.\n",
        "        \n",
        "        STEP-BY-STEP IMPLEMENTATION:\n",
        "        1. Calculate new data statistics:\n",
        "           - new_mean, new_std, new_min, new_max (same as baseline)\n",
        "        2. Perform statistical tests for each feature:\n",
        "           - KS test: from scipy.stats import ks_2samp (if available)\n",
        "           - Mean shift test: |new_mean - baseline_mean| / baseline_std > 2\n",
        "           - Std shift test: |new_std - baseline_std| / baseline_std > 0.5\n",
        "        3. Create result dictionary:\n",
        "           - \"drift_detected\": True if any feature shows drift\n",
        "           - \"feature_drift\": Dict with per-feature results\n",
        "           - \"summary\": Overall drift description\n",
        "        4. Store result in drift_history\n",
        "        \n",
        "        EXAMPLE RETURN:\n",
        "        ```python\n",
        "        {\n",
        "            \"drift_detected\": True,\n",
        "            \"feature_drift\": {\n",
        "                \"feature1\": {\"mean_drift\": True, \"std_drift\": False, \"ks_pvalue\": 0.001},\n",
        "                \"feature2\": {\"mean_drift\": False, \"std_drift\": True, \"ks_pvalue\": 0.3}\n",
        "            },\n",
        "            \"summary\": \"Drift detected in 2/3 features\"\n",
        "        }\n",
        "        ```\n",
        "        \n",
        "        IMPLEMENTATION HINTS:\n",
        "        - Use try-except for KS test (may not be available)\n",
        "        - Check each feature individually\n",
        "        - Use absolute values for difference checks\n",
        "        - Count how many features show drift\n",
        "        \"\"\"\n",
        "        ### BEGIN SOLUTION\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "        ### END SOLUTION\n",
        "    \n",
        "    def get_drift_history(self) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        TODO: Return the complete drift detection history.\n",
        "        \n",
        "        STEP-BY-STEP IMPLEMENTATION:\n",
        "        1. Return self.drift_history\n",
        "        2. Include timestamp and result for each detection\n",
        "        3. Format for easy analysis\n",
        "        \n",
        "        EXAMPLE RETURN:\n",
        "        ```python\n",
        "        [\n",
        "            {\n",
        "                \"timestamp\": datetime(2024, 1, 1, 12, 0),\n",
        "                \"result\": {\"drift_detected\": False, \"drift_count\": 0, ...}\n",
        "            },\n",
        "            {\n",
        "                \"timestamp\": datetime(2024, 1, 2, 12, 0),\n",
        "                \"result\": {\"drift_detected\": True, \"drift_count\": 2, ...}\n",
        "            }\n",
        "        ]\n",
        "        ```\n",
        "        \"\"\"\n",
        "        ### BEGIN SOLUTION\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "        ### END SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47f9a320",
      "metadata": {
        "cell_marker": "\"\"\"",
        "lines_to_next_cell": 1
      },
      "source": [
        "### \ud83e\uddea Test Your Drift Detector\n",
        "\n",
        "Once you implement the `DriftDetector` class above, run this cell to test it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80335034",
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "test-drift-detector",
          "locked": true,
          "points": 20,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def test_drift_detector():\n",
        "    \"\"\"Test DriftDetector implementation\"\"\"\n",
        "    print(\"\ud83d\udd2c Unit Test: Simple Drift Detection...\")\n",
        "    \n",
        "    # Create baseline data\n",
        "    np.random.seed(42)\n",
        "    baseline_data = np.random.normal(0, 1, (1000, 3))\n",
        "    feature_names = [\"feature1\", \"feature2\", \"feature3\"]\n",
        "    \n",
        "    detector = DriftDetector(baseline_data, feature_names)\n",
        "    \n",
        "    # Test initialization\n",
        "    assert detector.baseline_data.shape == (1000, 3)\n",
        "    assert len(detector.feature_names) == 3\n",
        "    assert detector.feature_names == feature_names\n",
        "    assert detector.significance_level == 0.05\n",
        "    \n",
        "    # Test no drift (similar data)\n",
        "    no_drift_data = np.random.normal(0, 1, (500, 3))\n",
        "    result = detector.detect_drift(no_drift_data)\n",
        "    \n",
        "    assert \"drift_detected\" in result\n",
        "    assert \"feature_drift\" in result\n",
        "    assert \"summary\" in result\n",
        "    assert len(result[\"feature_drift\"]) == 3\n",
        "    \n",
        "    # Test clear drift (shifted data)\n",
        "    drift_data = np.random.normal(3, 1, (500, 3))  # Mean shifted by 3\n",
        "    result = detector.detect_drift(drift_data)\n",
        "    \n",
        "    assert result[\"drift_detected\"] == True\n",
        "    assert result[\"drift_count\"] > 0\n",
        "    assert \"Drift detected\" in result[\"summary\"]\n",
        "    \n",
        "    # Check feature-level drift detection\n",
        "    for feature_name in feature_names:\n",
        "        feature_result = result[\"feature_drift\"][feature_name]\n",
        "        assert \"mean_drift\" in feature_result\n",
        "        assert \"std_drift\" in feature_result\n",
        "        assert \"mean_change\" in feature_result\n",
        "    \n",
        "    # Test drift history\n",
        "    history = detector.get_drift_history()\n",
        "    assert len(history) >= 2  # At least 2 drift checks\n",
        "    assert all(\"timestamp\" in entry for entry in history)\n",
        "    assert all(\"result\" in entry for entry in history)\n",
        "    \n",
        "    print(\"\u2705 DriftDetector initialization works correctly\")\n",
        "    print(\"\u2705 No-drift detection works (similar data)\")\n",
        "    print(\"\u2705 Clear drift detection works (shifted data)\")\n",
        "    print(\"\u2705 Feature-level drift analysis works\")\n",
        "    print(\"\u2705 Drift history tracking works\")\n",
        "    print(\"\ud83d\udcc8 Progress: Simple Drift Detection \u2713\")\n",
        "\n",
        "# Run the test\n",
        "test_drift_detector()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "033ca8cd",
      "metadata": {
        "cell_marker": "\"\"\"",
        "lines_to_next_cell": 1
      },
      "source": [
        "## Step 3: Retraining Trigger System - Automated Response to Issues\n",
        "\n",
        "### The Problem: Manual Intervention Required\n",
        "You can detect when models are failing, but someone needs to:\n",
        "- **Notice the alerts** (requires constant monitoring)\n",
        "- **Decide to retrain** (requires domain expertise)\n",
        "- **Execute retraining** (requires technical knowledge)\n",
        "- **Validate results** (requires ML expertise)\n",
        "\n",
        "### The Solution: Automated Retraining Pipeline\n",
        "Create a system that automatically responds to performance degradation:\n",
        "- **Threshold-based triggers**: Automatically start retraining when performance drops\n",
        "- **Reuse existing components**: Use your training pipeline from Module 09\n",
        "- **Intelligent scheduling**: Avoid unnecessary retraining\n",
        "- **Validation before deployment**: Ensure new models are actually better\n",
        "\n",
        "### What We'll Build\n",
        "A `RetrainingTrigger` that:\n",
        "1. **Monitors model performance** using ModelMonitor\n",
        "2. **Detects drift** using DriftDetector\n",
        "3. **Triggers retraining** when conditions are met\n",
        "4. **Orchestrates the process** using existing TinyTorch components\n",
        "\n",
        "### Real-World Applications\n",
        "- **A/B testing platforms**: Automatically update models based on performance\n",
        "- **Recommendation engines**: Retrain when user behavior changes\n",
        "- **Fraud detection**: Adapt to new fraud patterns automatically\n",
        "- **Predictive maintenance**: Update models as equipment ages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8a41682",
      "metadata": {
        "lines_to_next_cell": 1,
        "nbgrader": {
          "grade": false,
          "grade_id": "retraining-trigger",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "#| export\n",
        "class RetrainingTrigger:\n",
        "    \"\"\"\n",
        "    Automated retraining system that responds to model performance degradation.\n",
        "    \n",
        "    Orchestrates the complete retraining workflow using existing TinyTorch components.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model, training_data, validation_data, trainer_class=None):\n",
        "        \"\"\"\n",
        "        TODO: Initialize the RetrainingTrigger system.\n",
        "        \n",
        "        STEP-BY-STEP IMPLEMENTATION:\n",
        "        1. Store the model, training_data, and validation_data\n",
        "        2. Set up the trainer_class (use provided or default to simple trainer)\n",
        "        3. Initialize trigger conditions:\n",
        "           - accuracy_threshold: 0.85 (trigger retraining if accuracy < 85%)\n",
        "           - drift_threshold: 2 (trigger if drift detected in 2+ features)\n",
        "           - min_time_between_retrains: 24 hours (avoid too frequent retraining)\n",
        "        4. Initialize tracking variables:\n",
        "           - last_retrain_time: datetime.now()\n",
        "           - retrain_history: List[Dict] to store retraining results\n",
        "        \n",
        "        EXAMPLE USAGE:\n",
        "        ```python\n",
        "        trigger = RetrainingTrigger(model, train_data, val_data)\n",
        "        should_retrain = trigger.check_trigger_conditions(monitor, drift_detector)\n",
        "        if should_retrain:\n",
        "            new_model = trigger.execute_retraining()\n",
        "        ```\n",
        "        \n",
        "        IMPLEMENTATION HINTS:\n",
        "        - Store references to data for retraining\n",
        "        - Set reasonable default thresholds\n",
        "        - Use datetime for time tracking\n",
        "        - Initialize empty history list\n",
        "        \"\"\"\n",
        "        ### BEGIN SOLUTION\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "        ### END SOLUTION\n",
        "    \n",
        "    def check_trigger_conditions(self, monitor: ModelMonitor, drift_detector: DriftDetector) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        TODO: Check if retraining should be triggered.\n",
        "        \n",
        "        STEP-BY-STEP IMPLEMENTATION:\n",
        "        1. Get current time and check time since last retrain:\n",
        "           - time_since_last = (current_time - self.last_retrain_time).total_seconds()\n",
        "           - too_soon = time_since_last < self.min_time_between_retrains\n",
        "        2. Check monitor alerts:\n",
        "           - Get alerts from monitor.check_alerts()\n",
        "           - accuracy_trigger = alerts[\"accuracy_alert\"]\n",
        "        3. Check drift status:\n",
        "           - Get latest drift from drift_detector.drift_history\n",
        "           - drift_trigger = drift_count >= self.drift_threshold\n",
        "        4. Determine overall trigger status:\n",
        "           - should_retrain = (accuracy_trigger or drift_trigger) and not too_soon\n",
        "        5. Return comprehensive result dictionary\n",
        "        \n",
        "        EXAMPLE RETURN:\n",
        "        ```python\n",
        "        {\n",
        "            \"should_retrain\": True,\n",
        "            \"accuracy_trigger\": True,\n",
        "            \"drift_trigger\": False,\n",
        "            \"time_trigger\": True,\n",
        "            \"reasons\": [\"Accuracy below threshold: 0.82 < 0.85\"],\n",
        "            \"time_since_last_retrain\": 86400\n",
        "        }\n",
        "        ```\n",
        "        \n",
        "        IMPLEMENTATION HINTS:\n",
        "        - Use .total_seconds() for time differences\n",
        "        - Collect all trigger reasons in a list\n",
        "        - Handle empty drift history gracefully\n",
        "        - Provide detailed feedback for debugging\n",
        "        \"\"\"\n",
        "        ### BEGIN SOLUTION\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "        ### END SOLUTION\n",
        "    \n",
        "    def execute_retraining(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        TODO: Execute the retraining process.\n",
        "        \n",
        "        STEP-BY-STEP IMPLEMENTATION:\n",
        "        1. Record start time and create result dictionary\n",
        "        2. Simulate training process:\n",
        "           - Create simple model (copy of original architecture)\n",
        "           - Simulate training with random improvement\n",
        "           - Calculate new performance (baseline + random improvement)\n",
        "        3. Validate new model:\n",
        "           - Compare old vs new performance\n",
        "           - Only deploy if new model is better\n",
        "        4. Update tracking:\n",
        "           - Update last_retrain_time\n",
        "           - Add entry to retrain_history\n",
        "        5. Return comprehensive result\n",
        "        \n",
        "        EXAMPLE RETURN:\n",
        "        ```python\n",
        "        {\n",
        "            \"success\": True,\n",
        "            \"old_accuracy\": 0.82,\n",
        "            \"new_accuracy\": 0.91,\n",
        "            \"improvement\": 0.09,\n",
        "            \"deployed\": True,\n",
        "            \"training_time\": 45.2,\n",
        "            \"timestamp\": datetime(2024, 1, 1, 12, 0)\n",
        "        }\n",
        "        ```\n",
        "        \n",
        "        IMPLEMENTATION HINTS:\n",
        "        - Use time.time() for timing\n",
        "        - Simulate realistic training time (random 30-60 seconds)\n",
        "        - Add random improvement (0.02-0.08 accuracy boost)\n",
        "        - Only deploy if new model is better\n",
        "        - Store detailed results for analysis\n",
        "        \"\"\"\n",
        "        ### BEGIN SOLUTION\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "        ### END SOLUTION\n",
        "    \n",
        "    def get_retraining_history(self) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        TODO: Return the complete retraining history.\n",
        "        \n",
        "        STEP-BY-STEP IMPLEMENTATION:\n",
        "        1. Return self.retrain_history\n",
        "        2. Include all retraining attempts with results\n",
        "        \n",
        "        EXAMPLE RETURN:\n",
        "        ```python\n",
        "        [\n",
        "            {\n",
        "                \"success\": True,\n",
        "                \"old_accuracy\": 0.82,\n",
        "                \"new_accuracy\": 0.89,\n",
        "                \"improvement\": 0.07,\n",
        "                \"deployed\": True,\n",
        "                \"training_time\": 42.1,\n",
        "                \"timestamp\": datetime(2024, 1, 1, 12, 0)\n",
        "            }\n",
        "        ]\n",
        "        ```\n",
        "        \"\"\"\n",
        "        ### BEGIN SOLUTION\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "        ### END SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76ec0ec8",
      "metadata": {
        "cell_marker": "\"\"\"",
        "lines_to_next_cell": 1
      },
      "source": [
        "### \ud83e\uddea Test Your Retraining Trigger\n",
        "\n",
        "Once you implement the `RetrainingTrigger` class above, run this cell to test it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7a7415b",
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "test-retraining-trigger",
          "locked": true,
          "points": 25,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def test_retraining_trigger():\n",
        "    \"\"\"Test RetrainingTrigger implementation\"\"\"\n",
        "    print(\"\ud83d\udd2c Unit Test: Retraining Trigger System...\")\n",
        "    \n",
        "    # Create mock model and data\n",
        "    model = \"mock_model\"\n",
        "    train_data = np.random.normal(0, 1, (1000, 10))\n",
        "    val_data = np.random.normal(0, 1, (200, 10))\n",
        "    \n",
        "    # Create retraining trigger\n",
        "    trigger = RetrainingTrigger(model, train_data, val_data)\n",
        "    \n",
        "    # Test initialization\n",
        "    assert trigger.model == model\n",
        "    assert trigger.accuracy_threshold == 0.82\n",
        "    assert trigger.drift_threshold == 1\n",
        "    assert trigger.min_time_between_retrains == 24 * 60 * 60\n",
        "    \n",
        "    # Create monitor and drift detector for testing\n",
        "    monitor = ModelMonitor(\"test_model\", baseline_accuracy=0.90)\n",
        "    baseline_data = np.random.normal(0, 1, (1000, 3))\n",
        "    drift_detector = DriftDetector(baseline_data)\n",
        "    \n",
        "    # Test no trigger conditions (good performance)\n",
        "    monitor.record_performance(accuracy=0.92, latency=150.0)\n",
        "    no_drift_data = np.random.normal(0, 1, (500, 3))\n",
        "    drift_detector.detect_drift(no_drift_data)\n",
        "    \n",
        "    conditions = trigger.check_trigger_conditions(monitor, drift_detector)\n",
        "    assert not conditions[\"should_retrain\"]\n",
        "    assert not conditions[\"accuracy_trigger\"]\n",
        "    assert not conditions[\"drift_trigger\"]\n",
        "    \n",
        "    # Test accuracy trigger\n",
        "    monitor.record_performance(accuracy=0.80, latency=150.0)  # Below threshold\n",
        "    conditions = trigger.check_trigger_conditions(monitor, drift_detector)\n",
        "    assert conditions[\"accuracy_trigger\"]\n",
        "    \n",
        "    # Test drift trigger\n",
        "    drift_data = np.random.normal(3, 1, (500, 3))  # Shifted data\n",
        "    drift_detector.detect_drift(drift_data)\n",
        "    conditions = trigger.check_trigger_conditions(monitor, drift_detector)\n",
        "    assert conditions[\"drift_trigger\"]\n",
        "    \n",
        "    # Test retraining execution\n",
        "    result = trigger.execute_retraining()\n",
        "    assert result[\"success\"] == True\n",
        "    assert \"old_accuracy\" in result\n",
        "    assert \"new_accuracy\" in result\n",
        "    assert \"improvement\" in result\n",
        "    assert \"deployed\" in result\n",
        "    assert \"training_time\" in result\n",
        "    assert \"timestamp\" in result\n",
        "    \n",
        "    # Test retraining history\n",
        "    history = trigger.get_retraining_history()\n",
        "    assert len(history) >= 1\n",
        "    assert all(\"timestamp\" in entry for entry in history)\n",
        "    assert all(\"success\" in entry for entry in history)\n",
        "    \n",
        "    print(\"\u2705 RetrainingTrigger initialization works correctly\")\n",
        "    print(\"\u2705 Trigger condition checking works\")\n",
        "    print(\"\u2705 Accuracy and drift triggers work\")\n",
        "    print(\"\u2705 Retraining execution works\")\n",
        "    print(\"\u2705 Retraining history tracking works\")\n",
        "    print(\"\ud83d\udcc8 Progress: Retraining Trigger System \u2713\")\n",
        "\n",
        "# Run the test\n",
        "test_retraining_trigger()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8afa78f9",
      "metadata": {
        "cell_marker": "\"\"\"",
        "lines_to_next_cell": 1
      },
      "source": [
        "## Step 4: Complete MLOps Pipeline - Integration and Deployment\n",
        "\n",
        "### The Problem: Disconnected Components\n",
        "You have built individual MLOps components, but they need to work together:\n",
        "- **ModelMonitor**: Tracks performance over time\n",
        "- **DriftDetector**: Identifies data distribution changes\n",
        "- **RetrainingTrigger**: Automates retraining decisions\n",
        "- **Need**: Integration layer that orchestrates everything\n",
        "\n",
        "### The Solution: Complete MLOps Pipeline\n",
        "Create a unified system that brings everything together:\n",
        "- **Unified interface**: Single entry point for all MLOps operations\n",
        "- **Automated workflows**: End-to-end automation from monitoring to deployment\n",
        "- **Integration with TinyTorch**: Uses all previous modules seamlessly\n",
        "- **Production-ready**: Handles edge cases and error conditions\n",
        "\n",
        "### What We'll Build\n",
        "An `MLOpsPipeline` that:\n",
        "1. **Integrates all components** into a cohesive system\n",
        "2. **Orchestrates the complete workflow** from monitoring to deployment\n",
        "3. **Provides simple API** for production use\n",
        "4. **Demonstrates the full TinyTorch ecosystem** working together\n",
        "\n",
        "### Real-World Applications\n",
        "- **End-to-end ML platforms**: MLflow, Kubeflow, SageMaker\n",
        "- **Production ML systems**: Netflix, Uber, Google's ML infrastructure\n",
        "- **Automated ML pipelines**: Continuous learning and deployment\n",
        "- **ML monitoring platforms**: Datadog, New Relic for ML systems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d2dc2b7",
      "metadata": {
        "lines_to_next_cell": 1,
        "nbgrader": {
          "grade": false,
          "grade_id": "mlops-pipeline",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "#| export\n",
        "class MLOpsPipeline:\n",
        "    \"\"\"\n",
        "    Complete MLOps pipeline that integrates all components.\n",
        "    \n",
        "    Orchestrates the full ML system lifecycle from monitoring to deployment.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model, training_data, validation_data, baseline_data):\n",
        "        \"\"\"\n",
        "        TODO: Initialize the complete MLOps pipeline.\n",
        "        \n",
        "        STEP-BY-STEP IMPLEMENTATION:\n",
        "        1. Store all input data and model\n",
        "        2. Initialize all MLOps components:\n",
        "           - ModelMonitor with baseline accuracy\n",
        "           - DriftDetector with baseline data\n",
        "           - RetrainingTrigger with model and data\n",
        "        3. Set up pipeline configuration:\n",
        "           - monitoring_interval: 3600 (1 hour)\n",
        "           - auto_retrain: True\n",
        "           - deploy_threshold: 0.02 (2% improvement required)\n",
        "        4. Initialize pipeline state:\n",
        "           - pipeline_active: False\n",
        "           - last_check_time: datetime.now()\n",
        "           - deployment_history: []\n",
        "        \n",
        "        EXAMPLE USAGE:\n",
        "        ```python\n",
        "        pipeline = MLOpsPipeline(model, train_data, val_data, baseline_data)\n",
        "        pipeline.start_monitoring()\n",
        "        status = pipeline.check_system_health()\n",
        "        ```\n",
        "        \n",
        "        IMPLEMENTATION HINTS:\n",
        "        - Calculate baseline_accuracy from validation data (use 0.9 as default)\n",
        "        - Use feature_names from data shape\n",
        "        - Set reasonable defaults for all parameters\n",
        "        - Initialize all components in __init__\n",
        "        \"\"\"\n",
        "        ### BEGIN SOLUTION\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "        ### END SOLUTION\n",
        "    \n",
        "    def start_monitoring(self):\n",
        "        \"\"\"\n",
        "        TODO: Start the MLOps monitoring pipeline.\n",
        "        \n",
        "        STEP-BY-STEP IMPLEMENTATION:\n",
        "        1. Set pipeline_active = True\n",
        "        2. Update last_check_time = datetime.now()\n",
        "        3. Log pipeline start\n",
        "        4. Return status dictionary\n",
        "        \n",
        "        EXAMPLE RETURN:\n",
        "        ```python\n",
        "        {\n",
        "            \"status\": \"started\",\n",
        "            \"pipeline_active\": True,\n",
        "            \"start_time\": datetime(2024, 1, 1, 12, 0),\n",
        "            \"message\": \"MLOps pipeline started successfully\"\n",
        "        }\n",
        "        ```\n",
        "        \"\"\"\n",
        "        ### BEGIN SOLUTION\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "        ### END SOLUTION\n",
        "    \n",
        "    def check_system_health(self, new_data: Optional[np.ndarray] = None, current_accuracy: Optional[float] = None) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        TODO: Check complete system health and trigger actions if needed.\n",
        "        \n",
        "        STEP-BY-STEP IMPLEMENTATION:\n",
        "        1. Check if pipeline is active, return early if not\n",
        "        2. Record current performance in monitor (if provided)\n",
        "        3. Check for drift (if new_data provided)\n",
        "        4. Check trigger conditions\n",
        "        5. Execute retraining if needed (and auto_retrain is True)\n",
        "        6. Return comprehensive system status\n",
        "        \n",
        "        EXAMPLE RETURN:\n",
        "        ```python\n",
        "        {\n",
        "            \"pipeline_active\": True,\n",
        "            \"current_accuracy\": 0.87,\n",
        "            \"drift_detected\": True,\n",
        "            \"retraining_triggered\": True,\n",
        "            \"new_model_deployed\": True,\n",
        "            \"system_healthy\": True,\n",
        "            \"last_check\": datetime(2024, 1, 1, 12, 0),\n",
        "            \"actions_taken\": [\"drift_detected\", \"retraining_executed\", \"model_deployed\"]\n",
        "        }\n",
        "        ```\n",
        "        \n",
        "        IMPLEMENTATION HINTS:\n",
        "        - Use default values if parameters not provided\n",
        "        - Track all actions taken during health check\n",
        "        - Update last_check_time\n",
        "        - Return comprehensive status for debugging\n",
        "        \"\"\"\n",
        "        ### BEGIN SOLUTION\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "        ### END SOLUTION\n",
        "    \n",
        "    def get_pipeline_status(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        TODO: Get comprehensive pipeline status and history.\n",
        "        \n",
        "        STEP-BY-STEP IMPLEMENTATION:\n",
        "        1. Get status from all components:\n",
        "           - Monitor alerts and trends\n",
        "           - Drift detection history\n",
        "           - Retraining history\n",
        "           - Deployment history\n",
        "        2. Calculate summary statistics:\n",
        "           - Total deployments\n",
        "           - Average accuracy improvement\n",
        "           - Time since last check\n",
        "        3. Return comprehensive status\n",
        "        \n",
        "        EXAMPLE RETURN:\n",
        "        ```python\n",
        "        {\n",
        "            \"pipeline_active\": True,\n",
        "            \"total_deployments\": 3,\n",
        "            \"average_improvement\": 0.05,\n",
        "            \"time_since_last_check\": 300,\n",
        "            \"recent_alerts\": [...],\n",
        "            \"drift_history\": [...],\n",
        "            \"deployment_history\": [...]\n",
        "        }\n",
        "        ```\n",
        "        \"\"\"\n",
        "        ### BEGIN SOLUTION\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "        ### END SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "509f6f56",
      "metadata": {
        "cell_marker": "\"\"\"",
        "lines_to_next_cell": 1
      },
      "source": [
        "### \ud83e\uddea Test Your Complete MLOps Pipeline\n",
        "\n",
        "Once you implement the `MLOpsPipeline` class above, run this cell to test it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e2fc09f",
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "test-mlops-pipeline",
          "locked": true,
          "points": 35,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def test_mlops_pipeline():\n",
        "    \"\"\"Test complete MLOps pipeline\"\"\"\n",
        "    print(\"\ud83d\udd2c Unit Test: Complete MLOps Pipeline...\")\n",
        "    \n",
        "    # Create test data\n",
        "    model = \"test_model\"\n",
        "    train_data = np.random.normal(0, 1, (1000, 5))\n",
        "    val_data = np.random.normal(0, 1, (200, 5))\n",
        "    baseline_data = np.random.normal(0, 1, (1000, 5))\n",
        "    \n",
        "    # Create pipeline\n",
        "    pipeline = MLOpsPipeline(model, train_data, val_data, baseline_data)\n",
        "    \n",
        "    # Test initialization\n",
        "    assert pipeline.model == model\n",
        "    assert pipeline.pipeline_active == False\n",
        "    assert hasattr(pipeline, 'monitor')\n",
        "    assert hasattr(pipeline, 'drift_detector')\n",
        "    assert hasattr(pipeline, 'retrain_trigger')\n",
        "    \n",
        "    # Test start monitoring\n",
        "    start_result = pipeline.start_monitoring()\n",
        "    assert start_result[\"status\"] == \"started\"\n",
        "    assert start_result[\"pipeline_active\"] == True\n",
        "    assert pipeline.pipeline_active == True\n",
        "    \n",
        "    # Test system health check (no issues)\n",
        "    health = pipeline.check_system_health(\n",
        "        new_data=np.random.normal(0, 1, (100, 5)),\n",
        "        current_accuracy=0.92\n",
        "    )\n",
        "    assert health[\"pipeline_active\"] == True\n",
        "    assert health[\"current_accuracy\"] == 0.92\n",
        "    assert \"actions_taken\" in health\n",
        "    \n",
        "    # Test system health check (with issues)\n",
        "    health = pipeline.check_system_health(\n",
        "        new_data=np.random.normal(5, 2, (100, 5)),  # Heavily drifted data\n",
        "        current_accuracy=0.75  # Very low accuracy (well below 0.81 threshold)\n",
        "    )\n",
        "    assert health[\"pipeline_active\"] == True\n",
        "    assert health[\"drift_detected\"] == True\n",
        "    # Note: retraining_triggered depends on both accuracy and drift conditions\n",
        "    # For fast testing, we just verify the system detects issues\n",
        "    assert \"retraining_triggered\" in health\n",
        "    \n",
        "    # Test pipeline status\n",
        "    status = pipeline.get_pipeline_status()\n",
        "    assert status[\"pipeline_active\"] == True\n",
        "    assert \"total_deployments\" in status\n",
        "    assert \"average_improvement\" in status\n",
        "    assert \"time_since_last_check\" in status\n",
        "    assert \"recent_alerts\" in status\n",
        "    assert \"performance_trend\" in status\n",
        "    \n",
        "    print(\"\u2705 MLOpsPipeline initialization works correctly\")\n",
        "    print(\"\u2705 Pipeline start/stop functionality works\")\n",
        "    print(\"\u2705 System health checking works\")\n",
        "    print(\"\u2705 Drift detection and retraining integration works\")\n",
        "    print(\"\u2705 Pipeline status reporting works\")\n",
        "    print(\"\ud83d\udcc8 Progress: Complete MLOps Pipeline \u2713\")\n",
        "\n",
        "# Run the test\n",
        "test_mlops_pipeline()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0d40534",
      "metadata": {
        "cell_marker": "\"\"\"",
        "lines_to_next_cell": 1
      },
      "source": [
        "## \ud83c\udfaf Final Integration: Complete TinyTorch Ecosystem\n",
        "\n",
        "### The Full System in Action\n",
        "Let's demonstrate how all TinyTorch components work together in a complete MLOps pipeline:\n",
        "\n",
        "```python\n",
        "# Complete TinyTorch MLOps workflow\n",
        "from tinytorch.core.tensor import Tensor\n",
        "from tinytorch.core.networks import Sequential\n",
        "from tinytorch.core.layers import Dense  \n",
        "from tinytorch.core.activations import ReLU, Softmax\n",
        "from tinytorch.core.training import Trainer, CrossEntropyLoss\n",
        "from tinytorch.core.compression import quantize_layer_weights\n",
        "from tinytorch.core.benchmarking import TinyTorchPerf\n",
        "from tinytorch.core.mlops import MLOpsPipeline\n",
        "\n",
        "# 1. Build model (Modules 01-04)\n",
        "model = Sequential([\n",
        "    Dense(784, 128), ReLU(),\n",
        "    Dense(128, 64), ReLU(), \n",
        "    Dense(64, 10), Softmax()\n",
        "])\n",
        "\n",
        "# 2. Train model (Module 09)\n",
        "trainer = Trainer(model, CrossEntropyLoss(), learning_rate=0.001)\n",
        "trained_model = trainer.train(training_data, epochs=10)\n",
        "\n",
        "# 3. Compress model (Module 10)\n",
        "compressed_model = quantize_layer_weights(trained_model)\n",
        "\n",
        "# 4. Benchmark model (Module 12)\n",
        "perf = TinyTorchPerf()\n",
        "benchmark_results = perf.benchmark(compressed_model, test_data)\n",
        "\n",
        "# 5. Deploy with MLOps (Module 13)\n",
        "pipeline = MLOpsPipeline(compressed_model, training_data, validation_data, baseline_data)\n",
        "pipeline.start_monitoring()\n",
        "\n",
        "# 6. Monitor and maintain\n",
        "health = pipeline.check_system_health(new_data, current_accuracy=0.89)\n",
        "if health[\"new_model_deployed\"]:\n",
        "    print(\"\ud83d\ude80 New model deployed automatically!\")\n",
        "```\n",
        "\n",
        "### What Students Have Achieved\n",
        "By completing this module, you have:\n",
        "- **Built a complete ML system** from tensors to production deployment\n",
        "- **Integrated all TinyTorch components** into a cohesive workflow\n",
        "- **Implemented production-grade MLOps** with monitoring and automation\n",
        "- **Created self-maintaining systems** that adapt to changing conditions\n",
        "- **Mastered the full ML lifecycle** from development to production\n",
        "\n",
        "### Real-World Impact\n",
        "Your MLOps skills now enable:\n",
        "- **Automated model maintenance** reducing manual intervention by 90%\n",
        "- **Faster response to issues** from days to hours or minutes\n",
        "- **Improved model reliability** through continuous monitoring\n",
        "- **Scalable ML operations** that work across multiple models\n",
        "- **Production-ready deployment** with industry-standard practices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1436bb1",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "comprehensive-integration-test",
          "locked": false,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def test_comprehensive_integration():\n",
        "    \"\"\"Test complete integration of all TinyTorch components\"\"\"\n",
        "    print(\"\ud83d\udd2c Comprehensive Integration Test: Complete TinyTorch Ecosystem...\")\n",
        "    \n",
        "    # 1. Create synthetic data (simulating real ML dataset)\n",
        "    np.random.seed(42)\n",
        "    train_data = np.random.normal(0, 1, (1000, 10))\n",
        "    val_data = np.random.normal(0, 1, (200, 10))\n",
        "    baseline_data = np.random.normal(0, 1, (1000, 10))\n",
        "    \n",
        "    # 2. Create model architecture\n",
        "    model = \"TinyTorch_Production_Model\"\n",
        "    \n",
        "    # 3. Set up complete MLOps pipeline\n",
        "    pipeline = MLOpsPipeline(model, train_data, val_data, baseline_data)\n",
        "    \n",
        "    # 4. Start monitoring\n",
        "    start_result = pipeline.start_monitoring()\n",
        "    assert start_result[\"status\"] == \"started\"\n",
        "    print(\"\u2705 MLOps pipeline started successfully\")\n",
        "    \n",
        "    # 5. Simulate production monitoring cycle\n",
        "    print(\"\\n\ud83d\udd04 Simulating Production Monitoring Cycle...\")\n",
        "    \n",
        "    # Phase 1: Normal operation\n",
        "    health1 = pipeline.check_system_health(\n",
        "        new_data=np.random.normal(0, 1, (100, 10)),\n",
        "        current_accuracy=0.94\n",
        "    )\n",
        "    print(f\"   Phase 1 - Normal: Accuracy {health1['current_accuracy']}, Drift: {health1['drift_detected']}\")\n",
        "    \n",
        "    # Phase 2: Gradual degradation\n",
        "    health2 = pipeline.check_system_health(\n",
        "        new_data=np.random.normal(0.5, 1, (100, 10)),\n",
        "        current_accuracy=0.88\n",
        "    )\n",
        "    print(f\"   Phase 2 - Degradation: Accuracy {health2['current_accuracy']}, Drift: {health2['drift_detected']}\")\n",
        "    \n",
        "    # Phase 3: Significant drift and low accuracy\n",
        "    health3 = pipeline.check_system_health(\n",
        "        new_data=np.random.normal(2, 1, (100, 10)),\n",
        "        current_accuracy=0.79\n",
        "    )\n",
        "    print(f\"   Phase 3 - Critical: Accuracy {health3['current_accuracy']}, Drift: {health3['drift_detected']}\")\n",
        "    print(f\"   Retraining triggered: {health3['retraining_triggered']}\")\n",
        "    print(f\"   New model deployed: {health3['new_model_deployed']}\")\n",
        "    \n",
        "    # 6. Get final pipeline status\n",
        "    final_status = pipeline.get_pipeline_status()\n",
        "    print(f\"\\n\ud83d\udcca Final Pipeline Status:\")\n",
        "    print(f\"   Total deployments: {final_status['total_deployments']}\")\n",
        "    print(f\"   Average improvement: {final_status['average_improvement']:.3f}\")\n",
        "    print(f\"   System health: {health3['system_healthy']}\")\n",
        "    \n",
        "    # 7. Verify complete integration\n",
        "    assert final_status[\"pipeline_active\"] == True\n",
        "    assert len(final_status[\"deployment_history\"]) >= 0\n",
        "    assert \"drift_history\" in final_status\n",
        "    assert \"retrain_history\" in final_status\n",
        "    \n",
        "    print(\"\\n\u2705 Complete TinyTorch ecosystem integration successful!\")\n",
        "    print(\"\ud83c\udf89 All components working together seamlessly!\")\n",
        "    print(\"\ud83d\udcc8 Progress: Complete TinyTorch Ecosystem \u2713\")\n",
        "\n",
        "# Run the comprehensive test\n",
        "test_comprehensive_integration()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a71098d0",
      "metadata": {
        "cell_marker": "\"\"\""
      },
      "source": [
        "## \ud83e\uddea Auto-Discovery Testing\n",
        "\n",
        "The following cell automatically discovers and runs all test functions in this module:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9adf2fe0",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "auto-discovery-tests",
          "locked": false,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    from tito.tools.testing import run_module_tests_auto\n",
        "    \n",
        "    # Automatically discover and run all tests in this module\n",
        "    success = run_module_tests_auto(\"MLOps\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b58c7f9",
      "metadata": {
        "cell_marker": "\"\"\""
      },
      "source": [
        "## \ud83c\udfaf Module Summary: MLOps Production Systems\n",
        "\n",
        "Congratulations! You've successfully implemented a complete MLOps system for production ML lifecycle management:\n",
        "\n",
        "### What You've Built\n",
        "\u2705 **Model Monitor**: Performance tracking and drift detection\n",
        "\u2705 **Retraining Triggers**: Automated model updates based on performance thresholds\n",
        "\u2705 **MLOps Pipeline**: Complete production deployment and maintenance system\n",
        "\u2705 **Integration**: Orchestrates all TinyTorch components in production workflows\n",
        "\n",
        "### Key Concepts You've Learned\n",
        "- **Production ML systems** require continuous monitoring and maintenance\n",
        "- **Drift detection** identifies when models need retraining\n",
        "- **Automated workflows** respond to system degradation without manual intervention\n",
        "- **MLOps pipelines** integrate monitoring, training, and deployment\n",
        "- **System orchestration** coordinates complex ML component interactions\n",
        "\n",
        "### Real-World Applications\n",
        "- **Production AI**: Automated model maintenance at scale\n",
        "- **Enterprise ML**: Continuous monitoring and improvement systems\n",
        "- **Cloud deployment**: Industry-standard MLOps practices\n",
        "- **Model lifecycle**: Complete deployment and maintenance workflows\n",
        "\n",
        "### Connection to Industry Systems\n",
        "Your implementation mirrors production platforms:\n",
        "- **MLflow**: Model lifecycle management and experiment tracking\n",
        "- **Kubeflow**: Kubernetes-based ML workflows and pipelines\n",
        "- **Amazon SageMaker**: End-to-end ML platform with monitoring\n",
        "- **Google AI Platform**: Production ML services with automation\n",
        "\n",
        "### Next Steps\n",
        "1. **Export your code**: `tito export 13_mlops`\n",
        "2. **Test your implementation**: `tito test 13_mlops`\n",
        "3. **Deploy production systems**: Apply MLOps patterns to real-world ML projects\n",
        "4. **Complete TinyTorch**: You've mastered the full ML systems pipeline!\n",
        "\n",
        "**\ud83c\udf89 TinyTorch Journey Complete!** You've built a complete ML framework from tensors to production deployment. You're now ready to tackle real-world ML systems challenges!"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "main_language": "python"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8+"
    },
    "mystnb": {
      "execution_mode": "auto"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}