# Activations - Nonlinearity in Neural Networks

Welcome to the Activations module! This is where neural networks get their power through nonlinearity.

```{admonition} ğŸ¯ Learning Goals
:class: tip
- Understand why activation functions are essential for neural networks
- Implement the four most important activation functions: ReLU, Sigmoid, Tanh, and Softmax
- Visualize how activations transform data and enable complex learning
- See how activations work with layers to build powerful networks
- Master the NBGrader workflow with comprehensive testing
```


## Build â†’ Use â†’ Understand
1. **Build**: Activation functions that add nonlinearity
2. **Use**: Transform tensors and see immediate results
3. **Understand**: How nonlinearity enables complex pattern learning
## ğŸš€ Interactive Learning

Choose your preferred way to engage with this module:

````{grid} 1 2 3 3

```{grid-item-card} ğŸš€ Launch Binder
:link: https://mybinder.org/v2/gh/mlsysbook/TinyTorch/main?filepath=modules/source/02_activations/activations_dev.ipynb
:class-header: bg-light

Run this module interactively in your browser. No installation required!
```

```{grid-item-card} âš¡ Open in Colab  
:link: https://colab.research.google.com/github/mlsysbook/TinyTorch/blob/main/modules/source/02_activations/activations_dev.ipynb
:class-header: bg-light

Use Google Colab for GPU access and cloud compute power.
```

```{grid-item-card} ğŸ“– View Source
:link: https://github.com/mlsysbook/TinyTorch/blob/main/modules/source/02_activations/activations_dev.py
:class-header: bg-light

Browse the Python source code and understand the implementation.
```

````

```{admonition} ğŸ’¾ Save Your Progress
:class: tip
**Binder sessions are temporary!** Download your completed notebook when done, or switch to local development for persistent work.

Ready for serious development? â†’ [ğŸ—ï¸ Local Setup Guide](../usage-paths/serious-development.md)
```

