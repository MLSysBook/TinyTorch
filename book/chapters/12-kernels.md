# Kernels - Hardware-Optimized ML Operations

Welcome to the Kernels module! This is where we move beyond NumPy to understand how ML operations are optimized for modern hardware. You'll implement custom kernels that run faster than standard library functions.

```{admonition} 🎯 Learning Goals
:class: tip
- Understand why custom kernels matter for ML performance
- Implement vectorized operations using SIMD principles
- Master memory-efficient algorithms for better cache utilization
- Build parallel processing patterns for CPU and GPU-style computing
- Create performance profiling tools to measure and optimize code
- Apply kernel optimizations to compressed model operations
```


## Build → Use → Optimize
1. **Build**: Custom operations, vectorization, and memory optimization
2. **Use**: Apply optimized kernels to real ML workloads
3. **Optimize**: Profile, measure, and improve performance systematically
## 🚀 Interactive Learning

Choose your preferred way to engage with this module:

````{grid} 1 2 3 3

```{grid-item-card} 🚀 Launch Binder
:link: https://mybinder.org/v2/gh/mlsysbook/TinyTorch/main?filepath=modules/source/12_kernels/kernels_dev.ipynb
:class-header: bg-light

Run this module interactively in your browser. No installation required!
```

```{grid-item-card} ⚡ Open in Colab  
:link: https://colab.research.google.com/github/mlsysbook/TinyTorch/blob/main/modules/source/12_kernels/kernels_dev.ipynb
:class-header: bg-light

Use Google Colab for GPU access and cloud compute power.
```

```{grid-item-card} 📖 View Source
:link: https://github.com/mlsysbook/TinyTorch/blob/main/modules/source/12_kernels/kernels_dev.py
:class-header: bg-light

Browse the Python source code and understand the implementation.
```

````

```{admonition} 💾 Save Your Progress
:class: tip
**Binder sessions are temporary!** Download your completed notebook when done, or switch to local development for persistent work.

Ready for serious development? → [🏗️ Local Setup Guide](../usage-paths/serious-development.md)
```

