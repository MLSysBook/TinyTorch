
# TinyTorch Module Development Best Practices

## Core Principles

### Real Data, Real Systems
- **Use production datasets**: Students work with CIFAR-10, ImageNet, etc. - never mock/fake data
- **Show progress feedback**: Downloads, training need visual progress indicators
- **Cache for efficiency**: Download once, use repeatedly
- **Real-world scale**: Use actual dataset sizes, not toy examples
- **Systems thinking**: Consider performance, memory, caching, user experience

### Educational Excellence: Build ‚Üí Use ‚Üí Analyze/Test
- **"Build ‚Üí Use ‚Üí Analyze"**: Follow this cycle religiously with specific third-stage verbs
- **Test after each feature**: Unit tests immediately after implementation, not at the end
- **Progressive complexity**: Easy ‚Üí Medium ‚Üí Hard with clear difficulty indicators
- **Comprehensive guidance**: TODO sections with approach, examples, hints, systems thinking
- **Real-world connections**: Connect every concept to production ML engineering

## Module Structure (Based on Our Best Modules)

### Ideal Module Layout
```
modules/source/XX_module_name/
‚îú‚îÄ‚îÄ module_name_dev.py          # Main development file (Jupytext format)
‚îú‚îÄ‚îÄ module_name_dev.ipynb       # Generated notebook (auto-created)
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ test_module_name.py     # Unit tests ONLY for this module
‚îú‚îÄ‚îÄ README.md                   # Module overview and usage
‚îî‚îÄ‚îÄ module.yaml                 # Module metadata
```

### Test Organization (Critical)
- **Unit tests**: Inline in the notebook/Python code - immediate tests after each feature
- **Module-level tests**: `modules/source/XX_module/tests/` - test module in isolation with stubbed/fake data
- **Integration tests**: `tests/` (main directory) - real cross-module testing with actual dependencies
- **Test after each feature**: Write inline unit tests immediately after implementing each component
- **Module tests use fake data**: Module-level tests should stub out other modules with fake inputs/outputs

## Development Workflow: Test-Driven Feature Development

### 1. Feature Implementation Cycle
```python
# Step 1: Build the feature
class ComponentName:
    def method(self):
        # Implementation here
        pass

# Step 2: Use the feature (immediate inline unit test)
component = ComponentName()
result = component.method()
print(f"‚úÖ Component working: {result}")
assert result.shape == expected_shape  # Inline unit test

# Step 3: Analyze/Test the feature (more comprehensive inline testing)
def test_component_method():
    """Inline unit test for the specific method just implemented."""
    component = ComponentName()
    result = component.method()
    assert result.shape == expected_shape
    assert np.allclose(result.data, expected_data)
    print("‚úÖ Component unit test passed")

# Run the test immediately
test_component_method()
```

### 2. Build ‚Üí Use ‚Üí Analyze Pattern

Our best modules follow specific third-stage verbs:

#### **Build ‚Üí Use ‚Üí Reflect** (Early modules: Setup, Tensor)
- **Reflect**: Emphasizes metacognition and design trade-offs
- **Questions**: "Why did we design it this way?", "What are the trade-offs?"
- **Focus**: Systems thinking and architectural decisions

#### **Build ‚Üí Use ‚Üí Analyze** (Middle modules: Activations, Layers, Networks)
- **Analyze**: Technical depth with profiling, debugging, performance
- **Questions**: "How does this perform?", "What patterns emerge?"
- **Focus**: Technical understanding and optimization

#### **Build ‚Üí Use ‚Üí Optimize** (Advanced modules: CNN, DataLoader, Autograd)
- **Optimize**: Real systems iteration and improvement
- **Questions**: "How can we make this faster?", "What about memory usage?"
- **Focus**: Production-ready systems engineering

## Testing Architecture (Corrected)

### 1. Inline Unit Tests (Immediate After Each Feature)
```python
# In the notebook/Python development file
# Immediately after implementing each feature

class Sequential:
    def __init__(self, layers):
        self.layers = layers
    
    def __call__(self, x):
        for layer in self.layers:
            x = layer(x)
        return x

# INLINE UNIT TEST - Immediate testing
print("üî¨ Unit Test: Sequential Network...")
network = Sequential([Dense(3, 2), ReLU()])
x = Tensor([[1.0, 2.0, 3.0]])
y = network(x)

assert y.shape == (1, 2), f"Expected shape (1, 2), got {y.shape}"
assert np.all(y.data >= 0), "ReLU output should be non-negative"
print("‚úÖ Sequential network unit test passed")
```

### 2. Module-Level Tests (Isolated with Stubbed Data)
```python
# modules/source/04_networks/tests/test_networks.py
"""
Module-level tests for Networks module.
Tests the module in isolation using stubbed/fake data from other modules.
"""

import pytest
import numpy as np

# Create fake/stubbed versions of dependencies
class FakeTensor:
    """Stubbed Tensor class for isolated testing."""
    def __init__(self, data):
        self.data = np.array(data)
        self.shape = self.data.shape

class FakeLayer:
    """Stubbed Layer class for isolated testing."""
    def __init__(self, output_shape):
        self.output_shape = output_shape
    
    def __call__(self, x):
        # Return fake output with expected shape
        return FakeTensor(np.random.randn(*self.output_shape))

# Import the actual module being tested
from networks_dev import Sequential

class TestSequentialIsolated:
    """Test Sequential class in isolation with fake dependencies."""
    
    def test_sequential_with_fake_layers(self):
        """Test Sequential network with stubbed layers."""
        # Use fake layers that don't depend on other modules
        fake_layer1 = FakeLayer(output_shape=(1, 4))
        fake_layer2 = FakeLayer(output_shape=(1, 2))
        
        network = Sequential([fake_layer1, fake_layer2])
        fake_input = FakeTensor([[1.0, 2.0, 3.0]])
        
        output = network(fake_input)
        
        # Test only the Sequential logic, not the actual layers
        assert len(network.layers) == 2
        assert output.shape == (1, 2)
        
    def test_sequential_layer_composition(self):
        """Test that Sequential properly composes layers."""
        # Test with minimal fake layers
        layers = [FakeLayer((1, 3)), FakeLayer((1, 2))]
        network = Sequential(layers)
        
        assert network.layers == layers
        assert len(network.layers) == 2
```

### 3. Integration Tests (Real Cross-Module Testing)
```python
# tests/integration/test_ml_pipeline.py
"""
Integration tests using real implementations from all modules.
Tests how modules actually work together in realistic scenarios.
"""

import pytest
from tinytorch.core.tensor import Tensor  # Real Tensor
from tinytorch.core.layers import Dense   # Real Dense layer
from tinytorch.core.activations import ReLU  # Real ReLU
from tinytorch.core.networks import Sequential  # Real Sequential

class TestRealMLPipeline:
    """Test real ML pipeline with actual module implementations."""
    
    def test_tensor_to_network_integration(self):
        """Test real tensor flowing through real network."""
        # Real tensor
        x = Tensor([[1.0, 2.0, 3.0]])
        
        # Real network with real layers
        network = Sequential([
            Dense(input_size=3, output_size=4),
            ReLU(),
            Dense(input_size=4, output_size=2)
        ])
        
        # Real forward pass
        output = network(x)
        
        # Test real integration
        assert output.shape == (1, 2)
        assert isinstance(output, Tensor)
        assert np.all(output.data >= 0)  # ReLU ensures non-negative
```

## Student Implementation Structure

### Comprehensive TODO Pattern (Based on DataLoader Module)
```python
def student_method(self, params):
    """
    TODO: Clear, specific task description
    
    APPROACH:
    1. Concrete first step with specific guidance
    2. Concrete second step with specific guidance  
    3. Concrete third step with specific guidance
    
    EXAMPLE:
    Input: actual_data_example
    Expected: concrete_expected_output
    Your code should: specific_behavior_description
    
    HINTS:
    - Helpful guidance without giving code
    - Systems thinking consideration
    - Real-world connection
    
    SYSTEMS THINKING:
    - Performance consideration
    - Scalability question
    - User experience aspect
    """
    raise NotImplementedError("Student implementation required")
```

### Immediate Testing Pattern (Based on CNN Module)
```python
# %% [markdown]
"""
### üß™ Unit Test: Conv2D Layer

Let's test your Conv2D implementation immediately after building it!
**This is a unit test** - it tests one specific class in isolation.
"""

# %% nbgrader={"grade": true, "grade_id": "test-conv2d-immediate", "locked": true, "points": 10}
# Test Conv2D layer immediately after implementation
print("üî¨ Unit Test: Conv2D Layer...")

try:
    conv = Conv2D(kernel_size=(3, 3))
    x = Tensor(np.random.randn(1, 4, 4))
    y = conv(x)
    
    assert y.shape == (1, 2, 2), f"Expected shape (1, 2, 2), got {y.shape}"
    print("‚úÖ Conv2D layer produces correct output shape")
    
    # Test that kernels are properly initialized
    assert hasattr(conv, 'kernel'), "Conv2D should have kernel attribute"
    assert conv.kernel.shape == (3, 3), f"Kernel shape should be (3, 3), got {conv.kernel.shape}"
    print("‚úÖ Conv2D layer kernel initialization correct")
    
except Exception as e:
    print(f"‚ùå Conv2D layer test failed: {e}")
    raise

print("üéØ Conv2D layer behavior:")
print("   Applies convolution operation to input")
print("   Reduces spatial dimensions based on kernel size")
print("   Maintains batch dimension")
```

## Comprehensive Testing Strategy

### 1. Inline Unit Tests (Immediate After Each Feature)
- **When**: Immediately after implementing each component
- **What**: Test the specific feature with simple assertions
- **Where**: Inline in the development notebook/Python file
- **Purpose**: Ensure feature works before moving to next
- **Data**: Use simple, hardcoded test data

### 2. Module-Level Tests (Isolated Testing)
- **When**: After completing the entire module
- **What**: Test module components in isolation with stubbed dependencies
- **Where**: `modules/source/XX_module/tests/test_module.py`
- **Purpose**: Ensure module logic works independently of other modules
- **Data**: Use fake/stubbed data to avoid dependencies

### 3. Integration Tests (Real Cross-Module Testing)
- **When**: After multiple modules are complete
- **What**: Test how modules work together with real implementations
- **Where**: `tests/integration/` directory
- **Purpose**: Ensure modules integrate properly in ML workflows
- **Data**: Use real implementations and actual data

### 4. System Tests (Full End-to-End)
- **When**: After major functionality is complete
- **What**: Test complete ML pipelines from data to results
- **Where**: `tests/system/` directory
- **Purpose**: Ensure entire system works in production scenarios
- **Data**: Use real datasets and complete workflows

## Progress Feedback Pattern

```python
def _download_progress_hook(self, count, block_size, total_size):
    """Progress callback for downloads."""
    if total_size > 0:
        percent = min(100, (count * block_size * 100) // total_size)
        mb_downloaded = (count * block_size) / (1024 * 1024)
        mb_total = total_size / (1024 * 1024)
        
        # Visual progress bar
        bar_length = 50
        filled_length = int(bar_length * percent // 100)
        bar = '‚ñà' * filled_length + '‚ñë' * (bar_length - filled_length)
        
        print(f'\rüì• [{bar}] {percent}% ({mb_downloaded:.1f}/{mb_total:.1f} MB)', 
              end='', flush=True)
```

## Visual Feedback Pattern (Development Only)

```python
def show_data_samples(dataset, num_samples=8):
    """Show grid of actual data samples (development only)."""
    # NOT exported to package - development feedback only
    if not _should_show_plots():
        return
        
    fig, axes = plt.subplots(2, 4, figsize=(12, 6))
    for i in range(num_samples):
        image, label = dataset[i]
        # Show actual data with proper visualization
        axes[i//4, i%4].imshow(image.transpose(1, 2, 0))
        axes[i//4, i%4].set_title(f'Class: {label}')
    plt.show()
```

## Best Module Examples

### Networks Module (Our Gold Standard)
- **Structure**: Clear Sequential class with layers composition
- **Testing**: Immediate unit tests after each feature
- **Comprehensive**: Full test suite covering all scenarios
- **Educational**: Build ‚Üí Use ‚Üí Analyze pattern with technical depth
- **Real-world**: Actual network architectures (MLP, classification, regression)

### Layers Module (Excellent Foundation)
- **Structure**: Dense layer with proper matrix multiplication
- **Testing**: Unit tests for each operation (matmul, forward pass, initialization)
- **Comprehensive**: Integration tests with activations and networks
- **Educational**: Build ‚Üí Use ‚Üí Analyze with performance considerations
- **Real-world**: Production-ready linear transformations

### DataLoader Module (Systems Excellence)
- **Structure**: Abstract Dataset interface with concrete implementations
- **Testing**: Real data testing with CIFAR-10
- **Comprehensive**: Complete pipeline testing
- **Educational**: Build ‚Üí Use ‚Üí Optimize with systems thinking
- **Real-world**: Production data loading patterns

## Key Anti-Patterns to Avoid

### ‚ùå Don't Test Everything at the End
```python
# BAD: All tests at module end
def implement_all_features():
    # Implement feature 1
    # Implement feature 2
    # Implement feature 3
    
# Then test everything at once
def test_everything():
    # Test all features together
    
# GOOD: Inline unit tests after each feature
def implement_feature_1():
    # Implementation
    pass
    
# Immediate inline unit test
assert feature_1_works()
print("‚úÖ Feature 1 working")

def implement_feature_2():
    # Implementation
    pass
    
# Immediate inline unit test
assert feature_2_works()
print("‚úÖ Feature 2 working")
```

### ‚ùå Don't Use Real Dependencies in Module Tests
```python
# BAD: Module tests with real dependencies
# modules/source/04_networks/tests/test_networks.py
from tinytorch.core.layers import Dense  # Real dependency
from tinytorch.core.activations import ReLU  # Real dependency

def test_sequential_with_real_layers():
    # This creates coupling between modules
    network = Sequential([Dense(3, 2), ReLU()])
    
# GOOD: Module tests with stubbed dependencies
class FakeDense:
    def __call__(self, x):
        return FakeTensor(np.random.randn(1, 2))

class FakeReLU:
    def __call__(self, x):
        return x  # Simplified fake behavior

def test_sequential_with_fake_layers():
    # This tests only Sequential logic
    network = Sequential([FakeDense(), FakeReLU()])
```

### ‚ùå Don't Mix Testing Levels
```python
# BAD: Integration testing in module-level tests
# modules/source/04_networks/tests/test_networks.py
def test_networks_with_real_dataloader():
    # This should be in tests/integration/
    from tinytorch.core.dataloader import DataLoader
    # Testing cross-module integration
    
# GOOD: Keep testing levels separate
# modules/source/04_networks/tests/test_networks.py - Module-level with stubs
def test_sequential_isolated():
    # Test with fake data only
    
# tests/integration/test_ml_pipeline.py - Integration with real modules
def test_networks_with_real_dataloader():
    # Test real cross-module integration
```

### ‚ùå Don't Use Mock Data
```python
# BAD: Synthetic/mock data
class MockDataset:
    def __init__(self, size):
        self.data = np.random.randn(size, 784)  # Fake data
        
# GOOD: Real data
class CIFAR10Dataset:
    def __init__(self, root, train=True, download=True):
        self._download_if_needed()  # Real CIFAR-10 data
```

## Quality Standards

### Before Release Checklist
- [ ] Uses real data, not synthetic/mock data
- [ ] Includes progress feedback for long operations
- [ ] Visual feedback functions (development only, not exported)
- [ ] Inline unit tests after each feature implementation
- [ ] Module-level tests use stubbed/fake dependencies for isolation
- [ ] Integration tests use real cross-module implementations
- [ ] Clear separation: inline ‚Üí module ‚Üí integration ‚Üí system testing
- [ ] Follows "Build ‚Üí Use ‚Üí Analyze/Test" progression
- [ ] TODO guidance includes systems thinking
- [ ] Clean separation between development and exports

### Student Experience Requirements
- [ ] Clear learning progression with immediate inline feedback
- [ ] Inline unit tests provide confidence after each feature
- [ ] Module tests demonstrate isolation and stubbing concepts
- [ ] Integration tests show real-world module interactions
- [ ] Real-world relevance and connections
- [ ] Smooth transition to next modules
- [ ] Test-driven development workflow

## Success Metrics

**Students should be able to:**
- Write inline unit tests immediately after each feature
- Understand the difference between inline, module, integration, and system tests
- Create stubbed/fake dependencies for module-level testing
- Explain what they built in simple terms
- Modify code to solve related problems
- Connect module concepts to real ML systems
- Debug issues by understanding the different testing levels

**Modules should achieve:**
- High student engagement and completion rates
- Clear testing patterns with immediate inline feedback
- Proper isolation in module-level tests using stubs
- Realistic integration testing with real dependencies
- Smooth progression to next modules
- Real-world relevance and production quality
- Consistent test-driven development workflow

---

**Remember**: We're teaching ML systems engineering with proper testing architecture. Inline unit tests provide immediate feedback after each feature, module-level tests use stubbed dependencies for isolation, integration tests use real cross-module implementations, and system tests validate complete workflows. Follow the "Build ‚Üí Use ‚Üí Analyze/Test" cycle with proper testing separation.

## Development Workflow Summary

### Complete Module Development Cycle

1. **Setup Phase**
   ```bash
   cd modules/source/XX_module_name/
   # Work in module_name_dev.py (Jupytext format)
   # Module tests go in tests/test_module_name.py (with stubs)
   # Integration tests go in tests/integration/ (with real modules)
   ```

2. **Feature Development Phase** (Repeat for each component)
   ```python
   # Step 1: Build the feature
   class NewComponent:
       def method(self):
           # Implementation
           pass
   
   # Step 2: Use the feature (immediate inline unit test)
   component = NewComponent()
   result = component.method()
   assert result.shape == expected_shape  # Inline unit test
   print(f"‚úÖ {component.__class__.__name__} working: {result}")
   
   # Step 3: Analyze/Test the feature (more comprehensive inline testing)
   def test_new_component():
       component = NewComponent()
       result = component.method()
       assert result.shape == expected_shape
       assert np.allclose(result.data, expected_data)
       print("‚úÖ Component inline unit test passed")
   
   # Run the test immediately
   test_new_component()
   ```

3. **Module Completion Phase**
   ```bash
   # Run module-level tests (with stubbed dependencies)
   python -m pytest modules/source/XX_module_name/tests/test_module_name.py -v
   
   # Export to package
   tito package nbdev --export XX_module_name
   ```

4. **Integration Testing Phase**
   ```bash
   # Run integration tests (with real cross-module dependencies)
   python -m pytest tests/integration/ -v
   
   # Run system tests (full end-to-end)
   python -m pytest tests/system/ -v
   ```

### Daily Development Rhythm

- **Morning**: Review previous day's inline tests, ensure all passing
- **Feature work**: Build ‚Üí Use ‚Üí Inline Test for each component
- **Module work**: Create stubbed module-level tests for isolation
- **Integration work**: Test real cross-module interactions
- **End of day**: Run all test levels, commit working features

### Quality Gates

- **Feature Level**: Inline unit test must pass immediately
- **Module Level**: Stubbed isolation tests must pass before export
- **Integration Level**: Real cross-module tests must pass before merge
- **System Level**: Full end-to-end tests must pass before release

This workflow ensures students understand proper testing architecture while building confidence incrementally through immediate feedback.
