
# TinyTorch Module Development & Testing Best Practices

## Core Principles

### Real Data, Real Systems
- **Use production datasets**: Students work with CIFAR-10, ImageNet, etc. - never mock/fake data
- **Show progress feedback**: Downloads, training need visual progress indicators
- **Cache for efficiency**: Download once, use repeatedly
- **Real-world scale**: Use actual dataset sizes, not toy examples
- **Systems thinking**: Consider performance, memory, caching, user experience

### Educational Excellence: Build → Use → Analyze/Test
- **"Build → Use → Analyze"**: Follow this cycle religiously with specific third-stage verbs
- **Test after each feature**: Inline unit tests immediately after implementation, not at the end
- **Progressive complexity**: Easy → Medium → Hard with clear difficulty indicators
- **Comprehensive guidance**: TODO sections with approach, examples, hints, systems thinking
- **Real-world connections**: Connect every concept to production ML engineering

### Testing Architecture (ALWAYS USE PYTEST)
- **ALWAYS USE PYTEST**: TinyTorch uses pytest as the standard testing framework for all tests
- **ALWAYS USE REAL DATA**: Tests must use actual datasets (CIFAR-10, ImageNet), not mock/synthetic data
- **Four-tier testing**: Inline → Module → Integration → System with proper separation
- **Immediate feedback**: Inline tests provide confidence after each feature
- **Isolation**: Module tests use stubbed dependencies to test logic in isolation

## Module Structure (Based on Our Best Modules)

### Ideal Module Layout
```
modules/source/XX_module_name/
├── module_name_dev.py          # Main development file (Jupytext format)
├── module_name_dev.ipynb       # Generated notebook (auto-created)
├── tests/
│   └── test_module_name.py     # Module-level tests with stubbed dependencies
├── README.md                   # Module overview and usage
└── module.yaml                 # Module metadata
```

### Testing File Structure
- **Module tests**: `modules/source/XX_module/tests/test_module.py` - pytest with stubbed dependencies
- **Integration tests**: `tests/integration/` - pytest with real cross-module testing
- **System tests**: `tests/system/` - pytest with full end-to-end workflows
- **Package tests**: `tinytorch/tests/` - pytest for exported package functionality

## Development Workflow: Test-Driven Feature Development

### 1. Feature Implementation Cycle
```python
# Step 1: Build the feature
class ComponentName:
    def method(self):
        # Implementation here
        pass

# Step 2: Use the feature (immediate inline unit test)
component = ComponentName()
result = component.method()
assert result.shape == expected_shape  # Inline unit test
print(f"✅ Component working: {result}")

# Step 3: Analyze/Test the feature (more comprehensive inline testing)
def test_component_method():
    """Inline unit test for the specific method just implemented."""
    component = ComponentName()
    result = component.method()
    assert result.shape == expected_shape
    assert np.allclose(result.data, expected_data)
    print("✅ Component inline unit test passed")

# Run the test immediately
test_component_method()
```

### 2. Build → Use → Analyze Pattern

Our best modules follow specific third-stage verbs:

#### **Build → Use → Reflect** (Early modules: Setup, Tensor)
- **Reflect**: Emphasizes metacognition and design trade-offs
- **Questions**: "Why did we design it this way?", "What are the trade-offs?"
- **Focus**: Systems thinking and architectural decisions

#### **Build → Use → Analyze** (Middle modules: Activations, Layers, Networks)
- **Analyze**: Technical depth with profiling, debugging, performance
- **Questions**: "How does this perform?", "What patterns emerge?"
- **Focus**: Technical understanding and optimization

#### **Build → Use → Optimize** (Advanced modules: CNN, DataLoader, Autograd)
- **Optimize**: Real systems iteration and improvement
- **Questions**: "How can we make this faster?", "What about memory usage?"
- **Focus**: Production-ready systems engineering

## Testing Architecture (Comprehensive)

### Test Organization (Critical)
- **Inline unit tests**: In notebook/Python code - immediate tests after each feature
- **Module-level tests**: `modules/source/XX_module/tests/` - pytest with stubbed dependencies
- **Integration tests**: `tests/integration/` - pytest with real cross-module testing
- **System tests**: `tests/system/` - pytest with full end-to-end workflows
- **Package tests**: `tinytorch/tests/` - pytest for exported package functionality

### 1. Inline Unit Tests (Immediate After Each Feature)
```python
# In the notebook/Python development file
# Immediately after implementing each feature

class Sequential:
    def __init__(self, layers):
        self.layers = layers
    
    def __call__(self, x):
        for layer in self.layers:
            x = layer(x)
        return x

# INLINE UNIT TEST - Immediate testing
print("🔬 Unit Test: Sequential Network...")
network = Sequential([Dense(3, 2), ReLU()])
x = Tensor([[1.0, 2.0, 3.0]])
y = network(x)

assert y.shape == (1, 2), f"Expected shape (1, 2), got {y.shape}"
assert np.all(y.data >= 0), "ReLU output should be non-negative"
print("✅ Sequential network unit test passed")
```

### 2. Module-Level Tests (Isolated with Stubbed Data - PYTEST REQUIRED)
```python
# modules/source/04_networks/tests/test_networks.py
"""
Module-level tests for Networks module.
Tests the module in isolation using stubbed/fake data from other modules.
ALWAYS USE PYTEST - No exceptions, no manual testing.
"""

import pytest
import numpy as np
import sys
import os

# Add parent directory to path for module imports
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

# Create fake/stubbed versions of dependencies
class FakeTensor:
    """Stubbed Tensor class for isolated testing."""
    def __init__(self, data):
        self.data = np.array(data)
        self.shape = self.data.shape

class FakeLayer:
    """Stubbed Layer class for isolated testing."""
    def __init__(self, output_shape):
        self.output_shape = output_shape
    
    def __call__(self, x):
        # Return fake output with expected shape
        return FakeTensor(np.random.randn(*self.output_shape))

# Import from the module's development file
from networks_dev import Sequential

class TestSequentialIsolated:
    """Test Sequential class in isolation with fake dependencies."""
    
    def test_sequential_with_fake_layers(self):
        """Test Sequential network with stubbed layers."""
        # Use fake layers that don't depend on other modules
        fake_layer1 = FakeLayer(output_shape=(1, 4))
        fake_layer2 = FakeLayer(output_shape=(1, 2))
        
        network = Sequential([fake_layer1, fake_layer2])
        fake_input = FakeTensor([[1.0, 2.0, 3.0]])
        
        output = network(fake_input)
        
        # Test only the Sequential logic, not the actual layers
        assert len(network.layers) == 2
        assert output.shape == (1, 2)
        
    def test_sequential_layer_composition(self):
        """Test that Sequential properly composes layers."""
        # Test with minimal fake layers
        layers = [FakeLayer((1, 3)), FakeLayer((1, 2))]
        network = Sequential(layers)
        
        assert network.layers == layers
        assert len(network.layers) == 2
        
    def test_edge_cases(self):
        """Test edge cases and error conditions."""
        with pytest.raises(ValueError):
            Sequential([])  # Empty network should raise error
```

### 3. Integration Tests (Real Cross-Module Testing - PYTEST + REAL DATA)
```python
# tests/integration/test_ml_pipeline.py
"""
Integration tests using real implementations from all modules.
Tests how modules actually work together in realistic scenarios.
ALWAYS USE REAL DATA - No mocks, no synthetic data.
"""

import pytest
from tinytorch.core.tensor import Tensor  # Real Tensor
from tinytorch.core.layers import Dense   # Real Dense layer
from tinytorch.core.activations import ReLU  # Real ReLU
from tinytorch.core.networks import Sequential  # Real Sequential

class TestRealMLPipeline:
    """Test real ML pipeline with actual module implementations."""
    
    def test_tensor_to_network_integration(self):
        """Test real tensor flowing through real network."""
        # Real tensor
        x = Tensor([[1.0, 2.0, 3.0]])
        
        # Real network with real layers
        network = Sequential([
            Dense(input_size=3, output_size=4),
            ReLU(),
            Dense(input_size=4, output_size=2)
        ])
        
        # Real forward pass
        output = network(x)
        
        # Test real integration
        assert output.shape == (1, 2)
        assert isinstance(output, Tensor)
        assert np.all(output.data >= 0)  # ReLU ensures non-negative
        
    def test_cifar10_integration(self):
        """Test with actual CIFAR-10 data."""
        from tinytorch.core.dataloader import CIFAR10Dataset
        
        # Real dataset - no mocks allowed
        dataset = CIFAR10Dataset('data/cifar10/', train=True, download=True)
        assert len(dataset) == 50000  # Actual CIFAR-10 size
        
        image, label = dataset[0]
        assert image.shape == (3, 32, 32)  # Real image dimensions
        assert 0 <= label <= 9  # Real class labels
        
        # Test with real network
        network = Sequential([
            Dense(input_size=3072, output_size=128),  # Flattened 32x32x3
            ReLU(),
            Dense(input_size=128, output_size=10)
        ])
        
        # Real forward pass with real data
        flattened_image = image.reshape(1, -1)
        output = network(flattened_image)
        assert output.shape == (1, 10)
```

### 4. System Tests (Full End-to-End - PYTEST + REAL WORKFLOWS)
```python
# tests/system/test_complete_ml_system.py
"""
System tests for complete ML workflows.
Tests entire pipelines from data loading to model training.
ALWAYS USE REAL DATA and REAL WORKFLOWS.
"""

import pytest
from tinytorch.core.dataloader import DataLoader, CIFAR10Dataset
from tinytorch.core.networks import Sequential
from tinytorch.core.layers import Dense
from tinytorch.core.activations import ReLU
from tinytorch.core.optimizers import SGD  # When available

class TestCompleteMLSystem:
    """Test complete ML system end-to-end."""
    
    def test_full_training_pipeline(self):
        """Test complete training pipeline with real data."""
        # Real data loading
        dataset = CIFAR10Dataset('data/cifar10/', train=True, download=True)
        dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
        
        # Real network
        network = Sequential([
            Dense(input_size=3072, output_size=128),
            ReLU(),
            Dense(input_size=128, output_size=10)
        ])
        
        # Test complete training step with real data
        for batch_data, batch_labels in dataloader:
            # Real forward pass
            output = network(batch_data)
            assert output.shape == (32, 10)
            
            # Real loss computation (when available)
            # loss = criterion(output, batch_labels)
            # assert loss.item() > 0
            
            break  # Test one batch for system validation
```

### pytest Features to Use (REQUIRED)

- **Test classes** for organizing related tests
- **pytest fixtures** for setup/teardown
- **Parametrized tests** for testing multiple inputs
- **pytest.raises()** for exception testing
- **assert statements** with descriptive messages
- **Real data only** - No mocks, no synthetic data

### Running Tests (ALWAYS THROUGH PYTEST)

```bash
# Activate environment first
source bin/activate-tinytorch.sh

# Test specific module (recommended)
tito test --module networks

# Test all modules
tito test --all

# Run specific test file directly with pytest
python -m pytest modules/source/04_networks/tests/test_networks.py -v

# Run integration tests
python -m pytest tests/integration/ -v

# Run system tests
python -m pytest tests/system/ -v
```

## Student Implementation Structure

### Comprehensive TODO Pattern (Based on DataLoader Module)
```python
def student_method(self, params):
    """
    TODO: Clear, specific task description
    
    APPROACH:
    1. Concrete first step with specific guidance
    2. Concrete second step with specific guidance  
    3. Concrete third step with specific guidance
    
    EXAMPLE:
    Input: actual_data_example
    Expected: concrete_expected_output
    Your code should: specific_behavior_description
    
    HINTS:
    - Helpful guidance without giving code
    - Systems thinking consideration
    - Real-world connection
    
    SYSTEMS THINKING:
    - Performance consideration
    - Scalability question
    - User experience aspect
    """
    raise NotImplementedError("Student implementation required")
```

### Immediate Testing Pattern (Based on CNN Module)
```python
# %% [markdown]
"""
### 🧪 Unit Test: Conv2D Layer

Let's test your Conv2D implementation immediately after building it!
**This is a unit test** - it tests one specific class in isolation.
"""

# %% nbgrader={"grade": true, "grade_id": "test-conv2d-immediate", "locked": true, "points": 10}
# Test Conv2D layer immediately after implementation
print("🔬 Unit Test: Conv2D Layer...")

try:
    conv = Conv2D(kernel_size=(3, 3))
    x = Tensor(np.random.randn(1, 4, 4))
    y = conv(x)
    
    assert y.shape == (1, 2, 2), f"Expected shape (1, 2, 2), got {y.shape}"
    print("✅ Conv2D layer produces correct output shape")
    
    # Test that kernels are properly initialized
    assert hasattr(conv, 'kernel'), "Conv2D should have kernel attribute"
    assert conv.kernel.shape == (3, 3), f"Kernel shape should be (3, 3), got {conv.kernel.shape}"
    print("✅ Conv2D layer kernel initialization correct")
    
except Exception as e:
    print(f"❌ Conv2D layer test failed: {e}")
    raise

print("🎯 Conv2D layer behavior:")
print("   Applies convolution operation to input")
print("   Reduces spatial dimensions based on kernel size")
print("   Maintains batch dimension")
```

## Comprehensive Testing Strategy

### 1. Inline Unit Tests (Immediate After Each Feature)
- **When**: Immediately after implementing each component
- **What**: Test the specific feature with simple assertions
- **Where**: Inline in the development notebook/Python file
- **Purpose**: Ensure feature works before moving to next
- **Data**: Use simple, hardcoded test data

### 2. Module-Level Tests (Isolated Testing)
- **When**: After completing the entire module
- **What**: Test module components in isolation with stubbed dependencies
- **Where**: `modules/source/XX_module/tests/test_module.py`
- **Purpose**: Ensure module logic works independently of other modules
- **Data**: Use fake/stubbed data to avoid dependencies

### 3. Integration Tests (Real Cross-Module Testing)
- **When**: After multiple modules are complete
- **What**: Test how modules work together with real implementations
- **Where**: `tests/integration/` directory
- **Purpose**: Ensure modules integrate properly in ML workflows
- **Data**: Use real implementations and actual data

### 4. System Tests (Full End-to-End)
- **When**: After major functionality is complete
- **What**: Test complete ML pipelines from data to results
- **Where**: `tests/system/` directory
- **Purpose**: Ensure entire system works in production scenarios
- **Data**: Use real datasets and complete workflows

## Progress Feedback Pattern

```python
def _download_progress_hook(self, count, block_size, total_size):
    """Progress callback for downloads."""
    if total_size > 0:
        percent = min(100, (count * block_size * 100) // total_size)
        mb_downloaded = (count * block_size) / (1024 * 1024)
        mb_total = total_size / (1024 * 1024)
        
        # Visual progress bar
        bar_length = 50
        filled_length = int(bar_length * percent // 100)
        bar = '█' * filled_length + '░' * (bar_length - filled_length)
        
        print(f'\r📥 [{bar}] {percent}% ({mb_downloaded:.1f}/{mb_total:.1f} MB)', 
              end='', flush=True)
```

## Visual Feedback Pattern (Development Only)

```python
def show_data_samples(dataset, num_samples=8):
    """Show grid of actual data samples (development only)."""
    # NOT exported to package - development feedback only
    if not _should_show_plots():
        return
        
    fig, axes = plt.subplots(2, 4, figsize=(12, 6))
    for i in range(num_samples):
        image, label = dataset[i]
        # Show actual data with proper visualization
        axes[i//4, i%4].imshow(image.transpose(1, 2, 0))
        axes[i//4, i%4].set_title(f'Class: {label}')
    plt.show()
```

## Best Module Examples

### Networks Module (Our Gold Standard)
- **Structure**: Clear Sequential class with layers composition
- **Testing**: Immediate unit tests after each feature
- **Comprehensive**: Full test suite covering all scenarios
- **Educational**: Build → Use → Analyze pattern with technical depth
- **Real-world**: Actual network architectures (MLP, classification, regression)

### Layers Module (Excellent Foundation)
- **Structure**: Dense layer with proper matrix multiplication
- **Testing**: Unit tests for each operation (matmul, forward pass, initialization)
- **Comprehensive**: Integration tests with activations and networks
- **Educational**: Build → Use → Analyze with performance considerations
- **Real-world**: Production-ready linear transformations

### DataLoader Module (Systems Excellence)
- **Structure**: Abstract Dataset interface with concrete implementations
- **Testing**: Real data testing with CIFAR-10
- **Comprehensive**: Complete pipeline testing
- **Educational**: Build → Use → Optimize with systems thinking
- **Real-world**: Production data loading patterns

## Key Anti-Patterns to Avoid

### ❌ Don't Test Everything at the End
```python
# BAD: All tests at module end
def implement_all_features():
    # Implement feature 1
    # Implement feature 2
    # Implement feature 3
    
# Then test everything at once
def test_everything():
    # Test all features together
    
# GOOD: Inline unit tests after each feature
def implement_feature_1():
    # Implementation
    pass
    
# Immediate inline unit test
assert feature_1_works()
print("✅ Feature 1 working")

def implement_feature_2():
    # Implementation
    pass
    
# Immediate inline unit test
assert feature_2_works()
print("✅ Feature 2 working")
```

### ❌ Don't Use Real Dependencies in Module Tests
```python
# BAD: Module tests with real dependencies
# modules/source/04_networks/tests/test_networks.py
from tinytorch.core.layers import Dense  # Real dependency
from tinytorch.core.activations import ReLU  # Real dependency

def test_sequential_with_real_layers():
    # This creates coupling between modules
    network = Sequential([Dense(3, 2), ReLU()])
    
# GOOD: Module tests with stubbed dependencies
class FakeDense:
    def __call__(self, x):
        return FakeTensor(np.random.randn(1, 2))

class FakeReLU:
    def __call__(self, x):
        return x  # Simplified fake behavior

def test_sequential_with_fake_layers():
    # This tests only Sequential logic
    network = Sequential([FakeDense(), FakeReLU()])
```

### ❌ Don't Use Mock/Synthetic Data in Integration/System Tests
```python
# BAD: Mock/synthetic data in integration tests
class MockDataset:
    def __init__(self, size):
        self.data = np.random.randn(size, 784)  # Fake data
        
def test_mock_dataset():
    dataset = MockDataset(size=100)  # Meaningless test
    assert len(dataset) == 100
    
# GOOD: Real data in integration/system tests
def test_cifar10_dataset():
    """Test with actual CIFAR-10 data."""
    dataset = CIFAR10Dataset('data/cifar10/', train=True, download=True)
    assert len(dataset) == 50000  # Actual CIFAR-10 size
    image, label = dataset[0]
    assert image.shape == (3, 32, 32)  # Real image dimensions
    assert 0 <= label <= 9  # Real class labels
```

### ❌ Don't Mix Testing Levels
```python
# BAD: Integration testing in module-level tests
# modules/source/04_networks/tests/test_networks.py
def test_networks_with_real_dataloader():
    # This should be in tests/integration/
    from tinytorch.core.dataloader import DataLoader
    # Testing cross-module integration
    
# GOOD: Keep testing levels separate
# modules/source/04_networks/tests/test_networks.py - Module-level with stubs
def test_sequential_isolated():
    # Test with fake data only
    
# tests/integration/test_ml_pipeline.py - Integration with real modules
def test_networks_with_real_dataloader():
    # Test real cross-module integration
```

### ❌ Don't Skip pytest or Use Manual Testing
```python
# BAD: Manual testing or custom test runners
def manual_test():
    print("Testing component...")
    component = Component()
    if component.works():
        print("PASS")
    else:
        print("FAIL")
        
# BAD: Using unittest instead of pytest
import unittest
class TestComponent(unittest.TestCase):
    def test_component(self):
        self.assertTrue(component.works())
        
# GOOD: Always use pytest
import pytest

class TestComponent:
    """Test component functionality."""
    
    def test_component_works(self):
        """Test that component works correctly."""
        component = Component()
        assert component.works()
        
    def test_component_edge_cases(self):
        """Test edge cases and error conditions."""
        with pytest.raises(ValueError):
            Component(invalid_input)
```

## Quality Standards

### Before Release Checklist
- [ ] Uses real data, not synthetic/mock data (for integration/system tests)
- [ ] ALWAYS uses pytest for all testing - no exceptions
- [ ] Includes progress feedback for long operations
- [ ] Visual feedback functions (development only, not exported)
- [ ] Inline unit tests after each feature implementation
- [ ] Module-level tests use stubbed/fake dependencies for isolation
- [ ] Integration tests use real cross-module implementations with real data
- [ ] System tests use complete workflows with real datasets
- [ ] Clear separation: inline → module → integration → system testing
- [ ] Follows "Build → Use → Analyze/Test" progression
- [ ] TODO guidance includes systems thinking
- [ ] Clean separation between development and exports
- [ ] All test files follow pytest structure and patterns

### Student Experience Requirements
- [ ] Clear learning progression with immediate inline feedback
- [ ] Inline unit tests provide confidence after each feature
- [ ] Module tests demonstrate isolation and stubbing concepts
- [ ] Integration tests show real-world module interactions with real data
- [ ] System tests demonstrate complete ML workflows
- [ ] pytest provides consistent testing experience across all levels
- [ ] Real-world relevance and connections
- [ ] Smooth transition to next modules
- [ ] Test-driven development workflow

## Success Metrics

**Students should be able to:**
- Write inline unit tests immediately after each feature
- Understand the difference between inline, module, integration, and system tests
- Create stubbed/fake dependencies for module-level testing
- Explain what they built in simple terms
- Modify code to solve related problems
- Connect module concepts to real ML systems
- Debug issues by understanding the different testing levels

**Modules should achieve:**
- High student engagement and completion rates
- Clear testing patterns with immediate inline feedback
- Proper isolation in module-level tests using stubs
- Realistic integration testing with real dependencies
- Smooth progression to next modules
- Real-world relevance and production quality
- Consistent test-driven development workflow

---

**Remember**: We're teaching ML systems engineering with comprehensive testing architecture. Inline unit tests provide immediate feedback, module-level tests use stubbed dependencies for isolation, integration tests use real cross-module implementations with real data, and system tests validate complete workflows. ALWAYS use pytest for all testing levels. Follow the "Build → Use → Analyze/Test" cycle with proper testing separation.

## Development Workflow Summary

### Complete Module Development Cycle

1. **Setup Phase**
   ```bash
   cd modules/source/XX_module_name/
   # Work in module_name_dev.py (Jupytext format)
   # Module tests go in tests/test_module_name.py (pytest with stubs)
   # Integration tests go in tests/integration/ (pytest with real modules)
   # System tests go in tests/system/ (pytest with real workflows)
   ```

2. **Feature Development Phase** (Repeat for each component)
   ```python
   # Step 1: Build the feature
   class NewComponent:
       def method(self):
           # Implementation
           pass
   
   # Step 2: Use the feature (immediate inline unit test)
   component = NewComponent()
   result = component.method()
   assert result.shape == expected_shape  # Inline unit test
   print(f"✅ {component.__class__.__name__} working: {result}")
   
   # Step 3: Analyze/Test the feature (more comprehensive inline testing)
   def test_new_component():
       component = NewComponent()
       result = component.method()
       assert result.shape == expected_shape
       assert np.allclose(result.data, expected_data)
       print("✅ Component inline unit test passed")
   
   # Run the test immediately
   test_new_component()
   ```

3. **Module Completion Phase**
   ```bash
   # Run module-level tests (pytest with stubbed dependencies)
   python -m pytest modules/source/XX_module_name/tests/test_module_name.py -v
   
   # Export to package
   tito package nbdev --export XX_module_name
   ```

4. **Integration Testing Phase**
   ```bash
   # Run integration tests (pytest with real cross-module dependencies)
   python -m pytest tests/integration/ -v
   
   # Run system tests (pytest with full end-to-end workflows)
   python -m pytest tests/system/ -v
   ```

### Daily Development Rhythm

- **Morning**: Review previous day's inline tests, ensure all passing
- **Feature work**: Build → Use → Inline Test for each component
- **Module work**: Create pytest-based module tests with stubbed dependencies
- **Integration work**: Create pytest-based integration tests with real modules and real data
- **System work**: Create pytest-based system tests with complete workflows
- **End of day**: Run all test levels with pytest, commit working features

### Quality Gates

- **Feature Level**: Inline unit test must pass immediately
- **Module Level**: pytest tests with stubbed dependencies must pass before export
- **Integration Level**: pytest tests with real cross-module dependencies must pass before merge
- **System Level**: pytest tests with full end-to-end workflows must pass before release

### Testing Principles Summary

1. **ALWAYS use pytest** - No exceptions, no manual testing, no other frameworks
2. **Real data for integration/system** - No mocks in integration or system tests
3. **Stubbed dependencies for module tests** - Isolation through fake/stubbed components
4. **Immediate inline feedback** - Test every feature as you build it
5. **Four-tier architecture** - Inline → Module → Integration → System
6. **Proper separation** - Each level has distinct purpose and data requirements

This comprehensive workflow ensures students understand professional testing architecture while building confidence incrementally through immediate feedback and systematic validation at every level.
