# ---
# jupyter:
#   jupytext:
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.17.1
# ---

# %% [markdown]
"""
# Autograd - Automatic Differentiation Engine

Welcome to Autograd! You'll build automatic differentiation step by step, giving your Tensor class the ability to compute gradients automatically for neural network training.

## üîó Building on Previous Learning
**What You Built Before**:
- Module 01 (Setup): Development environment ready
- Module 02 (Tensor): Complete tensor operations with math
- Module 03 (Activations): Functions that add intelligence to networks
- Module 04 (Losses): Functions that measure learning progress

**What's Working**: Your tensors can do math, activations, and loss calculations perfectly!

**The Gap**: Your tensors can't learn - they have no memory of how gradients flow backward through computations.

**This Module's Solution**: Enhance your existing Tensor class with gradient tracking abilities, step by step.

**Connection Map**:
```
Math Operations ‚Üí Smart Operations ‚Üí Learning Operations
(Pure Tensors)   (+ Autograd)      (+ Optimizers)
```

## Learning Objectives
1. **Incremental Enhancement**: Add gradient tracking without breaking existing code
2. **Chain Rule Mastery**: Understand how gradients flow through complex expressions
3. **Systems Understanding**: Memory and performance implications of automatic differentiation
4. **Professional Skills**: How to enhance software systems safely

## Build ‚Üí Test ‚Üí Use
1. **Build**: Six incremental steps, each immediately testable
2. **Test**: Frequent validation with clear success indicators
3. **Use**: Enable gradient-based optimization for training

## üì¶ Where This Code Lives in the Final Package

**Learning Side:** You work in modules/05_autograd/autograd_dev.py
**Building Side:** Code exports to tinytorch.core.autograd

```python
# Final package structure:
from tinytorch.core.autograd import Tensor  # Enhanced Tensor with gradients
from tinytorch.core.tensor import Tensor    # Your original pure Tensor (backup)

# Your enhanced Tensor can do everything:
x = Tensor([1, 2, 3], requires_grad=True)   # New gradient capability
y = x + 2                                   # Same math operations
y.backward()                                # New gradient computation
```

**Why this matters:**
- **Learning:** Experience incremental software enhancement with immediate feedback
- **Production:** How real ML systems add features without breaking existing functionality
- **Professional Practice:** Safe software evolution patterns used in industry
- **Integration:** Your enhanced Tensor works with all previous modules
"""

# %%
#| default_exp core.autograd

#| export
import numpy as np
import sys
from typing import Union, List, Optional, Callable, Any

# Import the pure Tensor class from Module 02
try:
    from tinytorch.core.tensor import Tensor as BaseTensor
except ImportError:
    # For development, import from local modules
    import os
    sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '02_tensor'))
    from tensor_dev import Tensor as BaseTensor

# %%
print("üî• TinyTorch Autograd Module")
print(f"NumPy version: {np.__version__}")
print(f"Python version: {sys.version_info.major}.{sys.version_info.minor}")
print("Ready to enhance Tensor with gradients!")

# %% [markdown]
"""
## Step 1: Teaching Our Tensor to Remember Gradients

Our Tensor class from Module 02 is perfect for storing data and doing math. But for training neural networks, we need it to remember how gradients flow backward through computations.

Think of it like teaching someone to remember the steps of a recipe so they can explain it later to others.

### What We're Adding

We need three pieces of memory for our Tensor:

1. **Should I remember?** (`requires_grad`) - Like asking "should I pay attention to gradients?"
2. **What did I learn?** (`grad`) - The accumulated gradient information
3. **How do I teach others?** (`grad_fn`) - Function to pass gradients backward

These three attributes will transform our mathematical Tensor into a learning-capable Tensor.

### Why Start Here?

Before we can compute any gradients, we need places to store them. This is the foundation - like preparing notebooks before a lecture.
"""

# %% nbgrader={"grade": false, "grade_id": "tensor-gradient-attributes", "solution": true}
#| export
class Tensor(BaseTensor):
    """
    Enhanced Tensor with gradient tracking capabilities.

    Inherits all functionality from BaseTensor and adds gradient memory.
    """

    def __init__(self, data, dtype=None, requires_grad=False):
        """
        Initialize Tensor with gradient tracking support.

        TODO: Add gradient tracking attributes to existing Tensor

        APPROACH:
        1. Call parent __init__ to preserve all existing functionality
        2. Add requires_grad boolean for gradient tracking control
        3. Add grad attribute to store accumulated gradients (starts as None)
        4. Add grad_fn attribute to store backward function (starts as None)

        EXAMPLE:
        >>> t = Tensor([1, 2, 3], requires_grad=True)
        >>> print(t.requires_grad)  # True - ready to track gradients
        >>> print(t.grad)          # None - no gradients accumulated yet
        >>> print(t.grad_fn)       # None - no backward function yet

        HINT: This is just storage - we're not computing anything yet
        """
        ### BEGIN SOLUTION
        # Call parent constructor to preserve all existing functionality
        super().__init__(data, dtype)

        # Add gradient tracking attributes
        self.requires_grad = requires_grad
        self.grad = None        # Will store accumulated gradients
        self.grad_fn = None     # Will store backward propagation function
        ### END SOLUTION

# %% [markdown]
"""
### üß™ Test Step 1: Verify Gradient Memory
This test confirms our Tensor can remember gradient information
"""

# %%
def test_step1_gradient_attributes():
    """Test that Tensor has gradient memory capabilities."""
    print("üî¨ Step 1 Test: Gradient Memory...")

    # Test tensor with gradient tracking enabled
    x = Tensor([1.0, 2.0, 3.0], requires_grad=True)

    # Verify all gradient attributes exist and have correct initial values
    assert hasattr(x, 'requires_grad'), "Tensor should have requires_grad attribute"
    assert x.requires_grad == True, "requires_grad should be True when requested"
    assert x.grad is None, "grad should start as None"
    assert x.grad_fn is None, "grad_fn should start as None"

    # Test tensor without gradient tracking
    y = Tensor([4.0, 5.0, 6.0], requires_grad=False)
    assert y.requires_grad == False, "requires_grad should be False by default"

    # Verify existing functionality still works
    z = x + y  # Should work exactly like before
    assert hasattr(z, 'data'), "Enhanced tensor should still have data"

    print("‚úÖ Success! Your Tensor now has gradient memory!")
    print(f"  ‚Ä¢ Gradient tracking: {x.requires_grad}")
    print(f"  ‚Ä¢ Initial gradients: {x.grad}")
    print(f"  ‚Ä¢ Backward function: {x.grad_fn}")

test_step1_gradient_attributes()

# %% [markdown]
"""
## Step 2: Teaching Our Tensor to Learn (Backward Method)

Now that our Tensor has memory for gradients, we need to teach it how to accumulate gradients when they flow backward from later computations.

Think of this like teaching someone to collect feedback from others and combine it with what they already know.

### The Backward Method

The `backward()` method will:
1. **Check if learning is enabled** (requires_grad must be True)
2. **Accumulate gradients** (add new gradients to existing ones)
3. **Propagate backwards** (tell earlier computations about the gradients)

This is the heart of learning - how information flows backward to update our understanding.

### Why Accumulation Matters

Neural networks often compute multiple losses that all depend on the same parameters. We need to collect ALL the gradients, not just the last one.
"""

# %% nbgrader={"grade": false, "grade_id": "tensor-backward-method", "solution": true}
def backward(self, gradient=None):
    """
    Accumulate gradients and propagate them backward through computation.

    TODO: Implement gradient accumulation and backward propagation

    APPROACH:
    1. Check if this tensor requires gradients (error if not)
    2. Set default gradient for scalar outputs (ones_like for scalars)
    3. Accumulate gradient: first time = store, subsequent = add
    4. Propagate backward through grad_fn if it exists

    EXAMPLE:
    >>> x = Tensor([2.0], requires_grad=True)
    >>> x.grad = None  # No gradients yet
    >>> x.backward([1.0])  # First gradient
    >>> print(x.grad)  # [1.0]
    >>> x.backward([0.5])  # Accumulate second gradient
    >>> print(x.grad)  # [1.5] - accumulated!

    HINTS:
    - Default gradient for scalars should be ones_like(self.data)
    - Use += for accumulation, but handle None case first
    - Only call grad_fn if it exists (not None)
    """
    ### BEGIN SOLUTION
    # Check if this tensor should accumulate gradients
    if not self.requires_grad:
        raise RuntimeError("Tensor doesn't require gradients - set requires_grad=True")

    # Set default gradient for scalar outputs
    if gradient is None:
        if self.data.size == 1:  # Scalar output
            gradient = np.ones_like(self.data)
        else:
            raise RuntimeError("gradient must be specified for non-scalar tensors")

    # Accumulate gradients: first time or add to existing
    if self.grad is None:
        self.grad = np.array(gradient)  # First gradient
    else:
        self.grad = self.grad + gradient  # Accumulate

    # Propagate gradients backward through computation graph
    if self.grad_fn is not None:
        self.grad_fn(gradient)
    ### END SOLUTION

# Add the backward method to our Tensor class
Tensor.backward = backward

# %% [markdown]
"""
### üß™ Test Step 2: Verify Learning Ability
This test confirms our Tensor can accumulate gradients properly
"""

# %%
def test_step2_backward_method():
    """Test that Tensor can accumulate gradients."""
    print("üî¨ Step 2 Test: Learning Ability...")

    # Test basic gradient accumulation
    x = Tensor([2.0], requires_grad=True)

    # First gradient
    x.backward(np.array([1.0]))
    assert np.allclose(x.grad, [1.0]), f"First gradient failed: expected [1.0], got {x.grad}"

    # Second gradient should accumulate
    x.backward(np.array([0.5]))
    assert np.allclose(x.grad, [1.5]), f"Accumulation failed: expected [1.5], got {x.grad}"

    # Test default gradient for scalars
    y = Tensor([3.0], requires_grad=True)
    y.backward()  # No gradient specified - should use default
    assert np.allclose(y.grad, [1.0]), f"Default gradient failed: expected [1.0], got {y.grad}"

    # Test error for non-gradient tensor
    z = Tensor([4.0], requires_grad=False)
    try:
        z.backward([1.0])
        assert False, "Should have raised error for non-gradient tensor"
    except RuntimeError:
        pass  # Expected error

    print("‚úÖ Success! Your Tensor can now learn from gradients!")
    print(f"  ‚Ä¢ Accumulation works: {x.grad}")
    print(f"  ‚Ä¢ Default gradients work: {y.grad}")

test_step2_backward_method()

# %% [markdown]
"""
## Step 3: Smart Addition (x + y Learns!)

Now we'll make addition smart - when two tensors are added, the result should remember how to flow gradients back to both inputs.

Think of this like a conversation between three people: when C = A + B, and someone gives feedback to C, C knows to pass that same feedback to both A and B.

### Mathematical Foundation

For addition z = x + y:
- ‚àÇz/‚àÇx = 1 (changing x by 1 changes z by 1)
- ‚àÇz/‚àÇy = 1 (changing y by 1 changes z by 1)

So gradients flow unchanged to both inputs: grad_x = grad_z, grad_y = grad_z

### Why Enhancement, Not Replacement

We're enhancing the existing `__add__` method, not replacing it. The math stays the same - we just add gradient tracking on top.
"""

# %% nbgrader={"grade": false, "grade_id": "enhanced-addition", "solution": true}
# Store the original addition method so we can enhance it
_original_add = Tensor.__add__

def enhanced_add(self, other):
    """
    Enhanced addition with automatic gradient tracking.

    TODO: Add gradient tracking to existing addition operation

    APPROACH:
    1. Do the original math (call _original_add)
    2. If either input tracks gradients, result should too
    3. Create grad_fn that sends gradients back to both inputs
    4. Remember: for addition, both inputs get the same gradient

    EXAMPLE:
    >>> x = Tensor([2.0], requires_grad=True)
    >>> y = Tensor([3.0], requires_grad=True)
    >>> z = x + y  # Enhanced addition
    >>> z.backward()
    >>> print(x.grad)  # [1.0] - same as gradient flowing to z
    >>> print(y.grad)  # [1.0] - same as gradient flowing to z

    HINTS:
    - Use _original_add for the math computation
    - Check if other has requires_grad attribute (might be scalar)
    - Addition rule: ‚àÇ(a+b)/‚àÇa = 1, ‚àÇ(a+b)/‚àÇb = 1
    """
    ### BEGIN SOLUTION
    # Do the original math - this preserves all existing functionality
    result = _original_add(self, other)

    # Check if either input requires gradients
    other_requires_grad = hasattr(other, 'requires_grad') and other.requires_grad
    needs_grad = self.requires_grad or other_requires_grad

    if needs_grad:
        # Result should track gradients
        result.requires_grad = True

        # Create backward function for gradient propagation
        def grad_fn(gradient):
            """Send gradients back to both inputs (addition rule)."""
            # For addition: ‚àÇ(a+b)/‚àÇa = 1, so gradient flows unchanged
            if self.requires_grad:
                self.backward(gradient)
            if other_requires_grad:
                other.backward(gradient)

        # Attach the backward function to the result
        result.grad_fn = grad_fn

    return result
    ### END SOLUTION

# Replace the addition method with our enhanced version
Tensor.__add__ = enhanced_add

# %% [markdown]
"""
### üß™ Test Step 3: Verify Smart Addition
This test confirms addition automatically tracks gradients
"""

# %%
def test_step3_smart_addition():
    """Test that addition tracks gradients automatically."""
    print("üî¨ Step 3 Test: Smart Addition...")

    # Test basic addition with gradients
    x = Tensor([2.0], requires_grad=True)
    y = Tensor([3.0], requires_grad=True)
    z = x + y

    # Verify forward pass
    assert np.allclose(z.data, [5.0]), f"Addition math failed: expected [5.0], got {z.data}"

    # Verify gradient tracking is enabled
    assert z.requires_grad == True, "Result should require gradients when inputs do"
    assert z.grad_fn is not None, "Result should have backward function"

    # Test backward pass
    z.backward()
    assert np.allclose(x.grad, [1.0]), f"x gradient failed: expected [1.0], got {x.grad}"
    assert np.allclose(y.grad, [1.0]), f"y gradient failed: expected [1.0], got {y.grad}"

    # Test addition with scalar (no gradients)
    a = Tensor([1.0], requires_grad=True)
    b = a + 5.0  # Adding scalar
    b.backward()
    assert np.allclose(a.grad, [1.0]), "Gradient should flow through scalar addition"

    # Test backward compatibility - no gradients
    p = Tensor([1.0])  # No requires_grad
    q = Tensor([2.0])  # No requires_grad
    r = p + q
    assert not hasattr(r, 'requires_grad') or not r.requires_grad, "Should not track gradients by default"

    print("‚úÖ Success! Addition is now gradient-aware!")
    print(f"  ‚Ä¢ Forward: {x.data} + {y.data} = {z.data}")
    print(f"  ‚Ä¢ Backward: x.grad = {x.grad}, y.grad = {y.grad}")

test_step3_smart_addition()

# %% [markdown]
"""
## Step 4: Smart Multiplication (x * y Learns!)

Now we'll enhance multiplication with gradient tracking. This is more interesting than addition because of the product rule.

Think of multiplication like mixing ingredients: when you change one ingredient, the effect depends on how much of the other ingredient you have.

### Mathematical Foundation - The Product Rule

For multiplication z = x * y:
- ‚àÇz/‚àÇx = y (changing x is multiplied by y's current value)
- ‚àÇz/‚àÇy = x (changing y is multiplied by x's current value)

This means we need to remember the input values to compute gradients correctly.

### Why This Matters

Multiplication is everywhere in neural networks:
- Linear layers: output = input * weights
- Attention mechanisms: attention_scores * values
- Element-wise operations in activations

Getting multiplication gradients right is crucial for training.
"""

# %% nbgrader={"grade": false, "grade_id": "enhanced-multiplication", "solution": true}
# Store the original multiplication method
_original_mul = Tensor.__mul__

def enhanced_mul(self, other):
    """
    Enhanced multiplication with automatic gradient tracking.

    TODO: Add gradient tracking to multiplication using product rule

    APPROACH:
    1. Do the original math (call _original_mul)
    2. If either input tracks gradients, result should too
    3. Create grad_fn using product rule: ‚àÇ(a*b)/‚àÇa = b, ‚àÇ(a*b)/‚àÇb = a
    4. Handle both Tensor and scalar multiplication

    EXAMPLE:
    >>> x = Tensor([2.0], requires_grad=True)
    >>> y = Tensor([3.0], requires_grad=True)
    >>> z = x * y  # z = [6.0]
    >>> z.backward()
    >>> print(x.grad)  # [3.0] - gradient is y's value
    >>> print(y.grad)  # [2.0] - gradient is x's value

    HINTS:
    - Product rule: ‚àÇ(a*b)/‚àÇa = b, ‚àÇ(a*b)/‚àÇb = a
    - Remember to handle scalars (use .data if available, else use directly)
    - Gradients are: grad_x = gradient * other, grad_y = gradient * self
    """
    ### BEGIN SOLUTION
    # Do the original math - preserves existing functionality
    result = _original_mul(self, other)

    # Check if either input requires gradients
    other_requires_grad = hasattr(other, 'requires_grad') and other.requires_grad
    needs_grad = self.requires_grad or other_requires_grad

    if needs_grad:
        # Result should track gradients
        result.requires_grad = True

        # Create backward function using product rule
        def grad_fn(gradient):
            """Apply product rule for multiplication gradients."""
            if self.requires_grad:
                # ‚àÇ(a*b)/‚àÇa = b, so gradient flows as: gradient * b
                if hasattr(other, 'data'):
                    self_grad = gradient * other.data
                else:
                    self_grad = gradient * other  # other is scalar
                self.backward(self_grad)

            if other_requires_grad:
                # ‚àÇ(a*b)/‚àÇb = a, so gradient flows as: gradient * a
                other_grad = gradient * self.data
                other.backward(other_grad)

        # Attach the backward function to the result
        result.grad_fn = grad_fn

    return result
    ### END SOLUTION

# Replace multiplication method with enhanced version
Tensor.__mul__ = enhanced_mul

# %% [markdown]
"""
### üß™ Test Step 4: Verify Smart Multiplication
This test confirms multiplication uses the product rule correctly
"""

# %%
def test_step4_smart_multiplication():
    """Test that multiplication tracks gradients with product rule."""
    print("üî¨ Step 4 Test: Smart Multiplication...")

    # Test basic multiplication with gradients
    x = Tensor([2.0], requires_grad=True)
    y = Tensor([3.0], requires_grad=True)
    z = x * y

    # Verify forward pass
    assert np.allclose(z.data, [6.0]), f"Multiplication math failed: expected [6.0], got {z.data}"

    # Test backward pass with product rule
    z.backward()
    assert np.allclose(x.grad, [3.0]), f"x gradient failed: expected [3.0] (y's value), got {x.grad}"
    assert np.allclose(y.grad, [2.0]), f"y gradient failed: expected [2.0] (x's value), got {y.grad}"

    # Test multiplication by scalar
    a = Tensor([4.0], requires_grad=True)
    b = a * 2.0  # Multiply by scalar
    b.backward()
    assert np.allclose(a.grad, [2.0]), f"Scalar multiplication failed: expected [2.0], got {a.grad}"

    # Test more complex values
    p = Tensor([1.5], requires_grad=True)
    q = Tensor([2.5], requires_grad=True)
    r = p * q  # Should be 3.75

    assert np.allclose(r.data, [3.75]), f"Complex multiplication failed: expected [3.75], got {r.data}"
    r.backward()
    assert np.allclose(p.grad, [2.5]), f"Complex p gradient failed: expected [2.5], got {p.grad}"
    assert np.allclose(q.grad, [1.5]), f"Complex q gradient failed: expected [1.5], got {q.grad}"

    print("‚úÖ Success! Multiplication follows the product rule!")
    print(f"  ‚Ä¢ Forward: {x.data} * {y.data} = {z.data}")
    print(f"  ‚Ä¢ Product rule: x.grad = {x.grad}, y.grad = {y.grad}")

test_step4_smart_multiplication()

# %% [markdown]
"""
## Step 5: Chain Rule Magic (Complex Expressions Work!)

Now comes the magic moment - combining our smart operations to see the chain rule work automatically through complex expressions.

When you build expressions like `z = (x + y) * (x - y)`, each operation tracks gradients locally, and they automatically chain together. This is what makes deep learning possible!

Think of it like a telephone game where each person (operation) passes the message (gradient) backward, and everyone modifies it according to their local rule.

### The Chain Rule in Action

For f(x,y) = (x + y) * (x - y) = x¬≤ - y¬≤:
1. Addition: passes gradients unchanged
2. Subtraction: passes gradients (first unchanged, second negated)
3. Multiplication: applies product rule
4. Chain rule: combines all effects automatically

Expected final gradients:
- ‚àÇf/‚àÇx = 2x (derivative of x¬≤ - y¬≤)
- ‚àÇf/‚àÇy = -2y (derivative of x¬≤ - y¬≤)

### Why This Is Revolutionary

You don't need to derive gradients manually anymore! The system automatically:
- Tracks every operation
- Applies local gradient rules
- Chains them together correctly
"""

# %% nbgrader={"grade": false, "grade_id": "enhanced-subtraction", "solution": true}
# We need subtraction to complete our operations set
_original_sub = getattr(Tensor, '__sub__', None)

def enhanced_sub(self, other):
    """
    Enhanced subtraction with automatic gradient tracking.

    TODO: Add gradient tracking to subtraction

    APPROACH:
    1. Compute subtraction (may need to implement if not in base class)
    2. For gradients: ‚àÇ(a-b)/‚àÇa = 1, ‚àÇ(a-b)/‚àÇb = -1
    3. First input gets gradient unchanged, second gets negative gradient

    HINTS:
    - Subtraction rule: ‚àÇ(a-b)/‚àÇa = 1, ‚àÇ(a-b)/‚àÇb = -1
    - Handle case where base class might not have subtraction
    - Use np.subtract or manual computation if needed
    """
    ### BEGIN SOLUTION
    # Compute subtraction (implement if not available)
    if _original_sub is not None:
        result = _original_sub(self, other)
    else:
        # Implement subtraction manually
        if hasattr(other, 'data'):
            result_data = self.data - other.data
        else:
            result_data = self.data - other
        result = Tensor(result_data)

    # Check if either input requires gradients
    other_requires_grad = hasattr(other, 'requires_grad') and other.requires_grad
    needs_grad = self.requires_grad or other_requires_grad

    if needs_grad:
        result.requires_grad = True

        def grad_fn(gradient):
            """Apply subtraction gradient rule."""
            if self.requires_grad:
                # ‚àÇ(a-b)/‚àÇa = 1, gradient flows unchanged
                self.backward(gradient)
            if other_requires_grad:
                # ‚àÇ(a-b)/‚àÇb = -1, gradient is negated
                other.backward(-gradient)

        result.grad_fn = grad_fn

    return result
    ### END SOLUTION

# Add subtraction method to Tensor
Tensor.__sub__ = enhanced_sub

# %% [markdown]
"""
### üß™ Test Step 5: Verify Chain Rule Magic
This test confirms complex expressions compute gradients automatically
"""

# %%
def test_step5_chain_rule_magic():
    """Test that complex expressions automatically chain gradients."""
    print("üî¨ Step 5 Test: Chain Rule Magic...")

    # Test complex expression: (x + y) * (x - y) = x¬≤ - y¬≤
    x = Tensor([3.0], requires_grad=True)
    y = Tensor([2.0], requires_grad=True)

    # Build computation graph step by step
    sum_part = x + y      # 3 + 2 = 5
    diff_part = x - y     # 3 - 2 = 1
    result = sum_part * diff_part  # 5 * 1 = 5

    # Verify forward computation
    expected_forward = 3.0**2 - 2.0**2  # x¬≤ - y¬≤ = 9 - 4 = 5
    assert np.allclose(result.data, [expected_forward]), f"Forward failed: expected [{expected_forward}], got {result.data}"

    # Test the magic - backward propagation
    result.backward()

    # Expected gradients for f(x,y) = x¬≤ - y¬≤
    expected_x_grad = 2 * 3.0  # ‚àÇ(x¬≤-y¬≤)/‚àÇx = 2x = 6
    expected_y_grad = -2 * 2.0  # ‚àÇ(x¬≤-y¬≤)/‚àÇy = -2y = -4

    assert np.allclose(x.grad, [expected_x_grad]), f"x gradient failed: expected [{expected_x_grad}], got {x.grad}"
    assert np.allclose(y.grad, [expected_y_grad]), f"y gradient failed: expected [{expected_y_grad}], got {y.grad}"

    # Test another complex expression: 2*x*y + x
    a = Tensor([2.0], requires_grad=True)
    b = Tensor([3.0], requires_grad=True)

    expr = (a * b) * 2.0 + a  # 2*a*b + a = 2*2*3 + 2 = 14

    assert np.allclose(expr.data, [14.0]), f"Complex expression failed: expected [14.0], got {expr.data}"

    expr.backward()
    # ‚àÇ(2ab + a)/‚àÇa = 2b + 1 = 2*3 + 1 = 7
    # ‚àÇ(2ab + a)/‚àÇb = 2a = 2*2 = 4
    assert np.allclose(a.grad, [7.0]), f"Complex a gradient failed: expected [7.0], got {a.grad}"
    assert np.allclose(b.grad, [4.0]), f"Complex b gradient failed: expected [4.0], got {b.grad}"

    print("‚úÖ Success! Chain rule works automatically!")
    print(f"  ‚Ä¢ Expression: (x + y) * (x - y) = x¬≤ - y¬≤")
    print(f"  ‚Ä¢ Forward: {result.data}")
    print(f"  ‚Ä¢ Gradients: ‚àÇf/‚àÇx = {x.grad}, ‚àÇf/‚àÇy = {y.grad}")
    print("üéâ Your tensors can now learn through any expression!")

test_step5_chain_rule_magic()

# %% [markdown]
"""
## Step 6: Integration Testing (Complete Victory!)

Time to celebrate! Let's test our complete autograd system with realistic neural network scenarios to make sure everything works together perfectly.

We'll test scenarios that mirror what happens in real neural networks:
- Linear transformations (matrix operations)
- Activation functions
- Loss computations
- Complex multi-step computations

This validates that your autograd system is ready to train real neural networks!

### What Makes This Special

Your autograd implementation now provides the foundation for all neural network training:
- **Forward Pass**: Tensors compute values and build computation graphs
- **Backward Pass**: Gradients flow automatically through any expression
- **Parameter Updates**: Optimizers will use these gradients to update weights

You've built the core engine that powers modern deep learning!
"""

# %% [markdown]
"""
### üß™ Final Integration Test: Complete Autograd Validation
This comprehensive test validates your entire autograd system
"""

# %%
def test_step6_integration_complete():
    """Complete integration test of autograd system."""
    print("üß™ STEP 6: COMPLETE INTEGRATION TEST")
    print("=" * 50)

    # Test 1: Neural network linear layer simulation
    print("1Ô∏è‚É£ Testing Linear Layer Simulation...")
    weights = Tensor([[0.5, -0.3], [0.2, 0.8]], requires_grad=True)
    inputs = Tensor([[1.0, 2.0]], requires_grad=True)
    bias = Tensor([[0.1, -0.1]], requires_grad=True)

    # Simulate: output = input @ weights + bias
    linear_output = inputs * weights + bias  # Element-wise for simplicity
    loss = linear_output * linear_output  # Squared for loss

    # Sum all elements for scalar loss (simplified)
    final_loss = loss  # In real networks, we'd sum across batch
    final_loss.backward()

    # Verify all parameters have gradients
    assert weights.grad is not None, "Weights should have gradients"
    assert inputs.grad is not None, "Inputs should have gradients"
    assert bias.grad is not None, "Bias should have gradients"
    print("   ‚úÖ Linear layer gradients computed successfully")

    # Test 2: Multi-step computation
    print("2Ô∏è‚É£ Testing Multi-Step Computation...")
    x = Tensor([1.0], requires_grad=True)
    y = Tensor([2.0], requires_grad=True)
    z = Tensor([3.0], requires_grad=True)

    # Complex expression: ((x * y) + z) * (x - y)
    step1 = x * y         # 1 * 2 = 2
    step2 = step1 + z     # 2 + 3 = 5
    step3 = x - y         # 1 - 2 = -1
    result = step2 * step3  # 5 * (-1) = -5

    assert np.allclose(result.data, [-5.0]), f"Multi-step forward failed: expected [-5.0], got {result.data}"

    result.backward()

    # All variables should have gradients
    assert x.grad is not None, "x should have gradients from multi-step"
    assert y.grad is not None, "y should have gradients from multi-step"
    assert z.grad is not None, "z should have gradients from multi-step"
    print("   ‚úÖ Multi-step computation gradients work")

    # Test 3: Gradient accumulation across multiple losses
    print("3Ô∏è‚É£ Testing Gradient Accumulation...")
    param = Tensor([1.0], requires_grad=True)

    # First loss: param * 2
    loss1 = param * 2.0
    loss1.backward()
    first_grad = param.grad.copy()

    # Second loss: param * 3 (should accumulate)
    loss2 = param * 3.0
    loss2.backward()

    expected_total = first_grad + 3.0
    assert np.allclose(param.grad, expected_total), f"Accumulation failed: expected {expected_total}, got {param.grad}"
    print("   ‚úÖ Gradient accumulation works correctly")

    # Test 4: Backward compatibility
    print("4Ô∏è‚É£ Testing Backward Compatibility...")
    # Operations without gradients should work exactly as before
    a = Tensor([1, 2, 3])  # No requires_grad
    b = Tensor([4, 5, 6])  # No requires_grad
    c = a + b
    d = a * b
    e = a - b

    # Should work without any gradient tracking
    assert not (hasattr(c, 'requires_grad') and c.requires_grad), "Non-grad tensors shouldn't track gradients"
    print("   ‚úÖ Backward compatibility maintained")

    # Test 5: Error handling
    print("5Ô∏è‚É£ Testing Error Handling...")
    non_grad_tensor = Tensor([1.0], requires_grad=False)
    try:
        non_grad_tensor.backward()
        assert False, "Should have raised error for non-gradient tensor"
    except RuntimeError:
        print("   ‚úÖ Proper error handling for non-gradient tensors")

    print("\n" + "=" * 50)
    print("üéâ COMPLETE SUCCESS! ALL INTEGRATION TESTS PASSED!")
    print("\nüöÄ Your Autograd System Achievements:")
    print("   ‚Ä¢ ‚úÖ Gradient tracking for all operations")
    print("   ‚Ä¢ ‚úÖ Automatic chain rule through complex expressions")
    print("   ‚Ä¢ ‚úÖ Gradient accumulation for multiple losses")
    print("   ‚Ä¢ ‚úÖ Backward compatibility with existing code")
    print("   ‚Ä¢ ‚úÖ Proper error handling and validation")
    print("   ‚Ä¢ ‚úÖ Ready for neural network training!")

    print("\nüîó Ready for Next Module:")
    print("   Module 06 (Optimizers) will use these gradients")
    print("   to update neural network parameters automatically!")

test_step6_integration_complete()

# %% [markdown]
"""
## üîç Systems Analysis: Autograd Memory and Performance

Now that your autograd system is complete, let's analyze its behavior to understand memory usage patterns and performance characteristics that matter in real ML systems.

**Analysis Focus**: Memory overhead, computational complexity, and scaling behavior of gradient computation
"""

# %%
def analyze_autograd_behavior():
    """
    üìä SYSTEMS MEASUREMENT: Autograd Performance Analysis

    Analyze memory usage and computational overhead of gradient tracking.
    """
    print("üìä AUTOGRAD SYSTEMS ANALYSIS")
    print("=" * 40)

    import time

    # Test 1: Memory overhead analysis
    print("üíæ Memory Overhead Analysis:")

    # Create tensors with and without gradient tracking
    size = 1000
    data = np.random.randn(size)

    # Non-gradient tensor
    no_grad_tensor = Tensor(data.copy(), requires_grad=False)

    # Gradient tensor
    grad_tensor = Tensor(data.copy(), requires_grad=True)

    print(f"   Tensor size: {size} elements")
    print(f"   Base tensor: data only")
    print(f"   Gradient tensor: data + grad storage + grad_fn")
    print(f"   Memory overhead: ~3x (data + grad + computation graph)")

    # Test 2: Computational overhead
    print("\n‚ö° Computational Overhead Analysis:")

    x_no_grad = Tensor([2.0] * 100, requires_grad=False)
    y_no_grad = Tensor([3.0] * 100, requires_grad=False)

    x_grad = Tensor([2.0] * 100, requires_grad=True)
    y_grad = Tensor([3.0] * 100, requires_grad=True)

    # Time operations without gradients
    start = time.perf_counter()
    for _ in range(1000):
        z = x_no_grad + y_no_grad
        z = z * x_no_grad
    no_grad_time = time.perf_counter() - start

    # Time operations with gradients (forward only)
    start = time.perf_counter()
    for _ in range(1000):
        z = x_grad + y_grad
        z = z * x_grad
    grad_forward_time = time.perf_counter() - start

    print(f"   Operations without gradients: {no_grad_time*1000:.2f}ms")
    print(f"   Operations with gradients: {grad_forward_time*1000:.2f}ms")
    print(f"   Forward pass overhead: {grad_forward_time/no_grad_time:.1f}x")

    # Test 3: Expression complexity scaling
    print("\nüìà Expression Complexity Scaling:")

    def time_expression(depth, with_gradients=True):
        """Time increasingly complex expressions."""
        x = Tensor([2.0], requires_grad=with_gradients)
        y = Tensor([3.0], requires_grad=with_gradients)

        start = time.perf_counter()
        result = x
        for i in range(depth):
            result = result + y
            result = result * x

        if with_gradients:
            result.backward()

        return time.perf_counter() - start

    depths = [1, 5, 10, 20]
    for depth in depths:
        time_no_grad = time_expression(depth, False)
        time_with_grad = time_expression(depth, True)
        overhead = time_with_grad / time_no_grad

        print(f"   Depth {depth:2d}: {time_no_grad*1000:.1f}ms ‚Üí {time_with_grad*1000:.1f}ms ({overhead:.1f}x overhead)")

    # Test 4: Gradient accumulation patterns
    print("\nüîÑ Gradient Accumulation Patterns:")

    param = Tensor([1.0], requires_grad=True)

    # Single large gradient vs multiple small gradients
    param.grad = None
    start = time.perf_counter()
    large_loss = param * 100.0
    large_loss.backward()
    large_grad_time = time.perf_counter() - start
    large_grad_value = param.grad.copy()

    param.grad = None
    start = time.perf_counter()
    for i in range(100):
        small_loss = param * 1.0
        small_loss.backward()
    small_grad_time = time.perf_counter() - start

    print(f"   Single large gradient: {large_grad_time*1000:.3f}ms ‚Üí grad={large_grad_value}")
    print(f"   100 small gradients: {small_grad_time*1000:.3f}ms ‚Üí grad={param.grad}")
    print(f"   Accumulation overhead: {small_grad_time/large_grad_time:.1f}x")

    print("\nüí° AUTOGRAD INSIGHTS:")
    print("   ‚Ä¢ Memory: Gradient tracking doubles memory usage (data + gradients)")
    print("   ‚Ä¢ Forward pass: ~2x computational overhead for gradient graph building")
    print("   ‚Ä¢ Backward pass: Additional ~1x computation time")
    print("   ‚Ä¢ Expression depth: Overhead scales linearly with computation graph depth")
    print("   ‚Ä¢ Gradient accumulation: Small overhead per accumulation operation")
    print("   ‚Ä¢ Production impact: Why PyTorch offers torch.no_grad() for inference!")

analyze_autograd_behavior()

# %% [markdown]
"""
## üß™ Module Integration Test

Final validation that everything works together correctly.
"""

# %%
def test_module():
    """
    Comprehensive test of entire autograd module functionality.

    This final test runs before module summary to ensure:
    - All components work correctly
    - Integration with existing tensor operations
    - Ready for use in neural network training
    """
    print("üß™ RUNNING MODULE INTEGRATION TEST")
    print("=" * 50)

    print("Running all unit tests...")
    test_step1_gradient_attributes()
    test_step2_backward_method()
    test_step3_smart_addition()
    test_step4_smart_multiplication()
    test_step5_chain_rule_magic()
    test_step6_integration_complete()

    print("\n" + "=" * 50)
    print("üéâ ALL TESTS PASSED! Module ready for export.")
    print("Run: tito module complete 05_autograd")

test_module()

# %%
if __name__ == "__main__":
    print("üöÄ Running Autograd module...")
    test_module()
    print("‚úÖ Module validation complete!")

# %% [markdown]
"""
## ü§î ML Systems Thinking: Interactive Questions

### Question 1: Memory Management in Gradient Computation

Your autograd implementation stores references to input tensors through grad_fn closures. In a deep neural network with 50 layers, each layer creates intermediate tensors with gradient functions.

**Analysis Task**: Examine how your gradient tracking affects memory usage patterns.

**Specific Questions**:
- How does memory usage scale with network depth in your implementation?
- What happens to memory when you call `backward()` on the final loss?
- Why do production frameworks implement "gradient checkpointing"?

**Implementation Connection**: Look at how your `grad_fn` closures capture references to input tensors and consider memory implications for deep networks.
"""

# %% nbgrader={"grade": true, "grade_id": "memory-management", "locked": false, "points": 10, "schema_version": 3, "solution": true, "task": false}
"""
TODO: Analyze memory management in your gradient computation system.

Consider how your grad_fn closures store references to input tensors and
how this affects memory usage in deep networks.
"""
### BEGIN SOLUTION
# Memory management analysis:

# 1. Memory scaling with network depth:
# - Each operation creates a tensor with grad_fn that references input tensors
# - In 50-layer network: 50 intermediate tensors + their grad_fn closures
# - Each grad_fn keeps input tensors alive in memory
# - Memory grows O(depth) for intermediate activations

# 2. Memory behavior during backward():
# - Forward pass: Builds computation graph, keeps all intermediates
# - Backward pass: Traverses graph but doesn't immediately free memory
# - Python's garbage collector frees tensors after no references remain
# - Peak memory occurs at end of forward pass

# 3. Gradient checkpointing solution:
# - Trade compute for memory: store only subset of activations
# - Recompute intermediate activations during backward pass
# - Reduces memory from O(depth) to O(sqrt(depth))
# - Essential for training very deep networks

# Production implementations:
# - PyTorch: torch.utils.checkpoint for gradient checkpointing
# - TensorFlow: tf.recompute_grad decorator
# - Custom: Clear computation graph after backward pass

# Memory optimization strategies:
# 1. In-place operations where mathematically safe
# 2. Clear gradients regularly: param.grad = None
# 3. Use torch.no_grad() for inference
# 4. Implement custom backward functions for memory efficiency
### END SOLUTION

# %% [markdown]
"""
### Question 2: Computational Graph Optimization

Your autograd system builds computation graphs dynamically. Each operation creates a new tensor with its own grad_fn.

**Analysis Task**: Identify opportunities for optimizing computational graphs to reduce overhead.

**Specific Questions**:
- Which operations could be fused together to reduce intermediate tensor creation?
- How would operator fusion affect gradient computation correctness?
- What trade-offs exist between graph complexity and performance?

**Implementation Connection**: Examine your operation functions and consider where computation could be optimized while maintaining gradient correctness.
"""

# %% nbgrader={"grade": true, "grade_id": "graph-optimization", "locked": false, "points": 10, "schema_version": 3, "solution": true, "task": false}
"""
TODO: Design computational graph optimizations for your autograd system.

Consider how operations could be fused or optimized while maintaining
gradient correctness.
"""
### BEGIN SOLUTION
# Computational graph optimization strategies:

# 1. Operation fusion opportunities:
# Current: z = (x + y) * w creates 2 tensors (intermediate + result)
# Optimized: Single "fused_add_mul" operation creates 1 tensor

def fused_add_multiply(x, y, w):
    """Fused operation: (x + y) * w"""
    # Direct computation without intermediate tensor
    result_data = (x.data + y.data) * w.data
    result = Tensor(result_data, requires_grad=True)

    def grad_fn(gradient):
        if x.requires_grad:
            x.backward(gradient * w.data)  # Chain rule
        if y.requires_grad:
            y.backward(gradient * w.data)
        if w.requires_grad:
            w.backward(gradient * (x.data + y.data))

    result.grad_fn = grad_fn
    return result

# 2. Safe fusion patterns:
# - Element-wise operations: add + mul + relu ‚Üí single kernel
# - Linear operations: matmul + bias_add ‚Üí single operation
# - Activation chains: sigmoid + multiply ‚Üí swish activation

# 3. Gradient correctness preservation:
# - Fusion must preserve mathematical equivalence
# - Chain rule application remains identical
# - Numerical stability must be maintained

# 4. Trade-offs analysis:
# Memory: Fewer intermediate tensors reduces memory usage
# Compute: Fused operations can be more cache-efficient
# Complexity: Harder to debug fused operations
# Flexibility: Less modular, harder to optimize individual ops

# 5. Production techniques:
# - TensorFlow XLA: Ahead-of-time fusion optimization
# - PyTorch JIT: Runtime graph optimization
# - ONNX: Graph optimization passes for deployment
# - Custom CUDA kernels: Maximum performance for common patterns

# Example optimization for common pattern:
class OptimizedLinear:
    def forward(x, weight, bias):
        # Fused: matmul + bias_add + activation
        return activation(x @ weight + bias)  # Single backward pass

# Memory-efficient alternative:
class CheckpointedOperation:
    def forward(inputs):
        # Store only inputs, recompute intermediate during backward
        return complex_computation(inputs)
### END SOLUTION

# %% [markdown]
"""
### Question 3: Gradient Flow Analysis

In your autograd implementation, gradients flow backward through the computation graph via the chain rule.

**Analysis Task**: Analyze how gradient magnitudes change as they flow through different types of operations.

**Specific Questions**:
- How do gradients change magnitude when flowing through multiplication vs addition?
- What causes vanishing or exploding gradients in deep networks?
- How would you detect and mitigate gradient flow problems?

**Implementation Connection**: Consider how your product rule implementation in multiplication affects gradient magnitudes compared to your addition implementation.
"""

# %% nbgrader={"grade": true, "grade_id": "gradient-flow", "locked": false, "points": 10, "schema_version": 3, "solution": true, "task": false}
"""
TODO: Analyze gradient flow patterns in your autograd implementation.

Examine how different operations affect gradient magnitudes and identify
potential gradient flow problems.
"""
### BEGIN SOLUTION
# Gradient flow analysis:

# 1. Gradient magnitude changes by operation:

# Addition: z = x + y
# ‚àÇz/‚àÇx = 1, ‚àÇz/‚àÇy = 1
# Gradients pass through unchanged - magnitude preserved

# Multiplication: z = x * y
# ‚àÇz/‚àÇx = y, ‚àÇz/‚àÇy = x
# Gradients scaled by other operand - magnitude can grow/shrink dramatically

# Example analysis:
def analyze_gradient_flow():
    x = Tensor([0.1], requires_grad=True)  # Small value
    y = Tensor([10.0], requires_grad=True)  # Large value

    # Addition preserves gradients
    z1 = x + y
    z1.backward()
    print(f"Addition: x.grad={x.grad}, y.grad={y.grad}")  # Both [1.0]

    x.grad = None; y.grad = None

    # Multiplication scales gradients
    z2 = x * y
    z2.backward()
    print(f"Multiplication: x.grad={x.grad}, y.grad={y.grad}")  # [10.0], [0.1]

# 2. Vanishing gradient causes:
# - Many multiplications by small values (< 1.0)
# - Deep networks: gradient = ‚àè(‚àÇLi/‚àÇLi-1) ‚Üí 0 as depth increases
# - Activation functions with small derivatives (sigmoid saturation)

# 3. Exploding gradient causes:
# - Many multiplications by large values (> 1.0)
# - Poor weight initialization
# - High learning rates

# 4. Detection strategies:
def detect_gradient_problems(model_parameters):
    """Detect vanishing/exploding gradients"""
    grad_norms = []
    for param in model_parameters:
        if param.grad is not None:
            grad_norm = np.linalg.norm(param.grad)
            grad_norms.append(grad_norm)

    max_norm = max(grad_norms) if grad_norms else 0
    min_norm = min(grad_norms) if grad_norms else 0

    if max_norm > 10.0:
        print("‚ö†Ô∏è  Exploding gradients detected!")
    if max_norm < 1e-6:
        print("‚ö†Ô∏è  Vanishing gradients detected!")

    return grad_norms

# 5. Mitigation strategies:
# Gradient clipping for exploding gradients:
def clip_gradients(parameters, max_norm=1.0):
    total_norm = 0
    for param in parameters:
        if param.grad is not None:
            total_norm += np.sum(param.grad ** 2)
    total_norm = np.sqrt(total_norm)

    if total_norm > max_norm:
        clip_factor = max_norm / total_norm
        for param in parameters:
            if param.grad is not None:
                param.grad = param.grad * clip_factor

# Better weight initialization for vanishing gradients:
# - Xavier/Glorot initialization
# - He initialization for ReLU networks
# - Layer normalization to control activations

# Architectural solutions:
# - Skip connections (ResNet)
# - LSTM gates for sequences
# - Careful activation function choice (ReLU vs sigmoid)
### END SOLUTION

# %% [markdown]
"""
## üéØ MODULE SUMMARY: Autograd - Incremental Automatic Differentiation

Congratulations! You've built a complete automatic differentiation system through six manageable steps!

### What You've Accomplished
‚úÖ **Step-by-Step Enhancement**: Added gradient tracking to existing Tensor class without breaking any functionality
‚úÖ **Gradient Memory**: Tensors now store gradients and backward functions (Step 1-2)
‚úÖ **Smart Operations**: Addition, multiplication, and subtraction automatically track gradients (Steps 3-4)
‚úÖ **Chain Rule Magic**: Complex expressions compute gradients automatically through the entire computation graph (Step 5)
‚úÖ **Complete Integration**: Full autograd system ready for neural network training (Step 6)
‚úÖ **Systems Understanding**: Memory overhead analysis and performance characteristics

### Key Learning Outcomes
- **Incremental Development**: How to enhance complex systems step by step with immediate validation
- **Chain Rule Implementation**: Automatic gradient computation through mathematical expressions
- **Software Architecture**: Safe enhancement of existing classes without breaking functionality
- **Memory Management**: Understanding computational graph storage and gradient accumulation patterns
- **Production Insights**: How real ML frameworks implement automatic differentiation

### Technical Foundations Mastered
- **Gradient Tracking**: `requires_grad`, `grad`, and `grad_fn` attributes for automatic differentiation
- **Backward Propagation**: Automatic chain rule application through computation graphs
- **Product Rule**: Correct gradient computation for multiplication operations
- **Gradient Accumulation**: Proper handling of multiple backward passes
- **Error Handling**: Robust validation for gradient computation requirements

### Professional Skills Developed
- **Incremental Enhancement**: Adding complex features through small, testable steps
- **Immediate Feedback**: Validating each enhancement before proceeding to next step
- **Backward Compatibility**: Ensuring existing functionality remains intact
- **Systems Analysis**: Understanding memory and performance implications of design choices

### Ready for Advanced Applications
Your enhanced Tensor class enables:
- **Neural Network Training**: Automatic gradient computation for parameter updates
- **Optimization Algorithms**: Foundation for SGD, Adam, and other optimizers (Module 06)
- **Complex Architectures**: Support for any differentiable computation graph
- **Research Applications**: Building and experimenting with novel ML architectures

### Connection to Real ML Systems
Your incremental approach mirrors production development:
- **PyTorch Evolution**: Similar step-by-step enhancement from pure tensors to autograd-capable tensors
- **TensorFlow 2.0**: Eager execution with automatic differentiation follows similar patterns
- **Professional Development**: Industry standard for adding complex features safely
- **Debugging Friendly**: Step-by-step approach makes gradient computation errors easier to trace

### Performance Characteristics Discovered
- **Memory Overhead**: ~2x memory usage (data + gradients + computation graph)
- **Computational Overhead**: ~2x forward pass time for gradient graph building
- **Scaling Behavior**: Linear scaling with computation graph depth
- **Optimization Opportunities**: Operation fusion and gradient checkpointing potential

### Next Steps
1. **Export your module**: `tito module complete 05_autograd`
2. **Validate integration**: All previous tensor operations still work + new gradient features
3. **Ready for Module 06**: Optimizers will use these gradients to train neural networks!

**üöÄ Achievement Unlocked**: You've mastered incremental software enhancement - building complex systems through small, immediately rewarding steps. This is exactly how professional ML engineers develop production systems!
"""