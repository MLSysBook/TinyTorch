{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff78c820",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Module 2: Activations - Nonlinearity in Neural Networks\n",
    "\n",
    "Welcome to the Activations module! This is where neural networks get their power through nonlinearity.\n",
    "\n",
    "## Learning Goals\n",
    "- Understand why activation functions are essential for neural networks\n",
    "- Implement the four most important activation functions: ReLU, Sigmoid, Tanh, and Softmax\n",
    "- Visualize how activations transform data and enable complex learning\n",
    "- See how activations work with layers to build powerful networks\n",
    "- Master the NBGrader workflow with comprehensive testing\n",
    "\n",
    "## Build â†’ Use â†’ Understand\n",
    "1. **Build**: Activation functions that add nonlinearity\n",
    "2. **Use**: Transform tensors and see immediate results\n",
    "3. **Understand**: How nonlinearity enables complex pattern learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4054e6d",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "activations-imports",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| default_exp core.activations\n",
    "\n",
    "#| export\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from typing import Union, List\n",
    "\n",
    "# Import our Tensor class - try from package first, then from local module\n",
    "try:\n",
    "    from tinytorch.core.tensor import Tensor\n",
    "except ImportError:\n",
    "    # For development, import from local tensor module\n",
    "    sys.path.append(os.path.join(os.path.dirname(__file__), '..', '01_tensor'))\n",
    "    from tensor_dev import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443934a0",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "activations-setup",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "def _should_show_plots():\n",
    "    \"\"\"Check if we should show plots (disable during testing)\"\"\"\n",
    "    # Check multiple conditions that indicate we're in test mode\n",
    "    is_pytest = (\n",
    "        'pytest' in sys.modules or\n",
    "        'test' in sys.argv or\n",
    "        os.environ.get('PYTEST_CURRENT_TEST') is not None or\n",
    "        any('test' in arg for arg in sys.argv) or\n",
    "        any('pytest' in arg for arg in sys.argv)\n",
    "    )\n",
    "    \n",
    "    # Show plots in development mode (when not in test mode)\n",
    "    return not is_pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a040d4b8",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "activations-visualization",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "def visualize_activation_function(activation_fn, name: str, x_range: tuple = (-5, 5), num_points: int = 100):\n",
    "    \"\"\"Visualize an activation function's behavior\"\"\"\n",
    "    if not _should_show_plots():\n",
    "        return\n",
    "        \n",
    "    try:\n",
    "        \n",
    "        # Generate input values\n",
    "        x_vals = np.linspace(x_range[0], x_range[1], num_points)\n",
    "        \n",
    "        # Apply activation function\n",
    "        y_vals = []\n",
    "        for x in x_vals:\n",
    "            input_tensor = Tensor([[x]])\n",
    "            output = activation_fn(input_tensor)\n",
    "            y_vals.append(output.data.item())\n",
    "        \n",
    "        # Create plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(x_vals, y_vals, 'b-', linewidth=2, label=f'{name} Activation')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.xlabel('Input (x)')\n",
    "        plt.ylabel(f'{name}(x)')\n",
    "        plt.title(f'{name} Activation Function')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"   ğŸ“Š Matplotlib not available - skipping visualization\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸  Visualization error: {e}\")\n",
    "\n",
    "def visualize_activation_on_data(activation_fn, name: str, data: Tensor):\n",
    "    \"\"\"Show activation function applied to sample data\"\"\"\n",
    "    if not _should_show_plots():\n",
    "        return\n",
    "        \n",
    "    try:\n",
    "        output = activation_fn(data)\n",
    "        print(f\"   ğŸ“Š {name} Example:\")\n",
    "        print(f\"      Input:  {data.data.flatten()}\")\n",
    "        print(f\"      Output: {output.data.flatten()}\")\n",
    "        print(f\"      Range:  [{output.data.min():.3f}, {output.data.max():.3f}]\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸  Data visualization error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8273b5ee",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ğŸ“¦ Where This Code Lives in the Final Package\n",
    "\n",
    "**Learning Side:** You work in `modules/source/02_activations/activations_dev.py`  \n",
    "**Building Side:** Code exports to `tinytorch.core.activations`\n",
    "\n",
    "```python\n",
    "# Final package structure:\n",
    "from tinytorch.core.activations import ReLU, Sigmoid, Tanh, Softmax  # All activations together!\n",
    "from tinytorch.core.tensor import Tensor  # The foundation\n",
    "from tinytorch.core.layers import Dense, Conv2D  # Coming next!\n",
    "```\n",
    "\n",
    "**Why this matters:**\n",
    "- **Learning:** Focused modules for deep understanding\n",
    "- **Production:** Proper organization like PyTorch's `torch.nn.functional`\n",
    "- **Consistency:** All activation functions live together in `core.activations`\n",
    "- **Integration:** Works seamlessly with tensors and layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72728a3",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ğŸ§  The Mathematical Foundation of Nonlinearity\n",
    "\n",
    "### The Universal Approximation Theorem\n",
    "**Key Insight:** Neural networks with nonlinear activation functions can approximate any continuous function!\n",
    "\n",
    "```\n",
    "Without activation: f(x) = Wâ‚ƒ(Wâ‚‚(Wâ‚x + bâ‚) + bâ‚‚) + bâ‚ƒ = Wx + b (still linear!)\n",
    "With activation: f(x) = Wâ‚ƒÏƒ(Wâ‚‚Ïƒ(Wâ‚x + bâ‚) + bâ‚‚) + bâ‚ƒ (nonlinear!)\n",
    "```\n",
    "\n",
    "### Why Nonlinearity is Critical\n",
    "- **Linear Limitations**: Without activations, any deep network collapses to a single linear transformation\n",
    "- **Feature Learning**: Nonlinear functions create complex decision boundaries\n",
    "- **Representation Power**: Each layer can learn different levels of abstraction\n",
    "- **Biological Inspiration**: Neurons fire (activate) only above certain thresholds\n",
    "\n",
    "### Mathematical Properties We Care About\n",
    "- **Differentiability**: For gradient-based optimization\n",
    "- **Computational Efficiency**: Fast forward and backward passes\n",
    "- **Numerical Stability**: Avoiding vanishing/exploding gradients\n",
    "- **Sparsity**: Some activations (like ReLU) produce sparse representations\n",
    "\n",
    "### Connection to Real ML Systems\n",
    "Every major framework has these same activations:\n",
    "- **PyTorch**: `torch.nn.ReLU()`, `torch.nn.Sigmoid()`, etc.\n",
    "- **TensorFlow**: `tf.nn.relu()`, `tf.nn.sigmoid()`, etc.\n",
    "- **JAX**: `jax.nn.relu()`, `jax.nn.sigmoid()`, etc.\n",
    "- **TinyTorch**: `tinytorch.core.activations.ReLU()` (what we're building!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcc2c87",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 1: What is an Activation Function?\n",
    "\n",
    "### Definition\n",
    "An **activation function** is a mathematical function that adds nonlinearity to neural networks. It transforms the output of a layer before passing it to the next layer.\n",
    "\n",
    "### The Fundamental Problem: Why We Need Nonlinearity\n",
    "\n",
    "#### **The Linear Limitation**\n",
    "Without activation functions, neural networks are just linear transformations:\n",
    "\n",
    "```python\n",
    "# Without activation functions:\n",
    "layer1 = W1 @ x + b1    # Linear transformation\n",
    "layer2 = W2 @ layer1 + b2    # Another linear transformation\n",
    "layer3 = W3 @ layer2 + b3    # Yet another linear transformation\n",
    "\n",
    "# This is equivalent to:\n",
    "final_output = (W3 @ W2 @ W1) @ x + (W3 @ W2 @ b1 + W3 @ b2 + b3)\n",
    "#            = W_combined @ x + b_combined\n",
    "# Still just one linear transformation!\n",
    "```\n",
    "\n",
    "**No matter how many layers you stack, without activation functions, you can only learn linear relationships.**\n",
    "\n",
    "#### **The Nonlinearity Solution**\n",
    "Activation functions break this linearity:\n",
    "\n",
    "```python\n",
    "# With activation functions:\n",
    "layer1 = activation(W1 @ x + b1)      # Nonlinear transformation\n",
    "layer2 = activation(W2 @ layer1 + b2) # Another nonlinear transformation\n",
    "layer3 = activation(W3 @ layer2 + b3) # Complex nonlinear composition\n",
    "\n",
    "# This can approximate any continuous function!\n",
    "```\n",
    "\n",
    "### Biological Inspiration: How Neurons Really Work\n",
    "\n",
    "#### **The Biological Neuron**\n",
    "Real neurons in the brain exhibit nonlinear behavior:\n",
    "\n",
    "1. **Threshold behavior**: Neurons fire only when input exceeds a threshold\n",
    "2. **Saturation**: Neurons have maximum firing rates\n",
    "3. **Sparsity**: Most neurons are inactive most of the time\n",
    "4. **Adaptation**: Neurons adjust their sensitivity over time\n",
    "\n",
    "#### **Activation Functions as Neuron Models**\n",
    "- **ReLU**: Models threshold behavior (fire or don't fire)\n",
    "- **Sigmoid**: Models saturation (smooth transition from inactive to active)\n",
    "- **Tanh**: Models bipolar neurons (inhibitory and excitatory)\n",
    "- **Softmax**: Models competition between neurons (winner-take-all)\n",
    "\n",
    "### Mathematical Foundation: The Universal Approximation Theorem\n",
    "\n",
    "#### **The Theorem**\n",
    "**Any continuous function can be approximated by a neural network with:**\n",
    "- **One hidden layer**\n",
    "- **Enough neurons**\n",
    "- **Nonlinear activation functions**\n",
    "\n",
    "#### **Why This Matters**\n",
    "This theorem guarantees that neural networks with nonlinear activations can learn:\n",
    "- **Image recognition**: Mapping pixels to object classes\n",
    "- **Language understanding**: Mapping words to meanings\n",
    "- **Game playing**: Mapping board states to optimal moves\n",
    "- **Scientific modeling**: Mapping inputs to complex phenomena\n",
    "\n",
    "#### **The Catch**\n",
    "- **\"Enough neurons\"** might be exponentially large\n",
    "- **Deep networks** can approximate the same functions with fewer neurons\n",
    "- **Nonlinearity is essential** - linear networks can't do this\n",
    "\n",
    "### Real-World Impact: What Nonlinearity Enables\n",
    "\n",
    "#### **Computer Vision**\n",
    "```python\n",
    "# Linear model: Can only learn linear classifiers\n",
    "# \"Is this a cat?\" â†’ Only works if cats are linearly separable from dogs\n",
    "# Reality: Cats and dogs are NOT linearly separable in pixel space!\n",
    "\n",
    "# Nonlinear model: Can learn complex decision boundaries\n",
    "# \"Is this a cat?\" â†’ Can learn fur patterns, ear shapes, eye positions\n",
    "# Reality: Deep networks with ReLU can distinguish thousands of objects\n",
    "```\n",
    "\n",
    "#### **Natural Language Processing**\n",
    "```python\n",
    "# Linear model: Can only learn word co-occurrence\n",
    "# \"The movie was great\" â†’ Linear combination of word vectors\n",
    "# Problem: \"The movie was not great\" looks similar to linear model\n",
    "\n",
    "# Nonlinear model: Can understand context and negation\n",
    "# \"The movie was great\" vs \"The movie was not great\"\n",
    "# Solution: Transformers with nonlinear feedforward layers\n",
    "```\n",
    "\n",
    "#### **Game Playing**\n",
    "```python\n",
    "# Linear model: Can only learn linear strategies\n",
    "# Chess position â†’ Linear combination of piece values\n",
    "# Problem: Chess strategy is highly nonlinear (tactics, combinations)\n",
    "\n",
    "# Nonlinear model: Can learn complex strategies\n",
    "# Chess position â†’ Deep evaluation of patterns and tactics\n",
    "# Success: AlphaZero uses deep networks with ReLU\n",
    "```\n",
    "\n",
    "### Activation Function Properties: What Makes Them Work\n",
    "\n",
    "#### **1. Nonlinearity (Essential)**\n",
    "- **Definition**: f(ax + by) â‰  af(x) + bf(y)\n",
    "- **Why crucial**: Enables complex function approximation\n",
    "- **Example**: ReLU(2x) â‰  2Ã—ReLU(x) for negative x\n",
    "\n",
    "#### **2. Differentiability (Important)**\n",
    "- **Definition**: Function has well-defined derivatives\n",
    "- **Why important**: Enables gradient-based optimization\n",
    "- **Trade-off**: ReLU is not differentiable at 0, but works well in practice\n",
    "\n",
    "#### **3. Computational Efficiency (Practical)**\n",
    "- **Definition**: Fast to compute forward and backward passes\n",
    "- **Why important**: Training speed and inference speed\n",
    "- **Example**: ReLU is faster than sigmoid (no exponentials)\n",
    "\n",
    "#### **4. Gradient Properties (Critical)**\n",
    "- **Vanishing gradients**: Derivatives approach 0 (sigmoid, tanh)\n",
    "- **Exploding gradients**: Derivatives grow exponentially (rare)\n",
    "- **Gradient preservation**: Derivatives stay reasonable (ReLU)\n",
    "\n",
    "#### **5. Output Range (Application-dependent)**\n",
    "- **Bounded**: Output in fixed range (sigmoid: [0,1], tanh: [-1,1])\n",
    "- **Unbounded**: Output can be any value (ReLU: [0,âˆ))\n",
    "- **Probabilistic**: Output sums to 1 (softmax)\n",
    "\n",
    "### The Four Fundamental Activation Functions\n",
    "\n",
    "#### **1. ReLU (Rectified Linear Unit)**\n",
    "- **Formula**: f(x) = max(0, x)\n",
    "- **Use case**: Hidden layers in most networks\n",
    "- **Advantages**: Simple, fast, no vanishing gradients\n",
    "- **Disadvantages**: \"Dead neurons\" problem\n",
    "\n",
    "#### **2. Sigmoid**\n",
    "- **Formula**: f(x) = 1/(1 + e^(-x))\n",
    "- **Use case**: Binary classification output\n",
    "- **Advantages**: Smooth, probabilistic interpretation\n",
    "- **Disadvantages**: Vanishing gradients, computationally expensive\n",
    "\n",
    "#### **3. Tanh (Hyperbolic Tangent)**\n",
    "- **Formula**: f(x) = (e^x - e^(-x))/(e^x + e^(-x))\n",
    "- **Use case**: Hidden layers (better than sigmoid)\n",
    "- **Advantages**: Zero-centered, stronger gradients than sigmoid\n",
    "- **Disadvantages**: Still suffers from vanishing gradients\n",
    "\n",
    "#### **4. Softmax**\n",
    "- **Formula**: f(x_i) = e^(x_i) / Î£(e^(x_j))\n",
    "- **Use case**: Multi-class classification output\n",
    "- **Advantages**: Probabilistic, sums to 1\n",
    "- **Disadvantages**: Computationally expensive, can saturate\n",
    "\n",
    "### Modern Activation Function Evolution\n",
    "\n",
    "#### **Historical Timeline**\n",
    "1. **1943**: Threshold functions (McCulloch-Pitts neurons)\n",
    "2. **1960s**: Sigmoid functions (perceptrons)\n",
    "3. **1980s**: Tanh functions (backpropagation era)\n",
    "4. **2010s**: ReLU revolution (deep learning breakthrough)\n",
    "5. **2020s**: Advanced variants (Swish, GELU, Mish)\n",
    "\n",
    "#### **Why ReLU Won**\n",
    "- **Simplicity**: Just max(0, x)\n",
    "- **Speed**: No exponentials or divisions\n",
    "- **Gradients**: No vanishing gradient problem\n",
    "- **Sparsity**: Creates sparse representations\n",
    "- **Empirical success**: Works well in practice\n",
    "\n",
    "### Connection to Previous Modules\n",
    "\n",
    "#### **From Module 1 (Tensor)**\n",
    "- **Input**: Tensors from previous layers\n",
    "- **Output**: Transformed tensors for next layers\n",
    "- **Operations**: Element-wise transformations\n",
    "\n",
    "#### **To Module 3 (Layers)**\n",
    "- **Integration**: Layers + activations = nonlinear transformations\n",
    "- **Composition**: Stack layers with activations for deep networks\n",
    "- **Design**: Choose activation based on layer purpose\n",
    "\n",
    "### Visual Analogy: The Activation Function Zoo\n",
    "\n",
    "Think of activation functions as different types of **signal processors**:\n",
    "\n",
    "- **ReLU**: One-way valve (blocks negative, passes positive)\n",
    "- **Sigmoid**: Volume knob (smoothly adjusts from 0 to 1)\n",
    "- **Tanh**: Balanced amplifier (amplifies around 0, saturates at extremes)\n",
    "- **Softmax**: Probability distributor (converts scores to probabilities)\n",
    "\n",
    "Let's implement these essential nonlinear functions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8e5884",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 2: ReLU - The Workhorse of Deep Learning\n",
    "\n",
    "### What is ReLU?\n",
    "**ReLU (Rectified Linear Unit)** is the most popular activation function in deep learning.\n",
    "\n",
    "**Mathematical Definition:**\n",
    "```\n",
    "f(x) = max(0, x)\n",
    "```\n",
    "\n",
    "**In Plain English:**\n",
    "- If input is positive â†’ pass it through unchanged\n",
    "- If input is negative â†’ output zero\n",
    "\n",
    "### Why ReLU is Popular\n",
    "1. **Simple**: Easy to compute and understand\n",
    "2. **Fast**: No expensive operations (no exponentials)\n",
    "3. **Sparse**: Outputs many zeros, creating sparse representations\n",
    "4. **Gradient-friendly**: Gradient is either 0 or 1 (no vanishing gradient for positive inputs)\n",
    "\n",
    "### Real-World Analogy\n",
    "ReLU is like a **one-way valve** - it only lets positive \"pressure\" through, blocking negative values completely.\n",
    "\n",
    "### When to Use ReLU\n",
    "- **Hidden layers** in most neural networks (90% of cases)\n",
    "- **Convolutional layers** in image processing (CNNs)\n",
    "- **When you want sparse activations** (many zeros)\n",
    "- **Deep networks** (doesn't suffer from vanishing gradients)\n",
    "\n",
    "### Real-World Applications\n",
    "- **Image Classification**: ResNet, VGG, AlexNet all use ReLU\n",
    "- **Object Detection**: YOLO, R-CNN use ReLU in backbone networks\n",
    "- **Natural Language Processing**: Transformer models use ReLU in feedforward layers\n",
    "- **Recommendation Systems**: Deep collaborative filtering with ReLU\n",
    "\n",
    "### Mathematical Properties\n",
    "- **Derivative**: f'(x) = 1 if x > 0, else 0\n",
    "- **Range**: [0, âˆ)\n",
    "- **Sparsity**: Outputs exactly 0 for negative inputs\n",
    "- **Computational Cost**: O(1) - just a max operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a02aac",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "relu-class",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class ReLU:\n",
    "    \"\"\"\n",
    "    ReLU Activation Function: f(x) = max(0, x)\n",
    "    \n",
    "    The most popular activation function in deep learning.\n",
    "    Simple, fast, and effective for most applications.\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply ReLU activation: f(x) = max(0, x)\n",
    "        \n",
    "        TODO: Implement ReLU activation\n",
    "        \n",
    "        APPROACH:\n",
    "        1. For each element in the input tensor, apply max(0, element)\n",
    "        2. Return a new Tensor with the results\n",
    "        \n",
    "        EXAMPLE:\n",
    "        Input: Tensor([[-1, 0, 1, 2, -3]])\n",
    "        Expected: Tensor([[0, 0, 1, 2, 0]])\n",
    "        \n",
    "        HINTS:\n",
    "        - Use np.maximum(0, x.data) for element-wise max\n",
    "        - Remember to return a new Tensor object\n",
    "        - The shape should remain the same as input\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        result = np.maximum(0, x.data)\n",
    "        return Tensor(result)\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Make the class callable: relu(x) instead of relu.forward(x)\"\"\"\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0da09e9",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### ğŸ§ª Unit Test: ReLU Activation\n",
    "\n",
    "Let's test your ReLU implementation right away! This gives you immediate feedback on whether your activation function works correctly.\n",
    "\n",
    "**This is a unit test** - it tests one specific activation function (ReLU) in isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e369ace",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-relu-immediate",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test ReLU activation immediately after implementation\n",
    "print(\"ğŸ”¬ Unit Test: ReLU Activation...\")\n",
    "\n",
    "# Create ReLU instance\n",
    "relu = ReLU()\n",
    "\n",
    "# Test with mixed positive/negative values\n",
    "try:\n",
    "    test_input = Tensor([[-2, -1, 0, 1, 2]])\n",
    "    result = relu(test_input)\n",
    "    expected = np.array([[0, 0, 0, 1, 2]])\n",
    "    \n",
    "    assert np.array_equal(result.data, expected), f\"ReLU failed: expected {expected}, got {result.data}\"\n",
    "    print(f\"âœ… ReLU test: input {test_input.data} â†’ output {result.data}\")\n",
    "    \n",
    "    # Test that negative values become zero\n",
    "    assert np.all(result.data >= 0), \"ReLU should make all negative values zero\"\n",
    "    print(\"âœ… ReLU correctly zeros negative values\")\n",
    "    \n",
    "    # Test that positive values remain unchanged\n",
    "    positive_input = Tensor([[1, 2, 3, 4, 5]])\n",
    "    positive_result = relu(positive_input)\n",
    "    assert np.array_equal(positive_result.data, positive_input.data), \"ReLU should preserve positive values\"\n",
    "    print(\"âœ… ReLU preserves positive values\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ReLU test failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# Show visual example\n",
    "print(\"ğŸ¯ ReLU behavior:\")\n",
    "print(\"   Negative â†’ 0 (blocked)\")\n",
    "print(\"   Zero â†’ 0 (blocked)\")  \n",
    "print(\"   Positive â†’ unchanged (passed through)\")\n",
    "print(\"ğŸ“ˆ Progress: ReLU âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec61d918",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 3: Sigmoid - The Smooth Squasher\n",
    "\n",
    "### What is Sigmoid?\n",
    "**Sigmoid** is a smooth S-shaped function that squashes inputs to the range (0, 1).\n",
    "\n",
    "**Mathematical Definition:**\n",
    "```\n",
    "f(x) = 1 / (1 + e^(-x))\n",
    "```\n",
    "\n",
    "**Properties:**\n",
    "- **Range**: (0, 1) - never exactly 0 or 1\n",
    "- **Smooth**: Differentiable everywhere\n",
    "- **Monotonic**: Always increasing\n",
    "- **Centered**: Around 0.5\n",
    "\n",
    "### Why Sigmoid is Useful\n",
    "1. **Probabilistic**: Output can be interpreted as probabilities\n",
    "2. **Bounded**: Output is always between 0 and 1\n",
    "3. **Smooth**: Good for gradient-based optimization\n",
    "4. **Historical**: Was the standard before ReLU\n",
    "\n",
    "### Real-World Analogy\n",
    "Sigmoid is like a **soft switch** - it gradually turns on as input increases, unlike ReLU's hard cutoff.\n",
    "\n",
    "### Real-World Applications\n",
    "- **Binary Classification**: Final layer for yes/no decisions (spam detection, medical diagnosis)\n",
    "- **Logistic Regression**: The classic ML algorithm uses sigmoid\n",
    "- **Attention Mechanisms**: Gating mechanisms in LSTM/GRU\n",
    "- **Probability Estimation**: When you need outputs between 0 and 1\n",
    "\n",
    "### Mathematical Properties\n",
    "- **Derivative**: f'(x) = f(x)(1 - f(x)) - elegant and efficient!\n",
    "- **Range**: (0, 1) - never exactly 0 or 1\n",
    "- **Symmetry**: Sigmoid(0) = 0.5 (centered)\n",
    "- **Saturation**: Gradients approach 0 for large |x| (vanishing gradient problem)\n",
    "\n",
    "### When to Use Sigmoid\n",
    "- **Binary classification** (output layer)\n",
    "- **Gates** in LSTM/GRU networks\n",
    "- **When you need probabilistic outputs**\n",
    "- **Avoid in deep networks** (vanishing gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae68291e",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "sigmoid-class",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class Sigmoid:\n",
    "    \"\"\"\n",
    "    Sigmoid Activation Function: f(x) = 1 / (1 + e^(-x))\n",
    "    \n",
    "    Smooth S-shaped function that squashes inputs to (0, 1).\n",
    "    Useful for binary classification and probabilistic outputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply Sigmoid activation: f(x) = 1 / (1 + e^(-x))\n",
    "        \n",
    "        TODO: Implement Sigmoid activation with numerical stability\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Clip input values to prevent overflow (e.g., between -500 and 500)\n",
    "        2. Apply the sigmoid formula: 1 / (1 + exp(-x))\n",
    "        3. Return a new Tensor with the results\n",
    "        \n",
    "        EXAMPLE:\n",
    "        Input: Tensor([[-2, 0, 2]])\n",
    "        Expected: Tensor([[0.119, 0.5, 0.881]]) (approximately)\n",
    "        \n",
    "        HINTS:\n",
    "        - Use np.clip(x.data, -500, 500) for numerical stability\n",
    "        - Use np.exp() for the exponential function\n",
    "        - Be careful with very large/small inputs to avoid overflow\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        # Clip for numerical stability\n",
    "        clipped = np.clip(x.data, -500, 500)\n",
    "        result = 1 / (1 + np.exp(-clipped))\n",
    "        return Tensor(result)\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Make the class callable: sigmoid(x) instead of sigmoid.forward(x)\"\"\"\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f24f67",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### ğŸ§ª Unit Test: Sigmoid Activation\n",
    "\n",
    "Let's test your Sigmoid implementation! This should squash all values to the range (0, 1).\n",
    "\n",
    "**This is a unit test** - it tests one specific activation function (Sigmoid) in isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587fbfa1",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-sigmoid-immediate",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Sigmoid activation immediately after implementation\n",
    "print(\"ğŸ”¬ Unit Test: Sigmoid Activation...\")\n",
    "\n",
    "# Create Sigmoid instance\n",
    "sigmoid = Sigmoid()\n",
    "\n",
    "# Test with various inputs\n",
    "try:\n",
    "    test_input = Tensor([[-2, -1, 0, 1, 2]])\n",
    "    result = sigmoid(test_input)\n",
    "    \n",
    "    # Check that all outputs are between 0 and 1\n",
    "    assert np.all(result.data > 0), \"Sigmoid outputs should be > 0\"\n",
    "    assert np.all(result.data < 1), \"Sigmoid outputs should be < 1\"\n",
    "    print(f\"âœ… Sigmoid test: input {test_input.data} â†’ output {result.data}\")\n",
    "    \n",
    "    # Test specific values\n",
    "    zero_input = Tensor([[0]])\n",
    "    zero_result = sigmoid(zero_input)\n",
    "    assert np.allclose(zero_result.data, 0.5, atol=1e-6), f\"Sigmoid(0) should be 0.5, got {zero_result.data}\"\n",
    "    print(\"âœ… Sigmoid(0) = 0.5 (correct)\")\n",
    "    \n",
    "    # Test that it's monotonic (larger inputs give larger outputs)\n",
    "    small_input = Tensor([[-1]])\n",
    "    large_input = Tensor([[1]])\n",
    "    small_result = sigmoid(small_input)\n",
    "    large_result = sigmoid(large_input)\n",
    "    assert small_result.data < large_result.data, \"Sigmoid should be monotonic\"\n",
    "    print(\"âœ… Sigmoid is monotonic (increasing)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Sigmoid test failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# Show visual example\n",
    "print(\"ğŸ¯ Sigmoid behavior:\")\n",
    "print(\"   Large negative â†’ approaches 0\")\n",
    "print(\"   Zero â†’ 0.5\")\n",
    "print(\"   Large positive â†’ approaches 1\")\n",
    "print(\"ğŸ“ˆ Progress: ReLU âœ“, Sigmoid âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba540dc",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 4: Tanh - The Zero-Centered Squasher\n",
    "\n",
    "### What is Tanh?\n",
    "**Tanh (Hyperbolic Tangent)** is similar to Sigmoid but centered around zero.\n",
    "\n",
    "**Mathematical Definition:**\n",
    "```\n",
    "f(x) = tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "```\n",
    "\n",
    "**Properties:**\n",
    "- **Range**: (-1, 1) - symmetric around zero\n",
    "- **Zero-centered**: Output averages to zero\n",
    "- **Smooth**: Differentiable everywhere\n",
    "- **Stronger gradients**: Than sigmoid in some regions\n",
    "\n",
    "### Why Tanh is Useful\n",
    "1. **Zero-centered**: Better for training (gradients don't all have same sign)\n",
    "2. **Symmetric**: Treats positive and negative inputs equally\n",
    "3. **Stronger gradients**: Can help with training dynamics\n",
    "4. **Bounded**: Output is always between -1 and 1\n",
    "\n",
    "### Real-World Analogy\n",
    "Tanh is like a **balanced scale** - it can tip positive or negative, with zero as the neutral point.\n",
    "\n",
    "### When to Use Tanh\n",
    "- **Hidden layers** (alternative to ReLU)\n",
    "- **RNNs** (traditional choice)\n",
    "- **When you need zero-centered outputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4350fea3",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "tanh-class",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class Tanh:\n",
    "    \"\"\"\n",
    "    Tanh Activation Function: f(x) = tanh(x)\n",
    "    \n",
    "    Zero-centered S-shaped function that squashes inputs to (-1, 1).\n",
    "    Better than sigmoid for hidden layers due to zero-centered outputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply Tanh activation: f(x) = tanh(x)\n",
    "        \n",
    "        TODO: Implement Tanh activation\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Use NumPy's tanh function for numerical stability\n",
    "        2. Apply to the tensor data\n",
    "        3. Return a new Tensor with the results\n",
    "        \n",
    "        EXAMPLE:\n",
    "        Input: Tensor([[-2, 0, 2]])\n",
    "        Expected: Tensor([[-0.964, 0.0, 0.964]]) (approximately)\n",
    "        \n",
    "        HINTS:\n",
    "        - Use np.tanh(x.data) - NumPy handles the math\n",
    "        - Much simpler than implementing the formula manually\n",
    "        - NumPy's tanh is numerically stable\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        result = np.tanh(x.data)\n",
    "        return Tensor(result)\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Make the class callable: tanh(x) instead of tanh.forward(x)\"\"\"\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b0d5bc",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### ğŸ§ª Unit Test: Tanh Activation\n",
    "\n",
    "Let's test your Tanh implementation! This should squash all values to the range (-1, 1) and be zero-centered.\n",
    "\n",
    "**This is a unit test** - it tests one specific activation function (Tanh) in isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c34866",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-tanh-immediate",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Tanh activation immediately after implementation\n",
    "print(\"ğŸ”¬ Unit Test: Tanh Activation...\")\n",
    "\n",
    "# Create Tanh instance\n",
    "tanh = Tanh()\n",
    "\n",
    "# Test with various inputs\n",
    "try:\n",
    "    test_input = Tensor([[-2, -1, 0, 1, 2]])\n",
    "    result = tanh(test_input)\n",
    "    \n",
    "    # Check that all outputs are between -1 and 1\n",
    "    assert np.all(result.data > -1), \"Tanh outputs should be > -1\"\n",
    "    assert np.all(result.data < 1), \"Tanh outputs should be < 1\"\n",
    "    print(f\"âœ… Tanh test: input {test_input.data} â†’ output {result.data}\")\n",
    "    \n",
    "    # Test specific values\n",
    "    zero_input = Tensor([[0]])\n",
    "    zero_result = tanh(zero_input)\n",
    "    assert np.allclose(zero_result.data, 0.0, atol=1e-6), f\"Tanh(0) should be 0.0, got {zero_result.data}\"\n",
    "    print(\"âœ… Tanh(0) = 0.0 (zero-centered)\")\n",
    "    \n",
    "    # Test symmetry: tanh(-x) = -tanh(x)\n",
    "    pos_input = Tensor([[1]])\n",
    "    neg_input = Tensor([[-1]])\n",
    "    pos_result = tanh(pos_input)\n",
    "    neg_result = tanh(neg_input)\n",
    "    assert np.allclose(pos_result.data, -neg_result.data, atol=1e-6), \"Tanh should be symmetric\"\n",
    "    print(\"âœ… Tanh is symmetric: tanh(-x) = -tanh(x)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Tanh test failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# Show visual example\n",
    "print(\"ğŸ¯ Tanh behavior:\")\n",
    "print(\"   Large negative â†’ approaches -1\")\n",
    "print(\"   Zero â†’ 0.0 (zero-centered)\")\n",
    "print(\"   Large positive â†’ approaches 1\")\n",
    "print(\"ğŸ“ˆ Progress: ReLU âœ“, Sigmoid âœ“, Tanh âœ“\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff95c3f",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 5: Softmax - The Probability Converter\n",
    "\n",
    "### What is Softmax?\n",
    "**Softmax** converts a vector of numbers into a probability distribution.\n",
    "\n",
    "**Mathematical Definition:**\n",
    "```\n",
    "f(x_i) = e^(x_i) / Î£(e^(x_j)) for all j\n",
    "```\n",
    "\n",
    "**Properties:**\n",
    "- **Probabilities**: All outputs sum to 1\n",
    "- **Non-negative**: All outputs are â‰¥ 0\n",
    "- **Differentiable**: Smooth everywhere\n",
    "- **Competitive**: Amplifies differences between inputs\n",
    "\n",
    "### Why Softmax is Essential\n",
    "1. **Multi-class classification**: Converts logits to probabilities\n",
    "2. **Attention mechanisms**: Focuses on important elements\n",
    "3. **Interpretable**: Output can be understood as confidence\n",
    "4. **Competitive**: Emphasizes the largest input\n",
    "\n",
    "### Real-World Analogy\n",
    "Softmax is like **dividing a pie** - it takes any set of numbers and converts them into slices that sum to 100%.\n",
    "\n",
    "### When to Use Softmax\n",
    "- **Multi-class classification** (output layer)\n",
    "- **Attention mechanisms** in transformers\n",
    "- **When you need probability distributions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba3f4db",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "softmax-class",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class Softmax:\n",
    "    \"\"\"\n",
    "    Softmax Activation Function: f(x_i) = e^(x_i) / Î£(e^(x_j))\n",
    "    \n",
    "    Converts a vector of numbers into a probability distribution.\n",
    "    Essential for multi-class classification and attention mechanisms.\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply Softmax activation: f(x_i) = e^(x_i) / Î£(e^(x_j))\n",
    "        \n",
    "        TODO: Implement Softmax activation with numerical stability\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Subtract max value from inputs for numerical stability\n",
    "        2. Compute exponentials: e^(x_i - max)\n",
    "        3. Divide by sum of exponentials\n",
    "        4. Return a new Tensor with the results\n",
    "        \n",
    "        EXAMPLE:\n",
    "        Input: Tensor([[1, 2, 3]])\n",
    "        Expected: Tensor([[0.09, 0.24, 0.67]]) (approximately, sums to 1)\n",
    "        \n",
    "        HINTS:\n",
    "        - Use np.max(x.data, axis=-1, keepdims=True) for stability\n",
    "        - Use np.exp() for exponentials\n",
    "        - Use np.sum() for the denominator\n",
    "        - Make sure the result sums to 1 along the last axis\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        # Subtract max for numerical stability\n",
    "        x_max = np.max(x.data, axis=-1, keepdims=True)\n",
    "        x_shifted = x.data - x_max\n",
    "        \n",
    "        # Compute softmax\n",
    "        exp_x = np.exp(x_shifted)\n",
    "        sum_exp = np.sum(exp_x, axis=-1, keepdims=True)\n",
    "        result = exp_x / sum_exp\n",
    "        \n",
    "        return Tensor(result)\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Make the class callable: softmax(x) instead of softmax.forward(x)\"\"\"\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e575915",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### ğŸ§ª Unit Test: Softmax Activation\n",
    "\n",
    "Let's test your Softmax implementation! This should convert any vector into a probability distribution that sums to 1.\n",
    "\n",
    "**This is a unit test** - it tests one specific activation function (Softmax) in isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff3e424",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-softmax-immediate",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Softmax activation immediately after implementation\n",
    "print(\"ğŸ”¬ Unit Test: Softmax Activation...\")\n",
    "\n",
    "# Create Softmax instance\n",
    "softmax = Softmax()\n",
    "\n",
    "# Test with various inputs\n",
    "try:\n",
    "    test_input = Tensor([[1, 2, 3]])\n",
    "    result = softmax(test_input)\n",
    "    \n",
    "    # Check that all outputs are non-negative\n",
    "    assert np.all(result.data >= 0), \"Softmax outputs should be non-negative\"\n",
    "    print(f\"âœ… Softmax test: input {test_input.data} â†’ output {result.data}\")\n",
    "    \n",
    "    # Check that outputs sum to 1\n",
    "    sum_result = np.sum(result.data)\n",
    "    assert np.allclose(sum_result, 1.0, atol=1e-6), f\"Softmax should sum to 1, got {sum_result}\"\n",
    "    print(f\"âœ… Softmax sums to 1: {sum_result:.6f}\")\n",
    "    \n",
    "    # Test that larger inputs get higher probabilities\n",
    "    large_input = Tensor([[1, 2, 5]])  # 5 should get the highest probability\n",
    "    large_result = softmax(large_input)\n",
    "    max_idx = np.argmax(large_result.data)\n",
    "    assert max_idx == 2, f\"Largest input should get highest probability, got max at index {max_idx}\"\n",
    "    print(\"âœ… Softmax gives highest probability to largest input\")\n",
    "    \n",
    "    # Test numerical stability with large numbers\n",
    "    stable_input = Tensor([[1000, 1001, 1002]])\n",
    "    stable_result = softmax(stable_input)\n",
    "    assert not np.any(np.isnan(stable_result.data)), \"Softmax should be numerically stable\"\n",
    "    assert np.allclose(np.sum(stable_result.data), 1.0, atol=1e-6), \"Softmax should still sum to 1 with large inputs\"\n",
    "    print(\"âœ… Softmax is numerically stable with large inputs\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Softmax test failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# Show visual example\n",
    "print(\"ğŸ¯ Softmax behavior:\")\n",
    "print(\"   Converts any vector â†’ probability distribution\")\n",
    "print(\"   All outputs â‰¥ 0, sum = 1\")\n",
    "print(\"   Larger inputs â†’ higher probabilities\")\n",
    "print(\"ğŸ“ˆ Progress: ReLU âœ“, Sigmoid âœ“, Tanh âœ“, Softmax âœ“\")\n",
    "print(\"ğŸš€ All activation functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039170c1",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### ğŸ§ª Test Your Activation Functions\n",
    "\n",
    "Once you implement the activation functions above, run these cells to test them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452c927a",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-relu",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test ReLU activation\n",
    "print(\"Testing ReLU activation...\")\n",
    "\n",
    "relu = ReLU()\n",
    "\n",
    "# Test basic functionality\n",
    "input_tensor = Tensor([[-2, -1, 0, 1, 2]])\n",
    "output = relu(input_tensor)\n",
    "expected = np.array([[0, 0, 0, 1, 2]])\n",
    "assert np.array_equal(output.data, expected), f\"ReLU failed: expected {expected}, got {output.data}\"\n",
    "\n",
    "# Test with matrix\n",
    "matrix_input = Tensor([[-1, 2], [3, -4]])\n",
    "matrix_output = relu(matrix_input)\n",
    "expected_matrix = np.array([[0, 2], [3, 0]])\n",
    "assert np.array_equal(matrix_output.data, expected_matrix), f\"ReLU matrix failed: expected {expected_matrix}, got {matrix_output.data}\"\n",
    "\n",
    "# Test shape preservation\n",
    "assert output.shape == input_tensor.shape, f\"ReLU should preserve shape: input {input_tensor.shape}, output {output.shape}\"\n",
    "\n",
    "print(\"âœ… ReLU tests passed!\")\n",
    "print(f\"âœ… ReLU({input_tensor.data.flatten()}) = {output.data.flatten()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038bd4ab",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-sigmoid",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Sigmoid activation\n",
    "print(\"Testing Sigmoid activation...\")\n",
    "\n",
    "sigmoid = Sigmoid()\n",
    "\n",
    "# Test basic functionality\n",
    "input_tensor = Tensor([[0]])\n",
    "output = sigmoid(input_tensor)\n",
    "expected_value = 0.5\n",
    "assert abs(output.data.item() - expected_value) < 1e-6, f\"Sigmoid(0) should be 0.5, got {output.data.item()}\"\n",
    "\n",
    "# Test range bounds (allowing for floating-point precision at extremes)\n",
    "large_input = Tensor([[100]])\n",
    "large_output = sigmoid(large_input)\n",
    "assert 0 < large_output.data.item() <= 1, f\"Sigmoid output should be in (0,1], got {large_output.data.item()}\"\n",
    "\n",
    "small_input = Tensor([[-100]])\n",
    "small_output = sigmoid(small_input)\n",
    "assert 0 <= small_output.data.item() < 1, f\"Sigmoid output should be in [0,1), got {small_output.data.item()}\"\n",
    "\n",
    "# Test with multiple values\n",
    "multi_input = Tensor([[-2, 0, 2]])\n",
    "multi_output = sigmoid(multi_input)\n",
    "assert multi_output.shape == multi_input.shape, \"Sigmoid should preserve shape\"\n",
    "assert np.all((multi_output.data > 0) & (multi_output.data < 1)), \"All sigmoid outputs should be in (0,1)\"\n",
    "\n",
    "print(\"âœ… Sigmoid tests passed!\")\n",
    "print(f\"âœ… Sigmoid({multi_input.data.flatten()}) = {multi_output.data.flatten()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbb34b5",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-tanh",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Tanh activation\n",
    "print(\"Testing Tanh activation...\")\n",
    "\n",
    "tanh = Tanh()\n",
    "\n",
    "# Test basic functionality\n",
    "input_tensor = Tensor([[0]])\n",
    "output = tanh(input_tensor)\n",
    "expected_value = 0.0\n",
    "assert abs(output.data.item() - expected_value) < 1e-6, f\"Tanh(0) should be 0.0, got {output.data.item()}\"\n",
    "\n",
    "# Test range bounds (allowing for floating-point precision at extremes)\n",
    "large_input = Tensor([[100]])\n",
    "large_output = tanh(large_input)\n",
    "assert -1 <= large_output.data.item() <= 1, f\"Tanh output should be in [-1,1], got {large_output.data.item()}\"\n",
    "\n",
    "small_input = Tensor([[-100]])\n",
    "small_output = tanh(small_input)\n",
    "assert -1 <= small_output.data.item() <= 1, f\"Tanh output should be in [-1,1], got {small_output.data.item()}\"\n",
    "\n",
    "# Test symmetry: tanh(-x) = -tanh(x)\n",
    "test_input = Tensor([[2]])\n",
    "pos_output = tanh(test_input)\n",
    "neg_input = Tensor([[-2]])\n",
    "neg_output = tanh(neg_input)\n",
    "assert abs(pos_output.data.item() + neg_output.data.item()) < 1e-6, \"Tanh should be symmetric: tanh(-x) = -tanh(x)\"\n",
    "\n",
    "print(\"âœ… Tanh tests passed!\")\n",
    "print(f\"âœ… Tanh(Â±2) = Â±{abs(pos_output.data.item()):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969ebbce",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-softmax",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Softmax activation\n",
    "print(\"Testing Softmax activation...\")\n",
    "\n",
    "softmax = Softmax()\n",
    "\n",
    "# Test basic functionality\n",
    "input_tensor = Tensor([[1, 2, 3]])\n",
    "output = softmax(input_tensor)\n",
    "\n",
    "# Check that outputs sum to 1\n",
    "sum_output = np.sum(output.data)\n",
    "assert abs(sum_output - 1.0) < 1e-6, f\"Softmax outputs should sum to 1, got {sum_output}\"\n",
    "\n",
    "# Check that all outputs are positive\n",
    "assert np.all(output.data > 0), \"All softmax outputs should be positive\"\n",
    "\n",
    "# Check that larger inputs give larger outputs\n",
    "assert output.data[0, 2] > output.data[0, 1] > output.data[0, 0], \"Softmax should preserve order\"\n",
    "\n",
    "# Test with matrix (multiple rows)\n",
    "matrix_input = Tensor([[1, 2], [3, 4]])\n",
    "matrix_output = softmax(matrix_input)\n",
    "row_sums = np.sum(matrix_output.data, axis=1)\n",
    "assert np.allclose(row_sums, 1.0), f\"Each row should sum to 1, got {row_sums}\"\n",
    "\n",
    "print(\"âœ… Softmax tests passed!\")\n",
    "print(f\"âœ… Softmax({input_tensor.data.flatten()}) = {output.data.flatten()}\")\n",
    "print(f\"âœ… Sum = {np.sum(output.data):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b3787b",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-activation-integration",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test activation function integration\n",
    "print(\"Testing activation function integration...\")\n",
    "\n",
    "# Create test data\n",
    "test_data = Tensor([[-2, -1, 0, 1, 2]])\n",
    "\n",
    "# Test all activations\n",
    "relu = ReLU()\n",
    "sigmoid = Sigmoid()\n",
    "tanh = Tanh()\n",
    "softmax = Softmax()\n",
    "\n",
    "# Apply all activations\n",
    "relu_out = relu(test_data)\n",
    "sigmoid_out = sigmoid(test_data)\n",
    "tanh_out = tanh(test_data)\n",
    "softmax_out = softmax(test_data)\n",
    "\n",
    "# Check shapes are preserved\n",
    "assert relu_out.shape == test_data.shape, \"ReLU should preserve shape\"\n",
    "assert sigmoid_out.shape == test_data.shape, \"Sigmoid should preserve shape\"\n",
    "assert tanh_out.shape == test_data.shape, \"Tanh should preserve shape\"\n",
    "assert softmax_out.shape == test_data.shape, \"Softmax should preserve shape\"\n",
    "\n",
    "# Check ranges (allowing for floating-point precision at extremes)\n",
    "assert np.all(relu_out.data >= 0), \"ReLU outputs should be non-negative\"\n",
    "assert np.all((sigmoid_out.data >= 0) & (sigmoid_out.data <= 1)), \"Sigmoid outputs should be in [0,1]\"\n",
    "assert np.all((tanh_out.data >= -1) & (tanh_out.data <= 1)), \"Tanh outputs should be in [-1,1]\"\n",
    "assert np.all(softmax_out.data > 0), \"Softmax outputs should be positive\"\n",
    "\n",
    "# Test chaining (composition)\n",
    "chained = relu(sigmoid(test_data))\n",
    "assert chained.shape == test_data.shape, \"Chained activations should preserve shape\"\n",
    "\n",
    "print(\"âœ… Activation integration tests passed!\")\n",
    "print(f\"âœ… All activation functions work correctly\")\n",
    "print(f\"âœ… Input:   {test_data.data.flatten()}\")\n",
    "print(f\"âœ… ReLU:    {relu_out.data.flatten()}\")\n",
    "print(f\"âœ… Sigmoid: {sigmoid_out.data.flatten()}\")\n",
    "print(f\"âœ… Tanh:    {tanh_out.data.flatten()}\")\n",
    "print(f\"âœ… Softmax: {softmax_out.data.flatten()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ebc551",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## ğŸ§ª Comprehensive Testing: All Activation Functions\n",
    "\n",
    "Let's thoroughly test all your activation functions to make sure they work correctly in all scenarios.\n",
    "This comprehensive testing ensures your implementations are robust and ready for real ML applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d741aa",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-activations-comprehensive",
     "locked": true,
     "points": 25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_activations_comprehensive():\n",
    "    \"\"\"Comprehensive test of all activation functions.\"\"\"\n",
    "    print(\"ğŸ”¬ Testing all activation functions comprehensively...\")\n",
    "    \n",
    "    tests_passed = 0\n",
    "    total_tests = 12\n",
    "    \n",
    "    # Test 1: ReLU Basic Functionality\n",
    "    try:\n",
    "        relu = ReLU()\n",
    "        test_input = Tensor([[-2, -1, 0, 1, 2]])\n",
    "        result = relu(test_input)\n",
    "        expected = np.array([[0, 0, 0, 1, 2]])\n",
    "        \n",
    "        assert np.array_equal(result.data, expected), f\"ReLU failed: expected {expected}, got {result.data}\"\n",
    "        assert result.shape == test_input.shape, \"ReLU should preserve shape\"\n",
    "        assert np.all(result.data >= 0), \"ReLU outputs should be non-negative\"\n",
    "        \n",
    "        print(f\"âœ… ReLU basic: {test_input.data.flatten()} â†’ {result.data.flatten()}\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ReLU basic test failed: {e}\")\n",
    "    \n",
    "    # Test 2: ReLU Edge Cases\n",
    "    try:\n",
    "        relu = ReLU()\n",
    "        \n",
    "        # Test with zeros\n",
    "        zero_input = Tensor([[0, 0, 0]])\n",
    "        zero_result = relu(zero_input)\n",
    "        assert np.array_equal(zero_result.data, np.array([[0, 0, 0]])), \"ReLU(0) should be 0\"\n",
    "        \n",
    "        # Test with large values\n",
    "        large_input = Tensor([[1000, -1000]])\n",
    "        large_result = relu(large_input)\n",
    "        expected_large = np.array([[1000, 0]])\n",
    "        assert np.array_equal(large_result.data, expected_large), \"ReLU should handle large values\"\n",
    "        \n",
    "        # Test with matrix\n",
    "        matrix_input = Tensor([[-1, 2], [3, -4]])\n",
    "        matrix_result = relu(matrix_input)\n",
    "        expected_matrix = np.array([[0, 2], [3, 0]])\n",
    "        assert np.array_equal(matrix_result.data, expected_matrix), \"ReLU should work with matrices\"\n",
    "        \n",
    "        print(\"âœ… ReLU edge cases: zeros, large values, matrices\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ReLU edge cases failed: {e}\")\n",
    "    \n",
    "    # Test 3: Sigmoid Basic Functionality\n",
    "    try:\n",
    "        sigmoid = Sigmoid()\n",
    "        \n",
    "        # Test sigmoid(0) = 0.5\n",
    "        zero_input = Tensor([[0]])\n",
    "        zero_result = sigmoid(zero_input)\n",
    "        assert abs(zero_result.data.item() - 0.5) < 1e-6, f\"Sigmoid(0) should be 0.5, got {zero_result.data.item()}\"\n",
    "        \n",
    "        # Test range bounds\n",
    "        test_input = Tensor([[-10, -1, 0, 1, 10]])\n",
    "        result = sigmoid(test_input)\n",
    "        assert np.all((result.data > 0) & (result.data < 1)), \"Sigmoid outputs should be in (0,1)\"\n",
    "        assert result.shape == test_input.shape, \"Sigmoid should preserve shape\"\n",
    "        \n",
    "        print(f\"âœ… Sigmoid basic: range (0,1), sigmoid(0)=0.5\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Sigmoid basic test failed: {e}\")\n",
    "    \n",
    "    # Test 4: Sigmoid Properties\n",
    "    try:\n",
    "        sigmoid = Sigmoid()\n",
    "        \n",
    "        # Test monotonicity\n",
    "        inputs = Tensor([[-2, -1, 0, 1, 2]])\n",
    "        outputs = sigmoid(inputs)\n",
    "        output_values = outputs.data.flatten()\n",
    "        \n",
    "        # Check that outputs are increasing\n",
    "        for i in range(len(output_values) - 1):\n",
    "            assert output_values[i] < output_values[i + 1], \"Sigmoid should be monotonic increasing\"\n",
    "        \n",
    "        # Test numerical stability with extreme values\n",
    "        extreme_input = Tensor([[-1000, 1000]])\n",
    "        extreme_result = sigmoid(extreme_input)\n",
    "        assert not np.any(np.isnan(extreme_result.data)), \"Sigmoid should handle extreme values without NaN\"\n",
    "        assert not np.any(np.isinf(extreme_result.data)), \"Sigmoid should handle extreme values without Inf\"\n",
    "        \n",
    "        print(\"âœ… Sigmoid properties: monotonic, numerically stable\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Sigmoid properties failed: {e}\")\n",
    "    \n",
    "    # Test 5: Tanh Basic Functionality\n",
    "    try:\n",
    "        tanh = Tanh()\n",
    "        \n",
    "        # Test tanh(0) = 0\n",
    "        zero_input = Tensor([[0]])\n",
    "        zero_result = tanh(zero_input)\n",
    "        assert abs(zero_result.data.item() - 0.0) < 1e-6, f\"Tanh(0) should be 0.0, got {zero_result.data.item()}\"\n",
    "        \n",
    "        # Test range bounds\n",
    "        test_input = Tensor([[-10, -1, 0, 1, 10]])\n",
    "        result = tanh(test_input)\n",
    "        assert np.all((result.data >= -1) & (result.data <= 1)), \"Tanh outputs should be in [-1,1]\"\n",
    "        assert result.shape == test_input.shape, \"Tanh should preserve shape\"\n",
    "        \n",
    "        print(f\"âœ… Tanh basic: range [-1,1], tanh(0)=0\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Tanh basic test failed: {e}\")\n",
    "    \n",
    "    # Test 6: Tanh Symmetry\n",
    "    try:\n",
    "        tanh = Tanh()\n",
    "        \n",
    "        # Test symmetry: tanh(-x) = -tanh(x)\n",
    "        test_values = [1, 2, 3, 5]\n",
    "        for val in test_values:\n",
    "            pos_input = Tensor([[val]])\n",
    "            neg_input = Tensor([[-val]])\n",
    "            pos_result = tanh(pos_input)\n",
    "            neg_result = tanh(neg_input)\n",
    "            \n",
    "            assert abs(pos_result.data.item() + neg_result.data.item()) < 1e-6, f\"Tanh should be symmetric: tanh(-{val}) â‰  -tanh({val})\"\n",
    "        \n",
    "        # Test numerical stability\n",
    "        extreme_input = Tensor([[-1000, 1000]])\n",
    "        extreme_result = tanh(extreme_input)\n",
    "        assert not np.any(np.isnan(extreme_result.data)), \"Tanh should handle extreme values without NaN\"\n",
    "        \n",
    "        print(\"âœ… Tanh symmetry: tanh(-x) = -tanh(x), numerically stable\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Tanh symmetry failed: {e}\")\n",
    "    \n",
    "    # Test 7: Softmax Basic Functionality\n",
    "    try:\n",
    "        softmax = Softmax()\n",
    "        \n",
    "        # Test that outputs sum to 1\n",
    "        test_input = Tensor([[1, 2, 3]])\n",
    "        result = softmax(test_input)\n",
    "        sum_result = np.sum(result.data)\n",
    "        assert abs(sum_result - 1.0) < 1e-6, f\"Softmax outputs should sum to 1, got {sum_result}\"\n",
    "        \n",
    "        # Test that all outputs are positive\n",
    "        assert np.all(result.data > 0), \"All softmax outputs should be positive\"\n",
    "        \n",
    "        # Test that larger inputs give larger outputs\n",
    "        assert result.data[0, 2] > result.data[0, 1] > result.data[0, 0], \"Softmax should preserve order\"\n",
    "        \n",
    "        print(f\"âœ… Softmax basic: sums to 1, all positive, preserves order\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Softmax basic test failed: {e}\")\n",
    "    \n",
    "    # Test 8: Softmax with Multiple Rows\n",
    "    try:\n",
    "        softmax = Softmax()\n",
    "        \n",
    "        # Test with matrix (multiple rows)\n",
    "        matrix_input = Tensor([[1, 2, 3], [4, 5, 6]])\n",
    "        matrix_result = softmax(matrix_input)\n",
    "        \n",
    "        # Each row should sum to 1\n",
    "        row_sums = np.sum(matrix_result.data, axis=1)\n",
    "        assert np.allclose(row_sums, 1.0), f\"Each row should sum to 1, got {row_sums}\"\n",
    "        \n",
    "        # All values should be positive\n",
    "        assert np.all(matrix_result.data > 0), \"All softmax outputs should be positive\"\n",
    "        \n",
    "        # Test numerical stability with extreme values\n",
    "        extreme_input = Tensor([[1000, 1001, 1002]])\n",
    "        extreme_result = softmax(extreme_input)\n",
    "        assert not np.any(np.isnan(extreme_result.data)), \"Softmax should handle extreme values without NaN\"\n",
    "        assert abs(np.sum(extreme_result.data) - 1.0) < 1e-6, \"Softmax should still sum to 1 with extreme values\"\n",
    "        \n",
    "        print(\"âœ… Softmax matrices: each row sums to 1, numerically stable\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Softmax matrices failed: {e}\")\n",
    "    \n",
    "    # Test 9: Shape Preservation\n",
    "    try:\n",
    "        relu = ReLU()\n",
    "        sigmoid = Sigmoid()\n",
    "        tanh = Tanh()\n",
    "        softmax = Softmax()\n",
    "        \n",
    "        # Test different shapes\n",
    "        test_shapes = [\n",
    "            Tensor([[1]]),                    # 1x1\n",
    "            Tensor([[1, 2, 3]]),             # 1x3\n",
    "            Tensor([[1], [2], [3]]),         # 3x1\n",
    "            Tensor([[1, 2], [3, 4]]),        # 2x2\n",
    "            Tensor([[1, 2], [3, 4]]),        # 2x2\n",
    "        ]\n",
    "        \n",
    "        for i, test_tensor in enumerate(test_shapes):\n",
    "            original_shape = test_tensor.shape\n",
    "            \n",
    "            relu_result = relu(test_tensor)\n",
    "            sigmoid_result = sigmoid(test_tensor)\n",
    "            tanh_result = tanh(test_tensor)\n",
    "            softmax_result = softmax(test_tensor)\n",
    "            \n",
    "            assert relu_result.shape == original_shape, f\"ReLU shape mismatch for test {i}\"\n",
    "            assert sigmoid_result.shape == original_shape, f\"Sigmoid shape mismatch for test {i}\"\n",
    "            assert tanh_result.shape == original_shape, f\"Tanh shape mismatch for test {i}\"\n",
    "            assert softmax_result.shape == original_shape, f\"Softmax shape mismatch for test {i}\"\n",
    "        \n",
    "        print(\"âœ… Shape preservation: all activations preserve input shapes\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Shape preservation failed: {e}\")\n",
    "    \n",
    "    # Test 10: Function Composition\n",
    "    try:\n",
    "        relu = ReLU()\n",
    "        sigmoid = Sigmoid()\n",
    "        tanh = Tanh()\n",
    "        \n",
    "        # Test chaining activations\n",
    "        test_input = Tensor([[-2, -1, 0, 1, 2]])\n",
    "        \n",
    "        # Chain: input â†’ tanh â†’ relu\n",
    "        tanh_result = tanh(test_input)\n",
    "        relu_tanh_result = relu(tanh_result)\n",
    "        \n",
    "        # Chain: input â†’ sigmoid â†’ tanh\n",
    "        sigmoid_result = sigmoid(test_input)\n",
    "        tanh_sigmoid_result = tanh(sigmoid_result)\n",
    "        \n",
    "        # All should preserve shape\n",
    "        assert relu_tanh_result.shape == test_input.shape, \"Chained activations should preserve shape\"\n",
    "        assert tanh_sigmoid_result.shape == test_input.shape, \"Chained activations should preserve shape\"\n",
    "        \n",
    "        # Results should be valid\n",
    "        assert np.all(relu_tanh_result.data >= 0), \"ReLU after Tanh should be non-negative\"\n",
    "        assert np.all((tanh_sigmoid_result.data >= -1) & (tanh_sigmoid_result.data <= 1)), \"Tanh after Sigmoid should be in [-1,1]\"\n",
    "        \n",
    "        print(\"âœ… Function composition: activations can be chained together\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Function composition failed: {e}\")\n",
    "    \n",
    "    # Test 11: Real ML Scenario\n",
    "    try:\n",
    "        # Simulate a neural network layer output\n",
    "        logits = Tensor([[2.0, 1.0, 0.1]])  # Raw network outputs\n",
    "        \n",
    "        # Apply softmax for classification\n",
    "        softmax = Softmax()\n",
    "        probabilities = softmax(logits)\n",
    "        \n",
    "        # Check that we get valid probabilities\n",
    "        assert abs(np.sum(probabilities.data) - 1.0) < 1e-6, \"Probabilities should sum to 1\"\n",
    "        assert np.all(probabilities.data > 0), \"All probabilities should be positive\"\n",
    "        \n",
    "        # The highest logit should give the highest probability\n",
    "        max_logit_idx = np.argmax(logits.data)\n",
    "        max_prob_idx = np.argmax(probabilities.data)\n",
    "        assert max_logit_idx == max_prob_idx, \"Highest logit should give highest probability\"\n",
    "        \n",
    "        # Apply ReLU to hidden layer\n",
    "        hidden_activations = Tensor([[-0.5, 0.8, -1.2, 2.1]])\n",
    "        relu = ReLU()\n",
    "        relu_output = relu(hidden_activations)\n",
    "        \n",
    "        # Should zero out negative values\n",
    "        expected_relu = np.array([[0.0, 0.8, 0.0, 2.1]])\n",
    "        assert np.array_equal(relu_output.data, expected_relu), \"ReLU should zero negative values\"\n",
    "        \n",
    "        print(\"âœ… Real ML scenario: classification probabilities, hidden layer activation\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Real ML scenario failed: {e}\")\n",
    "    \n",
    "    # Test 12: Performance and Stability\n",
    "    try:\n",
    "        # Test with large tensors\n",
    "        large_input = Tensor(np.random.randn(100, 50))\n",
    "        \n",
    "        relu = ReLU()\n",
    "        sigmoid = Sigmoid()\n",
    "        tanh = Tanh()\n",
    "        softmax = Softmax()\n",
    "        \n",
    "        # All should handle large tensors\n",
    "        relu_large = relu(large_input)\n",
    "        sigmoid_large = sigmoid(large_input)\n",
    "        tanh_large = tanh(large_input)\n",
    "        softmax_large = softmax(large_input)\n",
    "        \n",
    "        # Check for NaN or Inf\n",
    "        assert not np.any(np.isnan(relu_large.data)), \"ReLU should not produce NaN\"\n",
    "        assert not np.any(np.isnan(sigmoid_large.data)), \"Sigmoid should not produce NaN\"\n",
    "        assert not np.any(np.isnan(tanh_large.data)), \"Tanh should not produce NaN\"\n",
    "        assert not np.any(np.isnan(softmax_large.data)), \"Softmax should not produce NaN\"\n",
    "        \n",
    "        assert not np.any(np.isinf(relu_large.data)), \"ReLU should not produce Inf\"\n",
    "        assert not np.any(np.isinf(sigmoid_large.data)), \"Sigmoid should not produce Inf\"\n",
    "        assert not np.any(np.isinf(tanh_large.data)), \"Tanh should not produce Inf\"\n",
    "        assert not np.any(np.isinf(softmax_large.data)), \"Softmax should not produce Inf\"\n",
    "        \n",
    "        print(\"âœ… Performance and stability: large tensors handled without NaN/Inf\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Performance and stability failed: {e}\")\n",
    "    \n",
    "    # Results summary\n",
    "    print(f\"\\nğŸ“Š Activation Functions Results: {tests_passed}/{total_tests} tests passed\")\n",
    "    \n",
    "    if tests_passed == total_tests:\n",
    "        print(\"ğŸ‰ All activation function tests passed! Your implementations support:\")\n",
    "        print(\"  â€¢ ReLU: Fast, sparse activation for hidden layers\")\n",
    "        print(\"  â€¢ Sigmoid: Smooth probabilistic outputs (0,1)\")\n",
    "        print(\"  â€¢ Tanh: Zero-centered activation (-1,1)\")\n",
    "        print(\"  â€¢ Softmax: Probability distributions for classification\")\n",
    "        print(\"  â€¢ All functions preserve shapes and handle edge cases\")\n",
    "        print(\"  â€¢ Numerical stability with extreme values\")\n",
    "        print(\"  â€¢ Function composition for complex networks\")\n",
    "        print(\"ğŸ“ˆ Progress: All Activation Functions âœ“\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"âš ï¸  Some activation tests failed. Common issues:\")\n",
    "        print(\"  â€¢ Check mathematical formulas (especially sigmoid and tanh)\")\n",
    "        print(\"  â€¢ Verify numerical stability (clip extreme values)\")\n",
    "        print(\"  â€¢ Ensure proper shape preservation\")\n",
    "        print(\"  â€¢ Test with edge cases (zeros, large values)\")\n",
    "        print(\"  â€¢ Verify softmax sums to 1 for each row\")\n",
    "        return False\n",
    "\n",
    "# Run the comprehensive test\n",
    "success = test_activations_comprehensive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873decbc",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### ğŸ§ª Integration Test: Activation Functions in Neural Networks\n",
    "\n",
    "Let's test how your activation functions work in a realistic neural network scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29563aa9",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-activations-integration",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_activations_integration():\n",
    "    \"\"\"Integration test with realistic neural network scenario.\"\"\"\n",
    "    print(\"ğŸ”¬ Testing activation functions in neural network scenario...\")\n",
    "    \n",
    "    try:\n",
    "        print(\"ğŸ§  Simulating a 3-layer neural network...\")\n",
    "        \n",
    "        # Layer 1: Input data (batch of 3 samples, 4 features each)\n",
    "        input_data = Tensor([[1.0, -2.0, 3.0, -1.0],\n",
    "                           [2.0, 1.0, -1.0, 0.5],\n",
    "                           [-1.0, 3.0, 2.0, -0.5]])\n",
    "        print(f\"ğŸ“Š Input data shape: {input_data.shape}\")\n",
    "        \n",
    "        # Layer 2: Hidden layer with ReLU activation\n",
    "        # Simulate some linear transformation results\n",
    "        hidden_raw = Tensor([[2.1, -1.5, 0.8],\n",
    "                           [1.2, 3.4, -0.3],\n",
    "                           [-0.7, 2.8, 1.9]])\n",
    "        \n",
    "        relu = ReLU()\n",
    "        hidden_activated = relu(hidden_raw)\n",
    "        print(f\"âœ… Hidden layer (ReLU): {hidden_raw.data.flatten()[:3]} â†’ {hidden_activated.data.flatten()[:3]}\")\n",
    "        \n",
    "        # Verify ReLU worked correctly\n",
    "        assert np.all(hidden_activated.data >= 0), \"Hidden layer should have non-negative activations\"\n",
    "        \n",
    "        # Layer 3: Output layer for binary classification (sigmoid)\n",
    "        output_raw = Tensor([[0.8], [2.1], [-0.5]])\n",
    "        \n",
    "        sigmoid = Sigmoid()\n",
    "        output_probs = sigmoid(output_raw)\n",
    "        print(f\"âœ… Output layer (Sigmoid): {output_raw.data.flatten()} â†’ {output_probs.data.flatten()}\")\n",
    "        \n",
    "        # Verify sigmoid outputs are valid probabilities\n",
    "        assert np.all((output_probs.data > 0) & (output_probs.data < 1)), \"Output should be valid probabilities\"\n",
    "        \n",
    "        # Alternative: Multi-class classification with softmax\n",
    "        multiclass_raw = Tensor([[1.0, 2.0, 0.5],\n",
    "                               [0.1, 0.8, 2.1],\n",
    "                               [1.5, 0.3, 1.2]])\n",
    "        \n",
    "        softmax = Softmax()\n",
    "        class_probs = softmax(multiclass_raw)\n",
    "        print(f\"âœ… Multi-class output (Softmax): each row sums to {np.sum(class_probs.data, axis=1)}\")\n",
    "        \n",
    "        # Verify softmax outputs\n",
    "        row_sums = np.sum(class_probs.data, axis=1)\n",
    "        assert np.allclose(row_sums, 1.0), \"Each sample should have probabilities summing to 1\"\n",
    "        \n",
    "        # Test activation function chaining\n",
    "        print(\"\\nğŸ”— Testing activation function chaining...\")\n",
    "        \n",
    "        # Chain: Tanh â†’ ReLU (unusual but valid)\n",
    "        tanh = Tanh()\n",
    "        test_input = Tensor([[-2, -1, 0, 1, 2]])\n",
    "        \n",
    "        tanh_result = tanh(test_input)\n",
    "        relu_tanh_result = relu(tanh_result)\n",
    "        \n",
    "        print(f\"âœ… Tanh â†’ ReLU: {test_input.data.flatten()} â†’ {tanh_result.data.flatten()} â†’ {relu_tanh_result.data.flatten()}\")\n",
    "        \n",
    "        # Verify chaining worked\n",
    "        assert relu_tanh_result.shape == test_input.shape, \"Chained activations should preserve shape\"\n",
    "        assert np.all(relu_tanh_result.data >= 0), \"Final result should be non-negative (ReLU effect)\"\n",
    "        \n",
    "        # Test different activation choices\n",
    "        print(\"\\nğŸ¯ Testing activation function choices...\")\n",
    "        \n",
    "        # Compare different activations on same input\n",
    "        comparison_input = Tensor([[0.5, -0.5, 1.0, -1.0]])\n",
    "        \n",
    "        relu_comp = relu(comparison_input)\n",
    "        sigmoid_comp = sigmoid(comparison_input)\n",
    "        tanh_comp = tanh(comparison_input)\n",
    "        \n",
    "        print(f\"Input:   {comparison_input.data.flatten()}\")\n",
    "        print(f\"ReLU:    {relu_comp.data.flatten()}\")\n",
    "        print(f\"Sigmoid: {sigmoid_comp.data.flatten()}\")\n",
    "        print(f\"Tanh:    {tanh_comp.data.flatten()}\")\n",
    "        \n",
    "        # Show how different activations affect the same input\n",
    "        print(\"\\nğŸ“ˆ Activation function characteristics:\")\n",
    "        print(\"â€¢ ReLU: Sparse (many zeros), unbounded positive\")\n",
    "        print(\"â€¢ Sigmoid: Smooth, bounded (0,1), good for probabilities\")\n",
    "        print(\"â€¢ Tanh: Zero-centered (-1,1), symmetric\")\n",
    "        print(\"â€¢ Softmax: Probability distribution, sums to 1\")\n",
    "        \n",
    "        print(\"\\nğŸ‰ Integration test passed! Your activation functions work correctly in:\")\n",
    "        print(\"  â€¢ Multi-layer neural networks\")\n",
    "        print(\"  â€¢ Binary and multi-class classification\")\n",
    "        print(\"  â€¢ Function composition and chaining\")\n",
    "        print(\"  â€¢ Different architectural choices\")\n",
    "        print(\"ğŸ“ˆ Progress: All activation functions ready for neural networks!\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Integration test failed: {e}\")\n",
    "        print(\"\\nğŸ’¡ This suggests an issue with:\")\n",
    "        print(\"  â€¢ Basic activation function implementation\")\n",
    "        print(\"  â€¢ Shape handling in neural network context\")\n",
    "        print(\"  â€¢ Mathematical correctness of the functions\")\n",
    "        print(\"  â€¢ Check your activation function implementations\")\n",
    "        return False\n",
    "\n",
    "# Run the integration test\n",
    "success = test_activations_integration() and success\n",
    "\n",
    "# Print final summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ğŸ¯ ACTIVATION FUNCTIONS MODULE TESTING COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if success:\n",
    "    print(\"ğŸ‰ CONGRATULATIONS! All activation function tests passed!\")\n",
    "    print(\"\\nâœ… Your activation functions successfully implement:\")\n",
    "    print(\"  â€¢ ReLU: max(0, x) for sparse hidden layer activation\")\n",
    "    print(\"  â€¢ Sigmoid: 1/(1+e^(-x)) for binary classification\")\n",
    "    print(\"  â€¢ Tanh: tanh(x) for zero-centered activation\")\n",
    "    print(\"  â€¢ Softmax: probability distributions for multi-class classification\")\n",
    "    print(\"  â€¢ Numerical stability with extreme values\")\n",
    "    print(\"  â€¢ Shape preservation and function composition\")\n",
    "    print(\"  â€¢ Real neural network integration\")\n",
    "    print(\"\\nğŸš€ You're ready to build neural network layers!\")\n",
    "    print(\"ğŸ“ˆ Final Progress: Activation Functions Module âœ“ COMPLETE\")\n",
    "else:\n",
    "    print(\"âš ï¸  Some tests failed. Please review the error messages above.\")\n",
    "    print(\"\\nğŸ”§ To fix issues:\")\n",
    "    print(\"  1. Check the specific activation function that failed\")\n",
    "    print(\"  2. Review the mathematical formulas\")\n",
    "    print(\"  3. Verify numerical stability (especially for sigmoid/tanh)\")\n",
    "    print(\"  4. Test with edge cases (zeros, large values)\")\n",
    "    print(\"  5. Ensure softmax sums to 1\")\n",
    "    print(\"\\nğŸ’ª Keep going! These functions are the key to neural network power.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e77ef6",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ğŸ¯ Module Summary\n",
    "\n",
    "Congratulations! You've successfully implemented the core activation functions for TinyTorch:\n",
    "\n",
    "### What You've Accomplished\n",
    "âœ… **ReLU**: The workhorse activation for hidden layers  \n",
    "âœ… **Sigmoid**: Smooth probabilistic outputs for binary classification  \n",
    "âœ… **Tanh**: Zero-centered activation for better training dynamics  \n",
    "âœ… **Softmax**: Probability distributions for multi-class classification  \n",
    "âœ… **Integration**: All functions work together and preserve tensor shapes  \n",
    "\n",
    "### Key Concepts You've Learned\n",
    "- **Nonlinearity** is essential for neural networks to learn complex patterns\n",
    "- **ReLU** is simple, fast, and effective for most hidden layers\n",
    "- **Sigmoid** squashes outputs to (0,1) for probabilistic interpretation\n",
    "- **Tanh** is zero-centered and often better than sigmoid for hidden layers\n",
    "- **Softmax** converts logits to probability distributions\n",
    "- **Numerical stability** is crucial for functions with exponentials\n",
    "\n",
    "### Next Steps\n",
    "1. **Export your code**: `tito package nbdev --export 02_activations`\n",
    "2. **Test your implementation**: `tito module test 02_activations`\n",
    "3. **Use your activations**: \n",
    "   ```python\n",
    "   from tinytorch.core.activations import ReLU, Sigmoid, Tanh, Softmax\n",
    "   from tinytorch.core.tensor import Tensor\n",
    "   \n",
    "   relu = ReLU()\n",
    "   x = Tensor([[-1, 0, 1, 2]])\n",
    "   y = relu(x)  # Your activation in action!\n",
    "   ```\n",
    "4. **Move to Module 3**: Start building neural network layers!\n",
    "\n",
    "**Ready for the next challenge?** Let's combine tensors and activations to build the fundamental building blocks of neural networks!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
