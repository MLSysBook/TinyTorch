{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bf03147",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Module 3: Layers - Building Blocks of Neural Networks\n",
    "\n",
    "Welcome to the Layers module! This is where we build the fundamental components that stack together to form neural networks.\n",
    "\n",
    "## Learning Goals\n",
    "- Understand how matrix multiplication powers neural networks\n",
    "- Implement naive matrix multiplication from scratch for deep understanding\n",
    "- Build the Dense (Linear) layer - the foundation of all neural networks\n",
    "- Learn weight initialization strategies and their importance\n",
    "- See how layers compose with activations to create powerful networks\n",
    "\n",
    "## Build → Use → Understand\n",
    "1. **Build**: Matrix multiplication and Dense layers from scratch\n",
    "2. **Use**: Create and test layers with real data\n",
    "3. **Understand**: How linear transformations enable feature learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f34004",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "layers-imports",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| default_exp core.layers\n",
    "\n",
    "#| export\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from typing import Union, List, Tuple, Optional\n",
    "\n",
    "# Import our dependencies - try from package first, then local modules\n",
    "try:\n",
    "    from tinytorch.core.tensor import Tensor\n",
    "    from tinytorch.core.activations import ReLU, Sigmoid, Tanh, Softmax\n",
    "except ImportError:\n",
    "    # For development, import from local modules\n",
    "    sys.path.append(os.path.join(os.path.dirname(__file__), '..', '01_tensor'))\n",
    "    sys.path.append(os.path.join(os.path.dirname(__file__), '..', '02_activations'))\n",
    "    from tensor_dev import Tensor\n",
    "    from activations_dev import ReLU, Sigmoid, Tanh, Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce02580",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "layers-setup",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "def _should_show_plots():\n",
    "    \"\"\"Check if we should show plots (disable during testing)\"\"\"\n",
    "    # Check multiple conditions that indicate we're in test mode\n",
    "    is_pytest = (\n",
    "        'pytest' in sys.modules or\n",
    "        'test' in sys.argv or\n",
    "        os.environ.get('PYTEST_CURRENT_TEST') is not None or\n",
    "        any('test' in arg for arg in sys.argv) or\n",
    "        any('pytest' in arg for arg in sys.argv)\n",
    "    )\n",
    "    \n",
    "    # Show plots in development mode (when not in test mode)\n",
    "    return not is_pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2982ae1d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "layers-welcome",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"🔥 TinyTorch Layers Module\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Python version: {sys.version_info.major}.{sys.version_info.minor}\")\n",
    "print(\"Ready to build neural network layers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7bdf22",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 📦 Where This Code Lives in the Final Package\n",
    "\n",
    "**Learning Side:** You work in `modules/source/03_layers/layers_dev.py`  \n",
    "**Building Side:** Code exports to `tinytorch.core.layers`\n",
    "\n",
    "```python\n",
    "# Final package structure:\n",
    "from tinytorch.core.layers import Dense, Conv2D  # All layer types together!\n",
    "from tinytorch.core.tensor import Tensor  # The foundation\n",
    "from tinytorch.core.activations import ReLU, Sigmoid  # Nonlinearity\n",
    "```\n",
    "\n",
    "**Why this matters:**\n",
    "- **Learning:** Focused modules for deep understanding\n",
    "- **Production:** Proper organization like PyTorch's `torch.nn.Linear`\n",
    "- **Consistency:** All layer types live together in `core.layers`\n",
    "- **Integration:** Works seamlessly with tensors and activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809fbdeb",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 🧠 The Mathematical Foundation of Neural Layers\n",
    "\n",
    "### Linear Algebra at the Heart of ML\n",
    "Neural networks are fundamentally about **linear transformations** followed by **nonlinear activations**:\n",
    "\n",
    "```\n",
    "Layer: y = Wx + b (linear transformation)\n",
    "Activation: z = σ(y) (nonlinear transformation)\n",
    "```\n",
    "\n",
    "### Matrix Multiplication: The Engine of Deep Learning\n",
    "Every forward pass in a neural network involves matrix multiplication:\n",
    "- **Dense layers**: Matrix multiplication between inputs and weights\n",
    "- **Convolutional layers**: Convolution as matrix multiplication\n",
    "- **Attention**: Query-key-value matrix operations\n",
    "- **Transformers**: Self-attention through matrix operations\n",
    "\n",
    "### Why Matrix Multiplication Matters\n",
    "- **Parallel computation**: GPUs excel at matrix operations\n",
    "- **Batch processing**: Handle multiple samples simultaneously\n",
    "- **Feature learning**: Each row/column learns different patterns\n",
    "- **Composability**: Layers stack naturally through matrix chains\n",
    "\n",
    "### Connection to Real ML Systems\n",
    "Every framework optimizes matrix multiplication:\n",
    "- **PyTorch**: `torch.nn.Linear` uses optimized BLAS\n",
    "- **TensorFlow**: `tf.keras.layers.Dense` uses cuDNN\n",
    "- **JAX**: `jax.numpy.dot` uses XLA compilation\n",
    "- **TinyTorch**: `tinytorch.core.layers.Dense` (what we're building!)\n",
    "\n",
    "### Performance Considerations\n",
    "- **Memory layout**: Contiguous arrays for cache efficiency\n",
    "- **Vectorization**: SIMD operations for speed\n",
    "- **Parallelization**: Multi-threading and GPU acceleration\n",
    "- **Numerical stability**: Proper initialization and normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6970c3ff",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 1: Understanding Matrix Multiplication\n",
    "\n",
    "### What is Matrix Multiplication?\n",
    "Matrix multiplication is the **fundamental operation** that powers neural networks. When we multiply matrices A and B:\n",
    "\n",
    "```\n",
    "C = A @ B\n",
    "```\n",
    "\n",
    "Each element C[i,j] is the **dot product** of row i from A and column j from B.\n",
    "\n",
    "### The Mathematical Foundation: Linear Algebra in Neural Networks\n",
    "\n",
    "#### **Why Matrix Multiplication in Neural Networks?**\n",
    "Neural networks are fundamentally about **linear transformations** followed by **nonlinear activations**:\n",
    "\n",
    "```python\n",
    "# The core neural network operation:\n",
    "linear_output = weights @ input + bias    # Linear transformation (matrix multiplication)\n",
    "activation_output = activation_function(linear_output)  # Nonlinear transformation\n",
    "```\n",
    "\n",
    "#### **The Geometric Interpretation**\n",
    "Matrix multiplication represents **geometric transformations** in high-dimensional space:\n",
    "\n",
    "- **Rotation**: Changing the orientation of data\n",
    "- **Scaling**: Stretching or compressing along certain dimensions\n",
    "- **Projection**: Mapping to lower or higher dimensional spaces\n",
    "- **Translation**: Shifting data (via bias terms)\n",
    "\n",
    "#### **Why This Matters for Learning**\n",
    "Each layer learns to transform the input space to make the final task easier:\n",
    "\n",
    "```python\n",
    "# Example: Image classification\n",
    "raw_pixels → [Layer 1] → edges → [Layer 2] → shapes → [Layer 3] → objects → [Layer 4] → classes\n",
    "```\n",
    "\n",
    "### The Computational Perspective\n",
    "\n",
    "#### **Batch Processing Power**\n",
    "Matrix multiplication enables efficient batch processing:\n",
    "\n",
    "```python\n",
    "# Single sample (inefficient):\n",
    "for sample in batch:\n",
    "    output = weights @ sample + bias  # Process one at a time\n",
    "\n",
    "# Batch processing (efficient):\n",
    "batch_output = weights @ batch + bias  # Process all samples simultaneously\n",
    "```\n",
    "\n",
    "#### **Parallelization Benefits**\n",
    "- **CPU**: Multiple cores can compute different parts simultaneously\n",
    "- **GPU**: Thousands of cores excel at matrix operations\n",
    "- **TPU**: Specialized hardware designed for matrix multiplication\n",
    "- **Memory**: Contiguous memory access patterns improve cache efficiency\n",
    "\n",
    "#### **Computational Complexity**\n",
    "For matrices A(m×n) and B(n×p):\n",
    "- **Time complexity**: O(mnp) - cubic in the worst case\n",
    "- **Space complexity**: O(mp) - for the output matrix\n",
    "- **Optimization**: Modern libraries use optimized algorithms (Strassen, etc.)\n",
    "\n",
    "### Real-World Applications: Where Matrix Multiplication Shines\n",
    "\n",
    "#### **Computer Vision**\n",
    "```python\n",
    "# Convolutional layers can be expressed as matrix multiplication:\n",
    "# Image patches → Matrix A\n",
    "# Convolutional filters → Matrix B\n",
    "# Feature maps → Matrix C = A @ B\n",
    "```\n",
    "\n",
    "#### **Natural Language Processing**\n",
    "```python\n",
    "# Transformer attention mechanism:\n",
    "# Query matrix Q, Key matrix K, Value matrix V\n",
    "# Attention weights = softmax(Q @ K.T / sqrt(d_k))\n",
    "# Output = Attention_weights @ V\n",
    "```\n",
    "\n",
    "#### **Recommendation Systems**\n",
    "```python\n",
    "# Matrix factorization:\n",
    "# User-item matrix R ≈ User_factors @ Item_factors.T\n",
    "# Collaborative filtering through matrix operations\n",
    "```\n",
    "\n",
    "### The Algorithm: Understanding Every Step\n",
    "\n",
    "For matrices A(m×n) and B(n×p) → C(m×p):\n",
    "```python\n",
    "for i in range(m):        # For each row of A\n",
    "    for j in range(p):    # For each column of B\n",
    "        for k in range(n):  # Compute dot product\n",
    "            C[i,j] += A[i,k] * B[k,j]\n",
    "```\n",
    "\n",
    "#### **Visual Breakdown**\n",
    "```\n",
    "A = [[1, 2],     B = [[5, 6],     C = [[19, 22],\n",
    "     [3, 4]]          [7, 8]]          [43, 50]]\n",
    "\n",
    "C[0,0] = A[0,0]*B[0,0] + A[0,1]*B[1,0] = 1*5 + 2*7 = 19\n",
    "C[0,1] = A[0,0]*B[0,1] + A[0,1]*B[1,1] = 1*6 + 2*8 = 22\n",
    "C[1,0] = A[1,0]*B[0,0] + A[1,1]*B[1,0] = 3*5 + 4*7 = 43\n",
    "C[1,1] = A[1,0]*B[0,1] + A[1,1]*B[1,1] = 3*6 + 4*8 = 50\n",
    "```\n",
    "\n",
    "#### **Memory Access Pattern**\n",
    "- **Row-major order**: Access elements row by row for cache efficiency\n",
    "- **Cache locality**: Nearby elements are likely to be accessed together\n",
    "- **Blocking**: Divide large matrices into blocks for better cache usage\n",
    "\n",
    "### Performance Considerations: Making It Fast\n",
    "\n",
    "#### **Optimization Strategies**\n",
    "1. **Vectorization**: Use SIMD instructions for parallel element operations\n",
    "2. **Blocking**: Divide matrices into cache-friendly blocks\n",
    "3. **Loop unrolling**: Reduce loop overhead\n",
    "4. **Memory alignment**: Ensure data is aligned for optimal access\n",
    "\n",
    "#### **Modern Libraries**\n",
    "- **BLAS (Basic Linear Algebra Subprograms)**: Optimized matrix operations\n",
    "- **Intel MKL**: Highly optimized for Intel processors\n",
    "- **OpenBLAS**: Open-source optimized BLAS\n",
    "- **cuBLAS**: GPU-accelerated BLAS from NVIDIA\n",
    "\n",
    "#### **Why We Implement Naive Version**\n",
    "Understanding the basic algorithm helps you:\n",
    "- **Debug performance issues**: Know what's happening under the hood\n",
    "- **Optimize for specific cases**: Custom implementations for special matrices\n",
    "- **Understand complexity**: Appreciate the optimizations in modern libraries\n",
    "- **Educational value**: See the mathematical foundation clearly\n",
    "\n",
    "### Connection to Neural Network Architecture\n",
    "\n",
    "#### **Layer Composition**\n",
    "```python\n",
    "# Each layer is a matrix multiplication:\n",
    "layer1_output = W1 @ input + b1\n",
    "layer2_output = W2 @ layer1_output + b2\n",
    "layer3_output = W3 @ layer2_output + b3\n",
    "\n",
    "# This is equivalent to:\n",
    "final_output = W3 @ (W2 @ (W1 @ input + b1) + b2) + b3\n",
    "```\n",
    "\n",
    "#### **Gradient Flow**\n",
    "During backpropagation, gradients flow through matrix operations:\n",
    "```python\n",
    "# Forward: y = W @ x + b\n",
    "# Backward: \n",
    "# dW = dy @ x.T\n",
    "# dx = W.T @ dy\n",
    "# db = dy.sum(axis=0)\n",
    "```\n",
    "\n",
    "#### **Weight Initialization**\n",
    "Matrix multiplication behavior depends on weight initialization:\n",
    "- **Xavier/Glorot**: Maintains variance across layers\n",
    "- **He initialization**: Optimized for ReLU activations\n",
    "- **Orthogonal**: Preserves gradient norms\n",
    "\n",
    "Let's implement matrix multiplication to truly understand it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feec2a3d",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "matmul-naive",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def matmul_naive(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Naive matrix multiplication using explicit for-loops.\n",
    "    \n",
    "    This helps you understand what matrix multiplication really does!\n",
    "    \n",
    "    Args:\n",
    "        A: Matrix of shape (m, n)\n",
    "        B: Matrix of shape (n, p)\n",
    "        \n",
    "    Returns:\n",
    "        Matrix of shape (m, p) where C[i,j] = sum(A[i,k] * B[k,j] for k in range(n))\n",
    "        \n",
    "    TODO: Implement matrix multiplication using three nested for-loops.\n",
    "    \n",
    "    APPROACH:\n",
    "    1. Get the dimensions: m, n from A and n2, p from B\n",
    "    2. Check that n == n2 (matrices must be compatible)\n",
    "    3. Create output matrix C of shape (m, p) filled with zeros\n",
    "    4. Use three nested loops:\n",
    "       - i loop: rows of A (0 to m-1)\n",
    "       - j loop: columns of B (0 to p-1) \n",
    "       - k loop: shared dimension (0 to n-1)\n",
    "    5. For each (i,j), compute: C[i,j] += A[i,k] * B[k,j]\n",
    "    \n",
    "    EXAMPLE:\n",
    "    A = [[1, 2],     B = [[5, 6],\n",
    "         [3, 4]]          [7, 8]]\n",
    "    \n",
    "    C[0,0] = A[0,0]*B[0,0] + A[0,1]*B[1,0] = 1*5 + 2*7 = 19\n",
    "    C[0,1] = A[0,0]*B[0,1] + A[0,1]*B[1,1] = 1*6 + 2*8 = 22\n",
    "    C[1,0] = A[1,0]*B[0,0] + A[1,1]*B[1,0] = 3*5 + 4*7 = 43\n",
    "    C[1,1] = A[1,0]*B[0,1] + A[1,1]*B[1,1] = 3*6 + 4*8 = 50\n",
    "    \n",
    "    HINTS:\n",
    "    - Start with C = np.zeros((m, p))\n",
    "    - Use three nested for loops: for i in range(m): for j in range(p): for k in range(n):\n",
    "    - Accumulate the sum: C[i,j] += A[i,k] * B[k,j]\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    # Get matrix dimensions\n",
    "    m, n = A.shape\n",
    "    n2, p = B.shape\n",
    "    \n",
    "    # Check compatibility\n",
    "    if n != n2:\n",
    "        raise ValueError(f\"Incompatible matrix dimensions: A is {m}x{n}, B is {n2}x{p}\")\n",
    "    \n",
    "    # Initialize result matrix\n",
    "    C = np.zeros((m, p))\n",
    "    \n",
    "    # Triple nested loop for matrix multiplication\n",
    "    for i in range(m):\n",
    "        for j in range(p):\n",
    "            for k in range(n):\n",
    "                C[i, j] += A[i, k] * B[k, j]\n",
    "    \n",
    "    return C\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00da4888",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### 🧪 Unit Test: Matrix Multiplication\n",
    "\n",
    "Let's test your matrix multiplication implementation right away! This is the foundation of neural networks.\n",
    "\n",
    "**This is a unit test** - it tests one specific function (matmul_naive) in isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013b5c7d",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-matmul-immediate",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test matrix multiplication immediately after implementation\n",
    "print(\"🔬 Unit Test: Matrix Multiplication...\")\n",
    "\n",
    "# Test simple 2x2 case\n",
    "try:\n",
    "    A = np.array([[1, 2], [3, 4]], dtype=np.float32)\n",
    "    B = np.array([[5, 6], [7, 8]], dtype=np.float32)\n",
    "    \n",
    "    result = matmul_naive(A, B)\n",
    "    expected = np.array([[19, 22], [43, 50]], dtype=np.float32)\n",
    "    \n",
    "    assert np.allclose(result, expected), f\"Matrix multiplication failed: expected {expected}, got {result}\"\n",
    "    print(f\"✅ Simple 2x2 test: {A.tolist()} @ {B.tolist()} = {result.tolist()}\")\n",
    "    \n",
    "    # Compare with NumPy\n",
    "    numpy_result = A @ B\n",
    "    assert np.allclose(result, numpy_result), f\"Doesn't match NumPy: got {result}, expected {numpy_result}\"\n",
    "    print(\"✅ Matches NumPy's result\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Matrix multiplication test failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# Test different shapes\n",
    "try:\n",
    "    A2 = np.array([[1, 2, 3]], dtype=np.float32)  # 1x3\n",
    "    B2 = np.array([[4], [5], [6]], dtype=np.float32)  # 3x1\n",
    "    result2 = matmul_naive(A2, B2)\n",
    "    expected2 = np.array([[32]], dtype=np.float32)  # 1*4 + 2*5 + 3*6 = 32\n",
    "    \n",
    "    assert np.allclose(result2, expected2), f\"Different shapes failed: got {result2}, expected {expected2}\"\n",
    "    print(f\"✅ Different shapes test: {A2.tolist()} @ {B2.tolist()} = {result2.tolist()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Different shapes test failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# Show the algorithm in action\n",
    "print(\"🎯 Matrix multiplication algorithm:\")\n",
    "print(\"   C[i,j] = Σ(A[i,k] * B[k,j]) for all k\")\n",
    "print(\"   Triple nested loops compute each element\")\n",
    "print(\"📈 Progress: Matrix multiplication ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcce151",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 2: Building the Dense Layer\n",
    "\n",
    "Now let's build the **Dense layer**, the most fundamental building block of neural networks. A Dense layer performs a linear transformation: `y = Wx + b`\n",
    "\n",
    "### What is a Dense Layer?\n",
    "- **Linear transformation**: `y = Wx + b`\n",
    "- **W**: Weight matrix (learnable parameters)\n",
    "- **x**: Input tensor\n",
    "- **b**: Bias vector (learnable parameters)\n",
    "- **y**: Output tensor\n",
    "\n",
    "### Why Dense Layers Matter\n",
    "- **Universal approximation**: Can approximate any function with enough neurons\n",
    "- **Feature learning**: Each neuron learns a different feature\n",
    "- **Nonlinearity**: When combined with activation functions, becomes very powerful\n",
    "- **Foundation**: All other layers build on this concept\n",
    "\n",
    "### The Math\n",
    "For input x of shape (batch_size, input_size):\n",
    "- **W**: Weight matrix of shape (input_size, output_size)\n",
    "- **b**: Bias vector of shape (output_size)\n",
    "- **y**: Output of shape (batch_size, output_size)\n",
    "\n",
    "### Visual Example\n",
    "```\n",
    "Input: x = [1, 2, 3] (3 features)\n",
    "Weights: W = [[0.1, 0.2],    Bias: b = [0.1, 0.2]\n",
    "              [0.3, 0.4],\n",
    "              [0.5, 0.6]]\n",
    "\n",
    "Step 1: Wx = [0.1*1 + 0.3*2 + 0.5*3,  0.2*1 + 0.4*2 + 0.6*3]\n",
    "            = [2.2, 3.2]\n",
    "\n",
    "Step 2: y = Wx + b = [2.2 + 0.1, 3.2 + 0.2] = [2.3, 3.4]\n",
    "```\n",
    "\n",
    "Let's implement this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee225e74",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "dense-class",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class Dense:\n",
    "    \"\"\"\n",
    "    Dense (Linear) Layer: y = Wx + b\n",
    "    \n",
    "    The fundamental building block of neural networks.\n",
    "    Performs linear transformation: matrix multiplication + bias addition.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, output_size: int, use_bias: bool = True, \n",
    "                 use_naive_matmul: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize Dense layer with random weights.\n",
    "        \n",
    "        Args:\n",
    "            input_size: Number of input features\n",
    "            output_size: Number of output features\n",
    "            use_bias: Whether to include bias term (default: True)\n",
    "            use_naive_matmul: Whether to use naive matrix multiplication (for learning)\n",
    "            \n",
    "        TODO: Implement Dense layer initialization with proper weight initialization.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Store layer parameters (input_size, output_size, use_bias, use_naive_matmul)\n",
    "        2. Initialize weights with Xavier/Glorot initialization\n",
    "        3. Initialize bias to zeros (if use_bias=True)\n",
    "        4. Convert to float32 for consistency\n",
    "        \n",
    "        EXAMPLE:\n",
    "        Dense(3, 2) creates:\n",
    "        - weights: shape (3, 2) with small random values\n",
    "        - bias: shape (2,) with zeros\n",
    "        \n",
    "        HINTS:\n",
    "        - Use np.random.randn() for random initialization\n",
    "        - Scale weights by sqrt(2/(input_size + output_size)) for Xavier init\n",
    "        - Use np.zeros() for bias initialization\n",
    "        - Convert to float32 with .astype(np.float32)\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        # Store parameters\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.use_bias = use_bias\n",
    "        self.use_naive_matmul = use_naive_matmul\n",
    "        \n",
    "        # Xavier/Glorot initialization\n",
    "        scale = np.sqrt(2.0 / (input_size + output_size))\n",
    "        self.weights = np.random.randn(input_size, output_size).astype(np.float32) * scale\n",
    "        \n",
    "        # Initialize bias\n",
    "        if use_bias:\n",
    "            self.bias = np.zeros(output_size, dtype=np.float32)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass: y = Wx + b\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, input_size)\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, output_size)\n",
    "            \n",
    "        TODO: Implement matrix multiplication and bias addition.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Choose matrix multiplication method based on use_naive_matmul flag\n",
    "        2. Perform matrix multiplication: Wx\n",
    "        3. Add bias if use_bias=True\n",
    "        4. Return result wrapped in Tensor\n",
    "        \n",
    "        EXAMPLE:\n",
    "        Input x: Tensor([[1, 2, 3]])  # shape (1, 3)\n",
    "        Weights: shape (3, 2)\n",
    "        Output: Tensor([[val1, val2]])  # shape (1, 2)\n",
    "        \n",
    "        HINTS:\n",
    "        - Use self.use_naive_matmul to choose between matmul_naive and @\n",
    "        - x.data gives you the numpy array\n",
    "        - Use broadcasting for bias addition: result + self.bias\n",
    "        - Return Tensor(result) to wrap the result\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        # Matrix multiplication\n",
    "        if self.use_naive_matmul:\n",
    "            result = matmul_naive(x.data, self.weights)\n",
    "        else:\n",
    "            result = x.data @ self.weights\n",
    "        \n",
    "        # Add bias\n",
    "        if self.use_bias:\n",
    "            result += self.bias\n",
    "        \n",
    "        return Tensor(result)\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Make layer callable: layer(x) same as layer.forward(x)\"\"\"\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef64633",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### 🧪 Unit Test: Dense Layer\n",
    "\n",
    "Let's test your Dense layer implementation! This is the fundamental building block of neural networks.\n",
    "\n",
    "**This is a unit test** - it tests one specific class (Dense layer) in isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aff7744",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-dense-immediate",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Dense layer immediately after implementation\n",
    "print(\"🔬 Unit Test: Dense Layer...\")\n",
    "\n",
    "# Test basic Dense layer\n",
    "try:\n",
    "    layer = Dense(input_size=3, output_size=2, use_bias=True)\n",
    "    x = Tensor([[1, 2, 3]])  # batch_size=1, input_size=3\n",
    "    \n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Layer weights shape: {layer.weights.shape}\")\n",
    "    if layer.bias is not None:\n",
    "        print(f\"Layer bias shape: {layer.bias.shape}\")\n",
    "    \n",
    "    y = layer(x)\n",
    "    print(f\"Output shape: {y.shape}\")\n",
    "    print(f\"Output: {y}\")\n",
    "    \n",
    "    # Test shape compatibility\n",
    "    assert y.shape == (1, 2), f\"Output shape should be (1, 2), got {y.shape}\"\n",
    "    print(\"✅ Dense layer produces correct output shape\")\n",
    "    \n",
    "    # Test weights initialization\n",
    "    assert layer.weights.shape == (3, 2), f\"Weights shape should be (3, 2), got {layer.weights.shape}\"\n",
    "    if layer.bias is not None:\n",
    "        assert layer.bias.shape == (2,), f\"Bias shape should be (2,), got {layer.bias.shape}\"\n",
    "    print(\"✅ Dense layer has correct weight and bias shapes\")\n",
    "    \n",
    "    # Test that weights are not all zeros (proper initialization)\n",
    "    assert not np.allclose(layer.weights, 0), \"Weights should not be all zeros\"\n",
    "    if layer.bias is not None:\n",
    "        assert np.allclose(layer.bias, 0), \"Bias should be initialized to zeros\"\n",
    "    print(\"✅ Dense layer has proper weight initialization\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Dense layer test failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# Test without bias\n",
    "try:\n",
    "    layer_no_bias = Dense(input_size=2, output_size=1, use_bias=False)\n",
    "    x2 = Tensor([[1, 2]])\n",
    "    y2 = layer_no_bias(x2)\n",
    "    \n",
    "    assert y2.shape == (1, 1), f\"No bias output shape should be (1, 1), got {y2.shape}\"\n",
    "    assert layer_no_bias.bias is None, \"Bias should be None when use_bias=False\"\n",
    "    print(\"✅ Dense layer works without bias\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Dense layer no-bias test failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# Test naive matrix multiplication\n",
    "try:\n",
    "    layer_naive = Dense(input_size=2, output_size=2, use_naive_matmul=True)\n",
    "    x3 = Tensor([[1, 2]])\n",
    "    y3 = layer_naive(x3)\n",
    "    \n",
    "    assert y3.shape == (1, 2), f\"Naive matmul output shape should be (1, 2), got {y3.shape}\"\n",
    "    print(\"✅ Dense layer works with naive matrix multiplication\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Dense layer naive matmul test failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# Show the linear transformation in action\n",
    "print(\"🎯 Dense layer behavior:\")\n",
    "print(\"   y = Wx + b (linear transformation)\")\n",
    "print(\"   W: learnable weight matrix\")\n",
    "print(\"   b: learnable bias vector\")\n",
    "print(\"📈 Progress: Matrix multiplication ✓, Dense layer ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8bbac9",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### 🧪 Test Your Implementations\n",
    "\n",
    "Once you implement the functions above, run these cells to test them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aa2c39",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-matmul-naive",
     "locked": true,
     "points": 25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test matrix multiplication\n",
    "print(\"Testing matrix multiplication...\")\n",
    "\n",
    "# Test case 1: Simple 2x2 matrices\n",
    "A = np.array([[1, 2], [3, 4]], dtype=np.float32)\n",
    "B = np.array([[5, 6], [7, 8]], dtype=np.float32)\n",
    "\n",
    "result = matmul_naive(A, B)\n",
    "expected = np.array([[19, 22], [43, 50]], dtype=np.float32)\n",
    "\n",
    "print(f\"Matrix A:\\n{A}\")\n",
    "print(f\"Matrix B:\\n{B}\")\n",
    "print(f\"Your result:\\n{result}\")\n",
    "print(f\"Expected:\\n{expected}\")\n",
    "\n",
    "assert np.allclose(result, expected), f\"Result doesn't match expected: got {result}, expected {expected}\"\n",
    "\n",
    "# Test case 2: Compare with NumPy\n",
    "numpy_result = A @ B\n",
    "assert np.allclose(result, numpy_result), f\"Doesn't match NumPy result: got {result}, expected {numpy_result}\"\n",
    "\n",
    "# Test case 3: Different shapes\n",
    "A2 = np.array([[1, 2, 3]], dtype=np.float32)  # 1x3\n",
    "B2 = np.array([[4], [5], [6]], dtype=np.float32)  # 3x1\n",
    "result2 = matmul_naive(A2, B2)\n",
    "expected2 = np.array([[32]], dtype=np.float32)  # 1*4 + 2*5 + 3*6 = 32\n",
    "assert np.allclose(result2, expected2), f\"Different shapes failed: got {result2}, expected {expected2}\"\n",
    "\n",
    "print(\"✅ Matrix multiplication tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acf76ab",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-dense-layer",
     "locked": true,
     "points": 25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Dense layer\n",
    "print(\"Testing Dense layer...\")\n",
    "\n",
    "# Test basic Dense layer\n",
    "layer = Dense(input_size=3, output_size=2, use_bias=True)\n",
    "x = Tensor([[1, 2, 3]])  # batch_size=1, input_size=3\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Layer weights shape: {layer.weights.shape}\")\n",
    "if layer.bias is not None:\n",
    "    print(f\"Layer bias shape: {layer.bias.shape}\")\n",
    "else:\n",
    "    print(\"Layer bias: None\")\n",
    "\n",
    "y = layer(x)\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print(f\"Output: {y}\")\n",
    "\n",
    "# Test shape compatibility\n",
    "assert y.shape == (1, 2), f\"Output shape should be (1, 2), got {y.shape}\"\n",
    "\n",
    "# Test without bias\n",
    "layer_no_bias = Dense(input_size=2, output_size=1, use_bias=False)\n",
    "x2 = Tensor([[1, 2]])\n",
    "y2 = layer_no_bias(x2)\n",
    "assert y2.shape == (1, 1), f\"No bias output shape should be (1, 1), got {y2.shape}\"\n",
    "assert layer_no_bias.bias is None, \"Bias should be None when use_bias=False\"\n",
    "\n",
    "# Test naive matrix multiplication\n",
    "layer_naive = Dense(input_size=2, output_size=2, use_naive_matmul=True)\n",
    "x3 = Tensor([[1, 2]])\n",
    "y3 = layer_naive(x3)\n",
    "assert y3.shape == (1, 2), f\"Naive matmul output shape should be (1, 2), got {y3.shape}\"\n",
    "\n",
    "print(\"✅ Dense layer tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6796a9",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-layer-composition",
     "locked": true,
     "points": 25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test layer composition\n",
    "print(\"Testing layer composition...\")\n",
    "\n",
    "# Create a simple network: Dense → ReLU → Dense\n",
    "dense1 = Dense(input_size=3, output_size=2)\n",
    "relu = ReLU()\n",
    "dense2 = Dense(input_size=2, output_size=1)\n",
    "\n",
    "# Test input\n",
    "x = Tensor([[1, 2, 3]])\n",
    "print(f\"Input: {x}\")\n",
    "\n",
    "# Forward pass through the network\n",
    "h1 = dense1(x)\n",
    "print(f\"After Dense1: {h1}\")\n",
    "\n",
    "h2 = relu(h1)\n",
    "print(f\"After ReLU: {h2}\")\n",
    "\n",
    "h3 = dense2(h2)\n",
    "print(f\"After Dense2: {h3}\")\n",
    "\n",
    "# Test shapes\n",
    "assert h1.shape == (1, 2), f\"Dense1 output should be (1, 2), got {h1.shape}\"\n",
    "assert h2.shape == (1, 2), f\"ReLU output should be (1, 2), got {h2.shape}\"\n",
    "assert h3.shape == (1, 1), f\"Dense2 output should be (1, 1), got {h3.shape}\"\n",
    "\n",
    "# Test that ReLU actually applied (non-negative values)\n",
    "assert np.all(h2.data >= 0), \"ReLU should produce non-negative values\"\n",
    "\n",
    "print(\"✅ Layer composition tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e19bd59",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## 🧪 Comprehensive Testing: Matrix Multiplication and Dense Layers\n",
    "\n",
    "Let's thoroughly test your implementations to make sure they work correctly in all scenarios.\n",
    "This comprehensive testing ensures your layers are robust and ready for real neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46effbb",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-layers-comprehensive",
     "locked": true,
     "points": 30,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_layers_comprehensive():\n",
    "    \"\"\"Comprehensive test of matrix multiplication and Dense layers.\"\"\"\n",
    "    print(\"🔬 Testing matrix multiplication and Dense layers comprehensively...\")\n",
    "    \n",
    "    tests_passed = 0\n",
    "    total_tests = 10\n",
    "    \n",
    "    # Test 1: Matrix Multiplication Basic Cases\n",
    "    try:\n",
    "        # Test 2x2 matrices\n",
    "        A = np.array([[1, 2], [3, 4]], dtype=np.float32)\n",
    "        B = np.array([[5, 6], [7, 8]], dtype=np.float32)\n",
    "        result = matmul_naive(A, B)\n",
    "        expected = np.array([[19, 22], [43, 50]], dtype=np.float32)\n",
    "        \n",
    "        assert np.allclose(result, expected), f\"2x2 multiplication failed: expected {expected}, got {result}\"\n",
    "        \n",
    "        # Compare with NumPy\n",
    "        numpy_result = A @ B\n",
    "        assert np.allclose(result, numpy_result), f\"Doesn't match NumPy: expected {numpy_result}, got {result}\"\n",
    "        \n",
    "        print(f\"✅ Matrix multiplication 2x2: {A.shape} × {B.shape} = {result.shape}\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Matrix multiplication basic failed: {e}\")\n",
    "    \n",
    "    # Test 2: Matrix Multiplication Different Shapes\n",
    "    try:\n",
    "        # Test 1x3 × 3x1 = 1x1\n",
    "        A1 = np.array([[1, 2, 3]], dtype=np.float32)\n",
    "        B1 = np.array([[4], [5], [6]], dtype=np.float32)\n",
    "        result1 = matmul_naive(A1, B1)\n",
    "        expected1 = np.array([[32]], dtype=np.float32)  # 1*4 + 2*5 + 3*6 = 32\n",
    "        assert np.allclose(result1, expected1), f\"1x3 × 3x1 failed: expected {expected1}, got {result1}\"\n",
    "        \n",
    "        # Test 3x2 × 2x4 = 3x4\n",
    "        A2 = np.array([[1, 2], [3, 4], [5, 6]], dtype=np.float32)\n",
    "        B2 = np.array([[1, 2, 3, 4], [5, 6, 7, 8]], dtype=np.float32)\n",
    "        result2 = matmul_naive(A2, B2)\n",
    "        expected2 = A2 @ B2\n",
    "        assert np.allclose(result2, expected2), f\"3x2 × 2x4 failed: expected {expected2}, got {result2}\"\n",
    "        \n",
    "        print(f\"✅ Matrix multiplication shapes: (1,3)×(3,1), (3,2)×(2,4)\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Matrix multiplication shapes failed: {e}\")\n",
    "    \n",
    "    # Test 3: Matrix Multiplication Edge Cases\n",
    "    try:\n",
    "        # Test with zeros\n",
    "        A_zero = np.zeros((2, 3), dtype=np.float32)\n",
    "        B_zero = np.zeros((3, 2), dtype=np.float32)\n",
    "        result_zero = matmul_naive(A_zero, B_zero)\n",
    "        expected_zero = np.zeros((2, 2), dtype=np.float32)\n",
    "        assert np.allclose(result_zero, expected_zero), \"Zero matrix multiplication failed\"\n",
    "        \n",
    "        # Test with identity\n",
    "        A_id = np.array([[1, 2]], dtype=np.float32)\n",
    "        B_id = np.array([[1, 0], [0, 1]], dtype=np.float32)\n",
    "        result_id = matmul_naive(A_id, B_id)\n",
    "        expected_id = np.array([[1, 2]], dtype=np.float32)\n",
    "        assert np.allclose(result_id, expected_id), \"Identity matrix multiplication failed\"\n",
    "        \n",
    "        # Test with negative values\n",
    "        A_neg = np.array([[-1, 2]], dtype=np.float32)\n",
    "        B_neg = np.array([[3], [-4]], dtype=np.float32)\n",
    "        result_neg = matmul_naive(A_neg, B_neg)\n",
    "        expected_neg = np.array([[-11]], dtype=np.float32)  # -1*3 + 2*(-4) = -11\n",
    "        assert np.allclose(result_neg, expected_neg), \"Negative matrix multiplication failed\"\n",
    "        \n",
    "        print(\"✅ Matrix multiplication edge cases: zeros, identity, negatives\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Matrix multiplication edge cases failed: {e}\")\n",
    "    \n",
    "    # Test 4: Dense Layer Initialization\n",
    "    try:\n",
    "        # Test with bias\n",
    "        layer_bias = Dense(input_size=3, output_size=2, use_bias=True)\n",
    "        assert layer_bias.weights.shape == (3, 2), f\"Weights shape should be (3, 2), got {layer_bias.weights.shape}\"\n",
    "        assert layer_bias.bias is not None, \"Bias should not be None when use_bias=True\"\n",
    "        assert layer_bias.bias.shape == (2,), f\"Bias shape should be (2,), got {layer_bias.bias.shape}\"\n",
    "        \n",
    "        # Check weight initialization (should not be all zeros)\n",
    "        assert not np.allclose(layer_bias.weights, 0), \"Weights should not be all zeros\"\n",
    "        assert np.allclose(layer_bias.bias, 0), \"Bias should be initialized to zeros\"\n",
    "        \n",
    "        # Test without bias\n",
    "        layer_no_bias = Dense(input_size=4, output_size=3, use_bias=False)\n",
    "        assert layer_no_bias.weights.shape == (4, 3), f\"No-bias weights shape should be (4, 3), got {layer_no_bias.weights.shape}\"\n",
    "        assert layer_no_bias.bias is None, \"Bias should be None when use_bias=False\"\n",
    "        \n",
    "        print(\"✅ Dense layer initialization: weights, bias, shapes\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Dense layer initialization failed: {e}\")\n",
    "    \n",
    "    # Test 5: Dense Layer Forward Pass\n",
    "    try:\n",
    "        layer = Dense(input_size=3, output_size=2, use_bias=True)\n",
    "        \n",
    "        # Test single sample\n",
    "        x_single = Tensor([[1, 2, 3]])  # shape: (1, 3)\n",
    "        y_single = layer(x_single)\n",
    "        assert y_single.shape == (1, 2), f\"Single sample output should be (1, 2), got {y_single.shape}\"\n",
    "        \n",
    "        # Test batch of samples\n",
    "        x_batch = Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])  # shape: (3, 3)\n",
    "        y_batch = layer(x_batch)\n",
    "        assert y_batch.shape == (3, 2), f\"Batch output should be (3, 2), got {y_batch.shape}\"\n",
    "        \n",
    "        # Verify computation manually for single sample\n",
    "        expected_single = np.dot(x_single.data, layer.weights) + layer.bias\n",
    "        assert np.allclose(y_single.data, expected_single), \"Single sample computation incorrect\"\n",
    "        \n",
    "        print(\"✅ Dense layer forward pass: single sample, batch processing\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Dense layer forward pass failed: {e}\")\n",
    "    \n",
    "    # Test 6: Dense Layer Without Bias\n",
    "    try:\n",
    "        layer_no_bias = Dense(input_size=2, output_size=3, use_bias=False)\n",
    "        x = Tensor([[1, 2]])\n",
    "        y = layer_no_bias(x)\n",
    "        \n",
    "        assert y.shape == (1, 3), f\"No-bias output should be (1, 3), got {y.shape}\"\n",
    "        \n",
    "        # Verify computation (should be just matrix multiplication)\n",
    "        expected = np.dot(x.data, layer_no_bias.weights)\n",
    "        assert np.allclose(y.data, expected), \"No-bias computation incorrect\"\n",
    "        \n",
    "        print(\"✅ Dense layer without bias: correct computation\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Dense layer without bias failed: {e}\")\n",
    "    \n",
    "    # Test 7: Dense Layer with Naive Matrix Multiplication\n",
    "    try:\n",
    "        layer_naive = Dense(input_size=2, output_size=2, use_naive_matmul=True)\n",
    "        layer_optimized = Dense(input_size=2, output_size=2, use_naive_matmul=False)\n",
    "        \n",
    "        # Set same weights for comparison\n",
    "        layer_optimized.weights = layer_naive.weights.copy()\n",
    "        layer_optimized.bias = layer_naive.bias.copy() if layer_naive.bias is not None else None\n",
    "        \n",
    "        x = Tensor([[1, 2]])\n",
    "        y_naive = layer_naive(x)\n",
    "        y_optimized = layer_optimized(x)\n",
    "        \n",
    "        # Both should give same results\n",
    "        assert np.allclose(y_naive.data, y_optimized.data), \"Naive and optimized should give same results\"\n",
    "        \n",
    "        print(\"✅ Dense layer naive vs optimized: consistent results\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Dense layer naive matmul failed: {e}\")\n",
    "    \n",
    "    # Test 8: Layer Composition\n",
    "    try:\n",
    "        # Create a simple network: Dense → ReLU → Dense\n",
    "        dense1 = Dense(input_size=3, output_size=4)\n",
    "        relu = ReLU()\n",
    "        dense2 = Dense(input_size=4, output_size=2)\n",
    "        \n",
    "        x = Tensor([[1, -2, 3]])\n",
    "        \n",
    "        # Forward pass\n",
    "        h1 = dense1(x)\n",
    "        h2 = relu(h1)\n",
    "        h3 = dense2(h2)\n",
    "        \n",
    "        # Check shapes\n",
    "        assert h1.shape == (1, 4), f\"Dense1 output should be (1, 4), got {h1.shape}\"\n",
    "        assert h2.shape == (1, 4), f\"ReLU output should be (1, 4), got {h2.shape}\"\n",
    "        assert h3.shape == (1, 2), f\"Dense2 output should be (1, 2), got {h3.shape}\"\n",
    "        \n",
    "        # Check ReLU effect\n",
    "        assert np.all(h2.data >= 0), \"ReLU should produce non-negative values\"\n",
    "        \n",
    "        print(\"✅ Layer composition: Dense → ReLU → Dense pipeline\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Layer composition failed: {e}\")\n",
    "    \n",
    "    # Test 9: Different Layer Sizes\n",
    "    try:\n",
    "        # Test various layer sizes\n",
    "        test_configs = [\n",
    "            (1, 1),    # Minimal\n",
    "            (10, 5),   # Medium\n",
    "            (100, 50), # Large\n",
    "            (784, 128) # MNIST-like\n",
    "        ]\n",
    "        \n",
    "        for input_size, output_size in test_configs:\n",
    "            layer = Dense(input_size=input_size, output_size=output_size)\n",
    "            \n",
    "            # Test with single sample\n",
    "            x = Tensor(np.random.randn(1, input_size))\n",
    "            y = layer(x)\n",
    "            \n",
    "            assert y.shape == (1, output_size), f\"Size ({input_size}, {output_size}) failed: got {y.shape}\"\n",
    "            assert layer.weights.shape == (input_size, output_size), f\"Weights shape wrong for ({input_size}, {output_size})\"\n",
    "        \n",
    "        print(\"✅ Different layer sizes: (1,1), (10,5), (100,50), (784,128)\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Different layer sizes failed: {e}\")\n",
    "    \n",
    "    # Test 10: Real Neural Network Scenario\n",
    "    try:\n",
    "        # Simulate MNIST-like scenario: 784 → 128 → 64 → 10\n",
    "        input_layer = Dense(input_size=784, output_size=128)\n",
    "        hidden_layer = Dense(input_size=128, output_size=64)\n",
    "        output_layer = Dense(input_size=64, output_size=10)\n",
    "        \n",
    "        relu1 = ReLU()\n",
    "        relu2 = ReLU()\n",
    "        softmax = Softmax()\n",
    "        \n",
    "        # Simulate flattened MNIST image\n",
    "        x = Tensor(np.random.randn(32, 784))  # Batch of 32 images\n",
    "        \n",
    "        # Forward pass through network\n",
    "        h1 = input_layer(x)\n",
    "        h1_activated = relu1(h1)\n",
    "        h2 = hidden_layer(h1_activated)\n",
    "        h2_activated = relu2(h2)\n",
    "        logits = output_layer(h2_activated)\n",
    "        probabilities = softmax(logits)\n",
    "        \n",
    "        # Check final output\n",
    "        assert probabilities.shape == (32, 10), f\"Final output should be (32, 10), got {probabilities.shape}\"\n",
    "        \n",
    "        # Check that probabilities sum to 1 for each sample\n",
    "        row_sums = np.sum(probabilities.data, axis=1)\n",
    "        assert np.allclose(row_sums, 1.0), \"Each sample should have probabilities summing to 1\"\n",
    "        \n",
    "        # Check that all intermediate shapes are correct\n",
    "        assert h1.shape == (32, 128), f\"Hidden 1 shape should be (32, 128), got {h1.shape}\"\n",
    "        assert h2.shape == (32, 64), f\"Hidden 2 shape should be (32, 64), got {h2.shape}\"\n",
    "        assert logits.shape == (32, 10), f\"Logits shape should be (32, 10), got {logits.shape}\"\n",
    "        \n",
    "        print(\"✅ Real neural network scenario: MNIST-like 784→128→64→10 classification\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Real neural network scenario failed: {e}\")\n",
    "    \n",
    "    # Results summary\n",
    "    print(f\"\\n📊 Layers Module Results: {tests_passed}/{total_tests} tests passed\")\n",
    "    \n",
    "    if tests_passed == total_tests:\n",
    "        print(\"🎉 All layers tests passed! Your implementations support:\")\n",
    "        print(\"  • Matrix multiplication: naive implementation from scratch\")\n",
    "        print(\"  • Dense layers: linear transformations with learnable parameters\")\n",
    "        print(\"  • Weight initialization: proper random initialization\")\n",
    "        print(\"  • Bias handling: optional bias terms\")\n",
    "        print(\"  • Batch processing: multiple samples at once\")\n",
    "        print(\"  • Layer composition: building complete neural networks\")\n",
    "        print(\"  • Real ML scenarios: MNIST-like classification networks\")\n",
    "        print(\"📈 Progress: All Layer Functionality ✓\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"⚠️  Some layers tests failed. Common issues:\")\n",
    "        print(\"  • Check matrix multiplication implementation (triple nested loops)\")\n",
    "        print(\"  • Verify Dense layer forward pass (y = Wx + b)\")\n",
    "        print(\"  • Ensure proper weight initialization (not all zeros)\")\n",
    "        print(\"  • Check shape handling for different input/output sizes\")\n",
    "        print(\"  • Verify bias handling when use_bias=False\")\n",
    "        return False\n",
    "\n",
    "# Run the comprehensive test\n",
    "success = test_layers_comprehensive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2273e7ad",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### 🧪 Integration Test: Layers in Complete Neural Networks\n",
    "\n",
    "Let's test how your layers work in realistic neural network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e734364",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-layers-integration",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_layers_integration():\n",
    "    \"\"\"Integration test with complete neural network architectures.\"\"\"\n",
    "    print(\"🔬 Testing layers in complete neural network architectures...\")\n",
    "    \n",
    "    try:\n",
    "        print(\"🧠 Building and testing different network architectures...\")\n",
    "        \n",
    "        # Architecture 1: Simple Binary Classifier\n",
    "        print(\"\\n📊 Architecture 1: Binary Classification Network\")\n",
    "        binary_net = [\n",
    "            Dense(input_size=4, output_size=8),\n",
    "            ReLU(),\n",
    "            Dense(input_size=8, output_size=4),\n",
    "            ReLU(),\n",
    "            Dense(input_size=4, output_size=1),\n",
    "            Sigmoid()\n",
    "        ]\n",
    "        \n",
    "        # Test with batch of samples\n",
    "        x_binary = Tensor(np.random.randn(10, 4))  # 10 samples, 4 features\n",
    "        \n",
    "        # Forward pass through network\n",
    "        current = x_binary\n",
    "        for i, layer in enumerate(binary_net):\n",
    "            current = layer(current)\n",
    "            print(f\"  Layer {i}: {current.shape}\")\n",
    "        \n",
    "        # Verify final output is valid probabilities\n",
    "        assert current.shape == (10, 1), f\"Binary classifier output should be (10, 1), got {current.shape}\"\n",
    "        assert np.all((current.data >= 0) & (current.data <= 1)), \"Binary probabilities should be in [0,1]\"\n",
    "        \n",
    "        print(\"✅ Binary classification network: 4→8→4→1 with ReLU/Sigmoid\")\n",
    "        \n",
    "        # Architecture 2: Multi-class Classifier\n",
    "        print(\"\\n📊 Architecture 2: Multi-class Classification Network\")\n",
    "        multiclass_net = [\n",
    "            Dense(input_size=784, output_size=256),\n",
    "            ReLU(),\n",
    "            Dense(input_size=256, output_size=128),\n",
    "            ReLU(),\n",
    "            Dense(input_size=128, output_size=10),\n",
    "            Softmax()\n",
    "        ]\n",
    "        \n",
    "        # Simulate MNIST-like input\n",
    "        x_mnist = Tensor(np.random.randn(5, 784))  # 5 images, 784 pixels\n",
    "        \n",
    "        current = x_mnist\n",
    "        for i, layer in enumerate(multiclass_net):\n",
    "            current = layer(current)\n",
    "            print(f\"  Layer {i}: {current.shape}\")\n",
    "        \n",
    "        # Verify final output is valid probability distribution\n",
    "        assert current.shape == (5, 10), f\"Multi-class output should be (5, 10), got {current.shape}\"\n",
    "        row_sums = np.sum(current.data, axis=1)\n",
    "        assert np.allclose(row_sums, 1.0), \"Each sample should have probabilities summing to 1\"\n",
    "        \n",
    "        print(\"✅ Multi-class classification network: 784→256→128→10 with Softmax\")\n",
    "        \n",
    "        # Architecture 3: Deep Network\n",
    "        print(\"\\n📊 Architecture 3: Deep Network (5 layers)\")\n",
    "        deep_net = [\n",
    "            Dense(input_size=100, output_size=80),\n",
    "            ReLU(),\n",
    "            Dense(input_size=80, output_size=60),\n",
    "            ReLU(),\n",
    "            Dense(input_size=60, output_size=40),\n",
    "            ReLU(),\n",
    "            Dense(input_size=40, output_size=20),\n",
    "            ReLU(),\n",
    "            Dense(input_size=20, output_size=3),\n",
    "            Softmax()\n",
    "        ]\n",
    "        \n",
    "        x_deep = Tensor(np.random.randn(8, 100))  # 8 samples, 100 features\n",
    "        \n",
    "        current = x_deep\n",
    "        for i, layer in enumerate(deep_net):\n",
    "            current = layer(current)\n",
    "            if i % 2 == 0:  # Print every other layer to save space\n",
    "                print(f\"  Layer {i}: {current.shape}\")\n",
    "        \n",
    "        assert current.shape == (8, 3), f\"Deep network output should be (8, 3), got {current.shape}\"\n",
    "        \n",
    "        print(\"✅ Deep network: 100→80→60→40→20→3 with multiple ReLU layers\")\n",
    "        \n",
    "        # Test 4: Network with Different Activation Functions\n",
    "        print(\"\\n📊 Architecture 4: Mixed Activation Functions\")\n",
    "        mixed_net = [\n",
    "            Dense(input_size=6, output_size=4),\n",
    "            Tanh(),  # Zero-centered activation\n",
    "            Dense(input_size=4, output_size=3),\n",
    "            ReLU(),  # Sparse activation\n",
    "            Dense(input_size=3, output_size=2),\n",
    "            Sigmoid()  # Bounded activation\n",
    "        ]\n",
    "        \n",
    "        x_mixed = Tensor(np.random.randn(3, 6))\n",
    "        \n",
    "        current = x_mixed\n",
    "        for i, layer in enumerate(mixed_net):\n",
    "            current = layer(current)\n",
    "            print(f\"  Layer {i}: {current.shape}, range: [{np.min(current.data):.3f}, {np.max(current.data):.3f}]\")\n",
    "        \n",
    "        assert current.shape == (3, 2), f\"Mixed network output should be (3, 2), got {current.shape}\"\n",
    "        \n",
    "        print(\"✅ Mixed activations network: Tanh→ReLU→Sigmoid combinations\")\n",
    "        \n",
    "        # Test 5: Parameter Counting\n",
    "        print(\"\\n📊 Parameter Analysis\")\n",
    "        \n",
    "        def count_parameters(layer):\n",
    "            \"\"\"Count trainable parameters in a Dense layer.\"\"\"\n",
    "            if isinstance(layer, Dense):\n",
    "                weight_params = layer.weights.size\n",
    "                bias_params = layer.bias.size if layer.bias is not None else 0\n",
    "                return weight_params + bias_params\n",
    "            return 0\n",
    "        \n",
    "        # Count parameters in binary classifier\n",
    "        total_params = sum(count_parameters(layer) for layer in binary_net)\n",
    "        print(f\"Binary classifier parameters: {total_params}\")\n",
    "        \n",
    "        # Manual verification for first layer: 4*8 + 8 = 40\n",
    "        first_dense = binary_net[0]\n",
    "        expected_first = 4 * 8 + 8  # weights + bias\n",
    "        actual_first = count_parameters(first_dense)\n",
    "        assert actual_first == expected_first, f\"First layer params: expected {expected_first}, got {actual_first}\"\n",
    "        \n",
    "        print(\"✅ Parameter counting: weight and bias parameters calculated correctly\")\n",
    "        \n",
    "        # Test 6: Gradient Flow Preparation\n",
    "        print(\"\\n📊 Gradient Flow Preparation\")\n",
    "        \n",
    "        # Test that network can handle different input types\n",
    "        test_inputs = [\n",
    "            Tensor(np.zeros((1, 4))),      # All zeros\n",
    "            Tensor(np.ones((1, 4))),       # All ones\n",
    "            Tensor(np.random.randn(1, 4)), # Random\n",
    "            Tensor(np.random.randn(1, 4) * 10)  # Large values\n",
    "        ]\n",
    "        \n",
    "        for i, test_input in enumerate(test_inputs):\n",
    "            current = test_input\n",
    "            for layer in binary_net:\n",
    "                current = layer(current)\n",
    "            \n",
    "            # Check for numerical stability\n",
    "            assert not np.any(np.isnan(current.data)), f\"Input {i} produced NaN\"\n",
    "            assert not np.any(np.isinf(current.data)), f\"Input {i} produced Inf\"\n",
    "        \n",
    "        print(\"✅ Numerical stability: networks handle various input ranges\")\n",
    "        \n",
    "        print(\"\\n🎉 Integration test passed! Your layers work correctly in:\")\n",
    "        print(\"  • Binary classification networks\")\n",
    "        print(\"  • Multi-class classification networks\") \n",
    "        print(\"  • Deep networks with multiple hidden layers\")\n",
    "        print(\"  • Networks with mixed activation functions\")\n",
    "        print(\"  • Parameter counting and analysis\")\n",
    "        print(\"  • Numerical stability across input ranges\")\n",
    "        print(\"📈 Progress: Layers ready for complete neural networks!\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Integration test failed: {e}\")\n",
    "        print(\"\\n💡 This suggests an issue with:\")\n",
    "        print(\"  • Layer composition and chaining\")\n",
    "        print(\"  • Shape compatibility between layers\")\n",
    "        print(\"  • Activation function integration\")\n",
    "        print(\"  • Numerical stability in deep networks\")\n",
    "        print(\"  • Check your Dense layer and matrix multiplication\")\n",
    "        return False\n",
    "\n",
    "# Run the integration test\n",
    "success = test_layers_integration() and success\n",
    "\n",
    "# Print final summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"🎯 LAYERS MODULE TESTING COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if success:\n",
    "    print(\"🎉 CONGRATULATIONS! All layers tests passed!\")\n",
    "    print(\"\\n✅ Your layers module successfully implements:\")\n",
    "    print(\"  • Matrix multiplication: naive implementation from scratch\")\n",
    "    print(\"  • Dense layers: y = Wx + b linear transformations\")\n",
    "    print(\"  • Weight initialization: proper random weight setup\")\n",
    "    print(\"  • Bias handling: optional bias terms\")\n",
    "    print(\"  • Batch processing: efficient multi-sample computation\")\n",
    "    print(\"  • Layer composition: building complete neural networks\")\n",
    "    print(\"  • Integration: works with all activation functions\")\n",
    "    print(\"  • Real ML scenarios: MNIST-like classification networks\")\n",
    "    print(\"\\n🚀 You're ready to build complete neural network architectures!\")\n",
    "    print(\"📈 Final Progress: Layers Module ✓ COMPLETE\")\n",
    "else:\n",
    "    print(\"⚠️  Some tests failed. Please review the error messages above.\")\n",
    "    print(\"\\n🔧 To fix issues:\")\n",
    "    print(\"  1. Check your matrix multiplication implementation\")\n",
    "    print(\"  2. Verify Dense layer forward pass computation\")\n",
    "    print(\"  3. Ensure proper weight and bias initialization\")\n",
    "    print(\"  4. Test shape compatibility between layers\")\n",
    "    print(\"  5. Verify integration with activation functions\")\n",
    "    print(\"\\n💪 Keep building! These layers are the foundation of all neural networks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f722f340",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 🎯 Module Summary\n",
    "\n",
    "Congratulations! You've successfully implemented the core building blocks of neural networks:\n",
    "\n",
    "### What You've Accomplished\n",
    "✅ **Matrix Multiplication**: Implemented from scratch with triple nested loops  \n",
    "✅ **Dense Layer**: The fundamental linear transformation y = Wx + b  \n",
    "✅ **Weight Initialization**: Xavier/Glorot initialization for stable training  \n",
    "✅ **Layer Composition**: Combining layers with activations  \n",
    "✅ **Flexible Implementation**: Support for both naive and optimized matrix multiplication  \n",
    "\n",
    "### Key Concepts You've Learned\n",
    "- **Matrix multiplication** is the engine of neural networks\n",
    "- **Dense layers** perform linear transformations that learn features\n",
    "- **Weight initialization** is crucial for stable training\n",
    "- **Layer composition** creates powerful nonlinear functions\n",
    "- **Batch processing** enables efficient computation\n",
    "\n",
    "### Mathematical Foundations\n",
    "- **Linear algebra**: Matrix operations power all neural computations\n",
    "- **Universal approximation**: Dense layers can approximate any function\n",
    "- **Feature learning**: Each neuron learns different patterns\n",
    "- **Composability**: Simple operations combine to create complex behaviors\n",
    "\n",
    "### Next Steps\n",
    "1. **Export your code**: `tito package nbdev --export 03_layers`\n",
    "2. **Test your implementation**: `tito module test 03_layers`\n",
    "3. **Use your layers**: \n",
    "   ```python\n",
    "   from tinytorch.core.layers import Dense\n",
    "   from tinytorch.core.activations import ReLU\n",
    "   layer = Dense(10, 5)\n",
    "   activation = ReLU()\n",
    "   ```\n",
    "4. **Move to Module 4**: Start building complete neural networks!\n",
    "\n",
    "**Ready for the next challenge?** Let's compose these layers into complete neural network architectures!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
