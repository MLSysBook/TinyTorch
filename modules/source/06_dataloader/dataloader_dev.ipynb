{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fadfc3cc",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Module 6: DataLoader - Data Loading and Preprocessing\n",
    "\n",
    "Welcome to the DataLoader module! This is where you'll learn how to efficiently load, process, and manage data for machine learning systems.\n",
    "\n",
    "## Learning Goals\n",
    "- Understand data pipelines as the foundation of ML systems\n",
    "- Implement efficient data loading with memory management and batching\n",
    "- Build reusable dataset abstractions for different data types\n",
    "- Master the Dataset and DataLoader pattern used in all ML frameworks\n",
    "- Learn systems thinking for data engineering and I/O optimization\n",
    "\n",
    "## Build → Use → Understand\n",
    "1. **Build**: Create dataset classes and data loaders from scratch\n",
    "2. **Use**: Load real datasets and feed them to neural networks\n",
    "3. **Understand**: How data engineering affects system performance and scalability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ba1bd7",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "dataloader-imports",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| default_exp core.dataloader\n",
    "\n",
    "#| export\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import struct\n",
    "from typing import List, Tuple, Optional, Union, Iterator\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "# Import our building blocks - try package first, then local modules\n",
    "try:\n",
    "    from tinytorch.core.tensor import Tensor\n",
    "except ImportError:\n",
    "    # For development, import from local modules\n",
    "    sys.path.append(os.path.join(os.path.dirname(__file__), '..', '01_tensor'))\n",
    "    from tensor_dev import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e2e060",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "dataloader-setup",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "def _should_show_plots():\n",
    "    \"\"\"Check if we should show plots (disable during testing)\"\"\"\n",
    "    # Check multiple conditions that indicate we're in test mode\n",
    "    is_pytest = (\n",
    "        'pytest' in sys.modules or\n",
    "        'test' in sys.argv or\n",
    "        os.environ.get('PYTEST_CURRENT_TEST') is not None or\n",
    "        any('test' in arg for arg in sys.argv) or\n",
    "        any('pytest' in arg for arg in sys.argv)\n",
    "    )\n",
    "    \n",
    "    # Show plots in development mode (when not in test mode)\n",
    "    return not is_pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d2cae7",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "dataloader-welcome",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"🔥 TinyTorch DataLoader Module\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Python version: {sys.version_info.major}.{sys.version_info.minor}\")\n",
    "print(\"Ready to build data pipelines!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbbd0f0",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 📦 Where This Code Lives in the Final Package\n",
    "\n",
    "**Learning Side:** You work in `modules/source/06_dataloader/dataloader_dev.py`  \n",
    "**Building Side:** Code exports to `tinytorch.core.dataloader`\n",
    "\n",
    "```python\n",
    "# Final package structure:\n",
    "from tinytorch.core.dataloader import Dataset, DataLoader  # Data loading utilities!\n",
    "from tinytorch.core.tensor import Tensor  # Foundation\n",
    "from tinytorch.core.networks import Sequential  # Models to train\n",
    "```\n",
    "\n",
    "**Why this matters:**\n",
    "- **Learning:** Focused modules for deep understanding of data pipelines\n",
    "- **Production:** Proper organization like PyTorch's `torch.utils.data`\n",
    "- **Consistency:** All data loading utilities live together in `core.dataloader`\n",
    "- **Integration:** Works seamlessly with tensors and networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb33b8dd",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 🧠 The Mathematical Foundation of Data Engineering\n",
    "\n",
    "### The Data Pipeline Equation\n",
    "Every machine learning system follows this fundamental equation:\n",
    "\n",
    "```\n",
    "Model Performance = f(Data Quality × Data Quantity × Data Efficiency)\n",
    "```\n",
    "\n",
    "### Why Data Engineering is Critical\n",
    "- **Data is the fuel**: Without proper data pipelines, nothing else works\n",
    "- **I/O bottlenecks**: Data loading is often the biggest performance bottleneck\n",
    "- **Memory management**: How you handle data affects everything else\n",
    "- **Production reality**: Data pipelines are critical in real ML systems\n",
    "\n",
    "### The Three Pillars of Data Engineering\n",
    "1. **Abstraction**: Clean interfaces that hide complexity\n",
    "2. **Efficiency**: Minimize I/O and memory overhead\n",
    "3. **Scalability**: Handle datasets larger than memory\n",
    "\n",
    "### Connection to Real ML Systems\n",
    "Every framework uses the Dataset/DataLoader pattern:\n",
    "- **PyTorch**: `torch.utils.data.Dataset` and `torch.utils.data.DataLoader`\n",
    "- **TensorFlow**: `tf.data.Dataset` with efficient data pipelines\n",
    "- **JAX**: Custom data loading with `jax.numpy` integration\n",
    "- **TinyTorch**: `tinytorch.core.dataloader.Dataset` and `DataLoader` (what we're building!)\n",
    "\n",
    "### Performance Considerations\n",
    "- **Memory efficiency**: Handle datasets larger than RAM\n",
    "- **I/O optimization**: Read from disk efficiently with batching\n",
    "- **Caching strategies**: When to cache vs recompute\n",
    "- **Parallel processing**: Multi-threaded data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda7466f",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 1: Understanding Data Engineering\n",
    "\n",
    "### What is Data Engineering?\n",
    "**Data engineering** is the foundation of all machine learning systems. It involves loading, processing, and managing data efficiently so that models can learn from it.\n",
    "\n",
    "### The Fundamental Insight\n",
    "**Data engineering is about managing the flow of information through your system:**\n",
    "```\n",
    "Raw Data → Load → Preprocess → Batch → Feed to Model\n",
    "```\n",
    "\n",
    "### Real-World Examples\n",
    "- **Image datasets**: CIFAR-10, ImageNet, MNIST\n",
    "- **Text datasets**: Wikipedia, books, social media\n",
    "- **Tabular data**: CSV files, databases, spreadsheets\n",
    "- **Audio data**: Speech recordings, music files\n",
    "\n",
    "### Systems Thinking\n",
    "- **Memory efficiency**: Handle datasets larger than RAM\n",
    "- **I/O optimization**: Read from disk efficiently\n",
    "- **Batching strategies**: Trade-offs between memory and speed\n",
    "- **Caching**: When to cache vs recompute\n",
    "\n",
    "### Visual Intuition\n",
    "```\n",
    "Raw Files: [image1.jpg, image2.jpg, image3.jpg, ...]\n",
    "Load: [Tensor(32x32x3), Tensor(32x32x3), Tensor(32x32x3), ...]\n",
    "Batch: [Tensor(32, 32, 32, 3)]  # 32 images at once\n",
    "Model: Process batch efficiently\n",
    "```\n",
    "\n",
    "Let's start by building the most fundamental component: **Dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ed670e",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "dataset-class",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class Dataset:\n",
    "    \"\"\"\n",
    "    Base Dataset class: Abstract interface for all datasets.\n",
    "    \n",
    "    The fundamental abstraction for data loading in TinyTorch.\n",
    "    Students implement concrete datasets by inheriting from this class.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Get a single sample and label by index.\n",
    "        \n",
    "        Args:\n",
    "            index: Index of the sample to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (data, label) tensors\n",
    "            \n",
    "        TODO: Implement abstract method for getting samples.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. This is an abstract method - subclasses will implement it\n",
    "        2. Return a tuple of (data, label) tensors\n",
    "        3. Data should be the input features, label should be the target\n",
    "        \n",
    "        EXAMPLE:\n",
    "        dataset[0] should return (Tensor(image_data), Tensor(label))\n",
    "        \n",
    "        HINTS:\n",
    "        - This is an abstract method that subclasses must override\n",
    "        - Always return a tuple of (data, label) tensors\n",
    "        - Data contains the input features, label contains the target\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        # This is an abstract method - subclasses must implement it\n",
    "        raise NotImplementedError(\"Subclasses must implement __getitem__\")\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the total number of samples in the dataset.\n",
    "        \n",
    "        TODO: Implement abstract method for getting dataset size.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. This is an abstract method - subclasses will implement it\n",
    "        2. Return the total number of samples in the dataset\n",
    "        \n",
    "        EXAMPLE:\n",
    "        len(dataset) should return 50000 for CIFAR-10 training set\n",
    "        \n",
    "        HINTS:\n",
    "        - This is an abstract method that subclasses must override\n",
    "        - Return an integer representing the total number of samples\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        # This is an abstract method - subclasses must implement it\n",
    "        raise NotImplementedError(\"Subclasses must implement __len__\")\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def get_sample_shape(self) -> Tuple[int, ...]:\n",
    "        \"\"\"\n",
    "        Get the shape of a single data sample.\n",
    "        \n",
    "        TODO: Implement method to get sample shape.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Get the first sample using self[0]\n",
    "        2. Extract the data part (first element of tuple)\n",
    "        3. Return the shape of the data tensor\n",
    "        \n",
    "        EXAMPLE:\n",
    "        For CIFAR-10: returns (3, 32, 32) for RGB images\n",
    "        \n",
    "        HINTS:\n",
    "        - Use self[0] to get the first sample\n",
    "        - Extract data from the (data, label) tuple\n",
    "        - Return data.shape\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        # Get the first sample to determine shape\n",
    "        data, _ = self[0]\n",
    "        return data.shape\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def get_num_classes(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the number of classes in the dataset.\n",
    "        \n",
    "        TODO: Implement abstract method for getting number of classes.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. This is an abstract method - subclasses will implement it\n",
    "        2. Return the number of unique classes in the dataset\n",
    "        \n",
    "        EXAMPLE:\n",
    "        For CIFAR-10: returns 10 (classes 0-9)\n",
    "        \n",
    "        HINTS:\n",
    "        - This is an abstract method that subclasses must override\n",
    "        - Return the number of unique classes/categories\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        # This is an abstract method - subclasses must implement it\n",
    "        raise NotImplementedError(\"Subclasses must implement get_num_classes\")\n",
    "        ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2237b312",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### 🧪 Quick Test: Dataset Base Class\n",
    "\n",
    "Let's understand the Dataset interface! While we can't test the abstract class directly, we'll create a simple test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8246c9e",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-dataset-interface-immediate",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Dataset interface with a simple implementation\n",
    "print(\"🔬 Testing Dataset interface...\")\n",
    "\n",
    "# Create a minimal test dataset\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, size=5):\n",
    "        self.size = size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Simple test data: features are [index, index*2], label is index % 2\n",
    "        data = Tensor([index, index * 2])\n",
    "        label = Tensor([index % 2])\n",
    "        return data, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def get_num_classes(self):\n",
    "        return 2\n",
    "\n",
    "# Test the interface\n",
    "try:\n",
    "    test_dataset = TestDataset(size=5)\n",
    "    print(f\"Dataset created with size: {len(test_dataset)}\")\n",
    "    \n",
    "    # Test __getitem__\n",
    "    data, label = test_dataset[0]\n",
    "    print(f\"Sample 0: data={data}, label={label}\")\n",
    "    assert isinstance(data, Tensor), \"Data should be a Tensor\"\n",
    "    assert isinstance(label, Tensor), \"Label should be a Tensor\"\n",
    "    print(\"✅ Dataset __getitem__ works correctly\")\n",
    "    \n",
    "    # Test __len__\n",
    "    assert len(test_dataset) == 5, f\"Dataset length should be 5, got {len(test_dataset)}\"\n",
    "    print(\"✅ Dataset __len__ works correctly\")\n",
    "    \n",
    "    # Test get_num_classes\n",
    "    assert test_dataset.get_num_classes() == 2, f\"Should have 2 classes, got {test_dataset.get_num_classes()}\"\n",
    "    print(\"✅ Dataset get_num_classes works correctly\")\n",
    "    \n",
    "    # Test multiple samples\n",
    "    for i in range(3):\n",
    "        data, label = test_dataset[i]\n",
    "        expected_data = [i, i * 2]\n",
    "        expected_label = [i % 2]\n",
    "        assert np.array_equal(data.data, expected_data), f\"Data mismatch at index {i}\"\n",
    "        assert np.array_equal(label.data, expected_label), f\"Label mismatch at index {i}\"\n",
    "    print(\"✅ Dataset produces correct data for multiple samples\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Dataset interface test failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# Show the dataset pattern\n",
    "print(\"🎯 Dataset interface pattern:\")\n",
    "print(\"   __getitem__: Returns (data, label) tuple\")\n",
    "print(\"   __len__: Returns dataset size\")\n",
    "print(\"   get_num_classes: Returns number of classes\")\n",
    "print(\"📈 Progress: Dataset interface ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffb03fc",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 2: Building the DataLoader\n",
    "\n",
    "### What is a DataLoader?\n",
    "A **DataLoader** efficiently batches and iterates through datasets. It's the bridge between individual samples and the batched data that neural networks expect.\n",
    "\n",
    "### Why DataLoaders Matter\n",
    "- **Batching**: Groups samples for efficient GPU computation\n",
    "- **Shuffling**: Randomizes data order to prevent overfitting\n",
    "- **Memory efficiency**: Loads data on-demand rather than all at once\n",
    "- **Iteration**: Provides clean interface for training loops\n",
    "\n",
    "### The DataLoader Pattern\n",
    "```\n",
    "DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "for batch_data, batch_labels in dataloader:\n",
    "    # batch_data.shape: (32, ...)\n",
    "    # batch_labels.shape: (32,)\n",
    "    # Train on batch\n",
    "```\n",
    "\n",
    "### Real-World Applications\n",
    "- **Training loops**: Feed batches to neural networks\n",
    "- **Validation**: Evaluate models on held-out data\n",
    "- **Inference**: Process large datasets efficiently\n",
    "- **Data analysis**: Explore datasets systematically\n",
    "\n",
    "### Systems Thinking\n",
    "- **Batch size**: Trade-off between memory and speed\n",
    "- **Shuffling**: Prevents overfitting to data order\n",
    "- **Iteration**: Efficient looping through data\n",
    "- **Memory**: Manage large datasets that don't fit in RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a3b004",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "dataloader-class",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataLoader:\n",
    "    \"\"\"\n",
    "    DataLoader: Efficiently batch and iterate through datasets.\n",
    "    \n",
    "    Provides batching, shuffling, and efficient iteration over datasets.\n",
    "    Essential for training neural networks efficiently.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset: Dataset, batch_size: int = 32, shuffle: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize DataLoader.\n",
    "        \n",
    "        Args:\n",
    "            dataset: Dataset to load from\n",
    "            batch_size: Number of samples per batch\n",
    "            shuffle: Whether to shuffle data each epoch\n",
    "            \n",
    "        TODO: Store configuration and dataset.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Store dataset as self.dataset\n",
    "        2. Store batch_size as self.batch_size\n",
    "        3. Store shuffle as self.shuffle\n",
    "        \n",
    "        EXAMPLE:\n",
    "        DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "        \n",
    "        HINTS:\n",
    "        - Store all parameters as instance variables\n",
    "        - These will be used in __iter__ for batching\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def __iter__(self) -> Iterator[Tuple[Tensor, Tensor]]:\n",
    "        \"\"\"\n",
    "        Iterate through dataset in batches.\n",
    "        \n",
    "        Returns:\n",
    "            Iterator yielding (batch_data, batch_labels) tuples\n",
    "            \n",
    "        TODO: Implement batching and shuffling logic.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Create indices list: list(range(len(dataset)))\n",
    "        2. Shuffle indices if self.shuffle is True\n",
    "        3. Loop through indices in batch_size chunks\n",
    "        4. For each batch: collect samples, stack them, yield batch\n",
    "        \n",
    "        EXAMPLE:\n",
    "        for batch_data, batch_labels in dataloader:\n",
    "            # batch_data.shape: (batch_size, ...)\n",
    "            # batch_labels.shape: (batch_size,)\n",
    "        \n",
    "        HINTS:\n",
    "        - Use list(range(len(self.dataset))) for indices\n",
    "        - Use np.random.shuffle() if self.shuffle is True\n",
    "        - Loop in chunks of self.batch_size\n",
    "        - Collect samples and stack with np.stack()\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        # Create indices for all samples\n",
    "        indices = list(range(len(self.dataset)))\n",
    "        \n",
    "        # Shuffle if requested\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "        \n",
    "        # Iterate through indices in batches\n",
    "        for i in range(0, len(indices), self.batch_size):\n",
    "            batch_indices = indices[i:i + self.batch_size]\n",
    "            \n",
    "            # Collect samples for this batch\n",
    "            batch_data = []\n",
    "            batch_labels = []\n",
    "            \n",
    "            for idx in batch_indices:\n",
    "                data, label = self.dataset[idx]\n",
    "                batch_data.append(data.data)\n",
    "                batch_labels.append(label.data)\n",
    "            \n",
    "            # Stack into batch tensors\n",
    "            batch_data_array = np.stack(batch_data, axis=0)\n",
    "            batch_labels_array = np.stack(batch_labels, axis=0)\n",
    "            \n",
    "            yield Tensor(batch_data_array), Tensor(batch_labels_array)\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the number of batches per epoch.\n",
    "        \n",
    "        TODO: Calculate number of batches.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Get dataset size: len(self.dataset)\n",
    "        2. Divide by batch_size and round up\n",
    "        3. Use ceiling division: (n + batch_size - 1) // batch_size\n",
    "        \n",
    "        EXAMPLE:\n",
    "        Dataset size 100, batch size 32 → 4 batches\n",
    "        \n",
    "        HINTS:\n",
    "        - Use len(self.dataset) for dataset size\n",
    "        - Use ceiling division for exact batch count\n",
    "        - Formula: (dataset_size + batch_size - 1) // batch_size\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        # Calculate number of batches using ceiling division\n",
    "        dataset_size = len(self.dataset)\n",
    "        return (dataset_size + self.batch_size - 1) // self.batch_size\n",
    "        ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39c318e",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### 🧪 Quick Test: DataLoader\n",
    "\n",
    "Let's test your DataLoader implementation! This is the heart of efficient data loading for neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94a902c",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-dataloader-immediate",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test DataLoader immediately after implementation\n",
    "print(\"🔬 Testing DataLoader...\")\n",
    "\n",
    "# Use the test dataset from before\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, size=10):\n",
    "        self.size = size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = Tensor([index, index * 2])\n",
    "        label = Tensor([index % 3])  # 3 classes\n",
    "        return data, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def get_num_classes(self):\n",
    "        return 3\n",
    "\n",
    "# Test basic DataLoader functionality\n",
    "try:\n",
    "    dataset = TestDataset(size=10)\n",
    "    dataloader = DataLoader(dataset, batch_size=3, shuffle=False)\n",
    "    \n",
    "    print(f\"DataLoader created: batch_size={dataloader.batch_size}, shuffle={dataloader.shuffle}\")\n",
    "    print(f\"Number of batches: {len(dataloader)}\")\n",
    "    \n",
    "    # Test __len__\n",
    "    expected_batches = (10 + 3 - 1) // 3  # Ceiling division: 4 batches\n",
    "    assert len(dataloader) == expected_batches, f\"Should have {expected_batches} batches, got {len(dataloader)}\"\n",
    "    print(\"✅ DataLoader __len__ works correctly\")\n",
    "    \n",
    "    # Test iteration\n",
    "    batch_count = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch_data, batch_labels in dataloader:\n",
    "        batch_count += 1\n",
    "        batch_size = batch_data.shape[0]\n",
    "        total_samples += batch_size\n",
    "        \n",
    "        print(f\"Batch {batch_count}: data shape {batch_data.shape}, labels shape {batch_labels.shape}\")\n",
    "        \n",
    "        # Verify batch dimensions\n",
    "        assert len(batch_data.shape) == 2, f\"Batch data should be 2D, got {batch_data.shape}\"\n",
    "        assert len(batch_labels.shape) == 2, f\"Batch labels should be 2D, got {batch_labels.shape}\"\n",
    "        assert batch_data.shape[1] == 2, f\"Each sample should have 2 features, got {batch_data.shape[1]}\"\n",
    "        assert batch_labels.shape[1] == 1, f\"Each label should have 1 element, got {batch_labels.shape[1]}\"\n",
    "        \n",
    "    assert batch_count == expected_batches, f\"Should iterate {expected_batches} times, got {batch_count}\"\n",
    "    assert total_samples == 10, f\"Should process 10 total samples, got {total_samples}\"\n",
    "    print(\"✅ DataLoader iteration works correctly\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ DataLoader test failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# Test shuffling\n",
    "try:\n",
    "    dataloader_shuffle = DataLoader(dataset, batch_size=5, shuffle=True)\n",
    "    dataloader_no_shuffle = DataLoader(dataset, batch_size=5, shuffle=False)\n",
    "    \n",
    "    # Get first batch from each\n",
    "    batch1_shuffle = next(iter(dataloader_shuffle))\n",
    "    batch1_no_shuffle = next(iter(dataloader_no_shuffle))\n",
    "    \n",
    "    print(\"✅ DataLoader shuffling parameter works\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ DataLoader shuffling test failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# Test different batch sizes\n",
    "try:\n",
    "    small_loader = DataLoader(dataset, batch_size=2, shuffle=False)\n",
    "    large_loader = DataLoader(dataset, batch_size=8, shuffle=False)\n",
    "    \n",
    "    assert len(small_loader) == 5, f\"Small loader should have 5 batches, got {len(small_loader)}\"\n",
    "    assert len(large_loader) == 2, f\"Large loader should have 2 batches, got {len(large_loader)}\"\n",
    "    print(\"✅ DataLoader handles different batch sizes correctly\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ DataLoader batch size test failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# Show the DataLoader behavior\n",
    "print(\"🎯 DataLoader behavior:\")\n",
    "print(\"   Batches data for efficient processing\")\n",
    "print(\"   Handles shuffling and iteration\")\n",
    "print(\"   Provides clean interface for training loops\")\n",
    "print(\"📈 Progress: Dataset interface ✓, DataLoader ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1143391",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 3: Creating a Simple Dataset Example\n",
    "\n",
    "### Why We Need Concrete Examples\n",
    "Abstract classes are great for interfaces, but we need concrete implementations to understand how they work. Let's create a simple dataset for testing.\n",
    "\n",
    "### Design Principles\n",
    "- **Simple**: Easy to understand and debug\n",
    "- **Configurable**: Adjustable size and properties\n",
    "- **Predictable**: Deterministic data for testing\n",
    "- **Educational**: Shows the Dataset pattern clearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112dcf35",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "simple-dataset",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class SimpleDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Simple dataset for testing and demonstration.\n",
    "    \n",
    "    Generates synthetic data with configurable size and properties.\n",
    "    Perfect for understanding the Dataset pattern.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, size: int = 100, num_features: int = 4, num_classes: int = 3):\n",
    "        \"\"\"\n",
    "        Initialize SimpleDataset.\n",
    "        \n",
    "        Args:\n",
    "            size: Number of samples in the dataset\n",
    "            num_features: Number of features per sample\n",
    "            num_classes: Number of classes\n",
    "            \n",
    "        TODO: Initialize the dataset with synthetic data.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Store the configuration parameters\n",
    "        2. Generate synthetic data and labels\n",
    "        3. Make data deterministic for testing\n",
    "        \n",
    "        EXAMPLE:\n",
    "        SimpleDataset(size=100, num_features=4, num_classes=3)\n",
    "        creates 100 samples with 4 features each, 3 classes\n",
    "        \n",
    "        HINTS:\n",
    "        - Store size, num_features, num_classes as instance variables\n",
    "        - Use np.random.seed() for reproducible data\n",
    "        - Generate random data with np.random.randn()\n",
    "        - Generate random labels with np.random.randint()\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        self.size = size\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Set seed for reproducible data\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # Generate synthetic data\n",
    "        self.data = np.random.randn(size, num_features).astype(np.float32)\n",
    "        self.labels = np.random.randint(0, num_classes, size=size)\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Get a single sample and label by index.\n",
    "        \n",
    "        Args:\n",
    "            index: Index of the sample to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (data, label) tensors\n",
    "            \n",
    "        TODO: Return the sample and label at the given index.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Get data at index from self.data\n",
    "        2. Get label at index from self.labels\n",
    "        3. Convert to tensors and return as tuple\n",
    "        \n",
    "        EXAMPLE:\n",
    "        dataset[0] returns (Tensor([1.2, -0.5, 0.8, 0.1]), Tensor(2))\n",
    "        \n",
    "        HINTS:\n",
    "        - Use self.data[index] and self.labels[index]\n",
    "        - Convert to Tensor objects\n",
    "        - Return as tuple (data, label)\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        data = Tensor(self.data[index])\n",
    "        label = Tensor(self.labels[index])\n",
    "        return data, label\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the total number of samples in the dataset.\n",
    "        \n",
    "        TODO: Return the dataset size.\n",
    "        \n",
    "        HINTS:\n",
    "        - Return self.size\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        return self.size\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def get_num_classes(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the number of classes in the dataset.\n",
    "        \n",
    "        TODO: Return the number of classes.\n",
    "        \n",
    "        HINTS:\n",
    "        - Return self.num_classes\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        return self.num_classes\n",
    "        ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a82fa8",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## 🧪 Comprehensive DataLoader Testing Suite\n",
    "\n",
    "Let's test all data loading components thoroughly with realistic ML data scenarios!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5cf627",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "test-dataloader-comprehensive",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_dataset_interface():\n",
    "    \"\"\"Test 1: Dataset interface comprehensive testing\"\"\"\n",
    "    print(\"🔬 Testing Dataset Interface...\")\n",
    "    \n",
    "    # Test 1.1: Abstract base class behavior\n",
    "    try:\n",
    "        # Test that we can't instantiate abstract Dataset\n",
    "        try:\n",
    "            base_dataset = Dataset()\n",
    "            base_dataset[0]  # Should raise NotImplementedError\n",
    "            assert False, \"Should not be able to call abstract methods\"\n",
    "        except NotImplementedError:\n",
    "            print(\"✅ Abstract Dataset correctly raises NotImplementedError\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Abstract Dataset test failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Test 1.2: SimpleDataset implementation\n",
    "    try:\n",
    "        dataset = SimpleDataset(size=50, num_features=4, num_classes=3)\n",
    "        \n",
    "        # Test basic properties\n",
    "        assert len(dataset) == 50, f\"Dataset length should be 50, got {len(dataset)}\"\n",
    "        assert dataset.get_num_classes() == 3, f\"Should have 3 classes, got {dataset.get_num_classes()}\"\n",
    "        \n",
    "        # Test sample retrieval\n",
    "        data, label = dataset[0]\n",
    "        assert isinstance(data, Tensor), \"Data should be a Tensor\"\n",
    "        assert isinstance(label, Tensor), \"Label should be a Tensor\"\n",
    "        assert data.shape == (4,), f\"Data shape should be (4,), got {data.shape}\"\n",
    "        \n",
    "        # Test sample shape method\n",
    "        sample_shape = dataset.get_sample_shape()\n",
    "        assert sample_shape == (4,), f\"Sample shape should be (4,), got {sample_shape}\"\n",
    "        \n",
    "        print(\"✅ SimpleDataset implementation test passed\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ SimpleDataset implementation failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Test 1.3: Different dataset configurations\n",
    "    try:\n",
    "        # Small dataset\n",
    "        small_dataset = SimpleDataset(size=5, num_features=2, num_classes=2)\n",
    "        assert len(small_dataset) == 5, \"Small dataset length wrong\"\n",
    "        assert small_dataset.get_num_classes() == 2, \"Small dataset classes wrong\"\n",
    "        \n",
    "        # Large dataset\n",
    "        large_dataset = SimpleDataset(size=1000, num_features=10, num_classes=5)\n",
    "        assert len(large_dataset) == 1000, \"Large dataset length wrong\"\n",
    "        assert large_dataset.get_num_classes() == 5, \"Large dataset classes wrong\"\n",
    "        \n",
    "        # Test data consistency (seeded random)\n",
    "        data1, _ = small_dataset[0]\n",
    "        data2, _ = small_dataset[0]\n",
    "        assert np.allclose(data1.data, data2.data), \"Dataset should be deterministic\"\n",
    "        \n",
    "        print(\"✅ Different dataset configurations test passed\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Different dataset configurations failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Test 1.4: Edge cases and robustness\n",
    "    try:\n",
    "        # Test edge case: single sample\n",
    "        single_dataset = SimpleDataset(size=1, num_features=1, num_classes=1)\n",
    "        data, label = single_dataset[0]\n",
    "        assert data.shape == (1,), \"Single sample data shape wrong\"\n",
    "        assert isinstance(label.data, (int, np.integer)) or label.data.shape == (), \"Single sample label wrong\"\n",
    "        \n",
    "        # Test boundary indices\n",
    "        dataset = SimpleDataset(size=10, num_features=3, num_classes=2)\n",
    "        first_data, first_label = dataset[0]\n",
    "        last_data, last_label = dataset[9]\n",
    "        assert first_data.shape == (3,), \"First sample shape wrong\"\n",
    "        assert last_data.shape == (3,), \"Last sample shape wrong\"\n",
    "        \n",
    "        print(\"✅ Edge cases and robustness test passed\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Edge cases and robustness failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "    print(\"🎯 Dataset interface: All tests passed!\")\n",
    "    return True\n",
    "\n",
    "def test_dataloader_functionality():\n",
    "    \"\"\"Test 2: DataLoader functionality comprehensive testing\"\"\"\n",
    "    print(\"🔬 Testing DataLoader Functionality...\")\n",
    "    \n",
    "    # Test 2.1: Basic DataLoader operations\n",
    "    try:\n",
    "        dataset = SimpleDataset(size=32, num_features=4, num_classes=2)\n",
    "        dataloader = DataLoader(dataset, batch_size=8, shuffle=False)\n",
    "        \n",
    "        # Test initialization\n",
    "        assert dataloader.batch_size == 8, f\"Batch size should be 8, got {dataloader.batch_size}\"\n",
    "        assert dataloader.shuffle == False, f\"Shuffle should be False, got {dataloader.shuffle}\"\n",
    "        \n",
    "        # Test length calculation\n",
    "        expected_batches = (32 + 8 - 1) // 8  # Ceiling division: 4 batches\n",
    "        assert len(dataloader) == expected_batches, f\"Should have {expected_batches} batches, got {len(dataloader)}\"\n",
    "        \n",
    "        print(\"✅ Basic DataLoader operations test passed\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Basic DataLoader operations failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Test 2.2: Batch iteration and shapes\n",
    "    try:\n",
    "        dataset = SimpleDataset(size=25, num_features=3, num_classes=2)\n",
    "        dataloader = DataLoader(dataset, batch_size=10, shuffle=False)\n",
    "        \n",
    "        batch_count = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for batch_data, batch_labels in dataloader:\n",
    "            batch_count += 1\n",
    "            batch_size = batch_data.shape[0]\n",
    "            total_samples += batch_size\n",
    "            \n",
    "            # Check batch shapes\n",
    "            assert len(batch_data.shape) == 2, f\"Batch data should be 2D, got {batch_data.shape}\"\n",
    "            assert batch_data.shape[1] == 3, f\"Should have 3 features, got {batch_data.shape[1]}\"\n",
    "            assert batch_labels.shape[0] == batch_size, f\"Labels should match batch size\"\n",
    "            \n",
    "            # Check data types\n",
    "            assert isinstance(batch_data, Tensor), \"Batch data should be Tensor\"\n",
    "            assert isinstance(batch_labels, Tensor), \"Batch labels should be Tensor\"\n",
    "        \n",
    "        # Verify complete iteration\n",
    "        assert total_samples == 25, f\"Should process 25 samples, got {total_samples}\"\n",
    "        assert batch_count == 3, f\"Should have 3 batches, got {batch_count}\"  # 25/10 = 3 batches\n",
    "        \n",
    "        print(\"✅ Batch iteration and shapes test passed\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Batch iteration and shapes failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Test 2.3: Different batch sizes\n",
    "    try:\n",
    "        dataset = SimpleDataset(size=100, num_features=5, num_classes=3)\n",
    "        \n",
    "        # Small batches\n",
    "        small_loader = DataLoader(dataset, batch_size=7, shuffle=False)\n",
    "        assert len(small_loader) == 15, f\"Small loader should have 15 batches, got {len(small_loader)}\"  # 100/7 = 15\n",
    "        \n",
    "        # Large batches\n",
    "        large_loader = DataLoader(dataset, batch_size=30, shuffle=False)\n",
    "        assert len(large_loader) == 4, f\"Large loader should have 4 batches, got {len(large_loader)}\"  # 100/30 = 4\n",
    "        \n",
    "        # Single sample batches\n",
    "        single_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "        assert len(single_loader) == 100, f\"Single loader should have 100 batches, got {len(single_loader)}\"\n",
    "        \n",
    "        print(\"✅ Different batch sizes test passed\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Different batch sizes failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Test 2.4: Shuffling behavior\n",
    "    try:\n",
    "        dataset = SimpleDataset(size=20, num_features=2, num_classes=2)\n",
    "        \n",
    "        # Test with shuffling\n",
    "        loader_shuffle = DataLoader(dataset, batch_size=5, shuffle=True)\n",
    "        loader_no_shuffle = DataLoader(dataset, batch_size=5, shuffle=False)\n",
    "        \n",
    "        # Get multiple batches to test shuffling\n",
    "        shuffle_batches = list(loader_shuffle)\n",
    "        no_shuffle_batches = list(loader_no_shuffle)\n",
    "        \n",
    "        assert len(shuffle_batches) == len(no_shuffle_batches), \"Should have same number of batches\"\n",
    "        \n",
    "        # Test that all original samples are present (just reordered)\n",
    "        shuffle_all_data = np.concatenate([batch[0].data for batch in shuffle_batches])\n",
    "        no_shuffle_all_data = np.concatenate([batch[0].data for batch in no_shuffle_batches])\n",
    "        \n",
    "        assert shuffle_all_data.shape == no_shuffle_all_data.shape, \"Should have same total data shape\"\n",
    "        \n",
    "        print(\"✅ Shuffling behavior test passed\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Shuffling behavior failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "    print(\"🎯 DataLoader functionality: All tests passed!\")\n",
    "    return True\n",
    "\n",
    "def test_data_pipeline_scenarios():\n",
    "    \"\"\"Test 3: Real-world data pipeline scenarios\"\"\"\n",
    "    print(\"🔬 Testing Data Pipeline Scenarios...\")\n",
    "    \n",
    "    # Test 3.1: Image classification scenario\n",
    "    try:\n",
    "        # Simulate CIFAR-10 like dataset: 32x32 RGB images, 10 classes\n",
    "        image_dataset = SimpleDataset(size=1000, num_features=32*32*3, num_classes=10)\n",
    "        image_loader = DataLoader(image_dataset, batch_size=64, shuffle=True)\n",
    "        \n",
    "        # Test one epoch of training\n",
    "        epoch_samples = 0\n",
    "        for batch_data, batch_labels in image_loader:\n",
    "            epoch_samples += batch_data.shape[0]\n",
    "            \n",
    "            # Verify image batch properties\n",
    "            assert batch_data.shape[1] == 32*32*3, f\"Should have 3072 features (32x32x3), got {batch_data.shape[1]}\"\n",
    "            assert batch_data.shape[0] <= 64, f\"Batch size should be <= 64, got {batch_data.shape[0]}\"\n",
    "            \n",
    "            # Simulate forward pass\n",
    "            batch_size = batch_data.shape[0]\n",
    "            assert batch_labels.shape[0] == batch_size, \"Labels should match batch size\"\n",
    "        \n",
    "        assert epoch_samples == 1000, f\"Should process 1000 samples, got {epoch_samples}\"\n",
    "        print(\"✅ Image classification scenario test passed\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Image classification scenario failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Test 3.2: Text classification scenario\n",
    "    try:\n",
    "        # Simulate text classification: 512 token embeddings, 5 sentiment classes\n",
    "        text_dataset = SimpleDataset(size=500, num_features=512, num_classes=5)\n",
    "        text_loader = DataLoader(text_dataset, batch_size=32, shuffle=True)\n",
    "        \n",
    "        # Test batch processing\n",
    "        for batch_data, batch_labels in text_loader:\n",
    "            # Verify text batch properties\n",
    "            assert batch_data.shape[1] == 512, f\"Should have 512 features, got {batch_data.shape[1]}\"\n",
    "            \n",
    "            # Simulate text processing\n",
    "            batch_size = batch_data.shape[0]\n",
    "            assert batch_size <= 32, f\"Batch size should be <= 32, got {batch_size}\"\n",
    "            break  # Just test first batch\n",
    "        \n",
    "        print(\"✅ Text classification scenario test passed\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Text classification scenario failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Test 3.3: Tabular data scenario\n",
    "    try:\n",
    "        # Simulate tabular data: house prices with 20 features, 3 price ranges\n",
    "        tabular_dataset = SimpleDataset(size=200, num_features=20, num_classes=3)\n",
    "        tabular_loader = DataLoader(tabular_dataset, batch_size=16, shuffle=False)\n",
    "        \n",
    "        # Test systematic processing (no shuffling for tabular data)\n",
    "        batch_count = 0\n",
    "        for batch_data, batch_labels in tabular_loader:\n",
    "            batch_count += 1\n",
    "            \n",
    "            # Verify tabular batch properties\n",
    "            assert batch_data.shape[1] == 20, f\"Should have 20 features, got {batch_data.shape[1]}\"\n",
    "            \n",
    "            # Simulate tabular processing\n",
    "            batch_size = batch_data.shape[0]\n",
    "            assert batch_size <= 16, f\"Batch size should be <= 16, got {batch_size}\"\n",
    "        \n",
    "        expected_batches = (200 + 16 - 1) // 16  # 13 batches\n",
    "        assert batch_count == expected_batches, f\"Should have {expected_batches} batches, got {batch_count}\"\n",
    "        \n",
    "        print(\"✅ Tabular data scenario test passed\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Tabular data scenario failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Test 3.4: Small dataset scenario\n",
    "    try:\n",
    "        # Simulate small research dataset\n",
    "        small_dataset = SimpleDataset(size=50, num_features=10, num_classes=2)\n",
    "        small_loader = DataLoader(small_dataset, batch_size=8, shuffle=True)\n",
    "        \n",
    "        # Test multiple epochs\n",
    "        for epoch in range(3):\n",
    "            epoch_samples = 0\n",
    "            for batch_data, batch_labels in small_loader:\n",
    "                epoch_samples += batch_data.shape[0]\n",
    "                \n",
    "                # Verify small dataset properties\n",
    "                assert batch_data.shape[1] == 10, f\"Should have 10 features, got {batch_data.shape[1]}\"\n",
    "                \n",
    "            assert epoch_samples == 50, f\"Epoch {epoch}: should process 50 samples, got {epoch_samples}\"\n",
    "        \n",
    "        print(\"✅ Small dataset scenario test passed\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Small dataset scenario failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "    print(\"🎯 Data pipeline scenarios: All tests passed!\")\n",
    "    return True\n",
    "\n",
    "def test_integration_with_ml_workflow():\n",
    "    \"\"\"Test 4: Integration with ML workflow\"\"\"\n",
    "    print(\"🔬 Testing Integration with ML Workflow...\")\n",
    "    \n",
    "    # Test 4.1: Training loop integration\n",
    "    try:\n",
    "        # Create dataset for training\n",
    "        train_dataset = SimpleDataset(size=100, num_features=8, num_classes=3)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "        \n",
    "        # Simulate training loop\n",
    "        for epoch in range(2):\n",
    "            epoch_loss = 0\n",
    "            batch_count = 0\n",
    "            \n",
    "            for batch_data, batch_labels in train_loader:\n",
    "                batch_count += 1\n",
    "                \n",
    "                # Simulate forward pass\n",
    "                batch_size = batch_data.shape[0]\n",
    "                assert batch_data.shape == (batch_size, 8), f\"Batch data shape wrong: {batch_data.shape}\"\n",
    "                assert batch_labels.shape[0] == batch_size, f\"Batch labels shape wrong: {batch_labels.shape}\"\n",
    "                \n",
    "                # Simulate loss computation\n",
    "                mock_loss = np.random.random()\n",
    "                epoch_loss += mock_loss\n",
    "                \n",
    "                # Verify we can iterate through all batches\n",
    "                assert batch_count <= 5, f\"Too many batches: {batch_count}\"  # 100/20 = 5\n",
    "            \n",
    "            assert batch_count == 5, f\"Should have 5 batches per epoch, got {batch_count}\"\n",
    "        \n",
    "        print(\"✅ Training loop integration test passed\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Training loop integration failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Test 4.2: Validation loop integration\n",
    "    try:\n",
    "        # Create dataset for validation\n",
    "        val_dataset = SimpleDataset(size=50, num_features=8, num_classes=3)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False)  # No shuffle for validation\n",
    "        \n",
    "        # Simulate validation loop\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for batch_data, batch_labels in val_loader:\n",
    "            batch_size = batch_data.shape[0]\n",
    "            total_samples += batch_size\n",
    "            \n",
    "            # Simulate prediction\n",
    "            mock_predictions = np.random.randint(0, 3, size=batch_size)\n",
    "            mock_correct = np.random.randint(0, batch_size + 1)\n",
    "            total_correct += mock_correct\n",
    "            \n",
    "            # Verify batch properties\n",
    "            assert batch_data.shape[1] == 8, f\"Features should be 8, got {batch_data.shape[1]}\"\n",
    "            assert batch_labels.shape[0] == batch_size, f\"Labels should match batch size\"\n",
    "        \n",
    "        assert total_samples == 50, f\"Should validate 50 samples, got {total_samples}\"\n",
    "        \n",
    "        print(\"✅ Validation loop integration test passed\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Validation loop integration failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Test 4.3: Model inference integration\n",
    "    try:\n",
    "        # Create dataset for inference\n",
    "        test_dataset = SimpleDataset(size=30, num_features=5, num_classes=2)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=5, shuffle=False)\n",
    "        \n",
    "        # Simulate inference\n",
    "        all_predictions = []\n",
    "        \n",
    "        for batch_data, batch_labels in test_loader:\n",
    "            batch_size = batch_data.shape[0]\n",
    "            \n",
    "            # Simulate model inference\n",
    "            mock_predictions = np.random.random((batch_size, 2))  # 2 classes\n",
    "            all_predictions.append(mock_predictions)\n",
    "            \n",
    "            # Verify inference batch properties\n",
    "            assert batch_data.shape[1] == 5, f\"Features should be 5, got {batch_data.shape[1]}\"\n",
    "            assert batch_size <= 5, f\"Batch size should be <= 5, got {batch_size}\"\n",
    "        \n",
    "        # Verify all predictions collected\n",
    "        total_predictions = np.concatenate(all_predictions, axis=0)\n",
    "        assert total_predictions.shape == (30, 2), f\"Predictions shape should be (30, 2), got {total_predictions.shape}\"\n",
    "        \n",
    "        print(\"✅ Model inference integration test passed\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Model inference integration failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Test 4.4: Cross-validation scenario\n",
    "    try:\n",
    "        # Create dataset for cross-validation\n",
    "        full_dataset = SimpleDataset(size=100, num_features=6, num_classes=4)\n",
    "        \n",
    "        # Simulate 5-fold cross-validation\n",
    "        fold_size = 20\n",
    "        \n",
    "        for fold in range(5):\n",
    "            # Create train/val split simulation\n",
    "            train_size = 80  # 4 folds for training\n",
    "            val_size = 20    # 1 fold for validation\n",
    "            \n",
    "            train_dataset = SimpleDataset(size=train_size, num_features=6, num_classes=4)\n",
    "            val_dataset = SimpleDataset(size=val_size, num_features=6, num_classes=4)\n",
    "            \n",
    "            train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "            \n",
    "            # Verify fold setup\n",
    "            assert len(train_dataset) == train_size, f\"Train size wrong for fold {fold}\"\n",
    "            assert len(val_dataset) == val_size, f\"Val size wrong for fold {fold}\"\n",
    "            \n",
    "            # Test one iteration of each\n",
    "            train_batch = next(iter(train_loader))\n",
    "            val_batch = next(iter(val_loader))\n",
    "            \n",
    "            assert train_batch[0].shape[1] == 6, f\"Train features wrong for fold {fold}\"\n",
    "            assert val_batch[0].shape[1] == 6, f\"Val features wrong for fold {fold}\"\n",
    "        \n",
    "        print(\"✅ Cross-validation scenario test passed\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Cross-validation scenario failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "    print(\"🎯 ML workflow integration: All tests passed!\")\n",
    "    return True\n",
    "\n",
    "# Run all comprehensive tests\n",
    "def run_comprehensive_dataloader_tests():\n",
    "    \"\"\"Run all comprehensive DataLoader tests\"\"\"\n",
    "    print(\"🧪 Running Comprehensive DataLoader Test Suite...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    test_results = []\n",
    "    \n",
    "    # Run all test functions\n",
    "    test_results.append(test_dataset_interface())\n",
    "    test_results.append(test_dataloader_functionality())\n",
    "    test_results.append(test_data_pipeline_scenarios())\n",
    "    test_results.append(test_integration_with_ml_workflow())\n",
    "    \n",
    "    # Summary\n",
    "    print(\"=\" * 60)\n",
    "    print(\"📊 Test Results Summary:\")\n",
    "    print(f\"✅ Dataset Interface: {'PASSED' if test_results[0] else 'FAILED'}\")\n",
    "    print(f\"✅ DataLoader Functionality: {'PASSED' if test_results[1] else 'FAILED'}\")\n",
    "    print(f\"✅ Data Pipeline Scenarios: {'PASSED' if test_results[2] else 'FAILED'}\")\n",
    "    print(f\"✅ ML Workflow Integration: {'PASSED' if test_results[3] else 'FAILED'}\")\n",
    "    \n",
    "    all_passed = all(test_results)\n",
    "    print(f\"\\n🎯 Overall Result: {'ALL TESTS PASSED! 🎉' if all_passed else 'SOME TESTS FAILED ❌'}\")\n",
    "    \n",
    "    if all_passed:\n",
    "        print(\"\\n🚀 DataLoader Module Implementation Complete!\")\n",
    "        print(\"   ✓ Dataset interface working correctly\")\n",
    "        print(\"   ✓ DataLoader batching and iteration functional\")\n",
    "        print(\"   ✓ Real-world data pipeline scenarios tested\")\n",
    "        print(\"   ✓ ML workflow integration verified\")\n",
    "        print(\"\\n🎓 Ready for production ML data pipelines!\")\n",
    "    \n",
    "    return all_passed\n",
    "\n",
    "# Run the comprehensive test suite\n",
    "if __name__ == \"__main__\":\n",
    "    run_comprehensive_dataloader_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97a73a7",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### 🧪 Test Your Data Loading Implementations\n",
    "\n",
    "Once you implement the classes above, run these cells to test them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a145412",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-dataset",
     "locked": true,
     "points": 25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Dataset abstract class\n",
    "print(\"Testing Dataset abstract class...\")\n",
    "\n",
    "# Create a simple dataset\n",
    "dataset = SimpleDataset(size=10, num_features=3, num_classes=2)\n",
    "\n",
    "# Test basic functionality\n",
    "assert len(dataset) == 10, f\"Dataset length should be 10, got {len(dataset)}\"\n",
    "assert dataset.get_num_classes() == 2, f\"Number of classes should be 2, got {dataset.get_num_classes()}\"\n",
    "\n",
    "# Test sample retrieval\n",
    "data, label = dataset[0]\n",
    "assert isinstance(data, Tensor), \"Data should be a Tensor\"\n",
    "assert isinstance(label, Tensor), \"Label should be a Tensor\"\n",
    "assert data.shape == (3,), f\"Data shape should be (3,), got {data.shape}\"\n",
    "assert label.shape == (), f\"Label shape should be (), got {label.shape}\"\n",
    "\n",
    "# Test sample shape\n",
    "sample_shape = dataset.get_sample_shape()\n",
    "assert sample_shape == (3,), f\"Sample shape should be (3,), got {sample_shape}\"\n",
    "\n",
    "print(\"✅ Dataset tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d146e5",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-dataloader",
     "locked": true,
     "points": 25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test DataLoader\n",
    "print(\"Testing DataLoader...\")\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = SimpleDataset(size=50, num_features=4, num_classes=3)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Test dataloader length\n",
    "expected_batches = (50 + 8 - 1) // 8  # Ceiling division\n",
    "assert len(dataloader) == expected_batches, f\"DataLoader length should be {expected_batches}, got {len(dataloader)}\"\n",
    "\n",
    "# Test batch iteration\n",
    "batch_count = 0\n",
    "total_samples = 0\n",
    "\n",
    "for batch_data, batch_labels in dataloader:\n",
    "    batch_count += 1\n",
    "    batch_size = batch_data.shape[0]\n",
    "    total_samples += batch_size\n",
    "    \n",
    "    # Check batch shapes\n",
    "    assert batch_data.shape[1] == 4, f\"Batch data should have 4 features, got {batch_data.shape[1]}\"\n",
    "    assert batch_labels.shape[0] == batch_size, f\"Batch labels should match batch size, got {batch_labels.shape[0]}\"\n",
    "    \n",
    "    # Check that we don't exceed expected batches\n",
    "    assert batch_count <= expected_batches, f\"Too many batches: {batch_count} > {expected_batches}\"\n",
    "\n",
    "# Verify we processed all samples\n",
    "assert total_samples == 50, f\"Should process 50 samples total, got {total_samples}\"\n",
    "assert batch_count == expected_batches, f\"Should have {expected_batches} batches, got {batch_count}\"\n",
    "\n",
    "print(\"✅ DataLoader tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612b9f9e",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-dataloader-shuffle",
     "locked": true,
     "points": 25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test DataLoader shuffling\n",
    "print(\"Testing DataLoader shuffling...\")\n",
    "\n",
    "# Create dataset\n",
    "dataset = SimpleDataset(size=20, num_features=2, num_classes=2)\n",
    "\n",
    "# Test with shuffling\n",
    "dataloader_shuffle = DataLoader(dataset, batch_size=5, shuffle=True)\n",
    "dataloader_no_shuffle = DataLoader(dataset, batch_size=5, shuffle=False)\n",
    "\n",
    "# Get first batch from each\n",
    "batch_shuffle = next(iter(dataloader_shuffle))\n",
    "batch_no_shuffle = next(iter(dataloader_no_shuffle))\n",
    "\n",
    "# With different random seeds, shuffled batches should be different\n",
    "# (This is probabilistic, but very likely to be true)\n",
    "shuffle_data = batch_shuffle[0].data\n",
    "no_shuffle_data = batch_no_shuffle[0].data\n",
    "\n",
    "# Check that shapes are correct\n",
    "assert shuffle_data.shape == (5, 2), f\"Shuffled batch shape should be (5, 2), got {shuffle_data.shape}\"\n",
    "assert no_shuffle_data.shape == (5, 2), f\"No-shuffle batch shape should be (5, 2), got {no_shuffle_data.shape}\"\n",
    "\n",
    "print(\"✅ DataLoader shuffling tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc3ac23",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-integration",
     "locked": true,
     "points": 25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test complete data pipeline integration\n",
    "print(\"Testing complete data pipeline integration...\")\n",
    "\n",
    "# Create a larger dataset\n",
    "dataset = SimpleDataset(size=100, num_features=8, num_classes=5)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Simulate training loop\n",
    "epoch_samples = 0\n",
    "epoch_batches = 0\n",
    "\n",
    "for batch_data, batch_labels in dataloader:\n",
    "    epoch_batches += 1\n",
    "    epoch_samples += batch_data.shape[0]\n",
    "    \n",
    "    # Verify batch properties\n",
    "    assert batch_data.shape[1] == 8, f\"Features should be 8, got {batch_data.shape[1]}\"\n",
    "    assert len(batch_labels.shape) == 1, f\"Labels should be 1D, got shape {batch_labels.shape}\"\n",
    "    \n",
    "    # Verify data types\n",
    "    assert isinstance(batch_data, Tensor), \"Batch data should be Tensor\"\n",
    "    assert isinstance(batch_labels, Tensor), \"Batch labels should be Tensor\"\n",
    "\n",
    "# Verify we processed all data\n",
    "assert epoch_samples == 100, f\"Should process 100 samples, got {epoch_samples}\"\n",
    "expected_batches = (100 + 16 - 1) // 16\n",
    "assert epoch_batches == expected_batches, f\"Should have {expected_batches} batches, got {epoch_batches}\"\n",
    "\n",
    "print(\"✅ Complete data pipeline integration tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28295d58",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 🎯 Module Summary\n",
    "\n",
    "Congratulations! You've successfully implemented the core components of data loading systems:\n",
    "\n",
    "### What You've Accomplished\n",
    "✅ **Dataset Abstract Class**: The foundation interface for all data loading  \n",
    "✅ **DataLoader Implementation**: Efficient batching and iteration over datasets  \n",
    "✅ **SimpleDataset Example**: Concrete implementation showing the Dataset pattern  \n",
    "✅ **Complete Data Pipeline**: End-to-end data loading for neural network training  \n",
    "✅ **Systems Thinking**: Understanding memory efficiency, batching, and I/O optimization  \n",
    "\n",
    "### Key Concepts You've Learned\n",
    "- **Dataset pattern**: Abstract interface for consistent data access\n",
    "- **DataLoader pattern**: Efficient batching and iteration for training\n",
    "- **Memory efficiency**: Loading data on-demand rather than all at once\n",
    "- **Batching strategies**: Grouping samples for efficient GPU computation\n",
    "- **Shuffling**: Randomizing data order to prevent overfitting\n",
    "\n",
    "### Mathematical Foundations\n",
    "- **Batch processing**: Vectorized operations on multiple samples\n",
    "- **Memory management**: Handling datasets larger than available RAM\n",
    "- **I/O optimization**: Minimizing disk reads and memory allocation\n",
    "- **Stochastic sampling**: Random shuffling for better generalization\n",
    "\n",
    "### Real-World Applications\n",
    "- **Computer vision**: Loading image datasets like CIFAR-10, ImageNet\n",
    "- **Natural language processing**: Loading text datasets with tokenization\n",
    "- **Tabular data**: Loading CSV files and database records\n",
    "- **Audio processing**: Loading and preprocessing audio files\n",
    "- **Time series**: Loading sequential data with proper windowing\n",
    "\n",
    "### Connection to Production Systems\n",
    "- **PyTorch**: Your Dataset and DataLoader mirror `torch.utils.data`\n",
    "- **TensorFlow**: Similar concepts in `tf.data.Dataset`\n",
    "- **JAX**: Custom data loading with efficient batching\n",
    "- **MLOps**: Data pipelines are critical for production ML systems\n",
    "\n",
    "### Next Steps\n",
    "1. **Export your code**: `tito package nbdev --export 06_dataloader`\n",
    "2. **Test your implementation**: `tito module test 06_dataloader`\n",
    "3. **Use your data loading**: \n",
    "   ```python\n",
    "   from tinytorch.core.dataloader import Dataset, DataLoader, SimpleDataset\n",
    "   \n",
    "   # Create dataset and dataloader\n",
    "   dataset = SimpleDataset(size=1000, num_features=10, num_classes=3)\n",
    "   dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "   \n",
    "   # Training loop\n",
    "   for batch_data, batch_labels in dataloader:\n",
    "       # Train your network on batch_data, batch_labels\n",
    "       pass\n",
    "   ```\n",
    "4. **Build real datasets**: Extend Dataset for your specific data types\n",
    "5. **Optimize performance**: Add caching, parallel loading, and preprocessing\n",
    "\n",
    "**Ready for the next challenge?** You now have all the core components to build complete machine learning systems: tensors, activations, layers, networks, and data loading. The next modules will focus on training (autograd, optimizers) and advanced topics!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
