{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70e293d5",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Module 05: Autograd - Awakening the Gradient Engine\n",
    "\n",
    "Welcome to Module 05! Today you'll bring gradients to life and unlock automatic differentiation.\n",
    "\n",
    "## 🔗 Prerequisites & Progress\n",
    "**You've Built**: Tensor operations, activations, layers, and loss functions\n",
    "**You'll Build**: The autograd system that computes gradients automatically\n",
    "**You'll Enable**: Learning! Training! The ability to optimize neural networks!\n",
    "\n",
    "**Connection Map**:\n",
    "```\n",
    "Modules 01-04 → Autograd → Training (Module 06-07)\n",
    "(forward pass) (backward pass) (learning loops)\n",
    "```\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this module, you will:\n",
    "1. Implement the backward() method for Tensor to enable gradient computation\n",
    "2. Create a Function base class for operation tracking\n",
    "3. Build computation graphs for automatic differentiation\n",
    "4. Test gradient correctness and chain rule implementation\n",
    "\n",
    "**CRITICAL**: This module enhances the existing Tensor class by implementing its dormant gradient features!\n",
    "\n",
    "Let's awaken the gradient engine!\n",
    "\n",
    "## 📦 Where This Code Lives in the Final Package\n",
    "\n",
    "**Learning Side:** You work in modules/05_autograd/autograd_dev.py\n",
    "**Building Side:** Code exports to tinytorch.core.autograd\n",
    "\n",
    "```python\n",
    "# Final package structure:\n",
    "from tinytorch.core.autograd import Function  # This module - gradient computation\n",
    "from tinytorch.core.tensor import Tensor  # Enhanced with gradients from this module\n",
    "```\n",
    "\n",
    "**Why this matters:**\n",
    "- **Learning:** Complete autograd system enabling automatic differentiation\n",
    "- **Production:** PyTorch-style computational graph and backward pass\n",
    "- **Consistency:** All gradient operations in core.autograd\n",
    "- **Integration:** Enhances existing Tensor without breaking anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8ef1a7",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "imports",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#| default_exp core.autograd\n",
    "\n",
    "import numpy as np\n",
    "from typing import List, Optional, Callable\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Import the modern Tensor class\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__), '..', '01_tensor'))\n",
    "from tensor_dev import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56abee5",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 1. Introduction: What is Automatic Differentiation?\n",
    "\n",
    "Automatic differentiation (autograd) is the magic that makes neural networks learn. Instead of manually computing gradients for every parameter, autograd tracks operations and automatically computes gradients via the chain rule.\n",
    "\n",
    "### The Challenge\n",
    "In Module 04, you implemented a loss function. To train a model, you need:\n",
    "```\n",
    "Loss = f(W₃, f(W₂, f(W₁, x)))\n",
    "∂Loss/∂W₁ = ?  ∂Loss/∂W₂ = ?  ∂Loss/∂W₃ = ?\n",
    "```\n",
    "\n",
    "Manual gradient computation becomes impossible for complex models with millions of parameters.\n",
    "\n",
    "### The Solution: Computational Graphs\n",
    "```\n",
    "Forward Pass:  x → Linear₁ → ReLU → Linear₂ → Loss\n",
    "Backward Pass: ∇x ← ∇Linear₁ ← ∇ReLU ← ∇Linear₂ ← ∇Loss\n",
    "```\n",
    "\n",
    "**Complete Autograd Process Visualization:**\n",
    "```\n",
    "┌─ FORWARD PASS ──────────────────────────────────────────────┐\n",
    "│                                                             │\n",
    "│ x ──┬── W₁ ──┐                                              │\n",
    "│     │        ├──[Linear₁]──→ z₁ ──[ReLU]──→ a₁ ──┬── W₂ ──┐ │\n",
    "│     └── b₁ ──┘                               │        ├─→ Loss\n",
    "│                                              └── b₂ ──┘ │\n",
    "│                                                             │\n",
    "└─ COMPUTATION GRAPH BUILT ──────────────────────────────────┘\n",
    "                             │\n",
    "                             ▼\n",
    "┌─ BACKWARD PASS ─────────────────────────────────────────────┐\n",
    "│                                                             │\n",
    "│∇x ←┬← ∇W₁ ←┐                                               │\n",
    "│    │       ├←[Linear₁]←─ ∇z₁ ←[ReLU]← ∇a₁ ←┬← ∇W₂ ←┐      │\n",
    "│    └← ∇b₁ ←┘                             │       ├← ∇Loss  │\n",
    "│                                          └← ∇b₂ ←┘      │\n",
    "│                                                             │\n",
    "└─ GRADIENTS COMPUTED ───────────────────────────────────────┘\n",
    "\n",
    "Key Insight: Each [operation] stores how to compute its backward pass.\n",
    "The chain rule automatically flows gradients through the entire graph.\n",
    "```\n",
    "\n",
    "Each operation records how to compute its backward pass. The chain rule connects them all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4c7c87",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 2. Foundations: The Chain Rule in Action\n",
    "\n",
    "### Mathematical Foundation\n",
    "For composite functions: f(g(x)), the derivative is:\n",
    "```\n",
    "df/dx = (df/dg) × (dg/dx)\n",
    "```\n",
    "\n",
    "### Computational Graph Example\n",
    "```\n",
    "Simple computation: L = (x * y + 5)²\n",
    "\n",
    "Forward Pass:\n",
    "  x=2 ──┐\n",
    "        ├──[×]──→ z=6 ──[+5]──→ w=11 ──[²]──→ L=121\n",
    "  y=3 ──┘\n",
    "\n",
    "Backward Pass (Chain Rule in Action):\n",
    "  ∂L/∂x = ∂L/∂w × ∂w/∂z × ∂z/∂x\n",
    "        = 2w  ×  1  ×  y\n",
    "        = 2(11) × 1 × 3 = 66\n",
    "\n",
    "  ∂L/∂y = ∂L/∂w × ∂w/∂z × ∂z/∂y\n",
    "        = 2w  ×  1  ×  x\n",
    "        = 2(11) × 1 × 2 = 44\n",
    "\n",
    "Gradient Flow Visualization:\n",
    "  ∇x=66 ←──┐\n",
    "           ├──[×]←── ∇z=22 ←──[+]←── ∇w=22 ←──[²]←── ∇L=1\n",
    "  ∇y=44 ←──┘\n",
    "```\n",
    "\n",
    "### Memory Layout During Backpropagation\n",
    "```\n",
    "Computation Graph Memory Structure:\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│ Forward Pass (stored for backward)                      │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│ Node 1: x=2 (leaf, requires_grad=True) │ grad: None→66  │\n",
    "│ Node 2: y=3 (leaf, requires_grad=True) │ grad: None→44  │\n",
    "│ Node 3: z=x*y (MulFunction)            │ grad: None→22  │\n",
    "│         saved: (x=2, y=3)              │ inputs: [x,y]  │\n",
    "│ Node 4: w=z+5 (AddFunction)            │ grad: None→22  │\n",
    "│         saved: (z=6, 5)                │ inputs: [z]    │\n",
    "│ Node 5: L=w² (PowFunction)             │ grad: 1        │\n",
    "│         saved: (w=11)                  │ inputs: [w]    │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "\n",
    "Memory Cost: 2× parameters (data + gradients) + graph overhead\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca81534",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 3. Implementation: Building the Autograd Engine\n",
    "\n",
    "Let's implement the autograd system step by step. We'll enhance the existing Tensor class and create supporting infrastructure.\n",
    "\n",
    "### The Function Architecture\n",
    "\n",
    "Every differentiable operation needs two things:\n",
    "1. **Forward pass**: Compute the result\n",
    "2. **Backward pass**: Compute gradients for inputs\n",
    "\n",
    "```\n",
    "Function Class Design:\n",
    "┌─────────────────────────────────────┐\n",
    "│ Function (Base Class)               │\n",
    "├─────────────────────────────────────┤\n",
    "│ • save_for_backward()  ← Store data │\n",
    "│ • forward()           ← Compute     │\n",
    "│ • backward()          ← Gradients   │\n",
    "└─────────────────────────────────────┘\n",
    "          ↑\n",
    "    ┌─────┴─────┬─────────┬──────────┐\n",
    "    │           │         │          │\n",
    "┌───▼────┐ ┌────▼───┐ ┌───▼────┐ ┌───▼────┐\n",
    "│  Add   │ │  Mul   │ │ Matmul │ │  Sum   │\n",
    "│Function│ │Function│ │Function│ │Function│\n",
    "└────────┘ └────────┘ └────────┘ └────────┘\n",
    "```\n",
    "\n",
    "Each operation inherits from Function and implements specific gradient rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2374a63",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### Function Base Class - The Foundation of Autograd\n",
    "\n",
    "The Function class is the foundation that makes autograd possible. Every differentiable operation (addition, multiplication, etc.) inherits from this class.\n",
    "\n",
    "**Why Functions Matter:**\n",
    "- They remember inputs needed for backward pass\n",
    "- They implement forward computation\n",
    "- They implement gradient computation via backward()\n",
    "- They connect to form computation graphs\n",
    "\n",
    "**The Pattern:**\n",
    "```\n",
    "Forward:  inputs → Function.forward() → output\n",
    "Backward: grad_output → Function.backward() → grad_inputs\n",
    "```\n",
    "\n",
    "This pattern enables the chain rule to flow gradients through complex computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e83fb5",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "function-base",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class Function:\n",
    "    \"\"\"\n",
    "    Base class for differentiable operations.\n",
    "\n",
    "    Every operation that needs gradients (add, multiply, matmul, etc.)\n",
    "    will inherit from this class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize function with empty input tracking.\"\"\"\n",
    "        self.inputs = []\n",
    "        self.saved_tensors = []\n",
    "\n",
    "    def save_for_backward(self, *tensors):\n",
    "        \"\"\"\n",
    "        Save tensors needed for backward pass.\n",
    "\n",
    "        TODO: Store tensors that backward() will need\n",
    "\n",
    "        EXAMPLE:\n",
    "        In multiplication: y = a * b\n",
    "        We need to save 'a' and 'b' because:\n",
    "        ∂y/∂a = b and ∂y/∂b = a\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        self.saved_tensors = tensors\n",
    "        ### END SOLUTION\n",
    "\n",
    "    def forward(self, *inputs):\n",
    "        \"\"\"\n",
    "        Compute forward pass.\n",
    "\n",
    "        TODO: Implement in subclasses\n",
    "        This should be overridden by each specific operation.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Forward pass must be implemented by subclasses\")\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Compute backward pass.\n",
    "\n",
    "        TODO: Implement in subclasses\n",
    "\n",
    "        APPROACH:\n",
    "        1. Take gradient flowing backward (grad_output)\n",
    "        2. Apply chain rule with local gradients\n",
    "        3. Return gradients for inputs\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Backward pass must be implemented by subclasses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d390955",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### 🔬 Unit Test: Function Base Class\n",
    "This test validates our Function base class works correctly.\n",
    "**What we're testing**: Function initialization and interface\n",
    "**Why it matters**: Foundation for all differentiable operations\n",
    "**Expected**: Proper initialization and save_for_backward functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2df72b",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-function-base",
     "locked": true,
     "points": 10
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_function_base():\n",
    "    \"\"\"🔬 Test Function base class.\"\"\"\n",
    "    print(\"🔬 Unit Test: Function Base Class...\")\n",
    "\n",
    "    # Test initialization\n",
    "    func = Function()\n",
    "    assert func.inputs == []\n",
    "    assert func.saved_tensors == []\n",
    "\n",
    "    # Test save_for_backward\n",
    "    tensor1 = Tensor([1, 2, 3])\n",
    "    tensor2 = Tensor([4, 5, 6])\n",
    "    func.save_for_backward(tensor1, tensor2)\n",
    "    assert len(func.saved_tensors) == 2\n",
    "    assert func.saved_tensors[0] is tensor1\n",
    "    assert func.saved_tensors[1] is tensor2\n",
    "\n",
    "    print(\"✅ Function base class works correctly!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_unit_function_base()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e62bea",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Operation Functions - Implementing Gradient Rules\n",
    "\n",
    "Now we'll implement specific operations that compute gradients correctly. Each operation has mathematical rules for how gradients flow backward.\n",
    "\n",
    "**Gradient Flow Visualization:**\n",
    "```\n",
    "Addition (z = a + b):\n",
    "    ∂z/∂a = 1    ∂z/∂b = 1\n",
    "\n",
    "    a ──┐           grad_a ←──┐\n",
    "        ├─[+]─→ z          ├─[+]←── grad_z\n",
    "    b ──┘           grad_b ←──┘\n",
    "\n",
    "Multiplication (z = a * b):\n",
    "    ∂z/∂a = b    ∂z/∂b = a\n",
    "\n",
    "    a ──┐           grad_a = grad_z * b\n",
    "        ├─[×]─→ z\n",
    "    b ──┘           grad_b = grad_z * a\n",
    "\n",
    "Matrix Multiplication (Z = A @ B):\n",
    "    ∂Z/∂A = grad_Z @ B.T\n",
    "    ∂Z/∂B = A.T @ grad_Z\n",
    "\n",
    "    A ──┐           grad_A = grad_Z @ B.T\n",
    "        ├─[@]─→ Z\n",
    "    B ──┘           grad_B = A.T @ grad_Z\n",
    "```\n",
    "\n",
    "Each operation stores the inputs it needs for computing gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659f0192",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### AddFunction - Gradient Rules for Addition\n",
    "\n",
    "Addition is the simplest gradient operation: gradients flow unchanged to both inputs.\n",
    "\n",
    "**Mathematical Principle:**\n",
    "```\n",
    "If z = a + b, then:\n",
    "∂z/∂a = 1  (gradient of z w.r.t. a)\n",
    "∂z/∂b = 1  (gradient of z w.r.t. b)\n",
    "\n",
    "By chain rule:\n",
    "∂Loss/∂a = ∂Loss/∂z × ∂z/∂a = grad_output × 1 = grad_output\n",
    "∂Loss/∂b = ∂Loss/∂z × ∂z/∂b = grad_output × 1 = grad_output\n",
    "```\n",
    "\n",
    "**Broadcasting Challenge:**\n",
    "When tensors have different shapes, NumPy broadcasts automatically in forward pass,\n",
    "but we must \"unbroadcast\" gradients in backward pass to match original shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db506b35",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "operation-functions",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class AddFunction(Function):\n",
    "    \"\"\"Gradient computation for tensor addition.\"\"\"\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        \"\"\"\n",
    "        Forward pass: compute a + b\n",
    "\n",
    "        TODO: Implement addition forward pass\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        # Save inputs for backward pass (shapes might be needed)\n",
    "        self.save_for_backward(a, b)\n",
    "\n",
    "        # Compute addition\n",
    "        if isinstance(b, Tensor):\n",
    "            result = a.data + b.data\n",
    "        else:\n",
    "            result = a.data + b\n",
    "\n",
    "        return result\n",
    "        ### END SOLUTION\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients for addition\n",
    "\n",
    "        TODO: Implement addition backward pass\n",
    "\n",
    "        MATH: If z = a + b, then ∂z/∂a = 1 and ∂z/∂b = 1\n",
    "        So: ∂loss/∂a = ∂loss/∂z × 1 = grad_output\n",
    "            ∂loss/∂b = ∂loss/∂z × 1 = grad_output\n",
    "\n",
    "        BROADCASTING CHALLENGE:\n",
    "        If shapes differ, we need to sum gradients appropriately\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        a, b = self.saved_tensors\n",
    "\n",
    "        # Gradient for 'a' - same shape as grad_output initially\n",
    "        grad_a = grad_output\n",
    "\n",
    "        # Gradient for 'b' - same as grad_output initially\n",
    "        grad_b = grad_output\n",
    "\n",
    "        # Handle broadcasting: if original shapes differed, sum gradients\n",
    "        # For tensor + scalar case\n",
    "        if not isinstance(b, Tensor):\n",
    "            grad_b = np.sum(grad_output)\n",
    "        else:\n",
    "            # Handle shape differences due to broadcasting\n",
    "            if a.shape != grad_output.shape:\n",
    "                # Sum out added dimensions and squeeze\n",
    "                grad_a = _handle_broadcasting_backward(grad_a, a.shape)\n",
    "\n",
    "            if b.shape != grad_output.shape:\n",
    "                grad_b = _handle_broadcasting_backward(grad_b, b.shape)\n",
    "\n",
    "        return grad_a, grad_b\n",
    "        ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e99e3e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "source": [
    "\"\"\"\n",
    "## MulFunction - Gradient Rules for Element-wise Multiplication\n",
    "\n",
    "Element-wise multiplication follows the product rule of calculus.\n",
    "\n",
    "**Mathematical Principle:**\n",
    "```\n",
    "If z = a * b (element-wise), then:\n",
    "∂z/∂a = b  (gradient w.r.t. a equals the other input)\n",
    "∂z/∂b = a  (gradient w.r.t. b equals the other input)\n",
    "\n",
    "By chain rule:\n",
    "∂Loss/∂a = grad_output * b\n",
    "∂Loss/∂b = grad_output * a\n",
    "```\n",
    "\n",
    "**Visual Example:**\n",
    "```\n",
    "Forward:  a=[2,3] * b=[4,5] = z=[8,15]\n",
    "Backward: grad_z=[1,1]\n",
    "          grad_a = grad_z * b = [1,1] * [4,5] = [4,5]\n",
    "          grad_b = grad_z * a = [1,1] * [2,3] = [2,3]\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "class MulFunction(Function):\n",
    "    \"\"\"Gradient computation for tensor multiplication.\"\"\"\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        \"\"\"\n",
    "        Forward pass: compute a * b (element-wise)\n",
    "\n",
    "        TODO: Implement multiplication forward pass\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        self.save_for_backward(a, b)\n",
    "\n",
    "        if isinstance(b, Tensor):\n",
    "            result = a.data * b.data\n",
    "        else:\n",
    "            result = a.data * b\n",
    "\n",
    "        return result\n",
    "        ### END SOLUTION\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients for multiplication\n",
    "\n",
    "        TODO: Implement multiplication backward pass\n",
    "\n",
    "        MATH: If z = a * b, then:\n",
    "        ∂z/∂a = b and ∂z/∂b = a\n",
    "        So: ∂loss/∂a = grad_output * b\n",
    "            ∂loss/∂b = grad_output * a\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        a, b = self.saved_tensors\n",
    "\n",
    "        if isinstance(b, Tensor):\n",
    "            grad_a = grad_output * b.data\n",
    "            grad_b = grad_output * a.data\n",
    "\n",
    "            # Handle broadcasting\n",
    "            if a.shape != grad_output.shape:\n",
    "                grad_a = _handle_broadcasting_backward(grad_a, a.shape)\n",
    "            if b.shape != grad_output.shape:\n",
    "                grad_b = _handle_broadcasting_backward(grad_b, b.shape)\n",
    "        else:\n",
    "            # b is a scalar\n",
    "            grad_a = grad_output * b\n",
    "            grad_b = np.sum(grad_output * a.data)\n",
    "\n",
    "        return grad_a, grad_b\n",
    "        ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc612e2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "source": [
    "\"\"\"\n",
    "## MatmulFunction - Gradient Rules for Matrix Multiplication\n",
    "\n",
    "Matrix multiplication has more complex gradient rules based on matrix calculus.\n",
    "\n",
    "**Mathematical Principle:**\n",
    "```\n",
    "If Z = A @ B (matrix multiplication), then:\n",
    "∂Z/∂A = grad_Z @ B.T\n",
    "∂Z/∂B = A.T @ grad_Z\n",
    "```\n",
    "\n",
    "**Why These Rules Work:**\n",
    "```\n",
    "For element Z[i,j] = Σ_k A[i,k] * B[k,j]\n",
    "∂Z[i,j]/∂A[i,k] = B[k,j]  ← This gives us grad_Z @ B.T\n",
    "∂Z[i,j]/∂B[k,j] = A[i,k]  ← This gives us A.T @ grad_Z\n",
    "```\n",
    "\n",
    "**Dimension Analysis:**\n",
    "```\n",
    "Forward:  A(m×k) @ B(k×n) = Z(m×n)\n",
    "Backward: grad_Z(m×n) @ B.T(n×k) = grad_A(m×k) ✓\n",
    "          A.T(k×m) @ grad_Z(m×n) = grad_B(k×n) ✓\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "class MatmulFunction(Function):\n",
    "    \"\"\"Gradient computation for matrix multiplication.\"\"\"\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        \"\"\"\n",
    "        Forward pass: compute a @ b (matrix multiplication)\n",
    "\n",
    "        TODO: Implement matmul forward pass\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        self.save_for_backward(a, b)\n",
    "        result = np.dot(a.data, b.data)\n",
    "        return result\n",
    "        ### END SOLUTION\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients for matrix multiplication\n",
    "\n",
    "        TODO: Implement matmul backward pass\n",
    "\n",
    "        MATH: If Z = A @ B, then:\n",
    "        ∂Z/∂A = grad_output @ B.T\n",
    "        ∂Z/∂B = A.T @ grad_output\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        a, b = self.saved_tensors\n",
    "\n",
    "        # Gradient w.r.t. a: grad_output @ b.T\n",
    "        grad_a = np.dot(grad_output, b.data.T)\n",
    "\n",
    "        # Gradient w.r.t. b: a.T @ grad_output\n",
    "        grad_b = np.dot(a.data.T, grad_output)\n",
    "\n",
    "        return grad_a, grad_b\n",
    "        ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a21456b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "source": [
    "\"\"\"\n",
    "## SumFunction - Gradient Rules for Reduction Operations\n",
    "\n",
    "Sum operations reduce tensor dimensions, so gradients must be broadcast back.\n",
    "\n",
    "**Mathematical Principle:**\n",
    "```\n",
    "If z = sum(a), then ∂z/∂a[i] = 1 for all i\n",
    "Gradient is broadcasted from scalar result back to input shape.\n",
    "```\n",
    "\n",
    "**Gradient Broadcasting Examples:**\n",
    "```\n",
    "Case 1: Full sum\n",
    "  Forward:  a=[1,2,3] → sum() → z=6 (scalar)\n",
    "  Backward: grad_z=1 → broadcast → grad_a=[1,1,1]\n",
    "\n",
    "Case 2: Axis sum\n",
    "  Forward:  a=[[1,2],[3,4]] → sum(axis=0) → z=[4,6]\n",
    "  Backward: grad_z=[1,1] → broadcast → grad_a=[[1,1],[1,1]]\n",
    "\n",
    "Case 3: Keepdims\n",
    "  Forward:  a=[[1,2],[3,4]] → sum(axis=0,keepdims=True) → z=[[4,6]]\n",
    "  Backward: grad_z=[[1,1]] → broadcast → grad_a=[[1,1],[1,1]]\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "class SumFunction(Function):\n",
    "    \"\"\"Gradient computation for tensor sum.\"\"\"\n",
    "\n",
    "    def forward(self, a, axis=None, keepdims=False):\n",
    "        \"\"\"\n",
    "        Forward pass: compute tensor sum\n",
    "\n",
    "        TODO: Implement sum forward pass\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        self.save_for_backward(a)\n",
    "        self.axis = axis\n",
    "        self.keepdims = keepdims\n",
    "        self.input_shape = a.shape\n",
    "\n",
    "        result = np.sum(a.data, axis=axis, keepdims=keepdims)\n",
    "        return result\n",
    "        ### END SOLUTION\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients for sum\n",
    "\n",
    "        TODO: Implement sum backward pass\n",
    "\n",
    "        MATH: If z = sum(a), then ∂z/∂a[i] = 1 for all i\n",
    "        So gradient is broadcast back to original shape\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        # Sum distributes gradient to all input elements\n",
    "        # Need to broadcast grad_output back to input shape\n",
    "\n",
    "        if self.axis is None:\n",
    "            # Summed all elements - broadcast scalar back to input shape\n",
    "            grad_a = np.full(self.input_shape, grad_output)\n",
    "        else:\n",
    "            # Summed along specific axis - need to broadcast properly\n",
    "            grad_a = grad_output\n",
    "\n",
    "            # If keepdims=False, we need to expand the summed dimensions\n",
    "            if not self.keepdims:\n",
    "                if isinstance(self.axis, int):\n",
    "                    grad_a = np.expand_dims(grad_a, self.axis)\n",
    "                else:\n",
    "                    for ax in sorted(self.axis):\n",
    "                        grad_a = np.expand_dims(grad_a, ax)\n",
    "\n",
    "            # Broadcast to input shape\n",
    "            grad_a = np.broadcast_to(grad_a, self.input_shape)\n",
    "\n",
    "        return grad_a\n",
    "        ### END SOLUTION\n",
    "\n",
    "def _handle_broadcasting_backward(grad, target_shape):\n",
    "    \"\"\"\n",
    "    Helper function to handle gradient broadcasting.\n",
    "\n",
    "    When forward pass used broadcasting, we need to sum gradients\n",
    "    back to the original tensor's shape.\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    # Start with the gradient\n",
    "    result = grad\n",
    "\n",
    "    # Sum out dimensions that were broadcasted (added dimensions)\n",
    "    # If target has fewer dimensions, sum out the leading dimensions\n",
    "    while len(result.shape) > len(target_shape):\n",
    "        result = np.sum(result, axis=0)\n",
    "\n",
    "    # For dimensions that were size 1 in target but expanded in grad\n",
    "    for i, (grad_dim, target_dim) in enumerate(zip(result.shape, target_shape)):\n",
    "        if target_dim == 1 and grad_dim > 1:\n",
    "            result = np.sum(result, axis=i, keepdims=True)\n",
    "\n",
    "    return result\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b0c564",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### 🔬 Unit Test: Operation Functions\n",
    "This test validates our operation functions compute gradients correctly.\n",
    "**What we're testing**: Forward and backward passes for each operation\n",
    "**Why it matters**: These are the building blocks of autograd\n",
    "**Expected**: Correct gradients that satisfy mathematical definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534068f3",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-operation-functions",
     "locked": true,
     "points": 15
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_operation_functions():\n",
    "    \"\"\"🔬 Test operation functions.\"\"\"\n",
    "    print(\"🔬 Unit Test: Operation Functions...\")\n",
    "\n",
    "    # Test AddFunction\n",
    "    add_func = AddFunction()\n",
    "    a = Tensor([1, 2, 3])\n",
    "    b = Tensor([4, 5, 6])\n",
    "    result = add_func.forward(a, b)\n",
    "    expected = np.array([5, 7, 9])\n",
    "    assert np.allclose(result, expected)\n",
    "\n",
    "    grad_output = np.array([1, 1, 1])\n",
    "    grad_a, grad_b = add_func.backward(grad_output)\n",
    "    assert np.allclose(grad_a, grad_output)\n",
    "    assert np.allclose(grad_b, grad_output)\n",
    "\n",
    "    # Test MulFunction\n",
    "    mul_func = MulFunction()\n",
    "    result = mul_func.forward(a, b)\n",
    "    expected = np.array([4, 10, 18])\n",
    "    assert np.allclose(result, expected)\n",
    "\n",
    "    grad_a, grad_b = mul_func.backward(grad_output)\n",
    "    assert np.allclose(grad_a, b.data)  # grad w.r.t a = b\n",
    "    assert np.allclose(grad_b, a.data)  # grad w.r.t b = a\n",
    "\n",
    "    # Test MatmulFunction\n",
    "    matmul_func = MatmulFunction()\n",
    "    a_mat = Tensor([[1, 2], [3, 4]])\n",
    "    b_mat = Tensor([[5, 6], [7, 8]])\n",
    "    result = matmul_func.forward(a_mat, b_mat)\n",
    "    expected = np.array([[19, 22], [43, 50]])\n",
    "    assert np.allclose(result, expected)\n",
    "\n",
    "    grad_output = np.ones((2, 2))\n",
    "    grad_a, grad_b = matmul_func.backward(grad_output)\n",
    "    assert grad_a.shape == a_mat.shape\n",
    "    assert grad_b.shape == b_mat.shape\n",
    "\n",
    "    print(\"✅ Operation functions work correctly!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_unit_operation_functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08717fc2",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Enhancing Tensor with Autograd Capabilities\n",
    "\n",
    "Now we'll enhance the existing Tensor class to use these gradient functions and build computation graphs automatically.\n",
    "\n",
    "**Computation Graph Formation:**\n",
    "```\n",
    "Before Autograd:             After Autograd:\n",
    "  x → operation → y           x → [Function] → y\n",
    "                                     ↓\n",
    "                               Stores operation\n",
    "                               for backward pass\n",
    "```\n",
    "\n",
    "**The Enhancement Strategy:**\n",
    "1. **Add backward() method** - Triggers gradient computation\n",
    "2. **Enhance operations** - Replace simple ops with gradient-tracking versions\n",
    "3. **Track computation graphs** - Each tensor remembers how it was created\n",
    "4. **Maintain compatibility** - All existing code continues to work\n",
    "\n",
    "**Critical Design Decision:**\n",
    "We enhance the EXISTING Tensor class rather than creating a new one.\n",
    "This means:\n",
    "- ✅ All previous modules continue working unchanged\n",
    "- ✅ No import changes needed\n",
    "- ✅ Gradients are \"opt-in\" via requires_grad=True\n",
    "- ✅ No confusion between Tensor types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e7d03f",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### The Backward Pass Algorithm\n",
    "\n",
    "The backward() method implements reverse-mode automatic differentiation.\n",
    "\n",
    "**Algorithm Visualization:**\n",
    "```\n",
    "Computation Graph (Forward):\n",
    "  x₁ ──┐\n",
    "       ├─[op₁]── z₁ ──┐\n",
    "  x₂ ──┘              ├─[op₂]── y\n",
    "  x₃ ──────[op₃]── z₂ ──┘\n",
    "\n",
    "Gradient Flow (Backward):\n",
    "  ∇x₁ ←──┐\n",
    "         ├─[op₁.backward()]← ∇z₁ ←──┐\n",
    "  ∇x₂ ←──┘                      ├─[op₂.backward()]← ∇y\n",
    "  ∇x₃ ←────[op₃.backward()]← ∇z₂ ←──┘\n",
    "```\n",
    "\n",
    "**Backward Pass Steps:**\n",
    "1. Start from output tensor (∇y = 1)\n",
    "2. For each operation in reverse order:\n",
    "   - Apply chain rule: ∇inputs = operation.backward(∇output)\n",
    "   - Accumulate gradients (handle shared variables)\n",
    "   - Continue to parent tensors\n",
    "3. Gradients accumulate in tensor.grad attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f8911d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "tensor-enhancements",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def implement_tensor_backward_method():\n",
    "    \"\"\"\n",
    "    Implement the backward method for the Tensor class.\n",
    "\n",
    "    CRITICAL: We modify the Tensor class in place to activate gradient features.\n",
    "    The dormant features are now brought to life!\n",
    "    \"\"\"\n",
    "\n",
    "    def backward_implementation(self, gradient=None):\n",
    "        \"\"\"\n",
    "        Compute gradients for this tensor and all tensors in its computation graph.\n",
    "\n",
    "        TODO: Implement the backward pass\n",
    "\n",
    "        APPROACH:\n",
    "        1. Check if this tensor requires gradients\n",
    "        2. Initialize gradient if starting point\n",
    "        3. Traverse computation graph backwards\n",
    "        4. Apply chain rule at each step\n",
    "\n",
    "        EXAMPLE:\n",
    "        >>> x = Tensor([2.0], requires_grad=True)\n",
    "        >>> y = x * 3\n",
    "        >>> y.backward()\n",
    "        >>> print(x.grad)  # Should be [3.0]\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        if not self.requires_grad:\n",
    "            return\n",
    "\n",
    "        # Initialize gradient if this is the starting point\n",
    "        if gradient is None:\n",
    "            if self.data.shape == ():\n",
    "                # Scalar tensor\n",
    "                gradient = np.array(1.0)\n",
    "            else:\n",
    "                # Non-scalar: gradient should be ones of same shape\n",
    "                gradient = np.ones_like(self.data)\n",
    "\n",
    "        # Accumulate gradient\n",
    "        if self.grad is None:\n",
    "            self.grad = gradient\n",
    "        else:\n",
    "            self.grad = self.grad + gradient\n",
    "\n",
    "        # If this tensor has a gradient function, propagate backwards\n",
    "        if hasattr(self, 'grad_fn') and self.grad_fn is not None:\n",
    "            grads = self.grad_fn.backward(gradient)\n",
    "\n",
    "            # grads could be a single gradient or tuple of gradients\n",
    "            if not isinstance(grads, tuple):\n",
    "                grads = (grads,)\n",
    "\n",
    "            # Propagate to input tensors\n",
    "            if hasattr(self.grad_fn, 'inputs'):\n",
    "                for tensor, grad in zip(self.grad_fn.inputs, grads):\n",
    "                    if isinstance(tensor, Tensor) and tensor.requires_grad:\n",
    "                        tensor.backward(grad)\n",
    "        ### END SOLUTION\n",
    "\n",
    "    # Replace the placeholder backward method with the real implementation\n",
    "    Tensor.backward = backward_implementation\n",
    "    print(\"🚀 Tensor backward method activated!\")\n",
    "\n",
    "# Activate the backward method\n",
    "implement_tensor_backward_method()\n",
    "\n",
    "def create_gradient_tracking_tensor(data, requires_grad, grad_fn=None, inputs=None):\n",
    "    \"\"\"\n",
    "    Helper function to create tensors with gradient tracking.\n",
    "\n",
    "    This function helps operations create result tensors that properly\n",
    "    track gradients and maintain the computation graph.\n",
    "    \"\"\"\n",
    "    result = Tensor(data, requires_grad=requires_grad)\n",
    "\n",
    "    if requires_grad and grad_fn is not None:\n",
    "        result.grad_fn = grad_fn\n",
    "        if inputs is not None:\n",
    "            grad_fn.inputs = inputs\n",
    "\n",
    "    return result\n",
    "\n",
    "def enhance_tensor_operations():\n",
    "    \"\"\"\n",
    "    Enhance existing Tensor operations to support gradient tracking.\n",
    "\n",
    "    This modifies the existing methods to use gradient-tracking functions\n",
    "    when requires_grad=True.\n",
    "    \"\"\"\n",
    "\n",
    "    # Store original methods\n",
    "    original_add = Tensor.__add__\n",
    "    original_mul = Tensor.__mul__\n",
    "    original_matmul = Tensor.matmul\n",
    "    original_sum = Tensor.sum\n",
    "\n",
    "    def gradient_aware_add(self, other):\n",
    "        \"\"\"\n",
    "        Addition that tracks gradients when needed.\n",
    "        \"\"\"\n",
    "        # Check if gradient tracking is needed\n",
    "        requires_grad = self.requires_grad or (isinstance(other, Tensor) and other.requires_grad)\n",
    "\n",
    "        if requires_grad:\n",
    "            # Use gradient-tracking version\n",
    "            add_func = AddFunction()\n",
    "            result_data = add_func.forward(self, other)\n",
    "            inputs = [self, other] if isinstance(other, Tensor) else [self]\n",
    "            return create_gradient_tracking_tensor(result_data, requires_grad, add_func, inputs)\n",
    "        else:\n",
    "            # Use original method (no gradient tracking)\n",
    "            return original_add(self, other)\n",
    "\n",
    "    def gradient_aware_mul(self, other):\n",
    "        \"\"\"\n",
    "        Multiplication that tracks gradients when needed.\n",
    "        \"\"\"\n",
    "        requires_grad = self.requires_grad or (isinstance(other, Tensor) and other.requires_grad)\n",
    "\n",
    "        if requires_grad:\n",
    "            mul_func = MulFunction()\n",
    "            result_data = mul_func.forward(self, other)\n",
    "            inputs = [self, other] if isinstance(other, Tensor) else [self]\n",
    "            return create_gradient_tracking_tensor(result_data, requires_grad, mul_func, inputs)\n",
    "        else:\n",
    "            return original_mul(self, other)\n",
    "\n",
    "    def gradient_aware_matmul(self, other):\n",
    "        \"\"\"\n",
    "        Matrix multiplication that tracks gradients when needed.\n",
    "        \"\"\"\n",
    "        if not isinstance(other, Tensor):\n",
    "            raise TypeError(f\"Expected Tensor for matrix multiplication, got {type(other)}\")\n",
    "\n",
    "        requires_grad = self.requires_grad or other.requires_grad\n",
    "\n",
    "        if requires_grad:\n",
    "            matmul_func = MatmulFunction()\n",
    "            result_data = matmul_func.forward(self, other)\n",
    "            inputs = [self, other]\n",
    "            return create_gradient_tracking_tensor(result_data, requires_grad, matmul_func, inputs)\n",
    "        else:\n",
    "            return original_matmul(self, other)\n",
    "\n",
    "    def gradient_aware_sum(self, axis=None, keepdims=False):\n",
    "        \"\"\"\n",
    "        Sum that tracks gradients when needed.\n",
    "        \"\"\"\n",
    "        if self.requires_grad:\n",
    "            sum_func = SumFunction()\n",
    "            result_data = sum_func.forward(self, axis, keepdims)\n",
    "            inputs = [self]\n",
    "            return create_gradient_tracking_tensor(result_data, self.requires_grad, sum_func, inputs)\n",
    "        else:\n",
    "            return original_sum(self, axis, keepdims)\n",
    "\n",
    "    # Replace methods with gradient-aware versions\n",
    "    Tensor.__add__ = gradient_aware_add\n",
    "    Tensor.__mul__ = gradient_aware_mul\n",
    "    Tensor.matmul = gradient_aware_matmul\n",
    "    Tensor.sum = gradient_aware_sum\n",
    "\n",
    "    print(\"🚀 Tensor operations enhanced with gradient tracking!\")\n",
    "\n",
    "# Enhance the operations\n",
    "enhance_tensor_operations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae0aa2f",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### 🔬 Unit Test: Tensor Autograd Enhancement\n",
    "This test validates our enhanced Tensor class computes gradients correctly.\n",
    "**What we're testing**: Gradient computation and chain rule implementation\n",
    "**Why it matters**: This is the core of automatic differentiation\n",
    "**Expected**: Correct gradients for various operations and computation graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf2dc78",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-tensor-autograd",
     "locked": true,
     "points": 20
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_tensor_autograd():\n",
    "    \"\"\"🔬 Test Tensor autograd enhancement.\"\"\"\n",
    "    print(\"🔬 Unit Test: Tensor Autograd Enhancement...\")\n",
    "\n",
    "    # Test simple gradient computation\n",
    "    x = Tensor([2.0], requires_grad=True)\n",
    "    y = x * 3\n",
    "    z = y + 1  # z = 3x + 1, so dz/dx = 3\n",
    "\n",
    "    z.backward()\n",
    "    assert np.allclose(x.grad, [3.0]), f\"Expected [3.0], got {x.grad}\"\n",
    "\n",
    "    # Test matrix multiplication gradients\n",
    "    a = Tensor([[1.0, 2.0]], requires_grad=True)  # 1x2\n",
    "    b = Tensor([[3.0], [4.0]], requires_grad=True)  # 2x1\n",
    "    c = a.matmul(b)  # 1x1, result = [[11.0]]\n",
    "\n",
    "    c.backward()\n",
    "    assert np.allclose(a.grad, [[3.0, 4.0]]), f\"Expected [[3.0, 4.0]], got {a.grad}\"\n",
    "    assert np.allclose(b.grad, [[1.0], [2.0]]), f\"Expected [[1.0], [2.0]], got {b.grad}\"\n",
    "\n",
    "    # Test computation graph with multiple operations\n",
    "    x = Tensor([1.0, 2.0], requires_grad=True)\n",
    "    y = x * 2      # y = [2, 4]\n",
    "    z = y.sum()    # z = 6\n",
    "\n",
    "    z.backward()\n",
    "    assert np.allclose(x.grad, [2.0, 2.0]), f\"Expected [2.0, 2.0], got {x.grad}\"\n",
    "\n",
    "    print(\"✅ Tensor autograd enhancement works correctly!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_unit_tensor_autograd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b86a099",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 2
   },
   "source": [
    "## 4. Integration: Building Complex Computation Graphs\n",
    "\n",
    "Let's test how our autograd system handles complex neural network computations.\n",
    "\n",
    "### Complex Computation Graph Example\n",
    "\n",
    "Neural networks create complex computation graphs with shared parameters and multiple paths.\n",
    "\n",
    "**Detailed Neural Network Computation Graph:**\n",
    "```\n",
    "Forward Pass with Function Tracking:\n",
    "                    x (input)\n",
    "                    │ requires_grad=True\n",
    "           ┌────────▼────────┐\n",
    "           │ MatmulFunction  │ stores: (x, W₁)\n",
    "           │   h₁ = x @ W₁   │\n",
    "           └────────┬────────┘\n",
    "                    │ grad_fn=MatmulFunction\n",
    "           ┌────────▼────────┐\n",
    "           │  AddFunction    │ stores: (h₁, b₁)\n",
    "           │  z₁ = h₁ + b₁   │\n",
    "           └────────┬────────┘\n",
    "                    │ grad_fn=AddFunction\n",
    "           ┌────────▼────────┐\n",
    "           │  ReLU (manual)  │ Note: We'll implement\n",
    "           │ a₁ = max(0,z₁)  │ ReLUFunction later\n",
    "           └────────┬────────┘\n",
    "                    │\n",
    "           ┌────────▼────────┐\n",
    "           │ MatmulFunction  │ stores: (a₁, W₂)\n",
    "           │   h₂ = a₁ @ W₂  │\n",
    "           └────────┬────────┘\n",
    "                    │ grad_fn=MatmulFunction\n",
    "           ┌────────▼────────┐\n",
    "           │  AddFunction    │ stores: (h₂, b₂)\n",
    "           │   y = h₂ + b₂   │ (final output)\n",
    "           └─────────────────┘\n",
    "\n",
    "Backward Pass Chain Rule Application:\n",
    "                   ∇x ←─────────────────────────────┐\n",
    "                                                     │\n",
    "    ┌─────────────────────────────────────────────────────────┐\n",
    "    │ MatmulFunction.backward(∇h₁):                           │\n",
    "    │   ∇x = ∇h₁ @ W₁.T                                      │\n",
    "    │   ∇W₁ = x.T @ ∇h₁                                      │\n",
    "    └─────────────────┬───────────────────────────────────────┘\n",
    "                      │\n",
    "    ┌─────────────────▼───────────────────────────────────────┐\n",
    "    │ AddFunction.backward(∇z₁):                              │\n",
    "    │   ∇h₁ = ∇z₁  (gradient passes through unchanged)       │\n",
    "    │   ∇b₁ = ∇z₁                                            │\n",
    "    └─────────────────┬───────────────────────────────────────┘\n",
    "                      │\n",
    "    ┌─────────────────▼───────────────────────────────────────┐\n",
    "    │ Manual ReLU backward:                                   │\n",
    "    │   ∇z₁ = ∇a₁ * (z₁ > 0)  (zero out negative gradients) │\n",
    "    └─────────────────┬───────────────────────────────────────┘\n",
    "                      │\n",
    "    ┌─────────────────▼───────────────────────────────────────┐\n",
    "    │ MatmulFunction.backward(∇h₂):                           │\n",
    "    │   ∇a₁ = ∇h₂ @ W₂.T                                     │\n",
    "    │   ∇W₂ = a₁.T @ ∇h₂                                     │\n",
    "    └─────────────────┬───────────────────────────────────────┘\n",
    "                      │\n",
    "    ┌─────────────────▼───────────────────────────────────────┐\n",
    "    │ AddFunction.backward(∇y):                               │\n",
    "    │   ∇h₂ = ∇y  (gradient passes through unchanged)        │\n",
    "    │   ∇b₂ = ∇y                                             │\n",
    "    └─────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Key Autograd Concepts:**\n",
    "1. **Function Chaining**: Each operation creates a Function that stores inputs\n",
    "2. **Gradient Accumulation**: Multiple paths to a parameter accumulate gradients\n",
    "3. **Automatic Traversal**: backward() walks the graph in reverse topological order\n",
    "4. **Chain Rule**: Local gradients multiply according to calculus rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4231c8",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## 5. Systems Analysis: Memory and Performance of Autograd\n",
    "\n",
    "Understanding the computational and memory costs of automatic differentiation.\n",
    "\n",
    "### Autograd Memory Architecture\n",
    "\n",
    "**Memory Layout Comparison:**\n",
    "```\n",
    "Forward-Only Mode:\n",
    "┌─────────────┐\n",
    "│ Parameters  │ 4N bytes (float32)\n",
    "└─────────────┘\n",
    "\n",
    "Autograd Mode:\n",
    "┌─────────────┐\n",
    "│ Parameters  │ 4N bytes\n",
    "├─────────────┤\n",
    "│ Gradients   │ 4N bytes (additional)\n",
    "├─────────────┤\n",
    "│ Graph Nodes │ Variable overhead\n",
    "├─────────────┤\n",
    "│ Activations │ Depends on graph depth\n",
    "└─────────────┘\n",
    "Total: ~2-3× forward memory\n",
    "```\n",
    "\n",
    "**Computation Graph Memory Growth:**\n",
    "```\n",
    "Shallow Network (3 layers):\n",
    "  Graph: x → W₁ → ReLU → W₂ → ReLU → W₃ → loss\n",
    "  Memory: Base + 3 × (weights + activations)\n",
    "\n",
    "Deep Network (50 layers):\n",
    "  Graph: x → [W₁...W₅₀] → loss\n",
    "  Memory: Base + 50 × (weights + activations)\n",
    "\n",
    "Gradient Checkpointing (optimization):\n",
    "  Store only every K layers, recompute others\n",
    "  Memory: Base + K × (weights + activations)\n",
    "  Time: +20% compute, -80% memory\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c0ef4c",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "analyze-autograd-memory",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def analyze_autograd_memory():\n",
    "    \"\"\"📊 Analyze memory usage of autograd vs no-grad computation.\"\"\"\n",
    "    print(\"📊 Analyzing Autograd Memory Usage...\")\n",
    "\n",
    "    # Test different tensor sizes\n",
    "    sizes = [100, 500, 1000]\n",
    "\n",
    "    for size in sizes:\n",
    "        # Forward-only computation\n",
    "        x_no_grad = Tensor(np.random.randn(size, size), requires_grad=False)\n",
    "        y_no_grad = Tensor(np.random.randn(size, size), requires_grad=False)\n",
    "        z_no_grad = x_no_grad.matmul(y_no_grad)\n",
    "\n",
    "        # Forward + backward computation\n",
    "        x_grad = Tensor(np.random.randn(size, size), requires_grad=True)\n",
    "        y_grad = Tensor(np.random.randn(size, size), requires_grad=True)\n",
    "        z_grad = x_grad.matmul(y_grad)\n",
    "\n",
    "        # Memory analysis\n",
    "        no_grad_elements = x_no_grad.size + y_no_grad.size + z_no_grad.size\n",
    "        grad_elements = x_grad.size + y_grad.size + z_grad.size\n",
    "        grad_storage = x_grad.size + y_grad.size  # For gradients\n",
    "\n",
    "        print(f\"Size {size}×{size}:\")\n",
    "        print(f\"  No grad: {no_grad_elements:,} elements\")\n",
    "        print(f\"  With grad: {grad_elements + grad_storage:,} elements\")\n",
    "        print(f\"  Memory overhead: {grad_storage / no_grad_elements:.1%}\")\n",
    "\n",
    "    print(\"\\n💡 Autograd Memory Pattern:\")\n",
    "    print(\"- Each parameter tensor needs gradient storage (2× memory)\")\n",
    "    print(\"- Computation graph nodes add overhead\")\n",
    "    print(\"- Trade-off: 2× memory for automatic gradients\")\n",
    "\n",
    "# Function defined above, will be called in main block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013bd1d0",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "analyze-gradient-computation",
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def analyze_gradient_computation():\n",
    "    \"\"\"📊 Analyze computational cost of gradient computation.\"\"\"\n",
    "    print(\"📊 Analyzing Gradient Computation Cost...\")\n",
    "\n",
    "    import time\n",
    "\n",
    "    # Test computation times\n",
    "    size = 500\n",
    "    x = Tensor(np.random.randn(size, size), requires_grad=True)\n",
    "    y = Tensor(np.random.randn(size, size), requires_grad=True)\n",
    "\n",
    "    # Time forward pass\n",
    "    start_time = time.time()\n",
    "    z = x.matmul(y)\n",
    "    forward_time = time.time() - start_time\n",
    "\n",
    "    # Time backward pass\n",
    "    start_time = time.time()\n",
    "    z.backward()\n",
    "    backward_time = time.time() - start_time\n",
    "\n",
    "    print(f\"Matrix size: {size}×{size}\")\n",
    "    print(f\"Forward pass: {forward_time:.4f}s\")\n",
    "    print(f\"Backward pass: {backward_time:.4f}s\")\n",
    "    print(f\"Backward/Forward ratio: {backward_time/forward_time:.1f}×\")\n",
    "\n",
    "    print(f\"\\n💡 Gradient Computation Analysis:\")\n",
    "    print(f\"- Forward: O(n³) matrix multiplication\")\n",
    "    print(f\"- Backward: 2× O(n³) operations (gradients for both inputs)\")\n",
    "    print(f\"- Total training cost: ~3× forward-only computation\")\n",
    "\n",
    "# Function defined above, will be called in main block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a68e64",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## 🧪 Module Integration Test\n",
    "\n",
    "Final validation that everything works together correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbf2259",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": true,
     "grade_id": "module-integration",
     "locked": true,
     "points": 25
    }
   },
   "outputs": [],
   "source": [
    "def test_module():\n",
    "    \"\"\"\n",
    "    Comprehensive test of entire module functionality.\n",
    "\n",
    "    This final test runs before module summary to ensure:\n",
    "    - All unit tests pass\n",
    "    - Autograd works for complex computation graphs\n",
    "    - Module is ready for integration with TinyTorch\n",
    "    \"\"\"\n",
    "    print(\"🧪 RUNNING MODULE INTEGRATION TEST\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Run all unit tests\n",
    "    print(\"Running unit tests...\")\n",
    "    test_unit_function_base()\n",
    "    test_unit_operation_functions()\n",
    "    test_unit_tensor_autograd()\n",
    "\n",
    "    print(\"\\nRunning integration scenarios...\")\n",
    "\n",
    "    # Test 1: Multi-layer computation graph\n",
    "    print(\"🔬 Integration Test: Multi-layer Neural Network...\")\n",
    "\n",
    "    # Create a 3-layer computation: x -> Linear -> Linear -> Linear -> loss\n",
    "    x = Tensor([[1.0, 2.0]], requires_grad=True)\n",
    "    W1 = Tensor([[0.5, 0.3, 0.1], [0.2, 0.4, 0.6]], requires_grad=True)\n",
    "    b1 = Tensor([[0.1, 0.2, 0.3]], requires_grad=True)\n",
    "\n",
    "    # First layer\n",
    "    h1 = x.matmul(W1) + b1\n",
    "    assert h1.shape == (1, 3)\n",
    "    assert h1.requires_grad == True\n",
    "\n",
    "    # Second layer\n",
    "    W2 = Tensor([[0.1], [0.2], [0.3]], requires_grad=True)\n",
    "    h2 = h1.matmul(W2)\n",
    "    assert h2.shape == (1, 1)\n",
    "\n",
    "    # Compute simple loss (just square the output for testing)\n",
    "    loss = h2 * h2\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Verify all parameters have gradients\n",
    "    assert x.grad is not None\n",
    "    assert W1.grad is not None\n",
    "    assert b1.grad is not None\n",
    "    assert W2.grad is not None\n",
    "    assert x.grad.shape == x.shape\n",
    "    assert W1.grad.shape == W1.shape\n",
    "\n",
    "    print(\"✅ Multi-layer neural network gradients work!\")\n",
    "\n",
    "    # Test 2: Gradient accumulation\n",
    "    print(\"🔬 Integration Test: Gradient Accumulation...\")\n",
    "\n",
    "    x = Tensor([2.0], requires_grad=True)\n",
    "\n",
    "    # First computation\n",
    "    y1 = x * 3\n",
    "    y1.backward()\n",
    "    first_grad = x.grad.copy()\n",
    "\n",
    "    # Second computation (should accumulate)\n",
    "    y2 = x * 5\n",
    "    y2.backward()\n",
    "\n",
    "    assert np.allclose(x.grad, first_grad + 5.0), \"Gradients should accumulate\"\n",
    "    print(\"✅ Gradient accumulation works!\")\n",
    "\n",
    "    # Test 3: Complex mathematical operations\n",
    "    print(\"🔬 Integration Test: Complex Operations...\")\n",
    "\n",
    "    a = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
    "    b = Tensor([[2.0, 1.0], [1.0, 2.0]], requires_grad=True)\n",
    "\n",
    "    # Complex computation: ((a @ b) + a) * b\n",
    "    temp1 = a.matmul(b)  # Matrix multiplication\n",
    "    temp2 = temp1 + a    # Addition\n",
    "    result = temp2 * b   # Element-wise multiplication\n",
    "    final = result.sum() # Sum reduction\n",
    "\n",
    "    final.backward()\n",
    "\n",
    "    assert a.grad is not None\n",
    "    assert b.grad is not None\n",
    "    assert a.grad.shape == a.shape\n",
    "    assert b.grad.shape == b.shape\n",
    "\n",
    "    print(\"✅ Complex mathematical operations work!\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"🎉 ALL TESTS PASSED! Module ready for export.\")\n",
    "    print(\"Run: tito module complete 05_autograd\")\n",
    "\n",
    "# Test function defined above, will be called in main block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458d3d72",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Run comprehensive module test\n",
    "if __name__ == \"__main__\":\n",
    "    test_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abfe41a",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 🎯 MODULE SUMMARY: Autograd Engine\n",
    "\n",
    "Congratulations! You've built the gradient engine that makes neural networks learn!\n",
    "\n",
    "### Key Accomplishments\n",
    "- Implemented Function base class for tracking differentiable operations\n",
    "- Enhanced existing Tensor class with backward() method (no new classes!)\n",
    "- Built computation graph tracking for automatic differentiation\n",
    "- Created operation functions (Add, Mul, Matmul, Sum) with correct gradients\n",
    "- Tested complex multi-layer computation graphs with gradient propagation\n",
    "- All tests pass ✅ (validated by `test_module()`)\n",
    "\n",
    "### Ready for Next Steps\n",
    "Your autograd implementation enables optimization! The dormant gradient features from Module 01 are now fully active. Every tensor can track gradients, every operation builds computation graphs, and backward() computes gradients automatically.\n",
    "\n",
    "Export with: `tito module complete 05_autograd`\n",
    "\n",
    "**Next**: Module 06 will add optimizers (SGD, Adam) that use these gradients to actually train neural networks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
