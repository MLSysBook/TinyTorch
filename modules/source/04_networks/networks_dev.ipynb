{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b555ed6",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Module 4: Networks - Neural Network Architectures\n",
    "\n",
    "Welcome to the Networks module! This is where we compose layers into complete neural network architectures.\n",
    "\n",
    "## Learning Goals\n",
    "- Understand networks as function composition: `f(x) = layer_n(...layer_2(layer_1(x)))`\n",
    "- Build the Sequential network architecture for composing layers\n",
    "- Create common network patterns like MLPs (Multi-Layer Perceptrons)\n",
    "- Visualize network architectures and understand their capabilities\n",
    "- Master forward pass inference through complete networks\n",
    "\n",
    "## Build → Use → Understand\n",
    "1. **Build**: Sequential networks that compose layers into complete architectures\n",
    "2. **Use**: Create different network patterns and run inference\n",
    "3. **Understand**: How architecture design affects network behavior and capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1922d4e7",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "networks-imports",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| default_exp core.networks\n",
    "\n",
    "#| export\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from typing import List, Union, Optional, Callable\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import FancyBboxPatch, ConnectionPatch\n",
    "import seaborn as sns\n",
    "\n",
    "# Import all the building blocks we need - try package first, then local modules\n",
    "try:\n",
    "    from tinytorch.core.tensor import Tensor\n",
    "    from tinytorch.core.layers import Dense\n",
    "    from tinytorch.core.activations import ReLU, Sigmoid, Tanh, Softmax\n",
    "except ImportError:\n",
    "    # For development, import from local modules\n",
    "    sys.path.append(os.path.join(os.path.dirname(__file__), '..', '01_tensor'))\n",
    "    sys.path.append(os.path.join(os.path.dirname(__file__), '..', '02_activations'))\n",
    "    sys.path.append(os.path.join(os.path.dirname(__file__), '..', '03_layers'))\n",
    "    from tensor_dev import Tensor\n",
    "    from activations_dev import ReLU, Sigmoid, Tanh, Softmax\n",
    "    from layers_dev import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d70f82e",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "networks-setup",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "def _should_show_plots():\n",
    "    \"\"\"Check if we should show plots (disable during testing)\"\"\"\n",
    "    # Check multiple conditions that indicate we're in test mode\n",
    "    is_pytest = (\n",
    "        'pytest' in sys.modules or\n",
    "        'test' in sys.argv or\n",
    "        os.environ.get('PYTEST_CURRENT_TEST') is not None or\n",
    "        any('test' in arg for arg in sys.argv) or\n",
    "        any('pytest' in arg for arg in sys.argv)\n",
    "    )\n",
    "    \n",
    "    # Show plots in development mode (when not in test mode)\n",
    "    return not is_pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc723bcf",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "networks-welcome",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"🔥 TinyTorch Networks Module\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Python version: {sys.version_info.major}.{sys.version_info.minor}\")\n",
    "print(\"Ready to build neural network architectures!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafdd562",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 📦 Where This Code Lives in the Final Package\n",
    "\n",
    "**Learning Side:** You work in `modules/source/04_networks/networks_dev.py`  \n",
    "**Building Side:** Code exports to `tinytorch.core.networks`\n",
    "\n",
    "```python\n",
    "# Final package structure:\n",
    "from tinytorch.core.networks import Sequential, MLP  # Network architectures!\n",
    "from tinytorch.core.layers import Dense, Conv2D  # Building blocks\n",
    "from tinytorch.core.activations import ReLU, Sigmoid, Tanh  # Nonlinearity\n",
    "from tinytorch.core.tensor import Tensor  # Foundation\n",
    "```\n",
    "\n",
    "**Why this matters:**\n",
    "- **Learning:** Focused modules for deep understanding\n",
    "- **Production:** Proper organization like PyTorch's `torch.nn.Sequential`\n",
    "- **Consistency:** All network architectures live together in `core.networks`\n",
    "- **Integration:** Works seamlessly with layers, activations, and tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e712cd64",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 🧠 The Mathematical Foundation of Neural Networks\n",
    "\n",
    "### Function Composition at Scale\n",
    "Neural networks are fundamentally about **function composition**:\n",
    "\n",
    "```\n",
    "f(x) = f_n(f_{n-1}(...f_2(f_1(x))))\n",
    "```\n",
    "\n",
    "Each layer is a function, and the network is the composition of all these functions.\n",
    "\n",
    "### Why Function Composition is Powerful\n",
    "- **Modularity**: Each layer has a specific purpose\n",
    "- **Composability**: Simple functions combine to create complex behaviors\n",
    "- **Universal approximation**: Deep compositions can approximate any function\n",
    "- **Hierarchical learning**: Early layers learn simple features, later layers learn complex patterns\n",
    "\n",
    "### The Architecture Design Space\n",
    "Different arrangements of layers create different capabilities:\n",
    "- **Depth**: More layers → more complex representations\n",
    "- **Width**: More neurons per layer → more capacity per layer\n",
    "- **Connections**: How layers connect affects information flow\n",
    "- **Activation functions**: Add nonlinearity for complex patterns\n",
    "\n",
    "### Connection to Real ML Systems\n",
    "Every framework uses sequential composition:\n",
    "- **PyTorch**: `torch.nn.Sequential([layer1, layer2, layer3])`\n",
    "- **TensorFlow**: `tf.keras.Sequential([layer1, layer2, layer3])`\n",
    "- **JAX**: `jax.nn.Sequential([layer1, layer2, layer3])`\n",
    "- **TinyTorch**: `tinytorch.core.networks.Sequential([layer1, layer2, layer3])` (what we're building!)\n",
    "\n",
    "### Performance and Design Considerations\n",
    "- **Forward pass efficiency**: Sequential computation through layers\n",
    "- **Memory management**: Intermediate activations storage\n",
    "- **Gradient flow**: How information flows backward (for training)\n",
    "- **Architecture search**: Finding optimal network structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119d7fd3",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 1: What is a Network?\n",
    "\n",
    "### Definition\n",
    "A **network** is a composition of layers that transforms input data into output predictions. Think of it as a pipeline of transformations:\n",
    "\n",
    "```\n",
    "Input → Layer1 → Layer2 → Layer3 → Output\n",
    "```\n",
    "\n",
    "### The Mathematical Foundation: Function Composition Theory\n",
    "\n",
    "#### **Function Composition in Mathematics**\n",
    "In mathematics, function composition combines simple functions to create complex ones:\n",
    "\n",
    "```python\n",
    "# Mathematical composition: (f ∘ g)(x) = f(g(x))\n",
    "def compose(f, g):\n",
    "    return lambda x: f(g(x))\n",
    "\n",
    "# Neural network composition: h(x) = f_n(f_{n-1}(...f_2(f_1(x))))\n",
    "def network(layers):\n",
    "    return lambda x: reduce(lambda acc, layer: layer(acc), layers, x)\n",
    "```\n",
    "\n",
    "#### **Why Composition is Powerful**\n",
    "1. **Modularity**: Each layer has a specific, well-defined purpose\n",
    "2. **Composability**: Simple functions combine to create arbitrarily complex behaviors\n",
    "3. **Hierarchical learning**: Early layers learn simple features, later layers learn complex patterns\n",
    "4. **Universal approximation**: Deep compositions can approximate any continuous function\n",
    "\n",
    "#### **The Emergence of Intelligence**\n",
    "Complex behavior emerges from simple layer composition:\n",
    "\n",
    "```python\n",
    "# Example: Image classification\n",
    "raw_pixels → [Edge detectors] → [Shape detectors] → [Object detectors] → [Class predictor]\n",
    "     ↓              ↓                    ↓                    ↓                 ↓\n",
    "  [28x28]      [64 features]      [128 features]      [256 features]      [10 classes]\n",
    "```\n",
    "\n",
    "### Architectural Design Principles\n",
    "\n",
    "#### **1. Depth vs. Width Trade-offs**\n",
    "- **Deep networks**: More layers → more complex representations\n",
    "  - **Advantages**: Better feature hierarchies, parameter efficiency\n",
    "  - **Disadvantages**: Harder to train, gradient problems\n",
    "- **Wide networks**: More neurons per layer → more capacity per layer\n",
    "  - **Advantages**: Easier to train, parallel computation\n",
    "  - **Disadvantages**: More parameters, potential overfitting\n",
    "\n",
    "#### **2. Information Flow Patterns**\n",
    "```python\n",
    "# Sequential flow (what we're building):\n",
    "x → layer1 → layer2 → layer3 → output\n",
    "\n",
    "# Residual flow (advanced):\n",
    "x → layer1 → layer2 + x → layer3 → output\n",
    "\n",
    "# Attention flow (transformers):\n",
    "x → attention(x, x, x) → feedforward → output\n",
    "```\n",
    "\n",
    "#### **3. Activation Function Placement**\n",
    "```python\n",
    "# Standard pattern:\n",
    "linear_transformation → nonlinear_activation → next_layer\n",
    "\n",
    "# Why this works:\n",
    "# Linear + Linear = Linear (no increase in expressiveness)\n",
    "# Linear + Nonlinear + Linear = Nonlinear (exponential increase in expressiveness)\n",
    "```\n",
    "\n",
    "### Real-World Architecture Examples\n",
    "\n",
    "#### **Multi-Layer Perceptron (MLP)**\n",
    "```python\n",
    "# Classic feedforward network\n",
    "input → dense(512) → relu → dense(256) → relu → dense(10) → softmax\n",
    "```\n",
    "- **Use cases**: Tabular data, feature learning, classification\n",
    "- **Strengths**: Universal approximation, well-understood\n",
    "- **Weaknesses**: Doesn't exploit spatial/temporal structure\n",
    "\n",
    "#### **Convolutional Neural Network (CNN)**\n",
    "```python\n",
    "# Exploits spatial structure\n",
    "input → conv2d → relu → pool → conv2d → relu → pool → dense → softmax\n",
    "```\n",
    "- **Use cases**: Image processing, computer vision\n",
    "- **Strengths**: Translation invariance, parameter sharing\n",
    "- **Weaknesses**: Fixed receptive field, not great for sequences\n",
    "\n",
    "#### **Recurrent Neural Network (RNN)**\n",
    "```python\n",
    "# Processes sequences\n",
    "input_t → rnn_cell(hidden_{t-1}) → hidden_t → output_t\n",
    "```\n",
    "- **Use cases**: Natural language processing, time series\n",
    "- **Strengths**: Variable length sequences, memory\n",
    "- **Weaknesses**: Sequential computation, gradient problems\n",
    "\n",
    "#### **Transformer**\n",
    "```python\n",
    "# Attention-based processing\n",
    "input → attention → feedforward → attention → feedforward → output\n",
    "```\n",
    "- **Use cases**: Language models, machine translation\n",
    "- **Strengths**: Parallelizable, long-range dependencies\n",
    "- **Weaknesses**: Quadratic complexity, large memory requirements\n",
    "\n",
    "### The Network Design Process\n",
    "\n",
    "#### **1. Problem Analysis**\n",
    "- **Data type**: Images, text, tabular, time series?\n",
    "- **Task type**: Classification, regression, generation?\n",
    "- **Constraints**: Latency, memory, accuracy requirements?\n",
    "\n",
    "#### **2. Architecture Selection**\n",
    "- **Start simple**: Begin with basic MLP\n",
    "- **Add structure**: Incorporate domain-specific inductive biases\n",
    "- **Scale up**: Increase depth/width as needed\n",
    "\n",
    "#### **3. Component Design**\n",
    "- **Input layer**: Match data dimensions\n",
    "- **Hidden layers**: Gradual dimension reduction typical\n",
    "- **Output layer**: Match task requirements (classes, regression targets)\n",
    "- **Activation functions**: ReLU for hidden, task-specific for output\n",
    "\n",
    "#### **4. Optimization Considerations**\n",
    "- **Gradient flow**: Ensure gradients can flow through the network\n",
    "- **Computational efficiency**: Balance expressiveness with speed\n",
    "- **Memory usage**: Consider intermediate activation storage\n",
    "\n",
    "### Performance Characteristics\n",
    "\n",
    "#### **Forward Pass Complexity**\n",
    "For a network with L layers, each with n neurons:\n",
    "- **Time complexity**: O(L × n²) for dense layers\n",
    "- **Space complexity**: O(L × n) for activations\n",
    "- **Parallelization**: Each layer can be parallelized\n",
    "\n",
    "#### **Memory Management**\n",
    "```python\n",
    "# Memory usage during forward pass:\n",
    "input_memory = batch_size × input_size\n",
    "hidden_memory = batch_size × hidden_size × num_layers\n",
    "output_memory = batch_size × output_size\n",
    "total_memory = input_memory + hidden_memory + output_memory\n",
    "```\n",
    "\n",
    "#### **Computational Optimization**\n",
    "- **Batch processing**: Process multiple samples simultaneously\n",
    "- **Vectorization**: Use optimized matrix operations\n",
    "- **Hardware acceleration**: Leverage GPUs/TPUs for parallel computation\n",
    "\n",
    "### Connection to Previous Modules\n",
    "\n",
    "#### **From Module 1 (Tensor)**\n",
    "- **Data flow**: Tensors flow through the network\n",
    "- **Shape management**: Ensure compatible dimensions between layers\n",
    "\n",
    "#### **From Module 2 (Activations)**\n",
    "- **Nonlinearity**: Activation functions between layers enable complex learning\n",
    "- **Function choice**: Different activations for different purposes\n",
    "\n",
    "#### **From Module 3 (Layers)**\n",
    "- **Building blocks**: Layers are the fundamental components\n",
    "- **Composition**: Networks compose layers into complete architectures\n",
    "\n",
    "### Why Networks Matter: The Scaling Laws\n",
    "\n",
    "#### **Empirical Observations**\n",
    "- **More parameters**: Generally better performance (up to a point)\n",
    "- **More data**: Enables training of larger networks\n",
    "- **More compute**: Allows exploration of larger architectures\n",
    "\n",
    "#### **The Deep Learning Revolution**\n",
    "```python\n",
    "# Pre-2012: Shallow networks\n",
    "input → hidden(100) → output\n",
    "\n",
    "# Post-2012: Deep networks\n",
    "input → hidden(512) → hidden(512) → hidden(512) → ... → output\n",
    "```\n",
    "\n",
    "The key insight: **Depth enables hierarchical feature learning**\n",
    "\n",
    "Let's start building our Sequential network architecture!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f852d885",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "sequential-class",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class Sequential:\n",
    "    \"\"\"\n",
    "    Sequential Network: Composes layers in sequence\n",
    "    \n",
    "    The most fundamental network architecture.\n",
    "    Applies layers in order: f(x) = layer_n(...layer_2(layer_1(x)))\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layers: List):\n",
    "        \"\"\"\n",
    "        Initialize Sequential network with layers.\n",
    "        \n",
    "        Args:\n",
    "            layers: List of layers to compose in order\n",
    "            \n",
    "        TODO: Store the layers and implement forward pass\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Store the layers list as an instance variable\n",
    "        2. This creates the network architecture ready for forward pass\n",
    "        \n",
    "        EXAMPLE:\n",
    "        Sequential([Dense(3,4), ReLU(), Dense(4,2)])\n",
    "        creates a 3-layer network: Dense → ReLU → Dense\n",
    "        \n",
    "        HINTS:\n",
    "        - Store layers in self.layers\n",
    "        - This is the foundation for all network architectures\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        self.layers = layers\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through all layers in sequence.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor after passing through all layers\n",
    "            \n",
    "        TODO: Implement sequential forward pass through all layers\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Start with the input tensor\n",
    "        2. Apply each layer in sequence\n",
    "        3. Each layer's output becomes the next layer's input\n",
    "        4. Return the final output\n",
    "        \n",
    "        EXAMPLE:\n",
    "        Input: Tensor([[1, 2, 3]])\n",
    "        Layer1 (Dense): Tensor([[1.4, 2.8]])\n",
    "        Layer2 (ReLU): Tensor([[1.4, 2.8]])\n",
    "        Layer3 (Dense): Tensor([[0.7]])\n",
    "        Output: Tensor([[0.7]])\n",
    "        \n",
    "        HINTS:\n",
    "        - Use a for loop: for layer in self.layers:\n",
    "        - Apply each layer: x = layer(x)\n",
    "        - The output of one layer becomes input to the next\n",
    "        - Return the final result\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        # Apply each layer in sequence\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Make network callable: network(x) same as network.forward(x)\"\"\"\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247e43f4",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### 🧪 Unit Test: Sequential Network\n",
    "\n",
    "Let's test your Sequential network implementation! This is the foundation of all neural network architectures.\n",
    "\n",
    "**This is a unit test** - it tests one specific class (Sequential network) in isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0e7373",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-sequential-immediate",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test Sequential network immediately after implementation\n",
    "print(\"🔬 Unit Test: Sequential Network...\")\n",
    "\n",
    "# Create a simple 2-layer network: 3 → 4 → 2\n",
    "try:\n",
    "    network = Sequential([\n",
    "        Dense(input_size=3, output_size=4),\n",
    "        ReLU(),\n",
    "        Dense(input_size=4, output_size=2),\n",
    "        Sigmoid()\n",
    "    ])\n",
    "    \n",
    "    print(f\"Network created with {len(network.layers)} layers\")\n",
    "    print(\"✅ Sequential network creation successful\")\n",
    "    \n",
    "    # Test with sample data\n",
    "    x = Tensor([[1.0, 2.0, 3.0]])\n",
    "    print(f\"Input: {x}\")\n",
    "    \n",
    "    # Forward pass\n",
    "    y = network(x)\n",
    "    print(f\"Output: {y}\")\n",
    "    print(f\"Output shape: {y.shape}\")\n",
    "    \n",
    "    # Verify the network works\n",
    "    assert y.shape == (1, 2), f\"Expected shape (1, 2), got {y.shape}\"\n",
    "    print(\"✅ Sequential network produces correct output shape\")\n",
    "    \n",
    "    # Test that sigmoid output is in valid range\n",
    "    assert np.all(y.data >= 0) and np.all(y.data <= 1), \"Sigmoid output should be between 0 and 1\"\n",
    "    print(\"✅ Sequential network output is in valid range\")\n",
    "    \n",
    "    # Test that layers are stored correctly\n",
    "    assert len(network.layers) == 4, f\"Expected 4 layers, got {len(network.layers)}\"\n",
    "    print(\"✅ Sequential network stores layers correctly\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Sequential network test failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# Show the network architecture\n",
    "print(\"🎯 Sequential network behavior:\")\n",
    "print(\"   Applies layers in sequence: f(g(h(x)))\")\n",
    "print(\"   Input flows through each layer in order\")\n",
    "print(\"   Output of layer i becomes input of layer i+1\")\n",
    "print(\"📈 Progress: Sequential network ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b510197",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 2: Building Multi-Layer Perceptrons (MLPs)\n",
    "\n",
    "### What is an MLP?\n",
    "A **Multi-Layer Perceptron** is the classic neural network architecture:\n",
    "\n",
    "```\n",
    "Input → Dense → Activation → Dense → Activation → ... → Dense → Output\n",
    "```\n",
    "\n",
    "### Why MLPs are Important\n",
    "- **Universal approximation**: Can approximate any continuous function\n",
    "- **Foundation**: Basis for understanding all neural networks\n",
    "- **Versatile**: Works for classification, regression, and more\n",
    "- **Simple**: Easy to understand and implement\n",
    "\n",
    "### MLP Architecture Pattern\n",
    "```\n",
    "create_mlp(3, [4, 2], 1) creates:\n",
    "Dense(3→4) → ReLU → Dense(4→2) → ReLU → Dense(2→1) → Sigmoid\n",
    "```\n",
    "\n",
    "### Real-World Applications\n",
    "- **Tabular data**: Customer analytics, financial modeling\n",
    "- **Feature learning**: Learning representations from raw data\n",
    "- **Classification**: Spam detection, medical diagnosis\n",
    "- **Regression**: Price prediction, time series forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eab7ceb",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "create-mlp",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_mlp(input_size: int, hidden_sizes: List[int], output_size: int, \n",
    "               activation=ReLU, output_activation=Sigmoid) -> Sequential:\n",
    "    \"\"\"\n",
    "    Create a Multi-Layer Perceptron (MLP) network.\n",
    "    \n",
    "    Args:\n",
    "        input_size: Number of input features\n",
    "        hidden_sizes: List of hidden layer sizes\n",
    "        output_size: Number of output features\n",
    "        activation: Activation function for hidden layers (default: ReLU)\n",
    "        output_activation: Activation function for output layer (default: Sigmoid)\n",
    "        \n",
    "    Returns:\n",
    "        Sequential network with MLP architecture\n",
    "        \n",
    "    TODO: Implement MLP creation with alternating Dense and activation layers.\n",
    "    \n",
    "    APPROACH:\n",
    "    1. Start with an empty list of layers\n",
    "    2. Add layers in this pattern:\n",
    "       - Dense(input_size → first_hidden_size)\n",
    "       - Activation()\n",
    "       - Dense(first_hidden_size → second_hidden_size)\n",
    "       - Activation()\n",
    "       - ...\n",
    "       - Dense(last_hidden_size → output_size)\n",
    "       - Output_activation()\n",
    "    3. Return Sequential(layers)\n",
    "    \n",
    "    EXAMPLE:\n",
    "    create_mlp(3, [4, 2], 1) creates:\n",
    "    Dense(3→4) → ReLU → Dense(4→2) → ReLU → Dense(2→1) → Sigmoid\n",
    "    \n",
    "    HINTS:\n",
    "    - Start with layers = []\n",
    "    - Track current_size starting with input_size\n",
    "    - For each hidden_size: add Dense(current_size, hidden_size), then activation\n",
    "    - Finally add Dense(last_hidden_size, output_size), then output_activation\n",
    "    - Return Sequential(layers)\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    layers = []\n",
    "    current_size = input_size\n",
    "    \n",
    "    # Add hidden layers with activations\n",
    "    for hidden_size in hidden_sizes:\n",
    "        layers.append(Dense(current_size, hidden_size))\n",
    "        layers.append(activation())\n",
    "        current_size = hidden_size\n",
    "    \n",
    "    # Add output layer with output activation\n",
    "    layers.append(Dense(current_size, output_size))\n",
    "    layers.append(output_activation())\n",
    "    \n",
    "    return Sequential(layers)\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d61de3c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### 🧪 Unit Test: MLP Creation\n",
    "\n",
    "Let's test your MLP creation function! This builds complete neural networks with a single function call.\n",
    "\n",
    "**This is a unit test** - it tests one specific function (create_mlp) in isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5663b0e1",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-mlp-immediate",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test MLP creation immediately after implementation\n",
    "print(\"🔬 Unit Test: MLP Creation...\")\n",
    "\n",
    "# Create a simple MLP: 3 → 4 → 2 → 1\n",
    "try:\n",
    "    mlp = create_mlp(input_size=3, hidden_sizes=[4, 2], output_size=1)\n",
    "    \n",
    "    print(f\"MLP created with {len(mlp.layers)} layers\")\n",
    "    print(\"✅ MLP creation successful\")\n",
    "    \n",
    "    # Test the structure - should have 6 layers: Dense, ReLU, Dense, ReLU, Dense, Sigmoid\n",
    "    expected_layers = 6  # 3 Dense + 2 ReLU + 1 Sigmoid\n",
    "    assert len(mlp.layers) == expected_layers, f\"Expected {expected_layers} layers, got {len(mlp.layers)}\"\n",
    "    print(\"✅ MLP has correct number of layers\")\n",
    "    \n",
    "    # Test with sample data\n",
    "    x = Tensor([[1.0, 2.0, 3.0]])\n",
    "    y = mlp(x)\n",
    "    print(f\"MLP input: {x}\")\n",
    "    print(f\"MLP output: {y}\")\n",
    "    print(f\"MLP output shape: {y.shape}\")\n",
    "    \n",
    "    # Verify the output\n",
    "    assert y.shape == (1, 1), f\"Expected shape (1, 1), got {y.shape}\"\n",
    "    print(\"✅ MLP produces correct output shape\")\n",
    "    \n",
    "    # Test that sigmoid output is in valid range\n",
    "    assert np.all(y.data >= 0) and np.all(y.data <= 1), \"Sigmoid output should be between 0 and 1\"\n",
    "    print(\"✅ MLP output is in valid range\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ MLP creation test failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# Test different architectures\n",
    "try:\n",
    "    # Test shallow network\n",
    "    shallow_net = create_mlp(input_size=3, hidden_sizes=[4], output_size=1)\n",
    "    assert len(shallow_net.layers) == 4, f\"Shallow network should have 4 layers, got {len(shallow_net.layers)}\"\n",
    "    \n",
    "    # Test deep network  \n",
    "    deep_net = create_mlp(input_size=3, hidden_sizes=[4, 4, 4], output_size=1)\n",
    "    assert len(deep_net.layers) == 8, f\"Deep network should have 8 layers, got {len(deep_net.layers)}\"\n",
    "    \n",
    "    # Test wide network\n",
    "    wide_net = create_mlp(input_size=3, hidden_sizes=[10], output_size=1)\n",
    "    assert len(wide_net.layers) == 4, f\"Wide network should have 4 layers, got {len(wide_net.layers)}\"\n",
    "    \n",
    "    print(\"✅ Different MLP architectures work correctly\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ MLP architecture test failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# Show the MLP pattern\n",
    "print(\"🎯 MLP creation pattern:\")\n",
    "print(\"   Input → Dense → Activation → Dense → Activation → ... → Dense → Output_Activation\")\n",
    "print(\"   Automatically creates the complete architecture\")\n",
    "print(\"   Handles any number of hidden layers\")\n",
    "print(\"📈 Progress: Sequential network ✓, MLP creation ✓\")\n",
    "print(\"🚀 Complete neural networks ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0f702c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### 🧪 Test Your Network Implementations\n",
    "\n",
    "Once you implement the functions above, run these cells to test them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f9d26f",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-sequential",
     "locked": true,
     "points": 25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test the Sequential network\n",
    "print(\"Testing Sequential network...\")\n",
    "\n",
    "# Create a simple 2-layer network: 3 → 4 → 2\n",
    "network = Sequential([\n",
    "    Dense(input_size=3, output_size=4),\n",
    "    ReLU(),\n",
    "    Dense(input_size=4, output_size=2),\n",
    "    Sigmoid()\n",
    "])\n",
    "\n",
    "print(f\"Network created with {len(network.layers)} layers\")\n",
    "\n",
    "# Test with sample data\n",
    "x = Tensor([[1.0, 2.0, 3.0]])\n",
    "print(f\"Input: {x}\")\n",
    "\n",
    "# Forward pass\n",
    "y = network(x)\n",
    "print(f\"Output: {y}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "\n",
    "# Verify the network works\n",
    "assert y.shape == (1, 2), f\"Expected shape (1, 2), got {y.shape}\"\n",
    "assert np.all(y.data >= 0) and np.all(y.data <= 1), \"Sigmoid output should be between 0 and 1\"\n",
    "\n",
    "print(\"✅ Sequential network tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d282cd22",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-mlp",
     "locked": true,
     "points": 25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test MLP creation\n",
    "print(\"Testing MLP creation...\")\n",
    "\n",
    "# Create a simple MLP: 3 → 4 → 2 → 1\n",
    "mlp = create_mlp(input_size=3, hidden_sizes=[4, 2], output_size=1)\n",
    "\n",
    "print(f\"MLP created with {len(mlp.layers)} layers\")\n",
    "\n",
    "# Test the structure\n",
    "expected_layers = [\n",
    "    Dense,  # 3 → 4\n",
    "    ReLU,   # activation\n",
    "    Dense,  # 4 → 2\n",
    "    ReLU,   # activation\n",
    "    Dense,  # 2 → 1\n",
    "    Sigmoid # output activation\n",
    "]\n",
    "\n",
    "assert len(mlp.layers) == 6, f\"Expected 6 layers, got {len(mlp.layers)}\"\n",
    "\n",
    "# Test with sample data\n",
    "x = Tensor([[1.0, 2.0, 3.0]])\n",
    "y = mlp(x)\n",
    "print(f\"MLP output: {y}\")\n",
    "print(f\"MLP output shape: {y.shape}\")\n",
    "\n",
    "# Verify the output\n",
    "assert y.shape == (1, 1), f\"Expected shape (1, 1), got {y.shape}\"\n",
    "assert np.all(y.data >= 0) and np.all(y.data <= 1), \"Sigmoid output should be between 0 and 1\"\n",
    "\n",
    "print(\"✅ MLP creation tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf06ba1",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-network-comparison",
     "locked": true,
     "points": 25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test different network architectures\n",
    "print(\"Testing different network architectures...\")\n",
    "\n",
    "# Create networks with different architectures\n",
    "shallow_net = create_mlp(input_size=3, hidden_sizes=[4], output_size=1)\n",
    "deep_net = create_mlp(input_size=3, hidden_sizes=[4, 4, 4], output_size=1)\n",
    "wide_net = create_mlp(input_size=3, hidden_sizes=[10], output_size=1)\n",
    "\n",
    "# Test input\n",
    "x = Tensor([[1.0, 2.0, 3.0]])\n",
    "\n",
    "# Test all networks\n",
    "shallow_out = shallow_net(x)\n",
    "deep_out = deep_net(x)\n",
    "wide_out = wide_net(x)\n",
    "\n",
    "print(f\"Shallow network output: {shallow_out}\")\n",
    "print(f\"Deep network output: {deep_out}\")\n",
    "print(f\"Wide network output: {wide_out}\")\n",
    "\n",
    "# Verify all outputs are valid\n",
    "for name, output in [(\"Shallow\", shallow_out), (\"Deep\", deep_out), (\"Wide\", wide_out)]:\n",
    "    assert output.shape == (1, 1), f\"{name} network output shape should be (1, 1), got {output.shape}\"\n",
    "    assert np.all(output.data >= 0) and np.all(output.data <= 1), f\"{name} network output should be between 0 and 1\"\n",
    "\n",
    "print(\"✅ Network architecture comparison tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d626679",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 🎯 Module Summary\n",
    "\n",
    "Congratulations! You've successfully implemented complete neural network architectures:\n",
    "\n",
    "### What You've Accomplished\n",
    "✅ **Sequential Networks**: The fundamental architecture for composing layers  \n",
    "✅ **Function Composition**: Understanding how layers combine to create complex behaviors  \n",
    "✅ **MLP Creation**: Building Multi-Layer Perceptrons with flexible architectures  \n",
    "✅ **Architecture Patterns**: Creating shallow, deep, and wide networks  \n",
    "✅ **Forward Pass**: Complete inference through multi-layer networks  \n",
    "\n",
    "### Key Concepts You've Learned\n",
    "- **Networks are function composition**: Complex behavior from simple building blocks\n",
    "- **Sequential architecture**: The foundation of most neural networks\n",
    "- **MLP patterns**: Dense → Activation → Dense → Activation → Output\n",
    "- **Architecture design**: How depth and width affect network capability\n",
    "- **Forward pass**: How data flows through complete networks\n",
    "\n",
    "### Mathematical Foundations\n",
    "- **Function composition**: f(x) = f_n(...f_2(f_1(x)))\n",
    "- **Universal approximation**: MLPs can approximate any continuous function\n",
    "- **Hierarchical learning**: Early layers learn simple features, later layers learn complex patterns\n",
    "- **Nonlinearity**: Activation functions enable complex decision boundaries\n",
    "\n",
    "### Real-World Applications\n",
    "- **Classification**: Image recognition, spam detection, medical diagnosis\n",
    "- **Regression**: Price prediction, time series forecasting\n",
    "- **Feature learning**: Extracting meaningful representations from raw data\n",
    "- **Transfer learning**: Using pre-trained networks for new tasks\n",
    "\n",
    "### Next Steps\n",
    "1. **Export your code**: `tito package nbdev --export 04_networks`\n",
    "2. **Test your implementation**: `tito module test 04_networks`\n",
    "3. **Use your networks**: \n",
    "   ```python\n",
    "   from tinytorch.core.networks import Sequential, create_mlp\n",
    "   from tinytorch.core.layers import Dense\n",
    "   from tinytorch.core.activations import ReLU\n",
    "   \n",
    "   # Create custom network\n",
    "   network = Sequential([Dense(10, 5), ReLU(), Dense(5, 1)])\n",
    "   \n",
    "   # Create MLP\n",
    "   mlp = create_mlp(10, [20, 10], 1)\n",
    "   ```\n",
    "4. **Move to Module 5**: Start building convolutional networks for images!\n",
    "\n",
    "**Ready for the next challenge?** Let's add convolutional layers for image processing and build CNNs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacec0da",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## 🧪 Comprehensive Testing: Neural Network Architectures\n",
    "\n",
    "Let's thoroughly test your network implementations to ensure they work correctly in all scenarios.\n",
    "This comprehensive testing ensures your networks are robust and ready for real ML applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc3ae67",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-networks-comprehensive",
     "locked": true,
     "points": 30,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_networks_comprehensive():\n",
    "    \"\"\"Comprehensive test of Sequential networks and MLP creation.\"\"\"\n",
    "    print(\"🔬 Testing neural network architectures comprehensively...\")\n",
    "    \n",
    "    tests_passed = 0\n",
    "    total_tests = 10\n",
    "    \n",
    "    # Test 1: Sequential Network Creation and Structure\n",
    "    try:\n",
    "        # Create a simple 2-layer network\n",
    "        network = Sequential([\n",
    "            Dense(input_size=3, output_size=4),\n",
    "            ReLU(),\n",
    "            Dense(input_size=4, output_size=2),\n",
    "            Sigmoid()\n",
    "        ])\n",
    "        \n",
    "        assert len(network.layers) == 4, f\"Expected 4 layers, got {len(network.layers)}\"\n",
    "        \n",
    "        # Test layer types\n",
    "        assert isinstance(network.layers[0], Dense), \"First layer should be Dense\"\n",
    "        assert isinstance(network.layers[1], ReLU), \"Second layer should be ReLU\"\n",
    "        assert isinstance(network.layers[2], Dense), \"Third layer should be Dense\"\n",
    "        assert isinstance(network.layers[3], Sigmoid), \"Fourth layer should be Sigmoid\"\n",
    "        \n",
    "        print(\"✅ Sequential network creation and structure\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Sequential network creation failed: {e}\")\n",
    "    \n",
    "    # Test 2: Sequential Network Forward Pass\n",
    "    try:\n",
    "        network = Sequential([\n",
    "            Dense(input_size=3, output_size=4),\n",
    "            ReLU(),\n",
    "            Dense(input_size=4, output_size=2),\n",
    "            Sigmoid()\n",
    "        ])\n",
    "        \n",
    "        # Test single sample\n",
    "        x_single = Tensor([[1.0, 2.0, 3.0]])\n",
    "        y_single = network(x_single)\n",
    "        \n",
    "        assert y_single.shape == (1, 2), f\"Single sample output should be (1, 2), got {y_single.shape}\"\n",
    "        assert np.all((y_single.data >= 0) & (y_single.data <= 1)), \"Sigmoid output should be in [0,1]\"\n",
    "        \n",
    "        # Test batch processing\n",
    "        x_batch = Tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]])\n",
    "        y_batch = network(x_batch)\n",
    "        \n",
    "        assert y_batch.shape == (3, 2), f\"Batch output should be (3, 2), got {y_batch.shape}\"\n",
    "        assert np.all((y_batch.data >= 0) & (y_batch.data <= 1)), \"All batch outputs should be in [0,1]\"\n",
    "        \n",
    "        print(\"✅ Sequential network forward pass: single and batch\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Sequential network forward pass failed: {e}\")\n",
    "    \n",
    "    # Test 3: MLP Creation Basic Functionality\n",
    "    try:\n",
    "        # Create simple MLP: 3 → 4 → 2 → 1\n",
    "        mlp = create_mlp(input_size=3, hidden_sizes=[4, 2], output_size=1)\n",
    "        \n",
    "        # Should have 6 layers: Dense, ReLU, Dense, ReLU, Dense, Sigmoid\n",
    "        expected_layers = 6\n",
    "        assert len(mlp.layers) == expected_layers, f\"Expected {expected_layers} layers, got {len(mlp.layers)}\"\n",
    "        \n",
    "        # Test layer pattern\n",
    "        layer_types = [type(layer).__name__ for layer in mlp.layers]\n",
    "        expected_pattern = ['Dense', 'ReLU', 'Dense', 'ReLU', 'Dense', 'Sigmoid']\n",
    "        assert layer_types == expected_pattern, f\"Expected pattern {expected_pattern}, got {layer_types}\"\n",
    "        \n",
    "        # Test forward pass\n",
    "        x = Tensor([[1.0, 2.0, 3.0]])\n",
    "        y = mlp(x)\n",
    "        \n",
    "        assert y.shape == (1, 1), f\"MLP output should be (1, 1), got {y.shape}\"\n",
    "        assert np.all((y.data >= 0) & (y.data <= 1)), \"MLP output should be in [0,1]\"\n",
    "        \n",
    "        print(\"✅ MLP creation basic functionality\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"❌ MLP creation basic failed: {e}\")\n",
    "    \n",
    "    # Test 4: Different MLP Architectures\n",
    "    try:\n",
    "        # Test shallow network (1 hidden layer)\n",
    "        shallow_net = create_mlp(input_size=3, hidden_sizes=[4], output_size=1)\n",
    "        assert len(shallow_net.layers) == 4, f\"Shallow network should have 4 layers, got {len(shallow_net.layers)}\"\n",
    "        \n",
    "        # Test deep network (3 hidden layers)\n",
    "        deep_net = create_mlp(input_size=3, hidden_sizes=[4, 4, 4], output_size=1)\n",
    "        assert len(deep_net.layers) == 8, f\"Deep network should have 8 layers, got {len(deep_net.layers)}\"\n",
    "        \n",
    "        # Test wide network (1 large hidden layer)\n",
    "        wide_net = create_mlp(input_size=3, hidden_sizes=[20], output_size=1)\n",
    "        assert len(wide_net.layers) == 4, f\"Wide network should have 4 layers, got {len(wide_net.layers)}\"\n",
    "        \n",
    "        # Test very deep network\n",
    "        very_deep_net = create_mlp(input_size=3, hidden_sizes=[5, 5, 5, 5, 5], output_size=1)\n",
    "        assert len(very_deep_net.layers) == 12, f\"Very deep network should have 12 layers, got {len(very_deep_net.layers)}\"\n",
    "        \n",
    "        # Test all networks work\n",
    "        x = Tensor([[1.0, 2.0, 3.0]])\n",
    "        for name, net in [(\"Shallow\", shallow_net), (\"Deep\", deep_net), (\"Wide\", wide_net), (\"Very Deep\", very_deep_net)]:\n",
    "            y = net(x)\n",
    "            assert y.shape == (1, 1), f\"{name} network output shape should be (1, 1), got {y.shape}\"\n",
    "            assert np.all((y.data >= 0) & (y.data <= 1)), f\"{name} network output should be in [0,1]\"\n",
    "        \n",
    "        print(\"✅ Different MLP architectures: shallow, deep, wide, very deep\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Different MLP architectures failed: {e}\")\n",
    "    \n",
    "    # Test 5: MLP with Different Activation Functions\n",
    "    try:\n",
    "        # Test with Tanh activation\n",
    "        mlp_tanh = create_mlp(input_size=3, hidden_sizes=[4], output_size=1, activation=Tanh, output_activation=Sigmoid)\n",
    "        \n",
    "        # Check layer types\n",
    "        layer_types = [type(layer).__name__ for layer in mlp_tanh.layers]\n",
    "        expected_pattern = ['Dense', 'Tanh', 'Dense', 'Sigmoid']\n",
    "        assert layer_types == expected_pattern, f\"Tanh MLP pattern should be {expected_pattern}, got {layer_types}\"\n",
    "        \n",
    "        # Test forward pass\n",
    "        x = Tensor([[1.0, 2.0, 3.0]])\n",
    "        y = mlp_tanh(x)\n",
    "        assert y.shape == (1, 1), \"Tanh MLP should work correctly\"\n",
    "        \n",
    "        # Test with different output activation\n",
    "        mlp_tanh_out = create_mlp(input_size=3, hidden_sizes=[4], output_size=3, activation=ReLU, output_activation=Softmax)\n",
    "        y_multi = mlp_tanh_out(x)\n",
    "        assert y_multi.shape == (1, 3), \"Multi-output MLP should work\"\n",
    "        \n",
    "        # Check softmax properties\n",
    "        assert abs(np.sum(y_multi.data) - 1.0) < 1e-6, \"Softmax outputs should sum to 1\"\n",
    "        \n",
    "        print(\"✅ MLP with different activation functions: Tanh, Softmax\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"❌ MLP with different activations failed: {e}\")\n",
    "    \n",
    "    # Test 6: Network Layer Composition\n",
    "    try:\n",
    "        # Test that network correctly chains layers\n",
    "        network = Sequential([\n",
    "            Dense(input_size=4, output_size=3),\n",
    "            ReLU(),\n",
    "            Dense(input_size=3, output_size=2),\n",
    "            Tanh(),\n",
    "            Dense(input_size=2, output_size=1),\n",
    "            Sigmoid()\n",
    "        ])\n",
    "        \n",
    "        x = Tensor([[1.0, -1.0, 2.0, -2.0]])\n",
    "        \n",
    "        # Manual forward pass to verify composition\n",
    "        h1 = network.layers[0](x)  # Dense\n",
    "        h2 = network.layers[1](h1)  # ReLU\n",
    "        h3 = network.layers[2](h2)  # Dense\n",
    "        h4 = network.layers[3](h3)  # Tanh\n",
    "        h5 = network.layers[4](h4)  # Dense\n",
    "        h6 = network.layers[5](h5)  # Sigmoid\n",
    "        \n",
    "        # Compare with network forward pass\n",
    "        y_network = network(x)\n",
    "        \n",
    "        assert np.allclose(h6.data, y_network.data), \"Manual and network forward pass should match\"\n",
    "        \n",
    "        # Check intermediate shapes\n",
    "        assert h1.shape == (1, 3), f\"h1 shape should be (1, 3), got {h1.shape}\"\n",
    "        assert h2.shape == (1, 3), f\"h2 shape should be (1, 3), got {h2.shape}\"\n",
    "        assert h3.shape == (1, 2), f\"h3 shape should be (1, 2), got {h3.shape}\"\n",
    "        assert h4.shape == (1, 2), f\"h4 shape should be (1, 2), got {h4.shape}\"\n",
    "        assert h5.shape == (1, 1), f\"h5 shape should be (1, 1), got {h5.shape}\"\n",
    "        assert h6.shape == (1, 1), f\"h6 shape should be (1, 1), got {h6.shape}\"\n",
    "        \n",
    "        # Check activation effects\n",
    "        assert np.all(h2.data >= 0), \"ReLU should produce non-negative values\"\n",
    "        assert np.all((h4.data >= -1) & (h4.data <= 1)), \"Tanh should produce values in [-1,1]\"\n",
    "        assert np.all((h6.data >= 0) & (h6.data <= 1)), \"Sigmoid should produce values in [0,1]\"\n",
    "        \n",
    "        print(\"✅ Network layer composition: correct chaining and shapes\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Network layer composition failed: {e}\")\n",
    "    \n",
    "    # Test 7: Edge Cases and Robustness\n",
    "    try:\n",
    "        # Test with minimal network (1 layer)\n",
    "        minimal_net = Sequential([Dense(input_size=2, output_size=1)])\n",
    "        x_minimal = Tensor([[1.0, 2.0]])\n",
    "        y_minimal = minimal_net(x_minimal)\n",
    "        assert y_minimal.shape == (1, 1), \"Minimal network should work\"\n",
    "        \n",
    "        # Test with single neuron layers\n",
    "        single_neuron_net = create_mlp(input_size=1, hidden_sizes=[1], output_size=1)\n",
    "        x_single = Tensor([[5.0]])\n",
    "        y_single_neuron = single_neuron_net(x_single)\n",
    "        assert y_single_neuron.shape == (1, 1), \"Single neuron network should work\"\n",
    "        \n",
    "        # Test with large batch\n",
    "        large_net = create_mlp(input_size=10, hidden_sizes=[5], output_size=1)\n",
    "        x_large_batch = Tensor(np.random.randn(100, 10))\n",
    "        y_large_batch = large_net(x_large_batch)\n",
    "        assert y_large_batch.shape == (100, 1), \"Large batch should work\"\n",
    "        assert not np.any(np.isnan(y_large_batch.data)), \"Should not produce NaN\"\n",
    "        assert not np.any(np.isinf(y_large_batch.data)), \"Should not produce Inf\"\n",
    "        \n",
    "        print(\"✅ Edge cases: minimal networks, single neurons, large batches\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Edge cases failed: {e}\")\n",
    "    \n",
    "    # Test 8: Multi-class Classification Networks\n",
    "    try:\n",
    "        # Create multi-class classifier\n",
    "        classifier = create_mlp(input_size=4, hidden_sizes=[8, 6], output_size=3, output_activation=Softmax)\n",
    "        \n",
    "        # Test with batch of samples\n",
    "        x_multi = Tensor(np.random.randn(5, 4))\n",
    "        y_multi = classifier(x_multi)\n",
    "        \n",
    "        assert y_multi.shape == (5, 3), f\"Multi-class output should be (5, 3), got {y_multi.shape}\"\n",
    "        \n",
    "        # Check softmax properties for each sample\n",
    "        row_sums = np.sum(y_multi.data, axis=1)\n",
    "        assert np.allclose(row_sums, 1.0), \"Each sample should have probabilities summing to 1\"\n",
    "        assert np.all(y_multi.data > 0), \"All probabilities should be positive\"\n",
    "        \n",
    "        # Test that argmax gives valid class predictions\n",
    "        predictions = np.argmax(y_multi.data, axis=1)\n",
    "        assert np.all((predictions >= 0) & (predictions < 3)), \"Predictions should be valid class indices\"\n",
    "        \n",
    "        print(\"✅ Multi-class classification: softmax probabilities, valid predictions\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Multi-class classification failed: {e}\")\n",
    "    \n",
    "    # Test 9: Real ML Scenarios\n",
    "    try:\n",
    "        # Scenario 1: Binary classification (like spam detection)\n",
    "        spam_classifier = create_mlp(input_size=100, hidden_sizes=[50, 20], output_size=1, output_activation=Sigmoid)\n",
    "        \n",
    "        # Simulate email features\n",
    "        email_features = Tensor(np.random.randn(10, 100))\n",
    "        spam_probabilities = spam_classifier(email_features)\n",
    "        \n",
    "        assert spam_probabilities.shape == (10, 1), \"Spam classifier should output probabilities for each email\"\n",
    "        assert np.all((spam_probabilities.data >= 0) & (spam_probabilities.data <= 1)), \"Should output valid probabilities\"\n",
    "        \n",
    "        # Scenario 2: Image classification (like MNIST)\n",
    "        mnist_classifier = create_mlp(input_size=784, hidden_sizes=[256, 128], output_size=10, output_activation=Softmax)\n",
    "        \n",
    "        # Simulate flattened images\n",
    "        images = Tensor(np.random.randn(32, 784))  # Batch of 32 images\n",
    "        class_probabilities = mnist_classifier(images)\n",
    "        \n",
    "        assert class_probabilities.shape == (32, 10), \"MNIST classifier should output 10 class probabilities\"\n",
    "        \n",
    "        # Check softmax properties\n",
    "        batch_sums = np.sum(class_probabilities.data, axis=1)\n",
    "        assert np.allclose(batch_sums, 1.0), \"Each image should have class probabilities summing to 1\"\n",
    "        \n",
    "        # Scenario 3: Regression (like house price prediction)\n",
    "        price_predictor = Sequential([\n",
    "            Dense(input_size=8, output_size=16),\n",
    "            ReLU(),\n",
    "            Dense(input_size=16, output_size=8),\n",
    "            ReLU(),\n",
    "            Dense(input_size=8, output_size=1)  # No activation for regression\n",
    "        ])\n",
    "        \n",
    "        # Simulate house features\n",
    "        house_features = Tensor(np.random.randn(5, 8))\n",
    "        predicted_prices = price_predictor(house_features)\n",
    "        \n",
    "        assert predicted_prices.shape == (5, 1), \"Price predictor should output one price per house\"\n",
    "        \n",
    "        print(\"✅ Real ML scenarios: spam detection, image classification, price prediction\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Real ML scenarios failed: {e}\")\n",
    "    \n",
    "    # Test 10: Network Comparison and Analysis\n",
    "    try:\n",
    "        # Create networks with same total parameters but different architectures\n",
    "        x_test = Tensor([[1.0, 2.0, 3.0, 4.0]])\n",
    "        \n",
    "        # Wide network: 4 → 20 → 1 (parameters: 4*20 + 20 + 20*1 + 1 = 121)\n",
    "        wide_network = create_mlp(input_size=4, hidden_sizes=[20], output_size=1)\n",
    "        \n",
    "        # Deep network: 4 → 10 → 10 → 1 (parameters: 4*10 + 10 + 10*10 + 10 + 10*1 + 1 = 171)\n",
    "        deep_network = create_mlp(input_size=4, hidden_sizes=[10, 10], output_size=1)\n",
    "        \n",
    "        # Test both networks\n",
    "        wide_output = wide_network(x_test)\n",
    "        deep_output = deep_network(x_test)\n",
    "        \n",
    "        assert wide_output.shape == (1, 1), \"Wide network should produce correct output\"\n",
    "        assert deep_output.shape == (1, 1), \"Deep network should produce correct output\"\n",
    "        \n",
    "        # Both should be valid but potentially different\n",
    "        assert np.all((wide_output.data >= 0) & (wide_output.data <= 1)), \"Wide network output should be valid\"\n",
    "        assert np.all((deep_output.data >= 0) & (deep_output.data <= 1)), \"Deep network output should be valid\"\n",
    "        \n",
    "        # Test network complexity\n",
    "        def count_parameters(network):\n",
    "            total = 0\n",
    "            for layer in network.layers:\n",
    "                if isinstance(layer, Dense):\n",
    "                    total += layer.weights.size\n",
    "                    if layer.bias is not None:\n",
    "                        total += layer.bias.size\n",
    "            return total\n",
    "        \n",
    "        wide_params = count_parameters(wide_network)\n",
    "        deep_params = count_parameters(deep_network)\n",
    "        \n",
    "        assert wide_params > 0, \"Wide network should have parameters\"\n",
    "        assert deep_params > 0, \"Deep network should have parameters\"\n",
    "        \n",
    "        print(f\"✅ Network comparison: wide ({wide_params} params) vs deep ({deep_params} params)\")\n",
    "        tests_passed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Network comparison failed: {e}\")\n",
    "    \n",
    "    # Results summary\n",
    "    print(f\"\\n📊 Networks Module Results: {tests_passed}/{total_tests} tests passed\")\n",
    "    \n",
    "    if tests_passed == total_tests:\n",
    "        print(\"🎉 All network tests passed! Your implementations support:\")\n",
    "        print(\"  • Sequential networks: layer composition and chaining\")\n",
    "        print(\"  • MLP creation: flexible multi-layer perceptron architectures\")\n",
    "        print(\"  • Different architectures: shallow, deep, wide networks\")\n",
    "        print(\"  • Multiple activation functions: ReLU, Tanh, Sigmoid, Softmax\")\n",
    "        print(\"  • Multi-class classification: softmax probability distributions\")\n",
    "        print(\"  • Real ML scenarios: spam detection, image classification, regression\")\n",
    "        print(\"  • Network analysis: parameter counting and architecture comparison\")\n",
    "        print(\"📈 Progress: All Network Functionality ✓\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"⚠️  Some network tests failed. Common issues:\")\n",
    "        print(\"  • Check Sequential class layer composition\")\n",
    "        print(\"  • Verify create_mlp function layer creation pattern\")\n",
    "        print(\"  • Ensure proper activation function integration\")\n",
    "        print(\"  • Test forward pass through complete networks\")\n",
    "        print(\"  • Verify shape handling across all layers\")\n",
    "        return False\n",
    "\n",
    "# Run the comprehensive test\n",
    "success = test_networks_comprehensive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b3354d",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### 🧪 Integration Test: Complete Neural Network Applications\n",
    "\n",
    "Let's test your networks in realistic machine learning applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e243bc",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-networks-integration",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_networks_integration():\n",
    "    \"\"\"Integration test with complete neural network applications.\"\"\"\n",
    "    print(\"🔬 Testing networks in complete ML applications...\")\n",
    "    \n",
    "    try:\n",
    "        print(\"🧠 Building complete ML applications with neural networks...\")\n",
    "        \n",
    "        # Application 1: Iris Classification\n",
    "        print(\"\\n🌸 Application 1: Iris Classification (Multi-class)\")\n",
    "        iris_classifier = create_mlp(\n",
    "            input_size=4,      # 4 flower measurements\n",
    "            hidden_sizes=[8, 6], # Hidden layers\n",
    "            output_size=3,     # 3 iris species\n",
    "            output_activation=Softmax\n",
    "        )\n",
    "        \n",
    "        # Simulate iris data\n",
    "        iris_samples = Tensor([\n",
    "            [5.1, 3.5, 1.4, 0.2],  # Setosa-like\n",
    "            [7.0, 3.2, 4.7, 1.4],  # Versicolor-like\n",
    "            [6.3, 3.3, 6.0, 2.5]   # Virginica-like\n",
    "        ])\n",
    "        \n",
    "        iris_predictions = iris_classifier(iris_samples)\n",
    "        \n",
    "        assert iris_predictions.shape == (3, 3), \"Should predict 3 classes for 3 samples\"\n",
    "        \n",
    "        # Check that predictions are valid probabilities\n",
    "        row_sums = np.sum(iris_predictions.data, axis=1)\n",
    "        assert np.allclose(row_sums, 1.0), \"Each prediction should sum to 1\"\n",
    "        \n",
    "        # Get predicted classes\n",
    "        predicted_classes = np.argmax(iris_predictions.data, axis=1)\n",
    "        print(f\"  Predicted classes: {predicted_classes}\")\n",
    "        print(f\"  Confidence scores: {np.max(iris_predictions.data, axis=1)}\")\n",
    "        \n",
    "        print(\"✅ Iris classification: valid multi-class predictions\")\n",
    "        \n",
    "        # Application 2: Housing Price Prediction\n",
    "        print(\"\\n🏠 Application 2: Housing Price Prediction (Regression)\")\n",
    "        price_predictor = Sequential([\n",
    "            Dense(input_size=8, output_size=16),  # 8 house features\n",
    "            ReLU(),\n",
    "            Dense(input_size=16, output_size=8),\n",
    "            ReLU(),\n",
    "            Dense(input_size=8, output_size=1)    # 1 price output (no activation for regression)\n",
    "        ])\n",
    "        \n",
    "        # Simulate house features: [size, bedrooms, bathrooms, age, location_score, etc.]\n",
    "        house_data = Tensor([\n",
    "            [2000, 3, 2, 5, 8.5, 1, 0, 1],    # Large, new house\n",
    "            [1200, 2, 1, 20, 6.0, 0, 1, 0],   # Small, older house\n",
    "            [1800, 3, 2, 10, 7.5, 1, 0, 0]    # Medium house\n",
    "        ])\n",
    "        \n",
    "        predicted_prices = price_predictor(house_data)\n",
    "        \n",
    "        assert predicted_prices.shape == (3, 1), \"Should predict 1 price for each house\"\n",
    "        assert not np.any(np.isnan(predicted_prices.data)), \"Prices should not be NaN\"\n",
    "        \n",
    "        print(f\"  Predicted prices: {predicted_prices.data.flatten()}\")\n",
    "        print(\"✅ Housing price prediction: valid regression outputs\")\n",
    "        \n",
    "        # Application 3: Sentiment Analysis\n",
    "        print(\"\\n💭 Application 3: Sentiment Analysis (Binary Classification)\")\n",
    "        sentiment_analyzer = create_mlp(\n",
    "            input_size=100,    # 100 text features (like TF-IDF)\n",
    "            hidden_sizes=[50, 25], # Deep network for text\n",
    "            output_size=1,     # Binary sentiment (positive/negative)\n",
    "            output_activation=Sigmoid\n",
    "        )\n",
    "        \n",
    "        # Simulate text features for different reviews\n",
    "        review_features = Tensor(np.random.randn(5, 100))  # 5 reviews\n",
    "        sentiment_scores = sentiment_analyzer(review_features)\n",
    "        \n",
    "        assert sentiment_scores.shape == (5, 1), \"Should predict sentiment for each review\"\n",
    "        assert np.all((sentiment_scores.data >= 0) & (sentiment_scores.data <= 1)), \"Sentiment scores should be probabilities\"\n",
    "        \n",
    "        # Convert to sentiment labels\n",
    "        sentiment_labels = (sentiment_scores.data > 0.5).astype(int)\n",
    "        print(f\"  Sentiment predictions: {sentiment_labels.flatten()}\")\n",
    "        print(f\"  Confidence scores: {sentiment_scores.data.flatten()}\")\n",
    "        \n",
    "        print(\"✅ Sentiment analysis: valid binary classification\")\n",
    "        \n",
    "        # Application 4: MNIST-like Digit Recognition\n",
    "        print(\"\\n🔢 Application 4: Digit Recognition (Image Classification)\")\n",
    "        digit_classifier = create_mlp(\n",
    "            input_size=784,     # 28x28 flattened images\n",
    "            hidden_sizes=[256, 128, 64], # Deep network for images\n",
    "            output_size=10,     # 10 digits (0-9)\n",
    "            output_activation=Softmax\n",
    "        )\n",
    "        \n",
    "        # Simulate flattened digit images\n",
    "        digit_images = Tensor(np.random.randn(8, 784))  # 8 digit images\n",
    "        digit_predictions = digit_classifier(digit_images)\n",
    "        \n",
    "        assert digit_predictions.shape == (8, 10), \"Should predict 10 classes for each image\"\n",
    "        \n",
    "        # Check softmax properties\n",
    "        row_sums = np.sum(digit_predictions.data, axis=1)\n",
    "        assert np.allclose(row_sums, 1.0), \"Each prediction should sum to 1\"\n",
    "        \n",
    "        # Get predicted digits\n",
    "        predicted_digits = np.argmax(digit_predictions.data, axis=1)\n",
    "        confidence_scores = np.max(digit_predictions.data, axis=1)\n",
    "        \n",
    "        print(f\"  Predicted digits: {predicted_digits}\")\n",
    "        print(f\"  Confidence scores: {confidence_scores}\")\n",
    "        \n",
    "        print(\"✅ Digit recognition: valid multi-class image classification\")\n",
    "        \n",
    "        # Application 5: Network Architecture Comparison\n",
    "        print(\"\\n📊 Application 5: Architecture Comparison Study\")\n",
    "        \n",
    "        # Create different architectures for same task\n",
    "        architectures = {\n",
    "            \"Shallow\": create_mlp(4, [16], 3, output_activation=Softmax),\n",
    "            \"Medium\": create_mlp(4, [12, 8], 3, output_activation=Softmax),\n",
    "            \"Deep\": create_mlp(4, [8, 8, 8], 3, output_activation=Softmax),\n",
    "            \"Wide\": create_mlp(4, [24], 3, output_activation=Softmax)\n",
    "        }\n",
    "        \n",
    "        # Test all architectures on same data\n",
    "        test_data = Tensor([[1.0, 2.0, 3.0, 4.0]])\n",
    "        \n",
    "        for name, network in architectures.items():\n",
    "            prediction = network(test_data)\n",
    "            assert prediction.shape == (1, 3), f\"{name} network should output 3 classes\"\n",
    "            assert abs(np.sum(prediction.data) - 1.0) < 1e-6, f\"{name} network should output valid probabilities\"\n",
    "            \n",
    "            # Count parameters\n",
    "            param_count = sum(layer.weights.size + (layer.bias.size if hasattr(layer, 'bias') and layer.bias is not None else 0) \n",
    "                            for layer in network.layers if hasattr(layer, 'weights'))\n",
    "            \n",
    "            print(f\"  {name} network: {param_count} parameters, prediction: {prediction.data.flatten()}\")\n",
    "        \n",
    "        print(\"✅ Architecture comparison: all networks work with different complexities\")\n",
    "        \n",
    "        # Application 6: Transfer Learning Simulation\n",
    "        print(\"\\n🔄 Application 6: Transfer Learning Simulation\")\n",
    "        \n",
    "        # Create \"pre-trained\" feature extractor\n",
    "        feature_extractor = Sequential([\n",
    "            Dense(input_size=100, output_size=50),\n",
    "            ReLU(),\n",
    "            Dense(input_size=50, output_size=25),\n",
    "            ReLU()\n",
    "        ])\n",
    "        \n",
    "        # Create task-specific classifier\n",
    "        classifier_head = Sequential([\n",
    "            Dense(input_size=25, output_size=10),\n",
    "            ReLU(),\n",
    "            Dense(input_size=10, output_size=2),\n",
    "            Softmax()\n",
    "        ])\n",
    "        \n",
    "        # Simulate transfer learning pipeline\n",
    "        raw_data = Tensor(np.random.randn(3, 100))\n",
    "        \n",
    "        # Extract features\n",
    "        features = feature_extractor(raw_data)\n",
    "        assert features.shape == (3, 25), \"Feature extractor should output 25 features\"\n",
    "        \n",
    "        # Classify using extracted features\n",
    "        final_predictions = classifier_head(features)\n",
    "        assert final_predictions.shape == (3, 2), \"Classifier should output 2 classes\"\n",
    "        \n",
    "        row_sums = np.sum(final_predictions.data, axis=1)\n",
    "        assert np.allclose(row_sums, 1.0), \"Transfer learning predictions should be valid\"\n",
    "        \n",
    "        print(\"✅ Transfer learning simulation: modular network composition\")\n",
    "        \n",
    "        print(\"\\n🎉 Integration test passed! Your networks work correctly in:\")\n",
    "        print(\"  • Multi-class classification (Iris flowers)\")\n",
    "        print(\"  • Regression tasks (housing prices)\")\n",
    "        print(\"  • Binary classification (sentiment analysis)\")\n",
    "        print(\"  • Image classification (digit recognition)\")\n",
    "        print(\"  • Architecture comparison studies\")\n",
    "        print(\"  • Transfer learning scenarios\")\n",
    "        print(\"📈 Progress: Networks ready for real ML applications!\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Integration test failed: {e}\")\n",
    "        print(\"\\n💡 This suggests an issue with:\")\n",
    "        print(\"  • Network architecture composition\")\n",
    "        print(\"  • Forward pass through complete networks\")\n",
    "        print(\"  • Shape compatibility between layers\")\n",
    "        print(\"  • Activation function integration\")\n",
    "        print(\"  • Check your Sequential and create_mlp implementations\")\n",
    "        return False\n",
    "\n",
    "# Run the integration test\n",
    "success = test_networks_integration() and success\n",
    "\n",
    "# Print final summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"🎯 NETWORKS MODULE TESTING COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if success:\n",
    "    print(\"🎉 CONGRATULATIONS! All network tests passed!\")\n",
    "    print(\"\\n✅ Your networks module successfully implements:\")\n",
    "    print(\"  • Sequential networks: flexible layer composition\")\n",
    "    print(\"  • MLP creation: automated multi-layer perceptron building\")\n",
    "    print(\"  • Architecture flexibility: shallow, deep, wide networks\")\n",
    "    print(\"  • Multiple activations: ReLU, Tanh, Sigmoid, Softmax\")\n",
    "    print(\"  • Real ML applications: classification, regression, image recognition\")\n",
    "    print(\"  • Network analysis: parameter counting and architecture comparison\")\n",
    "    print(\"  • Transfer learning: modular network composition\")\n",
    "    print(\"\\n🚀 You're ready to tackle any neural network architecture!\")\n",
    "    print(\"📈 Final Progress: Networks Module ✓ COMPLETE\")\n",
    "else:\n",
    "    print(\"⚠️  Some tests failed. Please review the error messages above.\")\n",
    "    print(\"\\n🔧 To fix issues:\")\n",
    "    print(\"  1. Check your Sequential class implementation\")\n",
    "    print(\"  2. Verify create_mlp function layer creation\")\n",
    "    print(\"  3. Ensure proper forward pass through all layers\")\n",
    "    print(\"  4. Test shape compatibility between layers\")\n",
    "    print(\"  5. Verify activation function integration\")\n",
    "    print(\"\\n💪 Keep building! These networks are the foundation of modern AI.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0865036",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 🎯 Module Summary\n",
    "\n",
    "Congratulations! You've successfully implemented complete neural network architectures:\n",
    "\n",
    "### What You've Accomplished\n",
    "✅ **Sequential Networks**: The fundamental architecture for composing layers  \n",
    "✅ **Function Composition**: Understanding how layers combine to create complex behaviors  \n",
    "✅ **MLP Creation**: Building Multi-Layer Perceptrons with flexible architectures  \n",
    "✅ **Architecture Patterns**: Creating shallow, deep, and wide networks  \n",
    "✅ **Forward Pass**: Complete inference through multi-layer networks  \n",
    "\n",
    "### Key Concepts You've Learned\n",
    "- **Networks are function composition**: Complex behavior from simple building blocks\n",
    "- **Sequential architecture**: The foundation of most neural networks\n",
    "- **MLP patterns**: Dense → Activation → Dense → Activation → Output\n",
    "- **Architecture design**: How depth and width affect network capability\n",
    "- **Forward pass**: How data flows through complete networks\n",
    "\n",
    "### Mathematical Foundations\n",
    "- **Function composition**: f(x) = f_n(...f_2(f_1(x)))\n",
    "- **Universal approximation**: MLPs can approximate any continuous function\n",
    "- **Hierarchical learning**: Early layers learn simple features, later layers learn complex patterns\n",
    "- **Nonlinearity**: Activation functions enable complex decision boundaries\n",
    "\n",
    "### Real-World Applications\n",
    "- **Classification**: Image recognition, spam detection, medical diagnosis\n",
    "- **Regression**: Price prediction, time series forecasting\n",
    "- **Feature learning**: Extracting meaningful representations from raw data\n",
    "- **Transfer learning**: Using pre-trained networks for new tasks\n",
    "\n",
    "### Next Steps\n",
    "1. **Export your code**: `tito package nbdev --export 04_networks`\n",
    "2. **Test your implementation**: `tito module test 04_networks`\n",
    "3. **Use your networks**: \n",
    "   ```python\n",
    "   from tinytorch.core.networks import Sequential, create_mlp\n",
    "   from tinytorch.core.layers import Dense\n",
    "   from tinytorch.core.activations import ReLU\n",
    "   \n",
    "   # Create custom network\n",
    "   network = Sequential([Dense(10, 5), ReLU(), Dense(5, 1)])\n",
    "   \n",
    "   # Create MLP\n",
    "   mlp = create_mlp(10, [20, 10], 1)\n",
    "   ```\n",
    "4. **Move to Module 5**: Start building convolutional networks for images!\n",
    "\n",
    "**Ready for the next challenge?** Let's add convolutional layers for image processing and build CNNs!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
