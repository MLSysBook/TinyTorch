{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be8121ef",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Activations - Nonlinearity in Neural Networks\n",
    "\n",
    "Welcome to the Activations module! This is where neural networks get their power through nonlinearity.\n",
    "\n",
    "## Learning Goals\n",
    "- Understand why activation functions are essential for neural networks\n",
    "- Implement the four most important activation functions: ReLU, Sigmoid, Tanh, and Softmax\n",
    "- Visualize how activations transform data and enable complex learning\n",
    "- See how activations work with layers to build powerful networks\n",
    "- Master the NBGrader workflow with comprehensive testing\n",
    "\n",
    "## Build â†’ Use â†’ Understand\n",
    "1. **Build**: Activation functions that add nonlinearity\n",
    "2. **Use**: Transform tensors and see immediate results\n",
    "3. **Understand**: How nonlinearity enables complex pattern learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961899d6",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "activations-imports",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| default_exp core.activations\n",
    "\n",
    "#| export\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from typing import Union, List\n",
    "\n",
    "# Import our Tensor class - try from package first, then from local module\n",
    "try:\n",
    "    from tinytorch.core.tensor import Tensor\n",
    "except ImportError:\n",
    "    # For development, import from local tensor module\n",
    "    sys.path.append(os.path.join(os.path.dirname(__file__), '..', '01_tensor'))\n",
    "    from tensor_dev import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad2f6a1",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "activations-setup",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "def _should_show_plots():\n",
    "    \"\"\"Check if we should show plots (disable during testing)\"\"\"\n",
    "    # Check multiple conditions that indicate we're in test mode\n",
    "    is_pytest = (\n",
    "        'pytest' in sys.modules or\n",
    "        'test' in sys.argv or\n",
    "        os.environ.get('PYTEST_CURRENT_TEST') is not None or\n",
    "        any('test' in arg for arg in sys.argv) or\n",
    "        any('pytest' in arg for arg in sys.argv)\n",
    "    )\n",
    "    \n",
    "    # Show plots in development mode (when not in test mode)\n",
    "    return not is_pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d45d18",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "activations-welcome",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"ðŸ”¥ TinyTorch Activations Module\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Python version: {sys.version_info.major}.{sys.version_info.minor}\")\n",
    "print(\"Ready to build activation functions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a504e812",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ðŸ“¦ Where This Code Lives in the Final Package\n",
    "\n",
    "**Learning Side:** You work in `modules/source/02_activations/activations_dev.py`  \n",
    "**Building Side:** Code exports to `tinytorch.core.activations`\n",
    "\n",
    "```python\n",
    "# Final package structure:\n",
    "from tinytorch.core.activations import ReLU, Sigmoid, Tanh, Softmax\n",
    "from tinytorch.core.tensor import Tensor  # Foundation\n",
    "from tinytorch.core.layers import Dense  # Uses activations\n",
    "```\n",
    "\n",
    "**Why this matters:**\n",
    "- **Learning:** Focused modules for deep understanding\n",
    "- **Production:** Proper organization like PyTorch's `torch.nn.ReLU`\n",
    "- **Consistency:** All activation functions live together in `core.activations`\n",
    "- **Integration:** Works seamlessly with tensors and layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e5d721",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## What Are Activation Functions?\n",
    "\n",
    "### The Problem: Linear Limitations\n",
    "Without activation functions, neural networks can only learn linear relationships:\n",
    "```\n",
    "y = Wâ‚ Â· (Wâ‚‚ Â· (Wâ‚ƒ Â· x + bâ‚ƒ) + bâ‚‚) + bâ‚\n",
    "```\n",
    "\n",
    "This simplifies to just:\n",
    "```\n",
    "y = W_combined Â· x + b_combined\n",
    "```\n",
    "\n",
    "**A single linear function!** No matter how many layers you add, you can't learn complex patterns like:\n",
    "- Image recognition (nonlinear pixel relationships)\n",
    "- Language understanding (nonlinear word relationships) \n",
    "- Game playing (nonlinear strategy relationships)\n",
    "\n",
    "### The Solution: Nonlinearity\n",
    "Activation functions add nonlinearity between layers:\n",
    "```\n",
    "y = Wâ‚ Â· f(Wâ‚‚ Â· f(Wâ‚ƒ Â· x + bâ‚ƒ) + bâ‚‚) + bâ‚\n",
    "```\n",
    "\n",
    "Now each layer can learn complex transformations!\n",
    "\n",
    "### Real-World Impact\n",
    "- **Before activations**: Only linear classifiers (logistic regression)\n",
    "- **After activations**: Complex pattern recognition (deep learning revolution)\n",
    "\n",
    "### What We'll Build\n",
    "1. **ReLU**: The foundation of modern deep learning\n",
    "2. **Sigmoid**: Classic activation for binary classification\n",
    "3. **Tanh**: Centered activation for better gradients\n",
    "4. **Softmax**: Probability distributions for multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d72ee2",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 1: ReLU - The Foundation of Deep Learning\n",
    "\n",
    "### What is ReLU?\n",
    "**ReLU (Rectified Linear Unit)** is the most important activation function in deep learning:\n",
    "\n",
    "```\n",
    "f(x) = max(0, x)\n",
    "```\n",
    "\n",
    "- **Positive inputs**: Pass through unchanged\n",
    "- **Negative inputs**: Become zero\n",
    "- **Zero**: Stays zero\n",
    "\n",
    "### Why ReLU Revolutionized Deep Learning\n",
    "1. **Computational efficiency**: Just a max operation\n",
    "2. **No vanishing gradients**: Derivative is 1 for positive values\n",
    "3. **Sparsity**: Many neurons output exactly 0\n",
    "4. **Empirical success**: Works well in practice\n",
    "\n",
    "### Visual Understanding\n",
    "```\n",
    "Input:  [-2, -1, 0, 1, 2]\n",
    "ReLU:   [ 0,  0, 0, 1, 2]\n",
    "```\n",
    "\n",
    "### Real-World Applications\n",
    "- **Image classification**: ResNet, VGG, AlexNet\n",
    "- **Object detection**: YOLO, R-CNN\n",
    "- **Language models**: Transformer feedforward layers\n",
    "- **Recommendation**: Deep collaborative filtering\n",
    "\n",
    "### Mathematical Properties\n",
    "- **Derivative**: f'(x) = 1 if x > 0, else 0\n",
    "- **Range**: [0, âˆž)\n",
    "- **Sparsity**: Outputs exactly 0 for negative inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d036432",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "relu-class",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class ReLU:\n",
    "    \"\"\"\n",
    "    ReLU Activation Function: f(x) = max(0, x)\n",
    "    \n",
    "    The most popular activation function in deep learning.\n",
    "    Simple, fast, and effective for most applications.\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply ReLU activation: f(x) = max(0, x)\n",
    "        \n",
    "        TODO: Implement ReLU activation function.\n",
    "        \n",
    "        STEP-BY-STEP IMPLEMENTATION:\n",
    "        1. For each element in the input tensor, apply max(0, element)\n",
    "        2. Use NumPy's maximum function for efficient element-wise operation\n",
    "        3. Return a new tensor of the same type with the results\n",
    "        4. Preserve the input tensor's shape\n",
    "        \n",
    "        EXAMPLE USAGE:\n",
    "        ```python\n",
    "        relu = ReLU()\n",
    "        input_tensor = Tensor([[-2, -1, 0, 1, 2]])\n",
    "        output = relu(input_tensor)\n",
    "        print(output.data)  # [[0, 0, 0, 1, 2]]\n",
    "        ```\n",
    "        \n",
    "        IMPLEMENTATION HINTS:\n",
    "        - Use np.maximum(0, x.data) for element-wise max with 0\n",
    "        - Return the same type as input: return type(x)(result)\n",
    "        - The shape should remain the same as input\n",
    "        - Don't modify the input tensor (immutable operations)\n",
    "        \n",
    "        LEARNING CONNECTIONS:\n",
    "        - This is like torch.nn.ReLU() in PyTorch\n",
    "        - Used in virtually every modern neural network\n",
    "        - Enables deep networks by preventing vanishing gradients\n",
    "        - Creates sparse representations (many zeros)\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        result = np.maximum(0, x.data)\n",
    "        return type(x)(result)\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Make the class callable: relu(x) instead of relu.forward(x)\"\"\"\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d92ddc5",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### ðŸ§ª Test Your ReLU Implementation\n",
    "\n",
    "Once you implement the ReLU forward method above, run this cell to test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3049a30",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-relu-immediate",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_relu_activation():\n",
    "    \"\"\"Test ReLU activation function\"\"\"\n",
    "    print(\"ðŸ”¬ Unit Test: ReLU Activation...\")\n",
    "\n",
    "    # Create ReLU instance\n",
    "    relu = ReLU()\n",
    "\n",
    "    # Test with mixed positive/negative values\n",
    "    test_input = Tensor([[-2, -1, 0, 1, 2]])\n",
    "    result = relu(test_input)\n",
    "    expected = np.array([[0, 0, 0, 1, 2]])\n",
    "    \n",
    "    assert np.array_equal(result.data, expected), f\"ReLU failed: expected {expected}, got {result.data}\"\n",
    "    \n",
    "    # Test that negative values become zero\n",
    "    assert np.all(result.data >= 0), \"ReLU should make all negative values zero\"\n",
    "    \n",
    "    # Test that positive values remain unchanged\n",
    "    positive_input = Tensor([[1, 2, 3, 4, 5]])\n",
    "    positive_result = relu(positive_input)\n",
    "    assert np.array_equal(positive_result.data, positive_input.data), \"ReLU should preserve positive values\"\n",
    "    \n",
    "    # Test with 2D tensor\n",
    "    matrix_input = Tensor([[-1, 2], [3, -4]])\n",
    "    matrix_result = relu(matrix_input)\n",
    "    matrix_expected = np.array([[0, 2], [3, 0]])\n",
    "    assert np.array_equal(matrix_result.data, matrix_expected), \"ReLU should work with 2D tensors\"\n",
    "    \n",
    "    # Test shape preservation\n",
    "    assert matrix_result.shape == matrix_input.shape, \"ReLU should preserve input shape\"\n",
    "    \n",
    "    print(\"âœ… ReLU activation tests passed!\")\n",
    "    print(f\"âœ… Negative values correctly zeroed\")\n",
    "    print(f\"âœ… Positive values preserved\")\n",
    "    print(f\"âœ… Shape preservation working\")\n",
    "    print(f\"âœ… Works with multi-dimensional tensors\")\n",
    "\n",
    "# Run the test\n",
    "test_relu_activation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ff8f4e",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 2: Sigmoid - Classic Binary Classification\n",
    "\n",
    "### What is Sigmoid?\n",
    "**Sigmoid** is the classic activation function that maps any real number to (0, 1):\n",
    "\n",
    "```\n",
    "f(x) = 1 / (1 + e^(-x))\n",
    "```\n",
    "\n",
    "### Why Sigmoid Matters\n",
    "1. **Probability interpretation**: Outputs between 0 and 1\n",
    "2. **Smooth gradients**: Differentiable everywhere\n",
    "3. **Historical importance**: Enabled early neural networks\n",
    "4. **Binary classification**: Perfect for yes/no decisions\n",
    "\n",
    "### Visual Understanding\n",
    "```\n",
    "Input:  [-âˆž, -2, -1, 0, 1, 2, âˆž]\n",
    "Sigmoid:[0,  0.12, 0.27, 0.5, 0.73, 0.88, 1]\n",
    "```\n",
    "\n",
    "### Real-World Applications\n",
    "- **Binary classification**: Spam detection, medical diagnosis\n",
    "- **Gating mechanisms**: LSTM and GRU cells\n",
    "- **Output layers**: When you need probabilities\n",
    "- **Attention mechanisms**: Where to focus attention\n",
    "\n",
    "### Mathematical Properties\n",
    "- **Range**: (0, 1)\n",
    "- **Derivative**: f'(x) = f(x) Â· (1 - f(x))\n",
    "- **Centered**: f(0) = 0.5\n",
    "- **Symmetric**: f(-x) = 1 - f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96622d93",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "sigmoid-class",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class Sigmoid:\n",
    "    \"\"\"\n",
    "    Sigmoid Activation Function: f(x) = 1 / (1 + e^(-x))\n",
    "    \n",
    "    Maps any real number to the range (0, 1).\n",
    "    Useful for binary classification and probability outputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply Sigmoid activation: f(x) = 1 / (1 + e^(-x))\n",
    "        \n",
    "        TODO: Implement Sigmoid activation function.\n",
    "        \n",
    "        STEP-BY-STEP IMPLEMENTATION:\n",
    "        1. Compute the negative of input: -x.data\n",
    "        2. Compute the exponential: np.exp(-x.data)\n",
    "        3. Add 1 to the exponential: 1 + np.exp(-x.data)\n",
    "        4. Take the reciprocal: 1 / (1 + np.exp(-x.data))\n",
    "        5. Return as new Tensor\n",
    "        \n",
    "        EXAMPLE USAGE:\n",
    "        ```python\n",
    "        sigmoid = Sigmoid()\n",
    "        input_tensor = Tensor([[-2, -1, 0, 1, 2]])\n",
    "        output = sigmoid(input_tensor)\n",
    "        print(output.data)  # [[0.119, 0.269, 0.5, 0.731, 0.881]]\n",
    "        ```\n",
    "        \n",
    "        IMPLEMENTATION HINTS:\n",
    "        - Use np.exp() for exponential function\n",
    "        - Formula: 1 / (1 + np.exp(-x.data))\n",
    "        - Handle potential overflow with np.clip(-x.data, -500, 500)\n",
    "        - Return Tensor(result)\n",
    "        \n",
    "        LEARNING CONNECTIONS:\n",
    "        - This is like torch.nn.Sigmoid() in PyTorch\n",
    "        - Used in binary classification output layers\n",
    "        - Key component in LSTM and GRU gating mechanisms\n",
    "        - Historically important for early neural networks\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        # Clip to prevent overflow\n",
    "        clipped_input = np.clip(-x.data, -500, 500)\n",
    "        result = 1 / (1 + np.exp(clipped_input))\n",
    "        return type(x)(result)\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Make the class callable: sigmoid(x) instead of sigmoid.forward(x)\"\"\"\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fc932b",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### ðŸ§ª Test Your Sigmoid Implementation\n",
    "\n",
    "Once you implement the Sigmoid forward method above, run this cell to test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adc7dbd",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-sigmoid-immediate",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_sigmoid_activation():\n",
    "    \"\"\"Test Sigmoid activation function\"\"\"\n",
    "    print(\"ðŸ”¬ Unit Test: Sigmoid Activation...\")\n",
    "\n",
    "# Create Sigmoid instance\n",
    "    sigmoid = Sigmoid()\n",
    "\n",
    "    # Test with known values\n",
    "    test_input = Tensor([[0]])\n",
    "    result = sigmoid(test_input)\n",
    "    expected = 0.5\n",
    "    \n",
    "    assert abs(result.data[0][0] - expected) < 1e-6, f\"Sigmoid(0) should be 0.5, got {result.data[0][0]}\"\n",
    "    \n",
    "    # Test with positive and negative values\n",
    "    test_input = Tensor([[-2, -1, 0, 1, 2]])\n",
    "    result = sigmoid(test_input)\n",
    "    \n",
    "    # Check that all values are between 0 and 1\n",
    "    assert np.all(result.data > 0), \"Sigmoid output should be > 0\"\n",
    "    assert np.all(result.data < 1), \"Sigmoid output should be < 1\"\n",
    "    \n",
    "    # Test symmetry: sigmoid(-x) = 1 - sigmoid(x)\n",
    "    x_val = 1.0\n",
    "    pos_result = sigmoid(Tensor([[x_val]]))\n",
    "    neg_result = sigmoid(Tensor([[-x_val]]))\n",
    "    symmetry_check = abs(pos_result.data[0][0] + neg_result.data[0][0] - 1.0)\n",
    "    assert symmetry_check < 1e-6, \"Sigmoid should be symmetric around 0.5\"\n",
    "    \n",
    "    # Test with 2D tensor\n",
    "    matrix_input = Tensor([[-1, 1], [0, 2]])\n",
    "    matrix_result = sigmoid(matrix_input)\n",
    "    assert matrix_result.shape == matrix_input.shape, \"Sigmoid should preserve shape\"\n",
    "    \n",
    "    # Test extreme values (should not overflow)\n",
    "    extreme_input = Tensor([[-100, 100]])\n",
    "    extreme_result = sigmoid(extreme_input)\n",
    "    assert not np.any(np.isnan(extreme_result.data)), \"Sigmoid should handle extreme values\"\n",
    "    assert not np.any(np.isinf(extreme_result.data)), \"Sigmoid should not produce inf values\"\n",
    "    \n",
    "    print(\"âœ… Sigmoid activation tests passed!\")\n",
    "    print(f\"âœ… Outputs correctly bounded between 0 and 1\")\n",
    "    print(f\"âœ… Symmetric property verified\")\n",
    "    print(f\"âœ… Handles extreme values without overflow\")\n",
    "    print(f\"âœ… Shape preservation working\")\n",
    "\n",
    "# Run the test\n",
    "test_sigmoid_activation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ec7799",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 3: Tanh - Centered Activation\n",
    "\n",
    "### What is Tanh?\n",
    "**Tanh (Hyperbolic Tangent)** is similar to sigmoid but centered around zero:\n",
    "\n",
    "```\n",
    "f(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "```\n",
    "\n",
    "### Why Tanh is Better Than Sigmoid\n",
    "1. **Zero-centered**: Outputs range from -1 to 1\n",
    "2. **Better gradients**: Helps with gradient flow in deep networks\n",
    "3. **Faster convergence**: Less bias shift during training\n",
    "4. **Stronger gradients**: Maximum gradient is 1 vs 0.25 for sigmoid\n",
    "\n",
    "### Visual Understanding\n",
    "```\n",
    "Input: [-âˆž, -2, -1, 0, 1, 2, âˆž]\n",
    "Tanh:  [-1, -0.96, -0.76, 0, 0.76, 0.96, 1]\n",
    "```\n",
    "\n",
    "### Real-World Applications\n",
    "- **Hidden layers**: Better than sigmoid for internal activations\n",
    "- **RNN cells**: Classic RNN and LSTM use tanh\n",
    "- **Normalization**: When you need zero-centered outputs\n",
    "- **Feature scaling**: Maps inputs to [-1, 1] range\n",
    "\n",
    "### Mathematical Properties\n",
    "- **Range**: (-1, 1)\n",
    "- **Derivative**: f'(x) = 1 - f(x)Â²\n",
    "- **Zero-centered**: f(0) = 0\n",
    "- **Antisymmetric**: f(-x) = -f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7759b33e",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "tanh-class",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class Tanh:\n",
    "    \"\"\"\n",
    "    Tanh Activation Function: f(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "    \n",
    "    Zero-centered activation function with range (-1, 1).\n",
    "    Better gradient properties than sigmoid.\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply Tanh activation: f(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "        \n",
    "        TODO: Implement Tanh activation function.\n",
    "        \n",
    "        STEP-BY-STEP IMPLEMENTATION:\n",
    "        1. Use NumPy's built-in tanh function: np.tanh(x.data)\n",
    "        2. Alternatively, implement manually:\n",
    "           - Compute e^x and e^(-x)\n",
    "           - Calculate (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "        3. Return as new Tensor\n",
    "        \n",
    "        EXAMPLE USAGE:\n",
    "        ```python\n",
    "        tanh = Tanh()\n",
    "        input_tensor = Tensor([[-2, -1, 0, 1, 2]])\n",
    "        output = tanh(input_tensor)\n",
    "        print(output.data)  # [[-0.964, -0.762, 0, 0.762, 0.964]]\n",
    "        ```\n",
    "        \n",
    "        IMPLEMENTATION HINTS:\n",
    "        - Use np.tanh(x.data) for simplicity\n",
    "        - Manual implementation: (np.exp(x.data) - np.exp(-x.data)) / (np.exp(x.data) + np.exp(-x.data))\n",
    "        - Handle overflow by clipping inputs: np.clip(x.data, -500, 500)\n",
    "        - Return Tensor(result)\n",
    "        \n",
    "        LEARNING CONNECTIONS:\n",
    "        - This is like torch.nn.Tanh() in PyTorch\n",
    "        - Used in RNN, LSTM, and GRU cells\n",
    "        - Better than sigmoid for hidden layers\n",
    "        - Zero-centered outputs help with gradient flow\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        # Use NumPy's built-in tanh function\n",
    "        result = np.tanh(x.data)\n",
    "        return type(x)(result)\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Make the class callable: tanh(x) instead of tanh.forward(x)\"\"\"\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23418296",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### ðŸ§ª Test Your Tanh Implementation\n",
    "\n",
    "Once you implement the Tanh forward method above, run this cell to test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e237b49a",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-tanh-immediate",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_tanh_activation():\n",
    "    \"\"\"Test Tanh activation function\"\"\"\n",
    "    print(\"ðŸ”¬ Unit Test: Tanh Activation...\")\n",
    "\n",
    "# Create Tanh instance\n",
    "    tanh = Tanh()\n",
    "\n",
    "    # Test with zero (should be 0)\n",
    "    test_input = Tensor([[0]])\n",
    "    result = tanh(test_input)\n",
    "    expected = 0.0\n",
    "    \n",
    "    assert abs(result.data[0][0] - expected) < 1e-6, f\"Tanh(0) should be 0, got {result.data[0][0]}\"\n",
    "    \n",
    "    # Test with positive and negative values\n",
    "    test_input = Tensor([[-2, -1, 0, 1, 2]])\n",
    "    result = tanh(test_input)\n",
    "    \n",
    "    # Check that all values are between -1 and 1\n",
    "    assert np.all(result.data > -1), \"Tanh output should be > -1\"\n",
    "    assert np.all(result.data < 1), \"Tanh output should be < 1\"\n",
    "    \n",
    "    # Test antisymmetry: tanh(-x) = -tanh(x)\n",
    "    x_val = 1.5\n",
    "    pos_result = tanh(Tensor([[x_val]]))\n",
    "    neg_result = tanh(Tensor([[-x_val]]))\n",
    "    antisymmetry_check = abs(pos_result.data[0][0] + neg_result.data[0][0])\n",
    "    assert antisymmetry_check < 1e-6, \"Tanh should be antisymmetric\"\n",
    "    \n",
    "    # Test with 2D tensor\n",
    "    matrix_input = Tensor([[-1, 1], [0, 2]])\n",
    "    matrix_result = tanh(matrix_input)\n",
    "    assert matrix_result.shape == matrix_input.shape, \"Tanh should preserve shape\"\n",
    "    \n",
    "    # Test extreme values (should not overflow)\n",
    "    extreme_input = Tensor([[-100, 100]])\n",
    "    extreme_result = tanh(extreme_input)\n",
    "    assert not np.any(np.isnan(extreme_result.data)), \"Tanh should handle extreme values\"\n",
    "    assert not np.any(np.isinf(extreme_result.data)), \"Tanh should not produce inf values\"\n",
    "    \n",
    "    # Test that extreme values approach Â±1\n",
    "    assert abs(extreme_result.data[0][0] - (-1)) < 1e-6, \"Tanh(-âˆž) should approach -1\"\n",
    "    assert abs(extreme_result.data[0][1] - 1) < 1e-6, \"Tanh(âˆž) should approach 1\"\n",
    "    \n",
    "    print(\"âœ… Tanh activation tests passed!\")\n",
    "    print(f\"âœ… Outputs correctly bounded between -1 and 1\")\n",
    "    print(f\"âœ… Antisymmetric property verified\")\n",
    "    print(f\"âœ… Zero-centered (tanh(0) = 0)\")\n",
    "    print(f\"âœ… Handles extreme values correctly\")\n",
    "\n",
    "# Run the test\n",
    "test_tanh_activation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca26ddb0",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 4: Softmax - Probability Distributions\n",
    "\n",
    "### What is Softmax?\n",
    "**Softmax** converts a vector of real numbers into a probability distribution:\n",
    "\n",
    "```\n",
    "f(x_i) = e^(x_i) / Î£(e^(x_j))\n",
    "```\n",
    "\n",
    "### Why Softmax is Essential\n",
    "1. **Probability distribution**: Outputs sum to 1\n",
    "2. **Multi-class classification**: Choose one class from many\n",
    "3. **Interpretable**: Each output is a probability\n",
    "4. **Differentiable**: Enables gradient-based learning\n",
    "\n",
    "### Visual Understanding\n",
    "```\n",
    "Input:  [1, 2, 3]\n",
    "Softmax:[0.09, 0.24, 0.67]  # Sums to 1.0\n",
    "```\n",
    "\n",
    "### Real-World Applications\n",
    "- **Classification**: Image classification, text classification\n",
    "- **Language models**: Next word prediction\n",
    "- **Attention mechanisms**: Where to focus attention\n",
    "- **Reinforcement learning**: Action selection probabilities\n",
    "\n",
    "### Mathematical Properties\n",
    "- **Range**: (0, 1) for each output\n",
    "- **Constraint**: Î£(f(x_i)) = 1\n",
    "- **Argmax preservation**: Doesn't change relative ordering\n",
    "- **Temperature scaling**: Can be made sharper or softer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcc52c8",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "softmax-class",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class Softmax:\n",
    "    \"\"\"\n",
    "    Softmax Activation Function: f(x_i) = e^(x_i) / Î£(e^(x_j))\n",
    "    \n",
    "    Converts a vector of real numbers into a probability distribution.\n",
    "    Essential for multi-class classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply Softmax activation: f(x_i) = e^(x_i) / Î£(e^(x_j))\n",
    "        \n",
    "        TODO: Implement Softmax activation function.\n",
    "        \n",
    "        STEP-BY-STEP IMPLEMENTATION:\n",
    "        1. Handle empty input case\n",
    "        2. Subtract max value for numerical stability: x - max(x)\n",
    "        3. Compute exponentials: np.exp(x - max(x))\n",
    "        4. Compute sum of exponentials: np.sum(exp_values)\n",
    "        5. Divide each exponential by the sum: exp_values / sum\n",
    "        6. Return as same tensor type as input\n",
    "        \n",
    "        EXAMPLE USAGE:\n",
    "        ```python\n",
    "        softmax = Softmax()\n",
    "        input_tensor = Tensor([[1, 2, 3]])\n",
    "        output = softmax(input_tensor)\n",
    "        print(output.data)  # [[0.09, 0.24, 0.67]]\n",
    "        print(np.sum(output.data))  # 1.0\n",
    "        ```\n",
    "        \n",
    "        IMPLEMENTATION HINTS:\n",
    "        - Handle empty case: if x.data.size == 0: return type(x)(x.data.copy())\n",
    "        - Subtract max for numerical stability: x_shifted = x.data - np.max(x.data, axis=-1, keepdims=True)\n",
    "        - Compute exponentials: exp_values = np.exp(x_shifted)\n",
    "        - Sum along last axis: sum_exp = np.sum(exp_values, axis=-1, keepdims=True)\n",
    "        - Divide: result = exp_values / sum_exp\n",
    "        - Return same type as input: return type(x)(result)\n",
    "        \n",
    "        LEARNING CONNECTIONS:\n",
    "        - This is like torch.nn.Softmax() in PyTorch\n",
    "        - Used in classification output layers\n",
    "        - Key component in attention mechanisms\n",
    "        - Enables probability-based decision making\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        # Handle empty input\n",
    "        if x.data.size == 0:\n",
    "            return type(x)(x.data.copy())\n",
    "        \n",
    "        # Subtract max for numerical stability\n",
    "        x_shifted = x.data - np.max(x.data, axis=-1, keepdims=True)\n",
    "        \n",
    "        # Compute exponentials\n",
    "        exp_values = np.exp(x_shifted)\n",
    "        \n",
    "        # Sum along last axis\n",
    "        sum_exp = np.sum(exp_values, axis=-1, keepdims=True)\n",
    "        \n",
    "        # Divide to get probabilities\n",
    "        result = exp_values / sum_exp\n",
    "        \n",
    "        return type(x)(result)\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Make the class callable: softmax(x) instead of softmax.forward(x)\"\"\"\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747e807d",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### ðŸ§ª Test Your Softmax Implementation\n",
    "\n",
    "Once you implement the Softmax forward method above, run this cell to test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1695c1d7",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-softmax-immediate",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_softmax_activation():\n",
    "    \"\"\"Test Softmax activation function\"\"\"\n",
    "    print(\"ðŸ”¬ Unit Test: Softmax Activation...\")\n",
    "\n",
    "# Create Softmax instance\n",
    "    softmax = Softmax()\n",
    "\n",
    "    # Test with simple input\n",
    "    test_input = Tensor([[1, 2, 3]])\n",
    "    result = softmax(test_input)\n",
    "    \n",
    "    # Check that outputs sum to 1\n",
    "    output_sum = np.sum(result.data)\n",
    "    assert abs(output_sum - 1.0) < 1e-6, f\"Softmax outputs should sum to 1, got {output_sum}\"\n",
    "    \n",
    "    # Check that all outputs are positive\n",
    "    assert np.all(result.data > 0), \"Softmax outputs should be positive\"\n",
    "    assert np.all(result.data < 1), \"Softmax outputs should be less than 1\"\n",
    "    \n",
    "    # Test with uniform input (should give equal probabilities)\n",
    "    uniform_input = Tensor([[1, 1, 1]])\n",
    "    uniform_result = softmax(uniform_input)\n",
    "    expected_prob = 1.0 / 3.0\n",
    "    \n",
    "    for prob in uniform_result.data[0]:\n",
    "        assert abs(prob - expected_prob) < 1e-6, f\"Uniform input should give equal probabilities\"\n",
    "    \n",
    "    # Test with batch input (multiple samples)\n",
    "    batch_input = Tensor([[1, 2, 3], [4, 5, 6]])\n",
    "    batch_result = softmax(batch_input)\n",
    "    \n",
    "    # Check that each row sums to 1\n",
    "    for i in range(batch_input.shape[0]):\n",
    "        row_sum = np.sum(batch_result.data[i])\n",
    "        assert abs(row_sum - 1.0) < 1e-6, f\"Each row should sum to 1, row {i} sums to {row_sum}\"\n",
    "    \n",
    "    # Test numerical stability with large values\n",
    "    large_input = Tensor([[1000, 1001, 1002]])\n",
    "    large_result = softmax(large_input)\n",
    "    \n",
    "    assert not np.any(np.isnan(large_result.data)), \"Softmax should handle large values\"\n",
    "    assert not np.any(np.isinf(large_result.data)), \"Softmax should not produce inf values\"\n",
    "    \n",
    "    large_sum = np.sum(large_result.data)\n",
    "    assert abs(large_sum - 1.0) < 1e-6, \"Large values should still sum to 1\"\n",
    "\n",
    "# Test shape preservation\n",
    "    assert batch_result.shape == batch_input.shape, \"Softmax should preserve shape\"\n",
    "    \n",
    "    print(\"âœ… Softmax activation tests passed!\")\n",
    "    print(f\"âœ… Outputs sum to 1 (probability distribution)\")\n",
    "    print(f\"âœ… All outputs are positive\")\n",
    "    print(f\"âœ… Handles uniform inputs correctly\")\n",
    "    print(f\"âœ… Works with batch inputs\")\n",
    "    print(f\"âœ… Numerically stable with large values\")\n",
    "\n",
    "# Run the test\n",
    "test_softmax_activation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7807a6",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## ðŸŽ¯ Comprehensive Test: All Activations Working Together\n",
    "\n",
    "### Real-World Scenario\n",
    "Let's test how all activation functions work together in a realistic neural network scenario:\n",
    "\n",
    "- **Input processing**: Raw data transformation\n",
    "- **Hidden layers**: ReLU for internal processing\n",
    "- **Output layer**: Softmax for classification\n",
    "- **Comparison**: See how different activations transform the same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e64dc4b",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-activations-comprehensive",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_activations():\n",
    "    \"\"\"Test all activation functions working together\"\"\"\n",
    "    print(\"ðŸ”¬ Unit Test: Activation Functions Comprehensive Test...\")\n",
    "    \n",
    "    # Create instances of all activation functions\n",
    "    relu = ReLU()\n",
    "    sigmoid = Sigmoid()\n",
    "    tanh = Tanh()\n",
    "    softmax = Softmax()\n",
    "    \n",
    "    # Test data: simulating neural network layer outputs\n",
    "    test_data = Tensor([[-2, -1, 0, 1, 2]])\n",
    "    \n",
    "    # Apply each activation function\n",
    "    relu_result = relu(test_data)\n",
    "    sigmoid_result = sigmoid(test_data)\n",
    "    tanh_result = tanh(test_data)\n",
    "    softmax_result = softmax(test_data)\n",
    "    \n",
    "    # Test that all functions preserve input shape\n",
    "    assert relu_result.shape == test_data.shape, \"ReLU should preserve shape\"\n",
    "    assert sigmoid_result.shape == test_data.shape, \"Sigmoid should preserve shape\"\n",
    "    assert tanh_result.shape == test_data.shape, \"Tanh should preserve shape\"\n",
    "    assert softmax_result.shape == test_data.shape, \"Softmax should preserve shape\"\n",
    "    \n",
    "    # Test that all functions return Tensor objects\n",
    "    assert isinstance(relu_result, Tensor), \"ReLU should return Tensor\"\n",
    "    assert isinstance(sigmoid_result, Tensor), \"Sigmoid should return Tensor\"\n",
    "    assert isinstance(tanh_result, Tensor), \"Tanh should return Tensor\"\n",
    "    assert isinstance(softmax_result, Tensor), \"Softmax should return Tensor\"\n",
    "    \n",
    "    # Test ReLU properties\n",
    "    assert np.all(relu_result.data >= 0), \"ReLU output should be non-negative\"\n",
    "    \n",
    "    # Test Sigmoid properties\n",
    "    assert np.all(sigmoid_result.data > 0), \"Sigmoid output should be positive\"\n",
    "    assert np.all(sigmoid_result.data < 1), \"Sigmoid output should be less than 1\"\n",
    "    \n",
    "    # Test Tanh properties\n",
    "    assert np.all(tanh_result.data > -1), \"Tanh output should be > -1\"\n",
    "    assert np.all(tanh_result.data < 1), \"Tanh output should be < 1\"\n",
    "    \n",
    "    # Test Softmax properties\n",
    "    softmax_sum = np.sum(softmax_result.data)\n",
    "    assert abs(softmax_sum - 1.0) < 1e-6, \"Softmax outputs should sum to 1\"\n",
    "    \n",
    "    # Test chaining activations (realistic neural network scenario)\n",
    "    # Hidden layer with ReLU\n",
    "    hidden_output = relu(test_data)\n",
    "    \n",
    "    # Add some weights simulation (element-wise multiplication)\n",
    "    weights = Tensor([[0.5, 0.3, 0.8, 0.2, 0.7]])\n",
    "    weighted_output = hidden_output * weights\n",
    "    \n",
    "    # Final layer with Softmax\n",
    "    final_output = softmax(weighted_output)\n",
    "    \n",
    "    # Test that chained operations work\n",
    "    assert isinstance(final_output, Tensor), \"Chained operations should return Tensor\"\n",
    "    assert abs(np.sum(final_output.data) - 1.0) < 1e-6, \"Final output should be valid probability\"\n",
    "    \n",
    "    # Test with batch data (multiple samples)\n",
    "    batch_data = Tensor([\n",
    "    [-2, -1, 0, 1, 2],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [-1, 0, 1, 2, 3]\n",
    "    ])\n",
    "    \n",
    "    batch_softmax = softmax(batch_data)\n",
    "    \n",
    "    # Each row should sum to 1\n",
    "    for i in range(batch_data.shape[0]):\n",
    "        row_sum = np.sum(batch_softmax.data[i])\n",
    "        assert abs(row_sum - 1.0) < 1e-6, f\"Batch row {i} should sum to 1\"\n",
    "    \n",
    "    print(\"âœ… Activation functions comprehensive tests passed!\")\n",
    "    print(f\"âœ… All functions work together seamlessly\")\n",
    "    print(f\"âœ… Shape preservation across all activations\")\n",
    "    print(f\"âœ… Chained operations work correctly\")\n",
    "    print(f\"âœ… Batch processing works for all activations\")\n",
    "    print(f\"âœ… Ready for neural network integration!\")\n",
    "\n",
    "# Run the comprehensive test\n",
    "test_activations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4917e71a",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ðŸ§ª Module Testing\n",
    "\n",
    "Time to test your implementation! This section uses TinyTorch's standardized testing framework to ensure your implementation works correctly.\n",
    "\n",
    "**This testing section is locked** - it provides consistent feedback across all modules and cannot be modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cdd85e",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "standardized-testing",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STANDARDIZED MODULE TESTING - DO NOT MODIFY\n",
    "# This cell is locked to ensure consistent testing across all TinyTorch modules\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from tito.tools.testing import run_module_tests_auto\n",
    "    \n",
    "    # Automatically discover and run all tests in this module\n",
    "    success = run_module_tests_auto(\"Activations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792a556f",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ðŸŽ¯ Module Summary: Activation Functions Mastery!\n",
    "\n",
    "    Congratulations! You've successfully implemented all four essential activation functions:\n",
    "\n",
    "### âœ… What You've Built\n",
    "    - **ReLU**: The foundation of modern deep learning with sparsity and efficiency\n",
    "    - **Sigmoid**: Classic activation for binary classification and probability outputs\n",
    "    - **Tanh**: Zero-centered activation with better gradient properties\n",
    "    - **Softmax**: Probability distribution for multi-class classification\n",
    "\n",
    "### âœ… Key Learning Outcomes\n",
    "    - **Understanding**: Why nonlinearity is essential for neural networks\n",
    "    - **Implementation**: Built activation functions from scratch using NumPy\n",
    "    - **Testing**: Progressive validation with immediate feedback after each function\n",
    "    - **Integration**: Saw how activations work together in neural networks\n",
    "    - **Real-world context**: Understanding where each activation is used\n",
    "\n",
    "### âœ… Mathematical Mastery\n",
    "    - **ReLU**: f(x) = max(0, x) - Simple but powerful\n",
    "    - **Sigmoid**: f(x) = 1/(1 + e^(-x)) - Maps to (0,1)\n",
    "    - **Tanh**: f(x) = tanh(x) - Zero-centered, maps to (-1,1)\n",
    "    - **Softmax**: f(x_i) = e^(x_i)/Î£(e^(x_j)) - Probability distribution\n",
    "\n",
    "### âœ… Professional Skills Developed\n",
    "    - **Numerical stability**: Handling overflow and underflow\n",
    "    - **API design**: Consistent interfaces across all functions\n",
    "    - **Testing discipline**: Immediate validation after each implementation\n",
    "    - **Integration thinking**: Understanding how components work together\n",
    "\n",
    "### âœ… Ready for Next Steps\n",
    "    Your activation functions are now ready to power:\n",
    "    - **Dense layers**: Linear transformations with nonlinear activations\n",
    "    - **Convolutional layers**: Spatial feature extraction with ReLU\n",
    "    - **Network architectures**: Complete neural networks with proper activations\n",
    "    - **Training**: Gradient computation through activation functions\n",
    "\n",
    "### ðŸ”— Connection to Real ML Systems\n",
    "    Your implementations mirror production systems:\n",
    "    - **PyTorch**: `torch.nn.ReLU()`, `torch.nn.Sigmoid()`, `torch.nn.Tanh()`, `torch.nn.Softmax()`\n",
    "    - **TensorFlow**: `tf.nn.relu()`, `tf.nn.sigmoid()`, `tf.nn.tanh()`, `tf.nn.softmax()`\n",
    "    - **Industry applications**: Every major deep learning model uses these functions\n",
    "\n",
    "### ðŸŽ¯ The Power of Nonlinearity\n",
    "    You've unlocked the key to deep learning:\n",
    "    - **Before**: Linear models limited to simple patterns\n",
    "    - **After**: Nonlinear models can learn any pattern (universal approximation)\n",
    "\n",
    "    **Next Module**: Layers - Building blocks that combine your tensors and activations into powerful transformations!\n",
    "\n",
    "    Your activation functions are the key to neural network intelligence. Now let's build the layers that use them!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
