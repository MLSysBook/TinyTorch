{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91e3dfc1",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Activations - Nonlinearity in Neural Networks\n",
    "\n",
    "Welcome to the Activations module! This is where neural networks get their power through nonlinearity.\n",
    "\n",
    "## Learning Goals\n",
    "- Understand why activation functions are essential for neural networks\n",
    "- Implement the four most important activation functions: ReLU, Sigmoid, Tanh, and Softmax\n",
    "- Visualize how activations transform data and enable complex learning\n",
    "- See how activations work with layers to build powerful networks\n",
    "- Master the NBGrader workflow with comprehensive testing\n",
    "\n",
    "## Build → Use → Understand\n",
    "1. **Build**: Activation functions that add nonlinearity\n",
    "2. **Use**: Transform tensors and see immediate results\n",
    "3. **Understand**: How nonlinearity enables complex pattern learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0529372d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "activations-imports",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| default_exp core.activations\n",
    "\n",
    "#| export\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from typing import Union, List\n",
    "\n",
    "# Import our Tensor class - try from package first, then from local module\n",
    "try:\n",
    "    from tinytorch.core.tensor import Tensor\n",
    "except ImportError:\n",
    "    # For development, import from local tensor module\n",
    "    sys.path.append(os.path.join(os.path.dirname(__file__), '..', '01_tensor'))\n",
    "    from tensor_dev import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9b7e24",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "activations-welcome",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"🔥 TinyTorch Activations Module\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Python version: {sys.version_info.major}.{sys.version_info.minor}\")\n",
    "print(\"Ready to build activation functions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82a46e9",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 📦 Where This Code Lives in the Final Package\n",
    "\n",
    "**Learning Side:** You work in `modules/source/02_activations/activations_dev.py`  \n",
    "**Building Side:** Code exports to `tinytorch.core.activations`\n",
    "\n",
    "```python\n",
    "# Final package structure:\n",
    "from tinytorch.core.activations import ReLU, Sigmoid, Tanh, Softmax\n",
    "from tinytorch.core.tensor import Tensor  # Foundation\n",
    "from tinytorch.core.layers import Dense  # Uses activations\n",
    "```\n",
    "\n",
    "**Why this matters:**\n",
    "- **Learning:** Focused modules for deep understanding\n",
    "- **Production:** Proper organization like PyTorch's `torch.nn.ReLU`\n",
    "- **Consistency:** All activation functions live together in `core.activations`\n",
    "- **Integration:** Works seamlessly with tensors and layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccedc6c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## What Are Activation Functions? The Key to Neural Network Intelligence\n",
    "\n",
    "### 🎯 The Core Problem: Linear Limitations\n",
    "\n",
    "Without activation functions, neural networks are fundamentally limited. No matter how many layers you stack, they can only learn linear relationships:\n",
    "\n",
    "```\n",
    "Layer 1: h₁ = W₁ · x + b₁\n",
    "Layer 2: h₂ = W₂ · h₁ + b₂ = W₂ · (W₁ · x + b₁) + b₂\n",
    "Layer 3: h₃ = W₃ · h₂ + b₃ = W₃ · (W₂ · (W₁ · x + b₁) + b₂) + b₃\n",
    "```\n",
    "\n",
    "**Mathematical Reality**: This always simplifies to:\n",
    "```\n",
    "y = W_combined · x + b_combined\n",
    "```\n",
    "\n",
    "**A single linear transformation!** This means neural networks without activations cannot learn:\n",
    "- **Image patterns**: Recognizing curves, shapes, textures (nonlinear pixel relationships)\n",
    "- **Language patterns**: Understanding syntax, semantics, context (nonlinear word relationships)\n",
    "- **Strategic patterns**: Game playing, decision making (nonlinear strategy relationships)\n",
    "- **Any complex real-world pattern**: Most useful relationships in data are nonlinear\n",
    "\n",
    "### 🔑 The Solution: Strategic Nonlinearity\n",
    "\n",
    "Activation functions break this linear limitation by adding nonlinearity between layers:\n",
    "\n",
    "```\n",
    "Neural Network Flow:\n",
    "Input → Linear Layer → Activation → Linear Layer → Activation → ... → Output\n",
    "  x   →    W₁x + b₁   →    f(·)    →    W₂h₁ + b₂  →    f(·)    →     y\n",
    "```\n",
    "\n",
    "**Now each layer can learn complex transformations!**\n",
    "\n",
    "### 📊 Visual Understanding: The Power of Nonlinearity\n",
    "\n",
    "#### Linear Network (No Activations):\n",
    "```\n",
    "Input Space:        Decision Boundary:       Capability:\n",
    "     ·              ─────────────────────       Only straight lines\n",
    "   ·   ·                                      Cannot separate:\n",
    " ·   ·   ·          ─────────────────────       • XOR problem\n",
    "   ·   ·                                       • Circular patterns\n",
    "     ·              ─────────────────────       • Any curved boundary\n",
    "```\n",
    "\n",
    "#### Nonlinear Network (With Activations):\n",
    "```\n",
    "Input Space:        Decision Boundary:       Capability:\n",
    "     ·              ╭─────────────────╮       Complex curves\n",
    "   ·   ·            │  ╭─────────╮    │       Can separate:\n",
    " ·   ·   ·          │  │         │    │       • XOR problem\n",
    "   ·   ·            │  ╰─────────╯    │       • Circular patterns  \n",
    "     ·              ╰─────────────────╯       • Any complex shape\n",
    "```\n",
    "\n",
    "### 🏭 Real-World Impact: The Deep Learning Revolution\n",
    "\n",
    "**Before Activation Functions (Pre-2000s)**:\n",
    "- Limited to linear classifiers and simple perceptrons\n",
    "- Could not solve XOR problem (fundamental nonlinear pattern)\n",
    "- Shallow networks with limited capability\n",
    "- AI winter due to fundamental limitations\n",
    "\n",
    "**After Activation Functions (2000s-Present)**:\n",
    "- Deep learning revolution begins\n",
    "- Complex pattern recognition possible\n",
    "- State-of-the-art results in vision, language, games\n",
    "- Universal approximation theorem: can learn any function\n",
    "\n",
    "### 🎯 The Four Essential Activations We'll Master\n",
    "\n",
    "1. **ReLU (Rectified Linear Unit)**: `f(x) = max(0, x)`\n",
    "   - **The foundation** of modern deep learning\n",
    "   - **Used in**: ResNet, VGG, GPT, BERT, virtually every modern model\n",
    "   - **Why crucial**: Solves vanishing gradient problem, computationally efficient\n",
    "\n",
    "2. **Sigmoid**: `f(x) = 1/(1 + e^(-x))`  \n",
    "   - **The classic** activation for probability outputs\n",
    "   - **Used in**: Binary classification, LSTM gates, attention mechanisms\n",
    "   - **Why crucial**: Maps any input to probability range (0,1)\n",
    "\n",
    "3. **Tanh**: `f(x) = (e^x - e^(-x))/(e^x + e^(-x))`\n",
    "   - **Zero-centered** activation for better training\n",
    "   - **Used in**: LSTM cells, traditional neural networks, signal processing\n",
    "   - **Why crucial**: Better gradients due to zero-centered output\n",
    "\n",
    "4. **Softmax**: `f(x_i) = e^(x_i) / Σⱼ e^(x_j)`\n",
    "   - **Probability distribution** for multi-class problems  \n",
    "   - **Used in**: Classification heads, attention mechanisms, language models\n",
    "   - **Why crucial**: Converts logits to valid probability distributions\n",
    "\n",
    "### 🧠 Mathematical Foundation: Understanding the Functions\n",
    "\n",
    "Each activation function serves a specific purpose:\n",
    "\n",
    "#### **Activation Properties Table**\n",
    "```\n",
    "Function  | Range     | Key Property        | Primary Use\n",
    "----------|-----------|--------------------|-----------------\n",
    "ReLU      | [0, ∞)    | Sparse (many 0s)   | Hidden layers\n",
    "Sigmoid   | (0, 1)    | Probability-like    | Binary output\n",
    "Tanh      | (-1, 1)   | Zero-centered       | Hidden layers\n",
    "Softmax   | (0, 1)    | Sums to 1          | Multi-class output\n",
    "```\n",
    "\n",
    "#### **Gradient Properties (Critical for Training)**\n",
    "```\n",
    "Function  | Gradient   | Vanishing Gradient? | Training Efficiency\n",
    "----------|------------|--------------------|-----------------\n",
    "ReLU      | 1 or 0     | No (for x > 0)     | Excellent\n",
    "Sigmoid   | ≤ 0.25     | Yes (for large |x|) | Poor for deep nets\n",
    "Tanh      | ≤ 1        | Yes (for large |x|) | Better than sigmoid\n",
    "Softmax   | Complex    | No                 | Good for outputs\n",
    "```\n",
    "\n",
    "This mathematical foundation explains why ReLU revolutionized deep learning - it's the only activation that doesn't suffer from vanishing gradients!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633b7075",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 🔧 DEVELOPMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033de441",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 1: ReLU - The Foundation of Deep Learning\n",
    "\n",
    "### What is ReLU?\n",
    "**ReLU (Rectified Linear Unit)** is the most important activation function in deep learning:\n",
    "\n",
    "```\n",
    "f(x) = max(0, x)\n",
    "```\n",
    "\n",
    "- **Positive inputs**: Pass through unchanged\n",
    "- **Negative inputs**: Become zero\n",
    "- **Zero**: Stays zero\n",
    "\n",
    "### Why ReLU Revolutionized Deep Learning\n",
    "1. **Computational efficiency**: Just a max operation\n",
    "2. **No vanishing gradients**: Derivative is 1 for positive values\n",
    "3. **Sparsity**: Many neurons output exactly 0\n",
    "4. **Empirical success**: Works well in practice\n",
    "\n",
    "### Visual Understanding\n",
    "```\n",
    "Input:  [-2, -1, 0, 1, 2]\n",
    "ReLU:   [ 0,  0, 0, 1, 2]\n",
    "```\n",
    "\n",
    "### Real-World Applications\n",
    "- **Image classification**: ResNet, VGG, AlexNet\n",
    "- **Object detection**: YOLO, R-CNN\n",
    "- **Language models**: Transformer feedforward layers\n",
    "- **Recommendation**: Deep collaborative filtering\n",
    "\n",
    "### Mathematical Properties\n",
    "- **Derivative**: f'(x) = 1 if x > 0, else 0\n",
    "- **Range**: [0, ∞)\n",
    "- **Sparsity**: Outputs exactly 0 for negative inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c3795b",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "relu-class",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class ReLU:\n",
    "    \"\"\"\n",
    "    ReLU Activation Function: f(x) = max(0, x)\n",
    "    \n",
    "    The most popular activation function in deep learning.\n",
    "    Simple, fast, and effective for most applications.\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply ReLU activation: f(x) = max(0, x)\n",
    "        \n",
    "        TODO: Implement ReLU activation function.\n",
    "        \n",
    "        STEP-BY-STEP IMPLEMENTATION:\n",
    "        1. For each element in the input tensor, apply max(0, element)\n",
    "        2. Use NumPy's maximum function for efficient element-wise operation\n",
    "        3. Return a new tensor of the same type with the results\n",
    "        4. Preserve the input tensor's shape\n",
    "        \n",
    "        EXAMPLE USAGE:\n",
    "        ```python\n",
    "        relu = ReLU()\n",
    "        input_tensor = Tensor([[-2, -1, 0, 1, 2]])\n",
    "        output = relu(input_tensor)\n",
    "        print(output.data)  # [[0, 0, 0, 1, 2]]\n",
    "        ```\n",
    "        \n",
    "        IMPLEMENTATION HINTS:\n",
    "        - Use np.maximum(0, x.data) for element-wise max with 0\n",
    "        - Return the same type as input: return type(x)(result)\n",
    "        - The shape should remain the same as input\n",
    "        - Don't modify the input tensor (immutable operations)\n",
    "        \n",
    "        LEARNING CONNECTIONS:\n",
    "        - This is like torch.nn.ReLU() in PyTorch\n",
    "        - Used in virtually every modern neural network\n",
    "        - Enables deep networks by preventing vanishing gradients\n",
    "        - Creates sparse representations (many zeros)\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        result = np.maximum(0, x.data)\n",
    "        return type(x)(result)\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Make the class callable: relu(x) instead of relu.forward(x)\"\"\"\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5703c0e9",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### 🧪 Test Your ReLU Implementation\n",
    "\n",
    "Once you implement the ReLU forward method above, run this cell to test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c69ff2",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-relu-immediate",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_relu_activation():\n",
    "    \"\"\"Unit test for the ReLU activation function.\"\"\"\n",
    "    print(\"🔬 Unit Test: ReLU Activation...\")\n",
    "\n",
    "    # Create ReLU instance\n",
    "    relu = ReLU()\n",
    "\n",
    "    # Test with mixed positive/negative values\n",
    "    test_input = Tensor([[-2, -1, 0, 1, 2]])\n",
    "    result = relu(test_input)\n",
    "    expected = np.array([[0, 0, 0, 1, 2]])\n",
    "    \n",
    "    assert np.array_equal(result.data, expected), f\"ReLU failed: expected {expected}, got {result.data}\"\n",
    "    \n",
    "    # Test that negative values become zero\n",
    "    assert np.all(result.data >= 0), \"ReLU should make all negative values zero\"\n",
    "    \n",
    "    # Test that positive values remain unchanged\n",
    "    positive_input = Tensor([[1, 2, 3, 4, 5]])\n",
    "    positive_result = relu(positive_input)\n",
    "    assert np.array_equal(positive_result.data, positive_input.data), \"ReLU should preserve positive values\"\n",
    "    \n",
    "    # Test with 2D tensor\n",
    "    matrix_input = Tensor([[-1, 2], [3, -4]])\n",
    "    matrix_result = relu(matrix_input)\n",
    "    matrix_expected = np.array([[0, 2], [3, 0]])\n",
    "    assert np.array_equal(matrix_result.data, matrix_expected), \"ReLU should work with 2D tensors\"\n",
    "    \n",
    "    # Test shape preservation\n",
    "    assert matrix_result.shape == matrix_input.shape, \"ReLU should preserve input shape\"\n",
    "    \n",
    "    print(\"✅ ReLU activation tests passed!\")\n",
    "    print(f\"✅ Negative values correctly zeroed\")\n",
    "    print(f\"✅ Positive values preserved\")\n",
    "    print(f\"✅ Shape preservation working\")\n",
    "    print(f\"✅ Works with multi-dimensional tensors\")\n",
    "\n",
    "# Run the test\n",
    "test_unit_relu_activation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d351ac43",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### 🎯 Checkpoint: ReLU Mastery\n",
    "Congratulations! You've successfully implemented and tested the ReLU activation function. \n",
    "\n",
    "Before moving to the next activation, make sure you can:\n",
    "\n",
    "```python\n",
    "# Create and test ReLU with different input patterns\n",
    "relu = ReLU()\n",
    "\n",
    "# Test 1: Basic functionality\n",
    "basic_input = Tensor([[-3, -1, 0, 1, 3]])\n",
    "basic_output = relu(basic_input)\n",
    "print(f\"Input:  {basic_input.data}\")   # [[-3, -1,  0,  1,  3]]\n",
    "print(f\"Output: {basic_output.data}\")  # [[ 0,  0,  0,  1,  3]]\n",
    "\n",
    "# Test 2: 2D tensors (simulating mini-batch)\n",
    "batch_input = Tensor([[-2, 1], [3, -1]])\n",
    "batch_output = relu(batch_input)\n",
    "print(f\"Batch Output: {batch_output.data}\")  # [[0, 1], [3, 0]]\n",
    "\n",
    "# Test 3: Understand sparsity\n",
    "print(f\"Sparsity: {np.count_nonzero(batch_output.data == 0) / batch_output.size * 100:.1f}% zeros\")\n",
    "```\n",
    "\n",
    "**Key Understanding**: ReLU creates **sparse representations** - many outputs are exactly zero. This sparsity makes networks:\n",
    "- **Computationally efficient**: Skip zero computations\n",
    "- **Memory efficient**: Compress sparse representations  \n",
    "- **Biologically inspired**: Real neurons often stay silent\n",
    "\n",
    "You now have the foundation activation that powers modern deep learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e12123",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 2: Sigmoid - Classic Binary Classification\n",
    "\n",
    "### What is Sigmoid? The Probability Gateway\n",
    "\n",
    "**Sigmoid** is the classic S-shaped activation function that gracefully maps any real number to the probability range (0, 1):\n",
    "\n",
    "```\n",
    "f(x) = 1 / (1 + e^(-x))\n",
    "```\n",
    "\n",
    "### 📊 Visual Understanding: The Sigmoid Curve\n",
    "\n",
    "```\n",
    "Sigmoid Function Shape:\n",
    "\n",
    "     1.0 |                    ╭─────\n",
    "         |                 ╭──╯\n",
    "     0.8 |              ╭─╯\n",
    "         |           ╭─╯\n",
    "     0.6 |        ╭─╯\n",
    "         |      ╭╯\n",
    "     0.4 |   ╭─╯\n",
    "         |  ╱\n",
    "     0.2 |╱\n",
    "         ╱\n",
    "     0.0 ╱────────────────────────────\n",
    "        -4    -2     0     2     4\n",
    "```\n",
    "\n",
    "**Key Insight**: The sigmoid is nature's smooth switch - it gradually transitions from \"off\" (0) to \"on\" (1).\n",
    "\n",
    "### 🎯 Why Sigmoid Is Essential for ML\n",
    "\n",
    "#### **1. Probability Interpretation**\n",
    "```\n",
    "Raw Neural Network Output:  [-2.5, 0.3, 1.8, -0.7]  (any range)\n",
    "After Sigmoid:              [0.08, 0.57, 0.86, 0.33]  (valid probabilities)\n",
    "```\n",
    "\n",
    "#### **2. Smooth Decision Boundaries**\n",
    "Unlike step functions, sigmoid provides smooth transitions:\n",
    "```\n",
    "Hard Decision (Step):     0, 0, 0, 1, 1, 1  (abrupt jump)\n",
    "Soft Decision (Sigmoid):  0.01, 0.27, 0.73, 0.88, 0.95  (smooth transition)\n",
    "```\n",
    "\n",
    "#### **3. Binary Classification Perfect Fit**\n",
    "```\n",
    "For binary classification:\n",
    "- Output > 0.5 → Class 1 (positive)\n",
    "- Output < 0.5 → Class 0 (negative)  \n",
    "- Output = 0.5 → Uncertain (decision boundary)\n",
    "```\n",
    "\n",
    "### 🏭 Real-World Applications: Where Sigmoid Shines\n",
    "\n",
    "**Binary Classification Tasks**:\n",
    "- **Email spam detection**: Probability this email is spam\n",
    "- **Medical diagnosis**: Probability patient has condition\n",
    "- **Fraud detection**: Probability transaction is fraudulent\n",
    "- **A/B testing**: Probability user clicks/converts\n",
    "\n",
    "**Neural Network Components**:\n",
    "- **LSTM gates**: Forget gate, input gate, output gate decisions\n",
    "- **Attention mechanisms**: How much attention to pay to each element\n",
    "- **Probability outputs**: Final layer for binary classification\n",
    "\n",
    "### 🧠 Mathematical Deep Dive\n",
    "\n",
    "#### **Critical Properties**\n",
    "```\n",
    "Property          | Value/Formula        | ML Significance\n",
    "------------------|---------------------|----------------------------\n",
    "Range             | (0, 1)              | Valid probability space\n",
    "Derivative        | σ(x)·(1-σ(x))       | Self-referential gradient\n",
    "Maximum gradient  | 0.25 (at x=0)       | Vanishing gradient problem\n",
    "Symmetry point    | σ(0) = 0.5          | Natural decision boundary\n",
    "Saturation        | σ(±∞) ≈ 0 or 1      | Confident predictions\n",
    "```\n",
    "\n",
    "#### **The Vanishing Gradient Problem**\n",
    "```\n",
    "Input Range    | Sigmoid Output | Gradient Size | Training Impact\n",
    "---------------|----------------|---------------|----------------\n",
    "x ∈ [-1, 1]    | [0.27, 0.73]   | ~0.2         | Good learning\n",
    "x ∈ [-3, 3]    | [0.05, 0.95]   | ~0.05        | Slow learning  \n",
    "x ∈ [-5, 5]    | [0.007, 0.993] | ~0.007       | Very slow\n",
    "x ∈ [-10, 10]  | [~0, ~1]       | ~0.0001      | Learning stops\n",
    "```\n",
    "\n",
    "**This is why ReLU replaced sigmoid in hidden layers - sigmoid gradients vanish for large inputs!**\n",
    "\n",
    "### 🎯 When to Use Sigmoid vs ReLU\n",
    "\n",
    "```\n",
    "Use Case                     | Sigmoid | ReLU | Why?\n",
    "----------------------------|---------|------|---------------------------\n",
    "Hidden layers (deep nets)  |    ❌   |  ✅  | ReLU avoids vanishing gradients\n",
    "Binary classification output|    ✅   |  ❌  | Need probability interpretation\n",
    "LSTM/GRU gates             |    ✅   |  ❌  | Need smooth 0-1 gating\n",
    "Multi-class classification  |    ❌   |  ❌  | Use Softmax instead\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8c1c13",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "sigmoid-class",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class Sigmoid:\n",
    "    \"\"\"\n",
    "    Sigmoid Activation Function: f(x) = 1 / (1 + e^(-x))\n",
    "    \n",
    "    Maps any real number to the range (0, 1).\n",
    "    Useful for binary classification and probability outputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply Sigmoid activation: f(x) = 1 / (1 + e^(-x))\n",
    "        \n",
    "        TODO: Implement Sigmoid activation function.\n",
    "        \n",
    "        STEP-BY-STEP IMPLEMENTATION:\n",
    "        1. Compute the negative of input: -x.data\n",
    "        2. Compute the exponential: np.exp(-x.data)\n",
    "        3. Add 1 to the exponential: 1 + np.exp(-x.data)\n",
    "        4. Take the reciprocal: 1 / (1 + np.exp(-x.data))\n",
    "        5. Return as new Tensor\n",
    "        \n",
    "        EXAMPLE USAGE:\n",
    "        ```python\n",
    "        sigmoid = Sigmoid()\n",
    "        input_tensor = Tensor([[-2, -1, 0, 1, 2]])\n",
    "        output = sigmoid(input_tensor)\n",
    "        print(output.data)  # [[0.119, 0.269, 0.5, 0.731, 0.881]]\n",
    "        ```\n",
    "        \n",
    "        IMPLEMENTATION HINTS:\n",
    "        - Use np.exp() for exponential function\n",
    "        - Formula: 1 / (1 + np.exp(-x.data))\n",
    "        - Handle potential overflow with np.clip(-x.data, -500, 500)\n",
    "        - Return Tensor(result)\n",
    "        \n",
    "        LEARNING CONNECTIONS:\n",
    "        - This is like torch.nn.Sigmoid() in PyTorch\n",
    "        - Used in binary classification output layers\n",
    "        - Key component in LSTM and GRU gating mechanisms\n",
    "        - Historically important for early neural networks\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        # Clip to prevent overflow\n",
    "        clipped_input = np.clip(-x.data, -500, 500)\n",
    "        result = 1 / (1 + np.exp(clipped_input))\n",
    "        return type(x)(result)\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Make the class callable: sigmoid(x) instead of sigmoid.forward(x)\"\"\"\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166876a9",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### 🧪 Test Your Sigmoid Implementation\n",
    "\n",
    "Once you implement the Sigmoid forward method above, run this cell to test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d445dc",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-sigmoid-immediate",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_sigmoid_activation():\n",
    "    \"\"\"Unit test for the Sigmoid activation function.\"\"\"\n",
    "    print(\"🔬 Unit Test: Sigmoid Activation...\")\n",
    "\n",
    "# Create Sigmoid instance\n",
    "    sigmoid = Sigmoid()\n",
    "\n",
    "    # Test with known values\n",
    "    test_input = Tensor([[0]])\n",
    "    result = sigmoid(test_input)\n",
    "    expected = 0.5\n",
    "    \n",
    "    assert abs(result.data[0][0] - expected) < 1e-6, f\"Sigmoid(0) should be 0.5, got {result.data[0][0]}\"\n",
    "    \n",
    "    # Test with positive and negative values\n",
    "    test_input = Tensor([[-2, -1, 0, 1, 2]])\n",
    "    result = sigmoid(test_input)\n",
    "    \n",
    "    # Check that all values are between 0 and 1\n",
    "    assert np.all(result.data > 0), \"Sigmoid output should be > 0\"\n",
    "    assert np.all(result.data < 1), \"Sigmoid output should be < 1\"\n",
    "    \n",
    "    # Test symmetry: sigmoid(-x) = 1 - sigmoid(x)\n",
    "    x_val = 1.0\n",
    "    pos_result = sigmoid(Tensor([[x_val]]))\n",
    "    neg_result = sigmoid(Tensor([[-x_val]]))\n",
    "    symmetry_check = abs(pos_result.data[0][0] + neg_result.data[0][0] - 1.0)\n",
    "    assert symmetry_check < 1e-6, \"Sigmoid should be symmetric around 0.5\"\n",
    "    \n",
    "    # Test with 2D tensor\n",
    "    matrix_input = Tensor([[-1, 1], [0, 2]])\n",
    "    matrix_result = sigmoid(matrix_input)\n",
    "    assert matrix_result.shape == matrix_input.shape, \"Sigmoid should preserve shape\"\n",
    "    \n",
    "    # Test extreme values (should not overflow)\n",
    "    extreme_input = Tensor([[-100, 100]])\n",
    "    extreme_result = sigmoid(extreme_input)\n",
    "    assert not np.any(np.isnan(extreme_result.data)), \"Sigmoid should handle extreme values\"\n",
    "    assert not np.any(np.isinf(extreme_result.data)), \"Sigmoid should not produce inf values\"\n",
    "    \n",
    "    print(\"✅ Sigmoid activation tests passed!\")\n",
    "    print(f\"✅ Outputs correctly bounded between 0 and 1\")\n",
    "    print(f\"✅ Symmetric property verified\")\n",
    "    print(f\"✅ Handles extreme values without overflow\")\n",
    "    print(f\"✅ Shape preservation working\")\n",
    "\n",
    "# Run the test\n",
    "test_unit_sigmoid_activation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa6098b",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 3: Tanh - Centered Activation\n",
    "\n",
    "### What is Tanh?\n",
    "**Tanh (Hyperbolic Tangent)** is similar to sigmoid but centered around zero:\n",
    "\n",
    "```\n",
    "f(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "```\n",
    "\n",
    "### Why Tanh is Better Than Sigmoid\n",
    "1. **Zero-centered**: Outputs range from -1 to 1\n",
    "2. **Better gradients**: Helps with gradient flow in deep networks\n",
    "3. **Faster convergence**: Less bias shift during training\n",
    "4. **Stronger gradients**: Maximum gradient is 1 vs 0.25 for sigmoid\n",
    "\n",
    "### Visual Understanding\n",
    "```\n",
    "Input: [-∞, -2, -1, 0, 1, 2, ∞]\n",
    "Tanh:  [-1, -0.96, -0.76, 0, 0.76, 0.96, 1]\n",
    "```\n",
    "\n",
    "### Real-World Applications\n",
    "- **Hidden layers**: Better than sigmoid for internal activations\n",
    "- **RNN cells**: Classic RNN and LSTM use tanh\n",
    "- **Normalization**: When you need zero-centered outputs\n",
    "- **Feature scaling**: Maps inputs to [-1, 1] range\n",
    "\n",
    "### Mathematical Properties\n",
    "- **Range**: (-1, 1)\n",
    "- **Derivative**: f'(x) = 1 - f(x)²\n",
    "- **Zero-centered**: f(0) = 0\n",
    "- **Antisymmetric**: f(-x) = -f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d904e6",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "tanh-class",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class Tanh:\n",
    "    \"\"\"\n",
    "    Tanh Activation Function: f(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "    \n",
    "    Zero-centered activation function with range (-1, 1).\n",
    "    Better gradient properties than sigmoid.\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply Tanh activation: f(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "        \n",
    "        TODO: Implement Tanh activation function.\n",
    "        \n",
    "        STEP-BY-STEP IMPLEMENTATION:\n",
    "        1. Use NumPy's built-in tanh function: np.tanh(x.data)\n",
    "        2. Alternatively, implement manually:\n",
    "           - Compute e^x and e^(-x)\n",
    "           - Calculate (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "        3. Return as new Tensor\n",
    "        \n",
    "        EXAMPLE USAGE:\n",
    "        ```python\n",
    "        tanh = Tanh()\n",
    "        input_tensor = Tensor([[-2, -1, 0, 1, 2]])\n",
    "        output = tanh(input_tensor)\n",
    "        print(output.data)  # [[-0.964, -0.762, 0, 0.762, 0.964]]\n",
    "        ```\n",
    "        \n",
    "        IMPLEMENTATION HINTS:\n",
    "        - Use np.tanh(x.data) for simplicity\n",
    "        - Manual implementation: (np.exp(x.data) - np.exp(-x.data)) / (np.exp(x.data) + np.exp(-x.data))\n",
    "        - Handle overflow by clipping inputs: np.clip(x.data, -500, 500)\n",
    "        - Return Tensor(result)\n",
    "        \n",
    "        LEARNING CONNECTIONS:\n",
    "        - This is like torch.nn.Tanh() in PyTorch\n",
    "        - Used in RNN, LSTM, and GRU cells\n",
    "        - Better than sigmoid for hidden layers\n",
    "        - Zero-centered outputs help with gradient flow\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        # Use NumPy's built-in tanh function\n",
    "        result = np.tanh(x.data)\n",
    "        return type(x)(result)\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Make the class callable: tanh(x) instead of tanh.forward(x)\"\"\"\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61750d6a",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### 🧪 Test Your Tanh Implementation\n",
    "\n",
    "Once you implement the Tanh forward method above, run this cell to test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53f9db5",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-tanh-immediate",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_tanh_activation():\n",
    "    \"\"\"Unit test for the Tanh activation function.\"\"\"\n",
    "    print(\"🔬 Unit Test: Tanh Activation...\")\n",
    "\n",
    "# Create Tanh instance\n",
    "    tanh = Tanh()\n",
    "\n",
    "    # Test with zero (should be 0)\n",
    "    test_input = Tensor([[0]])\n",
    "    result = tanh(test_input)\n",
    "    expected = 0.0\n",
    "    \n",
    "    assert abs(result.data[0][0] - expected) < 1e-6, f\"Tanh(0) should be 0, got {result.data[0][0]}\"\n",
    "    \n",
    "    # Test with positive and negative values\n",
    "    test_input = Tensor([[-2, -1, 0, 1, 2]])\n",
    "    result = tanh(test_input)\n",
    "    \n",
    "    # Check that all values are between -1 and 1\n",
    "    assert np.all(result.data > -1), \"Tanh output should be > -1\"\n",
    "    assert np.all(result.data < 1), \"Tanh output should be < 1\"\n",
    "    \n",
    "    # Test antisymmetry: tanh(-x) = -tanh(x)\n",
    "    x_val = 1.5\n",
    "    pos_result = tanh(Tensor([[x_val]]))\n",
    "    neg_result = tanh(Tensor([[-x_val]]))\n",
    "    antisymmetry_check = abs(pos_result.data[0][0] + neg_result.data[0][0])\n",
    "    assert antisymmetry_check < 1e-6, \"Tanh should be antisymmetric\"\n",
    "    \n",
    "    # Test with 2D tensor\n",
    "    matrix_input = Tensor([[-1, 1], [0, 2]])\n",
    "    matrix_result = tanh(matrix_input)\n",
    "    assert matrix_result.shape == matrix_input.shape, \"Tanh should preserve shape\"\n",
    "    \n",
    "    # Test extreme values (should not overflow)\n",
    "    extreme_input = Tensor([[-100, 100]])\n",
    "    extreme_result = tanh(extreme_input)\n",
    "    assert not np.any(np.isnan(extreme_result.data)), \"Tanh should handle extreme values\"\n",
    "    assert not np.any(np.isinf(extreme_result.data)), \"Tanh should not produce inf values\"\n",
    "    \n",
    "    # Test that extreme values approach ±1\n",
    "    assert abs(extreme_result.data[0][0] - (-1)) < 1e-6, \"Tanh(-∞) should approach -1\"\n",
    "    assert abs(extreme_result.data[0][1] - 1) < 1e-6, \"Tanh(∞) should approach 1\"\n",
    "    \n",
    "    print(\"✅ Tanh activation tests passed!\")\n",
    "    print(f\"✅ Outputs correctly bounded between -1 and 1\")\n",
    "    print(f\"✅ Antisymmetric property verified\")\n",
    "    print(f\"✅ Zero-centered (tanh(0) = 0)\")\n",
    "    print(f\"✅ Handles extreme values correctly\")\n",
    "\n",
    "# Run the test\n",
    "test_unit_tanh_activation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccca9665",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 4: Softmax - Probability Distributions\n",
    "\n",
    "### What is Softmax?\n",
    "**Softmax** converts a vector of real numbers into a probability distribution:\n",
    "\n",
    "```\n",
    "f(x_i) = e^(x_i) / Σ(e^(x_j))\n",
    "```\n",
    "\n",
    "### Why Softmax is Essential\n",
    "1. **Probability distribution**: Outputs sum to 1\n",
    "2. **Multi-class classification**: Choose one class from many\n",
    "3. **Interpretable**: Each output is a probability\n",
    "4. **Differentiable**: Enables gradient-based learning\n",
    "\n",
    "### Visual Understanding\n",
    "```\n",
    "Input:  [1, 2, 3]\n",
    "Softmax:[0.09, 0.24, 0.67]  # Sums to 1.0\n",
    "```\n",
    "\n",
    "### Real-World Applications\n",
    "- **Classification**: Image classification, text classification\n",
    "- **Language models**: Next word prediction\n",
    "- **Attention mechanisms**: Where to focus attention\n",
    "- **Reinforcement learning**: Action selection probabilities\n",
    "\n",
    "### Mathematical Properties\n",
    "- **Range**: (0, 1) for each output\n",
    "- **Constraint**: Σ(f(x_i)) = 1\n",
    "- **Argmax preservation**: Doesn't change relative ordering\n",
    "- **Temperature scaling**: Can be made sharper or softer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e377a68b",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "softmax-class",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class Softmax:\n",
    "    \"\"\"\n",
    "    Softmax Activation Function: f(x_i) = e^(x_i) / Σ(e^(x_j))\n",
    "    \n",
    "    Converts a vector of real numbers into a probability distribution.\n",
    "    Essential for multi-class classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply Softmax activation: f(x_i) = e^(x_i) / Σ(e^(x_j))\n",
    "        \n",
    "        TODO: Implement Softmax activation function.\n",
    "        \n",
    "        STEP-BY-STEP IMPLEMENTATION:\n",
    "        1. Handle empty input case\n",
    "        2. Subtract max value for numerical stability: x - max(x)\n",
    "        3. Compute exponentials: np.exp(x - max(x))\n",
    "        4. Compute sum of exponentials: np.sum(exp_values)\n",
    "        5. Divide each exponential by the sum: exp_values / sum\n",
    "        6. Return as same tensor type as input\n",
    "        \n",
    "        EXAMPLE USAGE:\n",
    "        ```python\n",
    "        softmax = Softmax()\n",
    "        input_tensor = Tensor([[1, 2, 3]])\n",
    "        output = softmax(input_tensor)\n",
    "        print(output.data)  # [[0.09, 0.24, 0.67]]\n",
    "        print(np.sum(output.data))  # 1.0\n",
    "        ```\n",
    "        \n",
    "        IMPLEMENTATION HINTS:\n",
    "        - Handle empty case: if x.data.size == 0: return type(x)(x.data.copy())\n",
    "        - Subtract max for numerical stability: x_shifted = x.data - np.max(x.data, axis=-1, keepdims=True)\n",
    "        - Compute exponentials: exp_values = np.exp(x_shifted)\n",
    "        - Sum along last axis: sum_exp = np.sum(exp_values, axis=-1, keepdims=True)\n",
    "        - Divide: result = exp_values / sum_exp\n",
    "        - Return same type as input: return type(x)(result)\n",
    "        \n",
    "        LEARNING CONNECTIONS:\n",
    "        - This is like torch.nn.Softmax() in PyTorch\n",
    "        - Used in classification output layers\n",
    "        - Key component in attention mechanisms\n",
    "        - Enables probability-based decision making\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        # Handle empty input\n",
    "        if x.data.size == 0:\n",
    "            return type(x)(x.data.copy())\n",
    "        \n",
    "        # Subtract max for numerical stability\n",
    "        x_shifted = x.data - np.max(x.data, axis=-1, keepdims=True)\n",
    "        \n",
    "        # Compute exponentials\n",
    "        exp_values = np.exp(x_shifted)\n",
    "        \n",
    "        # Sum along last axis\n",
    "        sum_exp = np.sum(exp_values, axis=-1, keepdims=True)\n",
    "        \n",
    "        # Divide to get probabilities\n",
    "        result = exp_values / sum_exp\n",
    "        \n",
    "        return type(x)(result)\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Make the class callable: softmax(x) instead of softmax.forward(x)\"\"\"\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304e285c",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### 🧪 Test Your Softmax Implementation\n",
    "\n",
    "Once you implement the Softmax forward method above, run this cell to test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fb8acb",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-softmax-immediate",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_softmax_activation():\n",
    "    \"\"\"Unit test for the Softmax activation function.\"\"\"\n",
    "    print(\"🔬 Unit Test: Softmax Activation...\")\n",
    "\n",
    "# Create Softmax instance\n",
    "    softmax = Softmax()\n",
    "\n",
    "    # Test with simple input\n",
    "    test_input = Tensor([[1, 2, 3]])\n",
    "    result = softmax(test_input)\n",
    "    \n",
    "    # Check that outputs sum to 1\n",
    "    output_sum = np.sum(result.data)\n",
    "    assert abs(output_sum - 1.0) < 1e-6, f\"Softmax outputs should sum to 1, got {output_sum}\"\n",
    "    \n",
    "    # Check that all outputs are positive\n",
    "    assert np.all(result.data > 0), \"Softmax outputs should be positive\"\n",
    "    assert np.all(result.data < 1), \"Softmax outputs should be less than 1\"\n",
    "    \n",
    "    # Test with uniform input (should give equal probabilities)\n",
    "    uniform_input = Tensor([[1, 1, 1]])\n",
    "    uniform_result = softmax(uniform_input)\n",
    "    expected_prob = 1.0 / 3.0\n",
    "    \n",
    "    for prob in uniform_result.data[0]:\n",
    "        assert abs(prob - expected_prob) < 1e-6, f\"Uniform input should give equal probabilities\"\n",
    "    \n",
    "    # Test with batch input (multiple samples)\n",
    "    batch_input = Tensor([[1, 2, 3], [4, 5, 6]])\n",
    "    batch_result = softmax(batch_input)\n",
    "    \n",
    "    # Check that each row sums to 1\n",
    "    for i in range(batch_input.shape[0]):\n",
    "        row_sum = np.sum(batch_result.data[i])\n",
    "        assert abs(row_sum - 1.0) < 1e-6, f\"Each row should sum to 1, row {i} sums to {row_sum}\"\n",
    "    \n",
    "    # Test numerical stability with large values\n",
    "    large_input = Tensor([[1000, 1001, 1002]])\n",
    "    large_result = softmax(large_input)\n",
    "    \n",
    "    assert not np.any(np.isnan(large_result.data)), \"Softmax should handle large values\"\n",
    "    assert not np.any(np.isinf(large_result.data)), \"Softmax should not produce inf values\"\n",
    "    \n",
    "    large_sum = np.sum(large_result.data)\n",
    "    assert abs(large_sum - 1.0) < 1e-6, \"Large values should still sum to 1\"\n",
    "\n",
    "# Test shape preservation\n",
    "    assert batch_result.shape == batch_input.shape, \"Softmax should preserve shape\"\n",
    "    \n",
    "    print(\"✅ Softmax activation tests passed!\")\n",
    "    print(f\"✅ Outputs sum to 1 (probability distribution)\")\n",
    "    print(f\"✅ All outputs are positive\")\n",
    "    print(f\"✅ Handles uniform inputs correctly\")\n",
    "    print(f\"✅ Works with batch inputs\")\n",
    "    print(f\"✅ Numerically stable with large values\")\n",
    "\n",
    "# Run the test\n",
    "test_unit_softmax_activation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8270fd",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## 🎯 Comprehensive Test: All Activations Working Together\n",
    "\n",
    "### Real-World Scenario\n",
    "Let's test how all activation functions work together in a realistic neural network scenario:\n",
    "\n",
    "- **Input processing**: Raw data transformation\n",
    "- **Hidden layers**: ReLU for internal processing\n",
    "- **Output layer**: Softmax for classification\n",
    "- **Comparison**: See how different activations transform the same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c666f3",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": true,
     "grade_id": "test-activations-comprehensive",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_activations_comprehensive():\n",
    "    \"\"\"Comprehensive unit test for all activation functions working together.\"\"\"\n",
    "    print(\"🔬 Unit Test: Activation Functions Comprehensive Test...\")\n",
    "    \n",
    "    # Create instances of all activation functions\n",
    "    relu = ReLU()\n",
    "    sigmoid = Sigmoid()\n",
    "    tanh = Tanh()\n",
    "    softmax = Softmax()\n",
    "    \n",
    "    # Test data: simulating neural network layer outputs\n",
    "    test_data = Tensor([[-2, -1, 0, 1, 2]])\n",
    "    \n",
    "    # Apply each activation function\n",
    "    relu_result = relu(test_data)\n",
    "    sigmoid_result = sigmoid(test_data)\n",
    "    tanh_result = tanh(test_data)\n",
    "    softmax_result = softmax(test_data)\n",
    "    \n",
    "    # Test that all functions preserve input shape\n",
    "    assert relu_result.shape == test_data.shape, \"ReLU should preserve shape\"\n",
    "    assert sigmoid_result.shape == test_data.shape, \"Sigmoid should preserve shape\"\n",
    "    assert tanh_result.shape == test_data.shape, \"Tanh should preserve shape\"\n",
    "    assert softmax_result.shape == test_data.shape, \"Softmax should preserve shape\"\n",
    "    \n",
    "    # Test that all functions return Tensor objects\n",
    "    assert isinstance(relu_result, Tensor), \"ReLU should return Tensor\"\n",
    "    assert isinstance(sigmoid_result, Tensor), \"Sigmoid should return Tensor\"\n",
    "    assert isinstance(tanh_result, Tensor), \"Tanh should return Tensor\"\n",
    "    assert isinstance(softmax_result, Tensor), \"Softmax should return Tensor\"\n",
    "    \n",
    "    # Test ReLU properties\n",
    "    assert np.all(relu_result.data >= 0), \"ReLU output should be non-negative\"\n",
    "    \n",
    "    # Test Sigmoid properties\n",
    "    assert np.all(sigmoid_result.data > 0), \"Sigmoid output should be positive\"\n",
    "    assert np.all(sigmoid_result.data < 1), \"Sigmoid output should be less than 1\"\n",
    "    \n",
    "    # Test Tanh properties\n",
    "    assert np.all(tanh_result.data > -1), \"Tanh output should be > -1\"\n",
    "    assert np.all(tanh_result.data < 1), \"Tanh output should be < 1\"\n",
    "    \n",
    "    # Test Softmax properties\n",
    "    softmax_sum = np.sum(softmax_result.data)\n",
    "    assert abs(softmax_sum - 1.0) < 1e-6, \"Softmax outputs should sum to 1\"\n",
    "    \n",
    "    # Test chaining activations (realistic neural network scenario)\n",
    "    # Hidden layer with ReLU\n",
    "    hidden_output = relu(test_data)\n",
    "    \n",
    "    # Add some weights simulation (element-wise multiplication)\n",
    "    weights = Tensor([[0.5, 0.3, 0.8, 0.2, 0.7]])\n",
    "    weighted_output = hidden_output * weights\n",
    "    \n",
    "    # Final layer with Softmax\n",
    "    final_output = softmax(weighted_output)\n",
    "    \n",
    "    # Test that chained operations work\n",
    "    assert isinstance(final_output, Tensor), \"Chained operations should return Tensor\"\n",
    "    assert abs(np.sum(final_output.data) - 1.0) < 1e-6, \"Final output should be valid probability\"\n",
    "    \n",
    "    # Test with batch data (multiple samples)\n",
    "    batch_data = Tensor([\n",
    "    [-2, -1, 0, 1, 2],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [-1, 0, 1, 2, 3]\n",
    "    ])\n",
    "    \n",
    "    batch_softmax = softmax(batch_data)\n",
    "    \n",
    "    # Each row should sum to 1\n",
    "    for i in range(batch_data.shape[0]):\n",
    "        row_sum = np.sum(batch_softmax.data[i])\n",
    "        assert abs(row_sum - 1.0) < 1e-6, f\"Batch row {i} should sum to 1\"\n",
    "    \n",
    "    print(\"✅ Activation functions comprehensive tests passed!\")\n",
    "    print(f\"✅ All functions work together seamlessly\")\n",
    "    print(f\"✅ Shape preservation across all activations\")\n",
    "    print(f\"✅ Chained operations work correctly\")\n",
    "    print(f\"✅ Batch processing works for all activations\")\n",
    "    print(f\"✅ Ready for neural network integration!\")\n",
    "\n",
    "# Run the comprehensive test\n",
    "test_unit_activations_comprehensive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d92e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_module_activation_tensor_integration():\n",
    "    \"\"\"\n",
    "    Integration test for activation functions with Tensor operations.\n",
    "    \n",
    "    Tests that activation functions properly integrate with the Tensor class\n",
    "    and maintain compatibility for neural network operations.\n",
    "    \"\"\"\n",
    "    print(\"🔬 Running Integration Test: Activation-Tensor Integration...\")\n",
    "    \n",
    "    # Test 1: Activation functions preserve Tensor types\n",
    "    input_tensor = Tensor([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "    \n",
    "    relu_fn = ReLU()\n",
    "    sigmoid_fn = Sigmoid()\n",
    "    tanh_fn = Tanh()\n",
    "    \n",
    "    relu_result = relu_fn(input_tensor)\n",
    "    sigmoid_result = sigmoid_fn(input_tensor) \n",
    "    tanh_result = tanh_fn(input_tensor)\n",
    "    \n",
    "    assert isinstance(relu_result, Tensor), \"ReLU should return Tensor\"\n",
    "    assert isinstance(sigmoid_result, Tensor), \"Sigmoid should return Tensor\"\n",
    "    assert isinstance(tanh_result, Tensor), \"Tanh should return Tensor\"\n",
    "    \n",
    "    # Test 2: Activations work with matrix Tensors (neural network layers)\n",
    "    layer_output = Tensor([[1.0, -2.0, 3.0], \n",
    "                          [-1.0, 2.0, -3.0]])  # Simulating dense layer output\n",
    "    \n",
    "    relu_fn = ReLU()\n",
    "    activated = relu_fn(layer_output)\n",
    "    expected = np.array([[1.0, 0.0, 3.0], \n",
    "                        [0.0, 2.0, 0.0]])\n",
    "    \n",
    "    assert isinstance(activated, Tensor), \"Matrix activation should return Tensor\"\n",
    "    assert np.array_equal(activated.data, expected), \"Matrix ReLU should work correctly\"\n",
    "    \n",
    "    # Test 3: Softmax with classification scenario\n",
    "    logits = Tensor([[2.0, 1.0, 0.1],  # Batch of 2 samples\n",
    "                    [1.0, 3.0, 0.2]])   # Each with 3 classes\n",
    "    \n",
    "    softmax_fn = Softmax()\n",
    "    probabilities = softmax_fn(logits)\n",
    "    \n",
    "    assert isinstance(probabilities, Tensor), \"Softmax should return Tensor\"\n",
    "    assert probabilities.shape == logits.shape, \"Softmax should preserve shape\"\n",
    "    \n",
    "    # Each row should sum to 1 (probability distribution)\n",
    "    for i in range(logits.shape[0]):\n",
    "        row_sum = np.sum(probabilities.data[i])\n",
    "        assert abs(row_sum - 1.0) < 1e-6, f\"Probability row {i} should sum to 1\"\n",
    "    \n",
    "    # Test 4: Chaining tensor operations with activations\n",
    "    x = Tensor([1.0, 2.0, 3.0])\n",
    "    y = Tensor([4.0, 5.0, 6.0])\n",
    "    \n",
    "    # Simulate: dense layer output -> activation -> more operations\n",
    "    dense_sim = x * y  # Element-wise multiplication (simulating dense layer)\n",
    "    relu_fn = ReLU()\n",
    "    activated = relu_fn(dense_sim)  # Apply activation\n",
    "    final = activated + Tensor([1.0, 1.0, 1.0])  # More tensor operations\n",
    "    \n",
    "    expected_final = np.array([5.0, 11.0, 19.0])  # [4,10,18] -> relu -> +1 = [5,11,19]\n",
    "    \n",
    "    assert isinstance(final, Tensor), \"Chained operations should maintain Tensor type\"\n",
    "    assert np.array_equal(final.data, expected_final), \"Chained operations should work correctly\"\n",
    "    \n",
    "    print(\"✅ Integration Test Passed: Activation-Tensor integration works correctly.\")\n",
    "\n",
    "# Run the integration test\n",
    "test_module_activation_tensor_integration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060254f8",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 🎯 MODULE SUMMARY: Activation Functions\n",
    "\n",
    "    Congratulations! You've successfully implemented all four essential activation functions:\n",
    "\n",
    "### ✅ What You've Built\n",
    "    - **ReLU**: The foundation of modern deep learning with sparsity and efficiency\n",
    "    - **Sigmoid**: Classic activation for binary classification and probability outputs\n",
    "    - **Tanh**: Zero-centered activation with better gradient properties\n",
    "    - **Softmax**: Probability distribution for multi-class classification\n",
    "\n",
    "### ✅ Key Learning Outcomes\n",
    "    - **Understanding**: Why nonlinearity is essential for neural networks\n",
    "    - **Implementation**: Built activation functions from scratch using NumPy\n",
    "    - **Testing**: Progressive validation with immediate feedback after each function\n",
    "    - **Integration**: Saw how activations work together in neural networks\n",
    "    - **Real-world context**: Understanding where each activation is used\n",
    "\n",
    "### ✅ Mathematical Mastery\n",
    "    - **ReLU**: f(x) = max(0, x) - Simple but powerful\n",
    "    - **Sigmoid**: f(x) = 1/(1 + e^(-x)) - Maps to (0,1)\n",
    "    - **Tanh**: f(x) = tanh(x) - Zero-centered, maps to (-1,1)\n",
    "    - **Softmax**: f(x_i) = e^(x_i)/Σ(e^(x_j)) - Probability distribution\n",
    "\n",
    "### ✅ Professional Skills Developed\n",
    "    - **Numerical stability**: Handling overflow and underflow\n",
    "    - **API design**: Consistent interfaces across all functions\n",
    "    - **Testing discipline**: Immediate validation after each implementation\n",
    "    - **Integration thinking**: Understanding how components work together\n",
    "\n",
    "### ✅ Ready for Next Steps\n",
    "    Your activation functions are now ready to power:\n",
    "    - **Dense layers**: Linear transformations with nonlinear activations\n",
    "    - **Convolutional layers**: Spatial feature extraction with ReLU\n",
    "    - **Network architectures**: Complete neural networks with proper activations\n",
    "    - **Training**: Gradient computation through activation functions\n",
    "\n",
    "### 🔗 Connection to Real ML Systems\n",
    "    Your implementations mirror production systems:\n",
    "    - **PyTorch**: `torch.nn.ReLU()`, `torch.nn.Sigmoid()`, `torch.nn.Tanh()`, `torch.nn.Softmax()`\n",
    "    - **TensorFlow**: `tf.nn.relu()`, `tf.nn.sigmoid()`, `tf.nn.tanh()`, `tf.nn.softmax()`\n",
    "    - **Industry applications**: Every major deep learning model uses these functions\n",
    "\n",
    "### 🎯 The Power of Nonlinearity\n",
    "    You've unlocked the key to deep learning:\n",
    "    - **Before**: Linear models limited to simple patterns\n",
    "    - **After**: Nonlinear models can learn any pattern (universal approximation)\n",
    "\n",
    "    **Next Module**: Layers - Building blocks that combine your tensors and activations into powerful transformations!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db927a6",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### 🧪 Integration Test: Tensor → Activations Workflow\n",
    "\n",
    "This comprehensive test validates that your tensor and activation implementations work together seamlessly, simulating a realistic neural network forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b649dad",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "integration-test-tensor-activations",
     "locked": true,
     "points": 25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_tensor_activations_integration():\n",
    "    \"\"\"\n",
    "    Integration test validating end-to-end tensor + activations workflow.\n",
    "    \n",
    "    Simulates a realistic neural network scenario:\n",
    "    1. Create input data (Tensor)\n",
    "    2. Apply linear transformation (simulated weight multiplication)\n",
    "    3. Apply each activation function\n",
    "    4. Verify mathematical properties and integration\n",
    "    \"\"\"\n",
    "    print(\"🔬 Integration Test: Tensor ↔ Activations Integration...\")\n",
    "    \n",
    "    # Simulate realistic neural network data\n",
    "    # Batch of 3 samples, each with 4 features (mini-batch processing)\n",
    "    raw_input = Tensor([\n",
    "        [-2.5, -1.0,  0.0,  1.5],  # Sample 1: mixed positive/negative\n",
    "        [ 3.2,  0.5, -0.8,  2.1],  # Sample 2: mostly positive\n",
    "        [-1.8,  2.3, -3.1,  0.7]   # Sample 3: mixed with extreme values\n",
    "    ])\n",
    "    \n",
    "    print(f\"📊 Input shape: {raw_input.shape}\")\n",
    "    print(f\"📊 Input data:\\n{raw_input.data}\")\n",
    "    \n",
    "    # Simulate weight matrix (4 input features → 3 hidden units)\n",
    "    weights = Tensor([\n",
    "        [ 0.5, -0.3,  0.8],  # Feature 1 weights to 3 hidden units\n",
    "        [-0.2,  0.7,  0.1],  # Feature 2 weights  \n",
    "        [ 0.9, -0.5,  0.4],  # Feature 3 weights\n",
    "        [ 0.3,  0.8, -0.6]   # Feature 4 weights\n",
    "    ])\n",
    "    \n",
    "    # Simulate matrix multiplication (input @ weights)\n",
    "    # In real neural networks, this would be: output = input @ weights + bias\n",
    "    hidden_pre_activation = raw_input @ weights.data.T  # Transpose for correct dimensions\n",
    "    hidden_pre_activation = Tensor(hidden_pre_activation)\n",
    "    \n",
    "    print(f\"📊 Pre-activation shape: {hidden_pre_activation.shape}\")\n",
    "    print(f\"📊 Pre-activation values:\\n{hidden_pre_activation.data}\")\n",
    "    \n",
    "    # Test each activation function on the realistic data\n",
    "    relu = ReLU()\n",
    "    sigmoid = Sigmoid() \n",
    "    tanh = Tanh()\n",
    "    softmax = Softmax()\n",
    "    \n",
    "    # Apply all activations\n",
    "    relu_output = relu(hidden_pre_activation)\n",
    "    sigmoid_output = sigmoid(hidden_pre_activation)  \n",
    "    tanh_output = tanh(hidden_pre_activation)\n",
    "    softmax_output = softmax(hidden_pre_activation)\n",
    "    \n",
    "    print(\"\\n🔍 Testing activation outputs...\")\n",
    "    \n",
    "    # Validate shapes are preserved\n",
    "    assert relu_output.shape == hidden_pre_activation.shape, \"ReLU should preserve shape\"\n",
    "    assert sigmoid_output.shape == hidden_pre_activation.shape, \"Sigmoid should preserve shape\"\n",
    "    assert tanh_output.shape == hidden_pre_activation.shape, \"Tanh should preserve shape\"\n",
    "    assert softmax_output.shape == hidden_pre_activation.shape, \"Softmax should preserve shape\"\n",
    "    print(\"✅ Shape preservation: All activations maintain input dimensions\")\n",
    "    \n",
    "    # Validate mathematical properties\n",
    "    # ReLU properties\n",
    "    assert np.all(relu_output.data >= 0), \"ReLU outputs must be non-negative\"\n",
    "    sparsity = np.count_nonzero(relu_output.data == 0) / relu_output.size\n",
    "    print(f\"✅ ReLU sparsity: {sparsity*100:.1f}% zeros (good for efficiency)\")\n",
    "    \n",
    "    # Sigmoid properties  \n",
    "    assert np.all(sigmoid_output.data > 0), \"Sigmoid outputs must be positive\"\n",
    "    assert np.all(sigmoid_output.data < 1), \"Sigmoid outputs must be less than 1\"\n",
    "    sigmoid_range = [np.min(sigmoid_output.data), np.max(sigmoid_output.data)]\n",
    "    print(f\"✅ Sigmoid range: [{sigmoid_range[0]:.3f}, {sigmoid_range[1]:.3f}] ∈ (0,1)\")\n",
    "    \n",
    "    # Tanh properties\n",
    "    assert np.all(tanh_output.data > -1), \"Tanh outputs must be greater than -1\"  \n",
    "    assert np.all(tanh_output.data < 1), \"Tanh outputs must be less than 1\"\n",
    "    tanh_range = [np.min(tanh_output.data), np.max(tanh_output.data)]\n",
    "    print(f\"✅ Tanh range: [{tanh_range[0]:.3f}, {tanh_range[1]:.3f}] ∈ (-1,1)\")\n",
    "    \n",
    "    # Softmax properties (most important for multi-class classification)\n",
    "    for i in range(softmax_output.shape[0]):  # Check each sample\n",
    "        sample_sum = np.sum(softmax_output.data[i])\n",
    "        assert abs(sample_sum - 1.0) < 1e-6, f\"Softmax row {i} should sum to 1, got {sample_sum}\"\n",
    "    print(\"✅ Softmax probability: Each row sums to 1.0 (valid probability distribution)\")\n",
    "    \n",
    "    # Test activation chaining (realistic neural network scenario)\n",
    "    print(\"\\n🔗 Testing activation chaining (hidden → output layers)...\")\n",
    "    \n",
    "    # Hidden layer: ReLU activation (common choice)\n",
    "    hidden_output = relu(hidden_pre_activation)\n",
    "    \n",
    "    # Simulate output layer weights (3 hidden → 2 output classes)\n",
    "    output_weights = Tensor([\n",
    "        [0.6, -0.4],  # Hidden unit 1 → [class 0, class 1]\n",
    "        [-0.3, 0.8],  # Hidden unit 2 → [class 0, class 1]  \n",
    "        [0.5, 0.2]    # Hidden unit 3 → [class 0, class 1]\n",
    "    ])\n",
    "    \n",
    "    # Output layer pre-activation\n",
    "    output_pre_activation = hidden_output @ output_weights.data\n",
    "    output_pre_activation = Tensor(output_pre_activation) \n",
    "    \n",
    "    # Output layer: Softmax for classification\n",
    "    final_output = softmax(output_pre_activation)\n",
    "    \n",
    "    print(f\"📊 Final classification output shape: {final_output.shape}\")\n",
    "    print(f\"📊 Final probabilities:\\n{final_output.data}\")\n",
    "    \n",
    "    # Validate final output properties\n",
    "    assert final_output.shape == (3, 2), \"Should have 3 samples × 2 classes\"\n",
    "    for i in range(3):\n",
    "        sample_probs = final_output.data[i]\n",
    "        assert abs(np.sum(sample_probs) - 1.0) < 1e-6, f\"Sample {i} probabilities should sum to 1\"\n",
    "        assert np.all(sample_probs > 0), f\"Sample {i} should have positive probabilities\"\n",
    "        predicted_class = np.argmax(sample_probs)\n",
    "        confidence = np.max(sample_probs)\n",
    "        print(f\"✅ Sample {i+1}: Class {predicted_class}, Confidence {confidence:.3f}\")\n",
    "    \n",
    "    # Test tensor arithmetic integration\n",
    "    print(\"\\n🔢 Testing tensor arithmetic with activations...\")\n",
    "    \n",
    "    # Element-wise operations should work seamlessly\n",
    "    combined_output = relu_output + sigmoid_output * 0.5\n",
    "    assert isinstance(combined_output, Tensor), \"Arithmetic should return Tensor\"\n",
    "    assert combined_output.shape == relu_output.shape, \"Arithmetic should preserve shape\"\n",
    "    print(\"✅ Tensor arithmetic integration: Addition and multiplication work\")\n",
    "    \n",
    "    # Test broadcasting with activations\n",
    "    bias = Tensor([0.1, -0.05, 0.2])  # Shape: (3,)\n",
    "    biased_output = sigmoid_output + bias  # Should broadcast\n",
    "    assert biased_output.shape == sigmoid_output.shape, \"Broadcasting should work with activations\"\n",
    "    print(\"✅ Broadcasting integration: Bias addition works\")\n",
    "    \n",
    "    print(\"\\n🎯 Integration Test Results:\")\n",
    "    print(\"✅ Tensor creation and manipulation\")\n",
    "    print(\"✅ Matrix operations (simulated linear layers)\")  \n",
    "    print(\"✅ All activation functions working correctly\")\n",
    "    print(\"✅ Mathematical properties validated\")\n",
    "    print(\"✅ Realistic neural network forward pass\")\n",
    "    print(\"✅ Activation chaining (hidden → output)\")\n",
    "    print(\"✅ Tensor arithmetic with activations\")\n",
    "    print(\"✅ Broadcasting compatibility\")\n",
    "    \n",
    "    print(\"\\n🚀 Ready for real neural networks! Your tensor and activation implementations\")\n",
    "    print(\"   can handle the computational demands of modern deep learning.\")\n",
    "\n",
    "# Run the comprehensive integration test\n",
    "test_tensor_activations_integration()\n",
    "\n",
    "    Your activation functions are the key to neural network intelligence. Now let's build the layers that use them!\n",
    "\"\"\" "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
