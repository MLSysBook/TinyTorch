{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91e3dfc1",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Activations - Nonlinearity in Neural Networks\n",
    "\n",
    "Welcome to the Activations module! This is where neural networks get their power through nonlinearity.\n",
    "\n",
    "## Learning Goals\n",
    "- Understand why activation functions are essential for neural networks\n",
    "- Implement the four most important activation functions: ReLU, Sigmoid, Tanh, and Softmax\n",
    "- Visualize how activations transform data and enable complex learning\n",
    "- See how activations work with layers to build powerful networks\n",
    "- Master the NBGrader workflow with comprehensive testing\n",
    "\n",
    "## Build → Use → Understand\n",
    "1. **Build**: Activation functions that add nonlinearity\n",
    "2. **Use**: Transform tensors and see immediate results\n",
    "3. **Understand**: How nonlinearity enables complex pattern learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0529372d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "activations-imports",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| default_exp core.activations\n",
    "\n",
    "#| export\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from typing import Union, List\n",
    "\n",
    "# Import our Tensor class - try from package first, then from local module\n",
    "try:\n",
    "    from tinytorch.core.tensor import Tensor\n",
    "except ImportError:\n",
    "    # For development, import from local tensor module\n",
    "    sys.path.append(os.path.join(os.path.dirname(__file__), '..', '01_tensor'))\n",
    "    from tensor_dev import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9b7e24",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "activations-welcome",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"🔥 TinyTorch Activations Module\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Python version: {sys.version_info.major}.{sys.version_info.minor}\")\n",
    "print(\"Ready to build activation functions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82a46e9",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 📦 Where This Code Lives in the Final Package\n",
    "\n",
    "**Learning Side:** You work in `modules/source/02_activations/activations_dev.py`  \n",
    "**Building Side:** Code exports to `tinytorch.core.activations`\n",
    "\n",
    "```python\n",
    "# Final package structure:\n",
    "from tinytorch.core.activations import ReLU, Sigmoid, Tanh, Softmax\n",
    "from tinytorch.core.tensor import Tensor  # Foundation\n",
    "from tinytorch.core.layers import Dense  # Uses activations\n",
    "```\n",
    "\n",
    "**Why this matters:**\n",
    "- **Learning:** Focused modules for deep understanding\n",
    "- **Production:** Proper organization like PyTorch's `torch.nn.ReLU`\n",
    "- **Consistency:** All activation functions live together in `core.activations`\n",
    "- **Integration:** Works seamlessly with tensors and layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccedc6c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## What Are Activation Functions? The Key to Neural Network Intelligence\n",
    "\n",
    "### 🎯 The Core Problem: Linear Limitations\n",
    "\n",
    "Without activation functions, neural networks are fundamentally limited. No matter how many layers you stack, they can only learn linear relationships:\n",
    "\n",
    "```\n",
    "Layer 1: h₁ = W₁ · x + b₁\n",
    "Layer 2: h₂ = W₂ · h₁ + b₂ = W₂ · (W₁ · x + b₁) + b₂\n",
    "Layer 3: h₃ = W₃ · h₂ + b₃ = W₃ · (W₂ · (W₁ · x + b₁) + b₂) + b₃\n",
    "```\n",
    "\n",
    "**Mathematical Reality**: This always simplifies to:\n",
    "```\n",
    "y = W_combined · x + b_combined\n",
    "```\n",
    "\n",
    "**A single linear transformation!** This means neural networks without activations cannot learn:\n",
    "- **Image patterns**: Recognizing curves, shapes, textures (nonlinear pixel relationships)\n",
    "- **Language patterns**: Understanding syntax, semantics, context (nonlinear word relationships)\n",
    "- **Strategic patterns**: Game playing, decision making (nonlinear strategy relationships)\n",
    "- **Any complex real-world pattern**: Most useful relationships in data are nonlinear\n",
    "\n",
    "### 🔑 The Solution: Strategic Nonlinearity\n",
    "\n",
    "Activation functions break this linear limitation by adding nonlinearity between layers:\n",
    "\n",
    "```\n",
    "Neural Network Flow:\n",
    "Input → Linear Layer → Activation → Linear Layer → Activation → ... → Output\n",
    "  x   →    W₁x + b₁   →    f(·)    →    W₂h₁ + b₂  →    f(·)    →     y\n",
    "```\n",
    "\n",
    "**Now each layer can learn complex transformations!**\n",
    "\n",
    "### 📊 Visual Understanding: The Power of Nonlinearity\n",
    "\n",
    "#### Linear Network (No Activations):\n",
    "```\n",
    "Input Space:        Decision Boundary:       Capability:\n",
    "     ·              ─────────────────────       Only straight lines\n",
    "   ·   ·                                      Cannot separate:\n",
    " ·   ·   ·          ─────────────────────       • XOR problem\n",
    "   ·   ·                                       • Circular patterns\n",
    "     ·              ─────────────────────       • Any curved boundary\n",
    "```\n",
    "\n",
    "#### Nonlinear Network (With Activations):\n",
    "```\n",
    "Input Space:        Decision Boundary:       Capability:\n",
    "     ·              ╭─────────────────╮       Complex curves\n",
    "   ·   ·            │  ╭─────────╮    │       Can separate:\n",
    " ·   ·   ·          │  │         │    │       • XOR problem\n",
    "   ·   ·            │  ╰─────────╯    │       • Circular patterns  \n",
    "     ·              ╰─────────────────╯       • Any complex shape\n",
    "```\n",
    "\n",
    "### 🏭 Real-World Impact: The Deep Learning Revolution\n",
    "\n",
    "**Before Activation Functions (Pre-2000s)**:\n",
    "- Limited to linear classifiers and simple perceptrons\n",
    "- Could not solve XOR problem (fundamental nonlinear pattern)\n",
    "- Shallow networks with limited capability\n",
    "- AI winter due to fundamental limitations\n",
    "\n",
    "**After Activation Functions (2000s-Present)**:\n",
    "- Deep learning revolution begins\n",
    "- Complex pattern recognition possible\n",
    "- State-of-the-art results in vision, language, games\n",
    "- Universal approximation theorem: can learn any function\n",
    "\n",
    "### 🎯 The Four Essential Activations We'll Master\n",
    "\n",
    "1. **ReLU (Rectified Linear Unit)**: `f(x) = max(0, x)`\n",
    "   - **The foundation** of modern deep learning\n",
    "   - **Used in**: ResNet, VGG, GPT, BERT, virtually every modern model\n",
    "   - **Why crucial**: Solves vanishing gradient problem, computationally efficient\n",
    "\n",
    "2. **Sigmoid**: `f(x) = 1/(1 + e^(-x))`  \n",
    "   - **The classic** activation for probability outputs\n",
    "   - **Used in**: Binary classification, LSTM gates, attention mechanisms\n",
    "   - **Why crucial**: Maps any input to probability range (0,1)\n",
    "\n",
    "3. **Tanh**: `f(x) = (e^x - e^(-x))/(e^x + e^(-x))`\n",
    "   - **Zero-centered** activation for better training\n",
    "   - **Used in**: LSTM cells, traditional neural networks, signal processing\n",
    "   - **Why crucial**: Better gradients due to zero-centered output\n",
    "\n",
    "4. **Softmax**: `f(x_i) = e^(x_i) / Σⱼ e^(x_j)`\n",
    "   - **Probability distribution** for multi-class problems  \n",
    "   - **Used in**: Classification heads, attention mechanisms, language models\n",
    "   - **Why crucial**: Converts logits to valid probability distributions\n",
    "\n",
    "### 🧠 Mathematical Foundation: Understanding the Functions\n",
    "\n",
    "Each activation function serves a specific purpose:\n",
    "\n",
    "#### **Activation Properties Table**\n",
    "```\n",
    "Function  | Range     | Key Property        | Primary Use\n",
    "----------|-----------|--------------------|-----------------\n",
    "ReLU      | [0, ∞)    | Sparse (many 0s)   | Hidden layers\n",
    "Sigmoid   | (0, 1)    | Probability-like    | Binary output\n",
    "Tanh      | (-1, 1)   | Zero-centered       | Hidden layers\n",
    "Softmax   | (0, 1)    | Sums to 1          | Multi-class output\n",
    "```\n",
    "\n",
    "#### **Gradient Properties (Critical for Training)**\n",
    "```\n",
    "Function  | Gradient   | Vanishing Gradient? | Training Efficiency\n",
    "----------|------------|--------------------|-----------------\n",
    "ReLU      | 1 or 0     | No (for x > 0)     | Excellent\n",
    "Sigmoid   | ≤ 0.25     | Yes (for large |x|) | Poor for deep nets\n",
    "Tanh      | ≤ 1        | Yes (for large |x|) | Better than sigmoid\n",
    "Softmax   | Complex    | No                 | Good for outputs\n",
    "```\n",
    "\n",
    "This mathematical foundation explains why ReLU revolutionized deep learning - it's the only activation that doesn't suffer from vanishing gradients!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633b7075",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 🔧 DEVELOPMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033de441",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 1: ReLU - The Foundation of Deep Learning\n",
    "\n",
    "### What is ReLU?\n",
    "**ReLU (Rectified Linear Unit)** is the most important activation function in deep learning:\n",
    "\n",
    "```\n",
    "f(x) = max(0, x)\n",
    "```\n",
    "\n",
    "- **Positive inputs**: Pass through unchanged\n",
    "- **Negative inputs**: Become zero\n",
    "- **Zero**: Stays zero\n",
    "\n",
    "### Why ReLU Revolutionized Deep Learning\n",
    "1. **Computational efficiency**: Just a max operation\n",
    "2. **No vanishing gradients**: Derivative is 1 for positive values\n",
    "3. **Sparsity**: Many neurons output exactly 0\n",
    "4. **Empirical success**: Works well in practice\n",
    "\n",
    "### Visual Understanding\n",
    "```\n",
    "Input:  [-2, -1, 0, 1, 2]\n",
    "ReLU:   [ 0,  0, 0, 1, 2]\n",
    "```\n",
    "\n",
    "### Real-World Applications\n",
    "- **Image classification**: ResNet, VGG, AlexNet\n",
    "- **Object detection**: YOLO, R-CNN\n",
    "- **Language models**: Transformer feedforward layers\n",
    "- **Recommendation**: Deep collaborative filtering\n",
    "\n",
    "### Mathematical Properties\n",
    "- **Derivative**: f'(x) = 1 if x > 0, else 0\n",
    "- **Range**: [0, ∞)\n",
    "- **Sparsity**: Outputs exactly 0 for negative inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c3795b",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "relu-class",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class ReLU:\n",
    "    \"\"\"\n",
    "    ReLU Activation Function: f(x) = max(0, x)\n",
    "    \n",
    "    The most popular activation function in deep learning.\n",
    "    Simple, fast, and effective for most applications.\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply ReLU activation: f(x) = max(0, x)\n",
    "        \n",
    "        TODO: Implement ReLU activation function.\n",
    "        \n",
    "        STEP-BY-STEP IMPLEMENTATION:\n",
    "        1. For each element in the input tensor, apply max(0, element)\n",
    "        2. Use NumPy's maximum function for efficient element-wise operation\n",
    "        3. Return a new tensor of the same type with the results\n",
    "        4. Preserve the input tensor's shape\n",
    "        \n",
    "        EXAMPLE USAGE:\n",
    "        ```python\n",
    "        relu = ReLU()\n",
    "        input_tensor = Tensor([[-2, -1, 0, 1, 2]])\n",
    "        output = relu(input_tensor)\n",
    "        print(output.data)  # [[0, 0, 0, 1, 2]]\n",
    "        ```\n",
    "        \n",
    "        IMPLEMENTATION HINTS:\n",
    "        - Use np.maximum(0, x.data) for element-wise max with 0\n",
    "        - Return the same type as input: return type(x)(result)\n",
    "        - The shape should remain the same as input\n",
    "        - Don't modify the input tensor (immutable operations)\n",
    "        \n",
    "        LEARNING CONNECTIONS:\n",
    "        - This is like torch.nn.ReLU() in PyTorch\n",
    "        - Used in virtually every modern neural network\n",
    "        - Enables deep networks by preventing vanishing gradients\n",
    "        - Creates sparse representations (many zeros)\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        result = np.maximum(0, x.data)\n",
    "        return type(x)(result)\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Make the class callable: relu(x) instead of relu.forward(x)\"\"\"\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5703c0e9",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### 🧪 Test Your ReLU Implementation\n",
    "\n",
    "Once you implement the ReLU forward method above, run this cell to test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c69ff2",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-relu-immediate",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_relu_activation():\n",
    "    \"\"\"Unit test for the ReLU activation function.\"\"\"\n",
    "    print(\"🔬 Unit Test: ReLU Activation...\")\n",
    "\n",
    "    # Create ReLU instance\n",
    "    relu = ReLU()\n",
    "\n",
    "    # Test with mixed positive/negative values\n",
    "    test_input = Tensor([[-2, -1, 0, 1, 2]])\n",
    "    result = relu(test_input)\n",
    "    expected = np.array([[0, 0, 0, 1, 2]])\n",
    "    \n",
    "    assert np.array_equal(result.data, expected), f\"ReLU failed: expected {expected}, got {result.data}\"\n",
    "    \n",
    "    # Test that negative values become zero\n",
    "    assert np.all(result.data >= 0), \"ReLU should make all negative values zero\"\n",
    "    \n",
    "    # Test that positive values remain unchanged\n",
    "    positive_input = Tensor([[1, 2, 3, 4, 5]])\n",
    "    positive_result = relu(positive_input)\n",
    "    assert np.array_equal(positive_result.data, positive_input.data), \"ReLU should preserve positive values\"\n",
    "    \n",
    "    # Test with 2D tensor\n",
    "    matrix_input = Tensor([[-1, 2], [3, -4]])\n",
    "    matrix_result = relu(matrix_input)\n",
    "    matrix_expected = np.array([[0, 2], [3, 0]])\n",
    "    assert np.array_equal(matrix_result.data, matrix_expected), \"ReLU should work with 2D tensors\"\n",
    "    \n",
    "    # Test shape preservation\n",
    "    assert matrix_result.shape == matrix_input.shape, \"ReLU should preserve input shape\"\n",
    "    \n",
    "    print(\"✅ ReLU activation tests passed!\")\n",
    "    print(f\"✅ Negative values correctly zeroed\")\n",
    "    print(f\"✅ Positive values preserved\")\n",
    "    print(f\"✅ Shape preservation working\")\n",
    "    print(f\"✅ Works with multi-dimensional tensors\")\n",
    "\n",
    "# Run the test\n",
    "test_unit_relu_activation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d351ac43",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### 🎯 Checkpoint: ReLU Mastery\n",
    "Congratulations! You've successfully implemented and tested the ReLU activation function. \n",
    "\n",
    "Before moving to the next activation, make sure you can:\n",
    "\n",
    "```python\n",
    "# Create and test ReLU with different input patterns\n",
    "relu = ReLU()\n",
    "\n",
    "# Test 1: Basic functionality\n",
    "basic_input = Tensor([[-3, -1, 0, 1, 3]])\n",
    "basic_output = relu(basic_input)\n",
    "print(f\"Input:  {basic_input.data}\")   # [[-3, -1,  0,  1,  3]]\n",
    "print(f\"Output: {basic_output.data}\")  # [[ 0,  0,  0,  1,  3]]\n",
    "\n",
    "# Test 2: 2D tensors (simulating mini-batch)\n",
    "batch_input = Tensor([[-2, 1], [3, -1]])\n",
    "batch_output = relu(batch_input)\n",
    "print(f\"Batch Output: {batch_output.data}\")  # [[0, 1], [3, 0]]\n",
    "\n",
    "# Test 3: Understand sparsity\n",
    "print(f\"Sparsity: {np.count_nonzero(batch_output.data == 0) / batch_output.size * 100:.1f}% zeros\")\n",
    "```\n",
    "\n",
    "**Key Understanding**: ReLU creates **sparse representations** - many outputs are exactly zero. This sparsity makes networks:\n",
    "- **Computationally efficient**: Skip zero computations\n",
    "- **Memory efficient**: Compress sparse representations  \n",
    "- **Biologically inspired**: Real neurons often stay silent\n",
    "\n",
    "You now have the foundation activation that powers modern deep learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e12123",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 2: Sigmoid - Classic Binary Classification\n",
    "\n",
    "### What is Sigmoid? The Probability Gateway\n",
    "\n",
    "**Sigmoid** is the classic S-shaped activation function that gracefully maps any real number to the probability range (0, 1):\n",
    "\n",
    "```\n",
    "f(x) = 1 / (1 + e^(-x))\n",
    "```\n",
    "\n",
    "### 📊 Visual Understanding: The Sigmoid Curve\n",
    "\n",
    "```\n",
    "Sigmoid Function Shape:\n",
    "\n",
    "     1.0 |                    ╭─────\n",
    "         |                 ╭──╯\n",
    "     0.8 |              ╭─╯\n",
    "         |           ╭─╯\n",
    "     0.6 |        ╭─╯\n",
    "         |      ╭╯\n",
    "     0.4 |   ╭─╯\n",
    "         |  ╱\n",
    "     0.2 |╱\n",
    "         ╱\n",
    "     0.0 ╱────────────────────────────\n",
    "        -4    -2     0     2     4\n",
    "```\n",
    "\n",
    "**Key Insight**: The sigmoid is nature's smooth switch - it gradually transitions from \"off\" (0) to \"on\" (1).\n",
    "\n",
    "### 🎯 Why Sigmoid Is Essential for ML\n",
    "\n",
    "#### **1. Probability Interpretation**\n",
    "```\n",
    "Raw Neural Network Output:  [-2.5, 0.3, 1.8, -0.7]  (any range)\n",
    "After Sigmoid:              [0.08, 0.57, 0.86, 0.33]  (valid probabilities)\n",
    "```\n",
    "\n",
    "#### **2. Smooth Decision Boundaries**\n",
    "Unlike step functions, sigmoid provides smooth transitions:\n",
    "```\n",
    "Hard Decision (Step):     0, 0, 0, 1, 1, 1  (abrupt jump)\n",
    "Soft Decision (Sigmoid):  0.01, 0.27, 0.73, 0.88, 0.95  (smooth transition)\n",
    "```\n",
    "\n",
    "#### **3. Binary Classification Perfect Fit**\n",
    "```\n",
    "For binary classification:\n",
    "- Output > 0.5 → Class 1 (positive)\n",
    "- Output < 0.5 → Class 0 (negative)  \n",
    "- Output = 0.5 → Uncertain (decision boundary)\n",
    "```\n",
    "\n",
    "### 🏭 Real-World Applications: Where Sigmoid Shines\n",
    "\n",
    "**Binary Classification Tasks**:\n",
    "- **Email spam detection**: Probability this email is spam\n",
    "- **Medical diagnosis**: Probability patient has condition\n",
    "- **Fraud detection**: Probability transaction is fraudulent\n",
    "- **A/B testing**: Probability user clicks/converts\n",
    "\n",
    "**Neural Network Components**:\n",
    "- **LSTM gates**: Forget gate, input gate, output gate decisions\n",
    "- **Attention mechanisms**: How much attention to pay to each element\n",
    "- **Probability outputs**: Final layer for binary classification\n",
    "\n",
    "### 🧠 Mathematical Deep Dive\n",
    "\n",
    "#### **Critical Properties**\n",
    "```\n",
    "Property          | Value/Formula        | ML Significance\n",
    "------------------|---------------------|----------------------------\n",
    "Range             | (0, 1)              | Valid probability space\n",
    "Derivative        | σ(x)·(1-σ(x))       | Self-referential gradient\n",
    "Maximum gradient  | 0.25 (at x=0)       | Vanishing gradient problem\n",
    "Symmetry point    | σ(0) = 0.5          | Natural decision boundary\n",
    "Saturation        | σ(±∞) ≈ 0 or 1      | Confident predictions\n",
    "```\n",
    "\n",
    "#### **The Vanishing Gradient Problem**\n",
    "```\n",
    "Input Range    | Sigmoid Output | Gradient Size | Training Impact\n",
    "---------------|----------------|---------------|----------------\n",
    "x ∈ [-1, 1]    | [0.27, 0.73]   | ~0.2         | Good learning\n",
    "x ∈ [-3, 3]    | [0.05, 0.95]   | ~0.05        | Slow learning  \n",
    "x ∈ [-5, 5]    | [0.007, 0.993] | ~0.007       | Very slow\n",
    "x ∈ [-10, 10]  | [~0, ~1]       | ~0.0001      | Learning stops\n",
    "```\n",
    "\n",
    "**This is why ReLU replaced sigmoid in hidden layers - sigmoid gradients vanish for large inputs!**\n",
    "\n",
    "### 🎯 When to Use Sigmoid vs ReLU\n",
    "\n",
    "```\n",
    "Use Case                     | Sigmoid | ReLU | Why?\n",
    "----------------------------|---------|------|---------------------------\n",
    "Hidden layers (deep nets)  |    ❌   |  ✅  | ReLU avoids vanishing gradients\n",
    "Binary classification output|    ✅   |  ❌  | Need probability interpretation\n",
    "LSTM/GRU gates             |    ✅   |  ❌  | Need smooth 0-1 gating\n",
    "Multi-class classification  |    ❌   |  ❌  | Use Softmax instead\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8c1c13",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "sigmoid-class",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class Sigmoid:\n",
    "    \"\"\"\n",
    "    Sigmoid Activation Function: f(x) = 1 / (1 + e^(-x))\n",
    "    \n",
    "    Maps any real number to the range (0, 1).\n",
    "    Useful for binary classification and probability outputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply Sigmoid activation: f(x) = 1 / (1 + e^(-x))\n",
    "        \n",
    "        TODO: Implement Sigmoid activation function.\n",
    "        \n",
    "        STEP-BY-STEP IMPLEMENTATION:\n",
    "        1. Compute the negative of input: -x.data\n",
    "        2. Compute the exponential: np.exp(-x.data)\n",
    "        3. Add 1 to the exponential: 1 + np.exp(-x.data)\n",
    "        4. Take the reciprocal: 1 / (1 + np.exp(-x.data))\n",
    "        5. Return as new Tensor\n",
    "        \n",
    "        EXAMPLE USAGE:\n",
    "        ```python\n",
    "        sigmoid = Sigmoid()\n",
    "        input_tensor = Tensor([[-2, -1, 0, 1, 2]])\n",
    "        output = sigmoid(input_tensor)\n",
    "        print(output.data)  # [[0.119, 0.269, 0.5, 0.731, 0.881]]\n",
    "        ```\n",
    "        \n",
    "        IMPLEMENTATION HINTS:\n",
    "        - Use np.exp() for exponential function\n",
    "        - Formula: 1 / (1 + np.exp(-x.data))\n",
    "        - Handle potential overflow with np.clip(-x.data, -500, 500)\n",
    "        - Return Tensor(result)\n",
    "        \n",
    "        LEARNING CONNECTIONS:\n",
    "        - This is like torch.nn.Sigmoid() in PyTorch\n",
    "        - Used in binary classification output layers\n",
    "        - Key component in LSTM and GRU gating mechanisms\n",
    "        - Historically important for early neural networks\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        # Clip to prevent overflow\n",
    "        clipped_input = np.clip(-x.data, -500, 500)\n",
    "        result = 1 / (1 + np.exp(clipped_input))\n",
    "        return type(x)(result)\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Make the class callable: sigmoid(x) instead of sigmoid.forward(x)\"\"\"\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166876a9",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### 🧪 Test Your Sigmoid Implementation\n",
    "\n",
    "Once you implement the Sigmoid forward method above, run this cell to test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d445dc",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-sigmoid-immediate",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_sigmoid_activation():\n",
    "    \"\"\"Unit test for the Sigmoid activation function.\"\"\"\n",
    "    print(\"🔬 Unit Test: Sigmoid Activation...\")\n",
    "\n",
    "# Create Sigmoid instance\n",
    "    sigmoid = Sigmoid()\n",
    "\n",
    "    # Test with known values\n",
    "    test_input = Tensor([[0]])\n",
    "    result = sigmoid(test_input)\n",
    "    expected = 0.5\n",
    "    \n",
    "    assert abs(result.data[0][0] - expected) < 1e-6, f\"Sigmoid(0) should be 0.5, got {result.data[0][0]}\"\n",
    "    \n",
    "    # Test with positive and negative values\n",
    "    test_input = Tensor([[-2, -1, 0, 1, 2]])\n",
    "    result = sigmoid(test_input)\n",
    "    \n",
    "    # Check that all values are between 0 and 1\n",
    "    assert np.all(result.data > 0), \"Sigmoid output should be > 0\"\n",
    "    assert np.all(result.data < 1), \"Sigmoid output should be < 1\"\n",
    "    \n",
    "    # Test symmetry: sigmoid(-x) = 1 - sigmoid(x)\n",
    "    x_val = 1.0\n",
    "    pos_result = sigmoid(Tensor([[x_val]]))\n",
    "    neg_result = sigmoid(Tensor([[-x_val]]))\n",
    "    symmetry_check = abs(pos_result.data[0][0] + neg_result.data[0][0] - 1.0)\n",
    "    assert symmetry_check < 1e-6, \"Sigmoid should be symmetric around 0.5\"\n",
    "    \n",
    "    # Test with 2D tensor\n",
    "    matrix_input = Tensor([[-1, 1], [0, 2]])\n",
    "    matrix_result = sigmoid(matrix_input)\n",
    "    assert matrix_result.shape == matrix_input.shape, \"Sigmoid should preserve shape\"\n",
    "    \n",
    "    # Test extreme values (should not overflow)\n",
    "    extreme_input = Tensor([[-100, 100]])\n",
    "    extreme_result = sigmoid(extreme_input)\n",
    "    assert not np.any(np.isnan(extreme_result.data)), \"Sigmoid should handle extreme values\"\n",
    "    assert not np.any(np.isinf(extreme_result.data)), \"Sigmoid should not produce inf values\"\n",
    "    \n",
    "    print(\"✅ Sigmoid activation tests passed!\")\n",
    "    print(f\"✅ Outputs correctly bounded between 0 and 1\")\n",
    "    print(f\"✅ Symmetric property verified\")\n",
    "    print(f\"✅ Handles extreme values without overflow\")\n",
    "    print(f\"✅ Shape preservation working\")\n",
    "\n",
    "# Run the test\n",
    "test_unit_sigmoid_activation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa6098b",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 3: Tanh - Centered Activation\n",
    "\n",
    "### What is Tanh?\n",
    "**Tanh (Hyperbolic Tangent)** is similar to sigmoid but centered around zero:\n",
    "\n",
    "```\n",
    "f(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "```\n",
    "\n",
    "### Why Tanh is Better Than Sigmoid\n",
    "1. **Zero-centered**: Outputs range from -1 to 1\n",
    "2. **Better gradients**: Helps with gradient flow in deep networks\n",
    "3. **Faster convergence**: Less bias shift during training\n",
    "4. **Stronger gradients**: Maximum gradient is 1 vs 0.25 for sigmoid\n",
    "\n",
    "### Visual Understanding\n",
    "```\n",
    "Input: [-∞, -2, -1, 0, 1, 2, ∞]\n",
    "Tanh:  [-1, -0.96, -0.76, 0, 0.76, 0.96, 1]\n",
    "```\n",
    "\n",
    "### Real-World Applications\n",
    "- **Hidden layers**: Better than sigmoid for internal activations\n",
    "- **RNN cells**: Classic RNN and LSTM use tanh\n",
    "- **Normalization**: When you need zero-centered outputs\n",
    "- **Feature scaling**: Maps inputs to [-1, 1] range\n",
    "\n",
    "### Mathematical Properties\n",
    "- **Range**: (-1, 1)\n",
    "- **Derivative**: f'(x) = 1 - f(x)²\n",
    "- **Zero-centered**: f(0) = 0\n",
    "- **Antisymmetric**: f(-x) = -f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d904e6",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "tanh-class",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class Tanh:\n",
    "    \"\"\"\n",
    "    Tanh Activation Function: f(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "    \n",
    "    Zero-centered activation function with range (-1, 1).\n",
    "    Better gradient properties than sigmoid.\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply Tanh activation: f(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "        \n",
    "        TODO: Implement Tanh activation function.\n",
    "        \n",
    "        STEP-BY-STEP IMPLEMENTATION:\n",
    "        1. Use NumPy's built-in tanh function: np.tanh(x.data)\n",
    "        2. Alternatively, implement manually:\n",
    "           - Compute e^x and e^(-x)\n",
    "           - Calculate (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "        3. Return as new Tensor\n",
    "        \n",
    "        EXAMPLE USAGE:\n",
    "        ```python\n",
    "        tanh = Tanh()\n",
    "        input_tensor = Tensor([[-2, -1, 0, 1, 2]])\n",
    "        output = tanh(input_tensor)\n",
    "        print(output.data)  # [[-0.964, -0.762, 0, 0.762, 0.964]]\n",
    "        ```\n",
    "        \n",
    "        IMPLEMENTATION HINTS:\n",
    "        - Use np.tanh(x.data) for simplicity\n",
    "        - Manual implementation: (np.exp(x.data) - np.exp(-x.data)) / (np.exp(x.data) + np.exp(-x.data))\n",
    "        - Handle overflow by clipping inputs: np.clip(x.data, -500, 500)\n",
    "        - Return Tensor(result)\n",
    "        \n",
    "        LEARNING CONNECTIONS:\n",
    "        - This is like torch.nn.Tanh() in PyTorch\n",
    "        - Used in RNN, LSTM, and GRU cells\n",
    "        - Better than sigmoid for hidden layers\n",
    "        - Zero-centered outputs help with gradient flow\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        # Use NumPy's built-in tanh function\n",
    "        result = np.tanh(x.data)\n",
    "        return type(x)(result)\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Make the class callable: tanh(x) instead of tanh.forward(x)\"\"\"\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61750d6a",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### 🧪 Test Your Tanh Implementation\n",
    "\n",
    "Once you implement the Tanh forward method above, run this cell to test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53f9db5",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-tanh-immediate",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_tanh_activation():\n",
    "    \"\"\"Unit test for the Tanh activation function.\"\"\"\n",
    "    print(\"🔬 Unit Test: Tanh Activation...\")\n",
    "\n",
    "# Create Tanh instance\n",
    "    tanh = Tanh()\n",
    "\n",
    "    # Test with zero (should be 0)\n",
    "    test_input = Tensor([[0]])\n",
    "    result = tanh(test_input)\n",
    "    expected = 0.0\n",
    "    \n",
    "    assert abs(result.data[0][0] - expected) < 1e-6, f\"Tanh(0) should be 0, got {result.data[0][0]}\"\n",
    "    \n",
    "    # Test with positive and negative values\n",
    "    test_input = Tensor([[-2, -1, 0, 1, 2]])\n",
    "    result = tanh(test_input)\n",
    "    \n",
    "    # Check that all values are between -1 and 1\n",
    "    assert np.all(result.data > -1), \"Tanh output should be > -1\"\n",
    "    assert np.all(result.data < 1), \"Tanh output should be < 1\"\n",
    "    \n",
    "    # Test antisymmetry: tanh(-x) = -tanh(x)\n",
    "    x_val = 1.5\n",
    "    pos_result = tanh(Tensor([[x_val]]))\n",
    "    neg_result = tanh(Tensor([[-x_val]]))\n",
    "    antisymmetry_check = abs(pos_result.data[0][0] + neg_result.data[0][0])\n",
    "    assert antisymmetry_check < 1e-6, \"Tanh should be antisymmetric\"\n",
    "    \n",
    "    # Test with 2D tensor\n",
    "    matrix_input = Tensor([[-1, 1], [0, 2]])\n",
    "    matrix_result = tanh(matrix_input)\n",
    "    assert matrix_result.shape == matrix_input.shape, \"Tanh should preserve shape\"\n",
    "    \n",
    "    # Test extreme values (should not overflow)\n",
    "    extreme_input = Tensor([[-100, 100]])\n",
    "    extreme_result = tanh(extreme_input)\n",
    "    assert not np.any(np.isnan(extreme_result.data)), \"Tanh should handle extreme values\"\n",
    "    assert not np.any(np.isinf(extreme_result.data)), \"Tanh should not produce inf values\"\n",
    "    \n",
    "    # Test that extreme values approach ±1\n",
    "    assert abs(extreme_result.data[0][0] - (-1)) < 1e-6, \"Tanh(-∞) should approach -1\"\n",
    "    assert abs(extreme_result.data[0][1] - 1) < 1e-6, \"Tanh(∞) should approach 1\"\n",
    "    \n",
    "    print(\"✅ Tanh activation tests passed!\")\n",
    "    print(f\"✅ Outputs correctly bounded between -1 and 1\")\n",
    "    print(f\"✅ Antisymmetric property verified\")\n",
    "    print(f\"✅ Zero-centered (tanh(0) = 0)\")\n",
    "    print(f\"✅ Handles extreme values correctly\")\n",
    "\n",
    "# Run the test\n",
    "test_unit_tanh_activation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccca9665",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 4: Softmax - Probability Distributions\n",
    "\n",
    "### What is Softmax?\n",
    "**Softmax** converts a vector of real numbers into a probability distribution:\n",
    "\n",
    "```\n",
    "f(x_i) = e^(x_i) / Σ(e^(x_j))\n",
    "```\n",
    "\n",
    "### Why Softmax is Essential\n",
    "1. **Probability distribution**: Outputs sum to 1\n",
    "2. **Multi-class classification**: Choose one class from many\n",
    "3. **Interpretable**: Each output is a probability\n",
    "4. **Differentiable**: Enables gradient-based learning\n",
    "\n",
    "### Visual Understanding\n",
    "```\n",
    "Input:  [1, 2, 3]\n",
    "Softmax:[0.09, 0.24, 0.67]  # Sums to 1.0\n",
    "```\n",
    "\n",
    "### Real-World Applications\n",
    "- **Classification**: Image classification, text classification\n",
    "- **Language models**: Next word prediction\n",
    "- **Attention mechanisms**: Where to focus attention\n",
    "- **Reinforcement learning**: Action selection probabilities\n",
    "\n",
    "### Mathematical Properties\n",
    "- **Range**: (0, 1) for each output\n",
    "- **Constraint**: Σ(f(x_i)) = 1\n",
    "- **Argmax preservation**: Doesn't change relative ordering\n",
    "- **Temperature scaling**: Can be made sharper or softer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e377a68b",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "softmax-class",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class Softmax:\n",
    "    \"\"\"\n",
    "    Softmax Activation Function: f(x_i) = e^(x_i) / Σ(e^(x_j))\n",
    "    \n",
    "    Converts a vector of real numbers into a probability distribution.\n",
    "    Essential for multi-class classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply Softmax activation: f(x_i) = e^(x_i) / Σ(e^(x_j))\n",
    "        \n",
    "        TODO: Implement Softmax activation function.\n",
    "        \n",
    "        STEP-BY-STEP IMPLEMENTATION:\n",
    "        1. Handle empty input case\n",
    "        2. Subtract max value for numerical stability: x - max(x)\n",
    "        3. Compute exponentials: np.exp(x - max(x))\n",
    "        4. Compute sum of exponentials: np.sum(exp_values)\n",
    "        5. Divide each exponential by the sum: exp_values / sum\n",
    "        6. Return as same tensor type as input\n",
    "        \n",
    "        EXAMPLE USAGE:\n",
    "        ```python\n",
    "        softmax = Softmax()\n",
    "        input_tensor = Tensor([[1, 2, 3]])\n",
    "        output = softmax(input_tensor)\n",
    "        print(output.data)  # [[0.09, 0.24, 0.67]]\n",
    "        print(np.sum(output.data))  # 1.0\n",
    "        ```\n",
    "        \n",
    "        IMPLEMENTATION HINTS:\n",
    "        - Handle empty case: if x.data.size == 0: return type(x)(x.data.copy())\n",
    "        - Subtract max for numerical stability: x_shifted = x.data - np.max(x.data, axis=-1, keepdims=True)\n",
    "        - Compute exponentials: exp_values = np.exp(x_shifted)\n",
    "        - Sum along last axis: sum_exp = np.sum(exp_values, axis=-1, keepdims=True)\n",
    "        - Divide: result = exp_values / sum_exp\n",
    "        - Return same type as input: return type(x)(result)\n",
    "        \n",
    "        LEARNING CONNECTIONS:\n",
    "        - This is like torch.nn.Softmax() in PyTorch\n",
    "        - Used in classification output layers\n",
    "        - Key component in attention mechanisms\n",
    "        - Enables probability-based decision making\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        # Handle empty input\n",
    "        if x.data.size == 0:\n",
    "            return type(x)(x.data.copy())\n",
    "        \n",
    "        # Subtract max for numerical stability\n",
    "        x_shifted = x.data - np.max(x.data, axis=-1, keepdims=True)\n",
    "        \n",
    "        # Compute exponentials\n",
    "        exp_values = np.exp(x_shifted)\n",
    "        \n",
    "        # Sum along last axis\n",
    "        sum_exp = np.sum(exp_values, axis=-1, keepdims=True)\n",
    "        \n",
    "        # Divide to get probabilities\n",
    "        result = exp_values / sum_exp\n",
    "        \n",
    "        return type(x)(result)\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"Make the class callable: softmax(x) instead of softmax.forward(x)\"\"\"\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304e285c",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### 🧪 Test Your Softmax Implementation\n",
    "\n",
    "Once you implement the Softmax forward method above, run this cell to test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fb8acb",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-softmax-immediate",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_softmax_activation():\n",
    "    \"\"\"Unit test for the Softmax activation function.\"\"\"\n",
    "    print(\"🔬 Unit Test: Softmax Activation...\")\n",
    "\n",
    "# Create Softmax instance\n",
    "    softmax = Softmax()\n",
    "\n",
    "    # Test with simple input\n",
    "    test_input = Tensor([[1, 2, 3]])\n",
    "    result = softmax(test_input)\n",
    "    \n",
    "    # Check that outputs sum to 1\n",
    "    output_sum = np.sum(result.data)\n",
    "    assert abs(output_sum - 1.0) < 1e-6, f\"Softmax outputs should sum to 1, got {output_sum}\"\n",
    "    \n",
    "    # Check that all outputs are positive\n",
    "    assert np.all(result.data > 0), \"Softmax outputs should be positive\"\n",
    "    assert np.all(result.data < 1), \"Softmax outputs should be less than 1\"\n",
    "    \n",
    "    # Test with uniform input (should give equal probabilities)\n",
    "    uniform_input = Tensor([[1, 1, 1]])\n",
    "    uniform_result = softmax(uniform_input)\n",
    "    expected_prob = 1.0 / 3.0\n",
    "    \n",
    "    for prob in uniform_result.data[0]:\n",
    "        assert abs(prob - expected_prob) < 1e-6, f\"Uniform input should give equal probabilities\"\n",
    "    \n",
    "    # Test with batch input (multiple samples)\n",
    "    batch_input = Tensor([[1, 2, 3], [4, 5, 6]])\n",
    "    batch_result = softmax(batch_input)\n",
    "    \n",
    "    # Check that each row sums to 1\n",
    "    for i in range(batch_input.shape[0]):\n",
    "        row_sum = np.sum(batch_result.data[i])\n",
    "        assert abs(row_sum - 1.0) < 1e-6, f\"Each row should sum to 1, row {i} sums to {row_sum}\"\n",
    "    \n",
    "    # Test numerical stability with large values\n",
    "    large_input = Tensor([[1000, 1001, 1002]])\n",
    "    large_result = softmax(large_input)\n",
    "    \n",
    "    assert not np.any(np.isnan(large_result.data)), \"Softmax should handle large values\"\n",
    "    assert not np.any(np.isinf(large_result.data)), \"Softmax should not produce inf values\"\n",
    "    \n",
    "    large_sum = np.sum(large_result.data)\n",
    "    assert abs(large_sum - 1.0) < 1e-6, \"Large values should still sum to 1\"\n",
    "\n",
    "# Test shape preservation\n",
    "    assert batch_result.shape == batch_input.shape, \"Softmax should preserve shape\"\n",
    "    \n",
    "    print(\"✅ Softmax activation tests passed!\")\n",
    "    print(f\"✅ Outputs sum to 1 (probability distribution)\")\n",
    "    print(f\"✅ All outputs are positive\")\n",
    "    print(f\"✅ Handles uniform inputs correctly\")\n",
    "    print(f\"✅ Works with batch inputs\")\n",
    "    print(f\"✅ Numerically stable with large values\")\n",
    "\n",
    "# Run the test\n",
    "test_unit_softmax_activation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8270fd",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## 🎯 Comprehensive Test: All Activations Working Together\n",
    "\n",
    "### Real-World Scenario\n",
    "Let's test how all activation functions work together in a realistic neural network scenario:\n",
    "\n",
    "- **Input processing**: Raw data transformation\n",
    "- **Hidden layers**: ReLU for internal processing\n",
    "- **Output layer**: Softmax for classification\n",
    "- **Comparison**: See how different activations transform the same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c666f3",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": true,
     "grade_id": "test-activations-comprehensive",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_activations_comprehensive():\n",
    "    \"\"\"Comprehensive unit test for all activation functions working together.\"\"\"\n",
    "    print(\"🔬 Unit Test: Activation Functions Comprehensive Test...\")\n",
    "    \n",
    "    # Create instances of all activation functions\n",
    "    relu = ReLU()\n",
    "    sigmoid = Sigmoid()\n",
    "    tanh = Tanh()\n",
    "    softmax = Softmax()\n",
    "    \n",
    "    # Test data: simulating neural network layer outputs\n",
    "    test_data = Tensor([[-2, -1, 0, 1, 2]])\n",
    "    \n",
    "    # Apply each activation function\n",
    "    relu_result = relu(test_data)\n",
    "    sigmoid_result = sigmoid(test_data)\n",
    "    tanh_result = tanh(test_data)\n",
    "    softmax_result = softmax(test_data)\n",
    "    \n",
    "    # Test that all functions preserve input shape\n",
    "    assert relu_result.shape == test_data.shape, \"ReLU should preserve shape\"\n",
    "    assert sigmoid_result.shape == test_data.shape, \"Sigmoid should preserve shape\"\n",
    "    assert tanh_result.shape == test_data.shape, \"Tanh should preserve shape\"\n",
    "    assert softmax_result.shape == test_data.shape, \"Softmax should preserve shape\"\n",
    "    \n",
    "    # Test that all functions return Tensor objects\n",
    "    assert isinstance(relu_result, Tensor), \"ReLU should return Tensor\"\n",
    "    assert isinstance(sigmoid_result, Tensor), \"Sigmoid should return Tensor\"\n",
    "    assert isinstance(tanh_result, Tensor), \"Tanh should return Tensor\"\n",
    "    assert isinstance(softmax_result, Tensor), \"Softmax should return Tensor\"\n",
    "    \n",
    "    # Test ReLU properties\n",
    "    assert np.all(relu_result.data >= 0), \"ReLU output should be non-negative\"\n",
    "    \n",
    "    # Test Sigmoid properties\n",
    "    assert np.all(sigmoid_result.data > 0), \"Sigmoid output should be positive\"\n",
    "    assert np.all(sigmoid_result.data < 1), \"Sigmoid output should be less than 1\"\n",
    "    \n",
    "    # Test Tanh properties\n",
    "    assert np.all(tanh_result.data > -1), \"Tanh output should be > -1\"\n",
    "    assert np.all(tanh_result.data < 1), \"Tanh output should be < 1\"\n",
    "    \n",
    "    # Test Softmax properties\n",
    "    softmax_sum = np.sum(softmax_result.data)\n",
    "    assert abs(softmax_sum - 1.0) < 1e-6, \"Softmax outputs should sum to 1\"\n",
    "    \n",
    "    # Test chaining activations (realistic neural network scenario)\n",
    "    # Hidden layer with ReLU\n",
    "    hidden_output = relu(test_data)\n",
    "    \n",
    "    # Add some weights simulation (element-wise multiplication)\n",
    "    weights = Tensor([[0.5, 0.3, 0.8, 0.2, 0.7]])\n",
    "    weighted_output = hidden_output * weights\n",
    "    \n",
    "    # Final layer with Softmax\n",
    "    final_output = softmax(weighted_output)\n",
    "    \n",
    "    # Test that chained operations work\n",
    "    assert isinstance(final_output, Tensor), \"Chained operations should return Tensor\"\n",
    "    assert abs(np.sum(final_output.data) - 1.0) < 1e-6, \"Final output should be valid probability\"\n",
    "    \n",
    "    # Test with batch data (multiple samples)\n",
    "    batch_data = Tensor([\n",
    "    [-2, -1, 0, 1, 2],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [-1, 0, 1, 2, 3]\n",
    "    ])\n",
    "    \n",
    "    batch_softmax = softmax(batch_data)\n",
    "    \n",
    "    # Each row should sum to 1\n",
    "    for i in range(batch_data.shape[0]):\n",
    "        row_sum = np.sum(batch_softmax.data[i])\n",
    "        assert abs(row_sum - 1.0) < 1e-6, f\"Batch row {i} should sum to 1\"\n",
    "    \n",
    "    print(\"✅ Activation functions comprehensive tests passed!\")\n",
    "    print(f\"✅ All functions work together seamlessly\")\n",
    "    print(f\"✅ Shape preservation across all activations\")\n",
    "    print(f\"✅ Chained operations work correctly\")\n",
    "    print(f\"✅ Batch processing works for all activations\")\n",
    "    print(f\"✅ Ready for neural network integration!\")\n",
    "\n",
    "# Run the comprehensive test\n",
    "test_unit_activations_comprehensive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d92e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_module_activation_tensor_integration():\n",
    "    \"\"\"\n",
    "    Integration test for activation functions with Tensor operations.\n",
    "    \n",
    "    Tests that activation functions properly integrate with the Tensor class\n",
    "    and maintain compatibility for neural network operations.\n",
    "    \"\"\"\n",
    "    print(\"🔬 Running Integration Test: Activation-Tensor Integration...\")\n",
    "    \n",
    "    # Test 1: Activation functions preserve Tensor types\n",
    "    input_tensor = Tensor([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "    \n",
    "    relu_fn = ReLU()\n",
    "    sigmoid_fn = Sigmoid()\n",
    "    tanh_fn = Tanh()\n",
    "    \n",
    "    relu_result = relu_fn(input_tensor)\n",
    "    sigmoid_result = sigmoid_fn(input_tensor) \n",
    "    tanh_result = tanh_fn(input_tensor)\n",
    "    \n",
    "    assert isinstance(relu_result, Tensor), \"ReLU should return Tensor\"\n",
    "    assert isinstance(sigmoid_result, Tensor), \"Sigmoid should return Tensor\"\n",
    "    assert isinstance(tanh_result, Tensor), \"Tanh should return Tensor\"\n",
    "    \n",
    "    # Test 2: Activations work with matrix Tensors (neural network layers)\n",
    "    layer_output = Tensor([[1.0, -2.0, 3.0], \n",
    "                          [-1.0, 2.0, -3.0]])  # Simulating dense layer output\n",
    "    \n",
    "    relu_fn = ReLU()\n",
    "    activated = relu_fn(layer_output)\n",
    "    expected = np.array([[1.0, 0.0, 3.0], \n",
    "                        [0.0, 2.0, 0.0]])\n",
    "    \n",
    "    assert isinstance(activated, Tensor), \"Matrix activation should return Tensor\"\n",
    "    assert np.array_equal(activated.data, expected), \"Matrix ReLU should work correctly\"\n",
    "    \n",
    "    # Test 3: Softmax with classification scenario\n",
    "    logits = Tensor([[2.0, 1.0, 0.1],  # Batch of 2 samples\n",
    "                    [1.0, 3.0, 0.2]])   # Each with 3 classes\n",
    "    \n",
    "    softmax_fn = Softmax()\n",
    "    probabilities = softmax_fn(logits)\n",
    "    \n",
    "    assert isinstance(probabilities, Tensor), \"Softmax should return Tensor\"\n",
    "    assert probabilities.shape == logits.shape, \"Softmax should preserve shape\"\n",
    "    \n",
    "    # Each row should sum to 1 (probability distribution)\n",
    "    for i in range(logits.shape[0]):\n",
    "        row_sum = np.sum(probabilities.data[i])\n",
    "        assert abs(row_sum - 1.0) < 1e-6, f\"Probability row {i} should sum to 1\"\n",
    "    \n",
    "    # Test 4: Chaining tensor operations with activations\n",
    "    x = Tensor([1.0, 2.0, 3.0])\n",
    "    y = Tensor([4.0, 5.0, 6.0])\n",
    "    \n",
    "    # Simulate: dense layer output -> activation -> more operations\n",
    "    dense_sim = x * y  # Element-wise multiplication (simulating dense layer)\n",
    "    relu_fn = ReLU()\n",
    "    activated = relu_fn(dense_sim)  # Apply activation\n",
    "    final = activated + Tensor([1.0, 1.0, 1.0])  # More tensor operations\n",
    "    \n",
    "    expected_final = np.array([5.0, 11.0, 19.0])  # [4,10,18] -> relu -> +1 = [5,11,19]\n",
    "    \n",
    "    assert isinstance(final, Tensor), \"Chained operations should maintain Tensor type\"\n",
    "    assert np.array_equal(final.data, expected_final), \"Chained operations should work correctly\"\n",
    "    \n",
    "    print(\"✅ Integration Test Passed: Activation-Tensor integration works correctly.\")\n",
    "\n",
    "# Run the integration test\n",
    "test_module_activation_tensor_integration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060254f8",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 🎯 MODULE SUMMARY: Activation Functions\n",
    "\n",
    "    Congratulations! You've successfully implemented all four essential activation functions:\n",
    "\n",
    "### ✅ What You've Built\n",
    "    - **ReLU**: The foundation of modern deep learning with sparsity and efficiency\n",
    "    - **Sigmoid**: Classic activation for binary classification and probability outputs\n",
    "    - **Tanh**: Zero-centered activation with better gradient properties\n",
    "    - **Softmax**: Probability distribution for multi-class classification\n",
    "\n",
    "### ✅ Key Learning Outcomes\n",
    "    - **Understanding**: Why nonlinearity is essential for neural networks\n",
    "    - **Implementation**: Built activation functions from scratch using NumPy\n",
    "    - **Testing**: Progressive validation with immediate feedback after each function\n",
    "    - **Integration**: Saw how activations work together in neural networks\n",
    "    - **Real-world context**: Understanding where each activation is used\n",
    "\n",
    "### ✅ Mathematical Mastery\n",
    "    - **ReLU**: f(x) = max(0, x) - Simple but powerful\n",
    "    - **Sigmoid**: f(x) = 1/(1 + e^(-x)) - Maps to (0,1)\n",
    "    - **Tanh**: f(x) = tanh(x) - Zero-centered, maps to (-1,1)\n",
    "    - **Softmax**: f(x_i) = e^(x_i)/Σ(e^(x_j)) - Probability distribution\n",
    "\n",
    "### ✅ Professional Skills Developed\n",
    "    - **Numerical stability**: Handling overflow and underflow\n",
    "    - **API design**: Consistent interfaces across all functions\n",
    "    - **Testing discipline**: Immediate validation after each implementation\n",
    "    - **Integration thinking**: Understanding how components work together\n",
    "\n",
    "### ✅ Ready for Next Steps\n",
    "    Your activation functions are now ready to power:\n",
    "    - **Dense layers**: Linear transformations with nonlinear activations\n",
    "    - **Convolutional layers**: Spatial feature extraction with ReLU\n",
    "    - **Network architectures**: Complete neural networks with proper activations\n",
    "    - **Training**: Gradient computation through activation functions\n",
    "\n",
    "### 🔗 Connection to Real ML Systems\n",
    "    Your implementations mirror production systems:\n",
    "    - **PyTorch**: `torch.nn.ReLU()`, `torch.nn.Sigmoid()`, `torch.nn.Tanh()`, `torch.nn.Softmax()`\n",
    "    - **TensorFlow**: `tf.nn.relu()`, `tf.nn.sigmoid()`, `tf.nn.tanh()`, `tf.nn.softmax()`\n",
    "    - **Industry applications**: Every major deep learning model uses these functions\n",
    "\n",
    "### 🎯 The Power of Nonlinearity\n",
    "    You've unlocked the key to deep learning:\n",
    "    - **Before**: Linear models limited to simple patterns\n",
    "    - **After**: Nonlinear models can learn any pattern (universal approximation)\n",
    "\n",
    "    **Next Module**: Layers - Building blocks that combine your tensors and activations into powerful transformations!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w1mf9zs9cv9",
   "source": "## 🤔 ML Systems Thinking: Reflection Questions\n\nNow that you've built the nonlinear functions that enable neural network intelligence, reflect on how these simple mathematical operations power the AI revolution:\n\n### System Design - How does this fit into larger systems?\n1. **The Nonlinearity Bottleneck**: Your implementations show how ReLU, Sigmoid, Tanh, and Softmax each solve different problems. When OpenAI designs GPT architectures, how do they decide which activation goes where? What happens when you choose the wrong activation for a specific layer?\n\n2. **Activation Memory**: Your ReLU creates sparse outputs (many zeros). How does this sparsity enable frameworks to optimize memory usage in massive models? Why might this be crucial when training models with billions of parameters?\n\n3. **Numerical Stability**: Your Softmax includes overflow protection with clipping. How do production systems handle numerical stability at scale? What happens when a single unstable activation cascades through a 100-layer network?\n\n### Production ML - How is this used in real ML workflows?\n4. **Hardware Acceleration**: Your simple element-wise operations translate to GPU kernels. How do cloud providers optimize activation function compute for millions of concurrent neural network inferences? Why might ReLU be preferred on certain hardware?\n\n5. **Model Serving Latency**: Your activation functions add computational overhead. When serving real-time recommendations or autonomous vehicle decisions, how do activation choices affect response times? Which activations are fastest and why?\n\n6. **Gradient Behavior**: Your functions enable backpropagation (coming in Module 7). In production training of large language models, how do activation choices affect training stability and convergence speed? Why did ReLU revolutionize deep learning?\n\n### Framework Design - Why do frameworks make certain choices?\n7. **API Consistency**: Your callable classes (`relu(x)`) mirror PyTorch's design. How does consistent activation APIs enable researchers to experiment rapidly with different nonlinearities? What would happen if each activation had a different interface?\n\n8. **Automatic Differentiation**: Your forward-only implementations will connect to gradient computation. How do frameworks like PyTorch automatically track operations through activations to compute gradients? Why is this harder than it looks?\n\n9. **Activation Fusion**: Your separate activation calls could be optimized. How do production frameworks combine linear layers with activations into single GPU operations for speed? What trade-offs does this optimization create?\n\n### Performance & Scale - What happens when systems get large?\n10. **Activation Bottlenecks**: Your functions process tensors sequentially. When training transformer models with billions of parameters, how do activation computations get parallelized across multiple GPUs? What new challenges emerge at scale?\n\n11. **Memory vs Computation**: Your implementations store all intermediate results. How do techniques like gradient checkpointing trade activation memory for recomputation time? When do these trade-offs become critical?\n\n12. **Dynamic Activations**: Your static functions are fixed at definition. How might learned activations (like Swish or GELU) that adapt during training change the computational requirements of large-scale systems?\n\n**💡 Systems Insight**: The activation functions you built are the \"biological neurons\" of artificial intelligence—each simple nonlinear transformation enables networks to learn complex patterns. Your ReLU implementation, despite being just `max(0, x)`, is literally computing in millions of deployed models right now, powering everything from photo recognition to language translation.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "jip1wuridj",
   "source": "## ⚡ ML Systems: Performance Analysis & Optimization\n\nNow that you have working activation functions, let's develop **performance engineering skills**. This section teaches you to measure computational costs, understand scaling patterns, and think about production optimization.\n\n### **Learning Outcome**: *\"I understand performance trade-offs between different activation functions\"*\n\n---\n\n## Performance Profiling Tools (Light Implementation)\n\nAs an ML systems engineer, you need to understand which activation functions are fast vs slow, and why. Let's build simple tools to measure and compare performance.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "owul1fsk5t",
   "source": "#| export\nimport time\n\nclass ActivationProfiler:\n    \"\"\"\n    Performance profiling toolkit for activation functions.\n    \n    Helps ML engineers understand computational costs and optimize\n    neural network performance for production deployment.\n    \"\"\"\n    \n    def __init__(self):\n        self.results = {}\n        \n    def time_activation(self, activation_fn, tensor, activation_name, iterations=100):\n        \"\"\"\n        Time how long an activation function takes to run.\n        \n        TODO: Implement activation timing.\n        \n        STEP-BY-STEP IMPLEMENTATION:\n        1. Record start time using time.time()\n        2. Run the activation function for specified iterations\n        3. Record end time\n        4. Calculate average time per iteration\n        5. Return the average time in milliseconds\n        \n        EXAMPLE:\n        profiler = ActivationProfiler()\n        relu = ReLU()\n        test_tensor = Tensor(np.random.randn(1000, 1000))\n        avg_time = profiler.time_activation(relu, test_tensor, \"ReLU\")\n        print(f\"ReLU took {avg_time:.3f} ms on average\")\n        \n        HINTS:\n        - Use time.time() for timing\n        - Run multiple iterations for better accuracy\n        - Calculate: (end_time - start_time) / iterations * 1000 for ms\n        - Return the average time per call in milliseconds\n        \"\"\"\n        ### BEGIN SOLUTION\n        start_time = time.time()\n        \n        for _ in range(iterations):\n            result = activation_fn(tensor)\n        \n        end_time = time.time()\n        avg_time_ms = (end_time - start_time) / iterations * 1000\n        \n        return avg_time_ms\n        ### END SOLUTION\n    \n    def compare_activations(self, tensor_size=(1000, 1000), iterations=50):\n        \"\"\"\n        Compare performance of all activation functions.\n        \n        This function is PROVIDED to show systems analysis.\n        Students run it to understand performance differences.\n        \"\"\"\n        print(f\"⚡ ACTIVATION PERFORMANCE COMPARISON\")\n        print(f\"=\" * 50)\n        print(f\"Tensor size: {tensor_size}, Iterations: {iterations}\")\n        \n        # Create test tensor\n        test_tensor = Tensor(np.random.randn(*tensor_size))\n        tensor_mb = test_tensor.data.nbytes / (1024 * 1024)\n        print(f\"Test tensor: {tensor_mb:.2f} MB\")\n        \n        # Test all activation functions\n        activations = {\n            'ReLU': ReLU(),\n            'Sigmoid': Sigmoid(),\n            'Tanh': Tanh(),\n            'Softmax': Softmax()\n        }\n        \n        results = {}\n        for name, activation_fn in activations.items():\n            avg_time = self.time_activation(activation_fn, test_tensor, name, iterations)\n            results[name] = avg_time\n            print(f\"   {name:8}: {avg_time:.3f} ms\")\n        \n        # Calculate speed ratios relative to fastest\n        fastest_time = min(results.values())\n        fastest_name = min(results, key=results.get)\n        \n        print(f\"\\n📊 SPEED ANALYSIS:\")\n        for name, time_ms in sorted(results.items(), key=lambda x: x[1]):\n            speed_ratio = time_ms / fastest_time\n            if name == fastest_name:\n                print(f\"   {name:8}: {speed_ratio:.1f}x (fastest)\")\n            else:\n                print(f\"   {name:8}: {speed_ratio:.1f}x slower than {fastest_name}\")\n        \n        return results\n    \n    def analyze_scaling(self, activation_fn, activation_name, sizes=[100, 500, 1000]):\n        \"\"\"\n        Analyze how activation performance scales with tensor size.\n        \n        This function is PROVIDED to demonstrate scaling patterns.\n        Students use it to understand computational complexity.\n        \"\"\"\n        print(f\"\\n🔍 SCALING ANALYSIS: {activation_name}\")\n        print(f\"=\" * 40)\n        \n        scaling_results = []\n        \n        for size in sizes:\n            test_tensor = Tensor(np.random.randn(size, size))\n            avg_time = self.time_activation(activation_fn, test_tensor, activation_name, iterations=20)\n            \n            elements = size * size\n            time_per_element = avg_time / elements * 1e6  # microseconds per element\n            \n            result = {\n                'size': size,\n                'elements': elements,\n                'time_ms': avg_time,\n                'time_per_element_us': time_per_element\n            }\n            scaling_results.append(result)\n            \n            print(f\"   {size}x{size}: {avg_time:.3f}ms ({time_per_element:.3f}μs/element)\")\n        \n        # Analyze scaling pattern\n        if len(scaling_results) >= 2:\n            small = scaling_results[0]\n            large = scaling_results[-1]\n            \n            size_ratio = large['size'] / small['size']\n            time_ratio = large['time_ms'] / small['time_ms']\n            \n            print(f\"\\n📈 Scaling Pattern:\")\n            print(f\"   Size increased {size_ratio:.1f}x ({small['size']} → {large['size']})\")\n            print(f\"   Time increased {time_ratio:.1f}x\")\n            \n            if abs(time_ratio - size_ratio**2) < abs(time_ratio - size_ratio):\n                print(f\"   Pattern: O(n²) - linear in tensor size\")\n            else:\n                print(f\"   Pattern: ~O(n) - very efficient scaling\")\n        \n        return scaling_results\n\ndef benchmark_activation_suite():\n    \"\"\"\n    Comprehensive benchmark of all activation functions.\n    \n    This function is PROVIDED to show complete systems analysis.\n    Students run it to understand production performance implications.\n    \"\"\"\n    profiler = ActivationProfiler()\n    \n    print(\"🏆 COMPREHENSIVE ACTIVATION BENCHMARK\")\n    print(\"=\" * 60)\n    \n    # Test 1: Performance comparison\n    comparison_results = profiler.compare_activations(tensor_size=(800, 800), iterations=30)\n    \n    # Test 2: Scaling analysis for each activation\n    activations_to_test = [\n        (ReLU(), \"ReLU\"),\n        (Sigmoid(), \"Sigmoid\"),\n        (Tanh(), \"Tanh\")\n    ]\n    \n    for activation_fn, name in activations_to_test:\n        profiler.analyze_scaling(activation_fn, name, sizes=[200, 400, 600])\n    \n    # Test 3: Memory vs Performance trade-offs\n    print(f\"\\n💾 MEMORY vs PERFORMANCE ANALYSIS:\")\n    print(f\"=\" * 40)\n    \n    test_tensor = Tensor(np.random.randn(500, 500))\n    original_memory = test_tensor.data.nbytes / (1024 * 1024)\n    \n    for name, activation_fn in [(\"ReLU\", ReLU()), (\"Sigmoid\", Sigmoid())]:\n        start_time = time.time()\n        result = activation_fn(test_tensor)\n        end_time = time.time()\n        \n        result_memory = result.data.nbytes / (1024 * 1024)\n        time_ms = (end_time - start_time) * 1000\n        \n        print(f\"   {name}:\")\n        print(f\"     Input: {original_memory:.2f} MB\")\n        print(f\"     Output: {result_memory:.2f} MB\")\n        print(f\"     Memory overhead: {result_memory - original_memory:.2f} MB\")\n        print(f\"     Time: {time_ms:.3f} ms\")\n    \n    print(f\"\\n🎯 PRODUCTION INSIGHTS:\")\n    print(f\"   - ReLU is typically fastest (simple max operation)\")\n    print(f\"   - Sigmoid/Tanh slower due to exponential calculations\")\n    print(f\"   - All operations scale linearly with tensor size\")\n    print(f\"   - Memory usage doubles (input + output tensors)\")\n    print(f\"   - Choose activation based on accuracy vs speed trade-offs\")\n    \n    return comparison_results",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "t73cbjva7h",
   "source": "### 🎯 Learning Activity 1: Activation Timing Practice (Light Implementation)\n\n**Goal**: Learn to measure activation function performance and understand which operations are fast vs slow.\n\nComplete the `time_activation` method in the `ActivationProfiler` class above, then use it to compare activations.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "sf8amtccvqp",
   "source": "# Initialize the activation profiler\nprofiler = ActivationProfiler()\n\nprint(\"⚡ ACTIVATION PERFORMANCE ANALYSIS\")\nprint(\"=\" * 50)\n\n# Create test data\ntest_tensor = Tensor(np.random.randn(500, 500))  # Medium-sized tensor for testing\nprint(f\"Test tensor size: {test_tensor.shape}\")\nprint(f\"Memory footprint: {test_tensor.data.nbytes/(1024*1024):.2f} MB\")\n\n# Test individual activation timing\nprint(f\"\\n🎯 Individual Activation Timing:\")\nactivations_to_test = [\n    (ReLU(), \"ReLU\"),\n    (Sigmoid(), \"Sigmoid\"), \n    (Tanh(), \"Tanh\"),\n    (Softmax(), \"Softmax\")\n]\n\nindividual_results = {}\nfor activation_fn, name in activations_to_test:\n    # Students implement this timing call\n    avg_time = profiler.time_activation(activation_fn, test_tensor, name, iterations=50)\n    individual_results[name] = avg_time\n    print(f\"   {name:8}: {avg_time:.3f} ms average\")\n\n# Analyze the results  \nfastest = min(individual_results, key=individual_results.get)\nslowest = max(individual_results, key=individual_results.get)\nspeed_ratio = individual_results[slowest] / individual_results[fastest]\n\nprint(f\"\\n📊 PERFORMANCE INSIGHTS:\")\nprint(f\"   Fastest: {fastest} ({individual_results[fastest]:.3f} ms)\")\nprint(f\"   Slowest: {slowest} ({individual_results[slowest]:.3f} ms)\")\nprint(f\"   Speed difference: {speed_ratio:.1f}x\")\n\nprint(f\"\\n💡 WHY THE DIFFERENCE?\")\nprint(f\"   - ReLU: Just max(0, x) - simple comparison\")\nprint(f\"   - Sigmoid: Requires exponential calculation\")\nprint(f\"   - Tanh: Also exponential, but often optimized\")\nprint(f\"   - Softmax: Exponentials + division\")\n\nprint(f\"\\n🏭 PRODUCTION IMPLICATIONS:\")\nprint(f\"   - ReLU dominates modern deep learning (speed + effectiveness)\")\nprint(f\"   - Sigmoid/Tanh used where probability interpretation needed\")\nprint(f\"   - Speed matters: 1000 layers × speed difference = major impact\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "eig5k6g9wc",
   "source": "### 🎯 Learning Activity 2: Comprehensive Benchmarking (Review & Understand)\n\n**Goal**: Run a complete benchmark suite to understand production-level performance patterns and scaling behavior.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "o4di8zn6z0j",
   "source": "# Run comprehensive activation benchmarking\nbenchmark_results = benchmark_activation_suite()\n\nprint(f\"\\n🎯 KEY LEARNINGS FOR ML SYSTEMS ENGINEERS:\")\nprint(f\"=\" * 55)\n\nprint(f\"\\n1. 🚀 SPEED HIERARCHY:\")\nprint(f\"   ReLU → Tanh → Sigmoid → Softmax (typical ranking)\")\nprint(f\"   Why: Mathematical complexity determines speed\")\n\nprint(f\"\\n2. 📈 SCALING BEHAVIOR:\")\nprint(f\"   All activations scale O(n²) with tensor size\")\nprint(f\"   Linear in number of elements (good scalability)\")\n\nprint(f\"\\n3. 💾 MEMORY PATTERNS:\")\nprint(f\"   Each activation doubles memory (input + output)\")\nprint(f\"   Important for large models with limited GPU memory\")\n\nprint(f\"\\n4. 🏭 PRODUCTION CHOICES:\")\nprint(f\"   Hidden layers: ReLU (speed + gradient flow)\")\nprint(f\"   Binary output: Sigmoid (probability interpretation)\")\nprint(f\"   Multi-class: Softmax (probability distribution)\")\nprint(f\"   Special cases: Tanh (zero-centered outputs)\")\n\nprint(f\"\\n5. ⚖️ OPTIMIZATION TRADE-OFFS:\")\nprint(f\"   Accuracy vs Speed: Sometimes slower activations work better\")\nprint(f\"   Memory vs Compute: Could recompute instead of storing\")\nprint(f\"   Hardware specific: GPU vs CPU performance differs\")\n\nprint(f\"\\n💡 SYSTEMS ENGINEERING INSIGHT:\")\nprint(f\"These micro-optimizations matter at scale:\")\nprint(f\"- 1ms difference × 1M operations = 16 minutes\")\nprint(f\"- GPU utilization depends on operation efficiency\")\nprint(f\"- Memory bandwidth often more limiting than compute\")\nprint(f\"- Choice of activation affects entire training pipeline\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9db927a6",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### 🧪 Integration Test: Tensor → Activations Workflow\n",
    "\n",
    "This comprehensive test validates that your tensor and activation implementations work together seamlessly, simulating a realistic neural network forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b649dad",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "integration-test-tensor-activations",
     "locked": true,
     "points": 25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_tensor_activations_integration():\n",
    "    \"\"\"\n",
    "    Integration test validating end-to-end tensor + activations workflow.\n",
    "    \n",
    "    Simulates a realistic neural network scenario:\n",
    "    1. Create input data (Tensor)\n",
    "    2. Apply linear transformation (simulated weight multiplication)\n",
    "    3. Apply each activation function\n",
    "    4. Verify mathematical properties and integration\n",
    "    \"\"\"\n",
    "    print(\"🔬 Integration Test: Tensor ↔ Activations Integration...\")\n",
    "    \n",
    "    # Simulate realistic neural network data\n",
    "    # Batch of 3 samples, each with 4 features (mini-batch processing)\n",
    "    raw_input = Tensor([\n",
    "        [-2.5, -1.0,  0.0,  1.5],  # Sample 1: mixed positive/negative\n",
    "        [ 3.2,  0.5, -0.8,  2.1],  # Sample 2: mostly positive\n",
    "        [-1.8,  2.3, -3.1,  0.7]   # Sample 3: mixed with extreme values\n",
    "    ])\n",
    "    \n",
    "    print(f\"📊 Input shape: {raw_input.shape}\")\n",
    "    print(f\"📊 Input data:\\n{raw_input.data}\")\n",
    "    \n",
    "    # Simulate weight matrix (4 input features → 3 hidden units)\n",
    "    weights = Tensor([\n",
    "        [ 0.5, -0.3,  0.8],  # Feature 1 weights to 3 hidden units\n",
    "        [-0.2,  0.7,  0.1],  # Feature 2 weights  \n",
    "        [ 0.9, -0.5,  0.4],  # Feature 3 weights\n",
    "        [ 0.3,  0.8, -0.6]   # Feature 4 weights\n",
    "    ])\n",
    "    \n",
    "    # Simulate matrix multiplication (input @ weights)\n",
    "    # In real neural networks, this would be: output = input @ weights + bias\n",
    "    hidden_pre_activation = raw_input @ weights.data.T  # Transpose for correct dimensions\n",
    "    hidden_pre_activation = Tensor(hidden_pre_activation)\n",
    "    \n",
    "    print(f\"📊 Pre-activation shape: {hidden_pre_activation.shape}\")\n",
    "    print(f\"📊 Pre-activation values:\\n{hidden_pre_activation.data}\")\n",
    "    \n",
    "    # Test each activation function on the realistic data\n",
    "    relu = ReLU()\n",
    "    sigmoid = Sigmoid() \n",
    "    tanh = Tanh()\n",
    "    softmax = Softmax()\n",
    "    \n",
    "    # Apply all activations\n",
    "    relu_output = relu(hidden_pre_activation)\n",
    "    sigmoid_output = sigmoid(hidden_pre_activation)  \n",
    "    tanh_output = tanh(hidden_pre_activation)\n",
    "    softmax_output = softmax(hidden_pre_activation)\n",
    "    \n",
    "    print(\"\\n🔍 Testing activation outputs...\")\n",
    "    \n",
    "    # Validate shapes are preserved\n",
    "    assert relu_output.shape == hidden_pre_activation.shape, \"ReLU should preserve shape\"\n",
    "    assert sigmoid_output.shape == hidden_pre_activation.shape, \"Sigmoid should preserve shape\"\n",
    "    assert tanh_output.shape == hidden_pre_activation.shape, \"Tanh should preserve shape\"\n",
    "    assert softmax_output.shape == hidden_pre_activation.shape, \"Softmax should preserve shape\"\n",
    "    print(\"✅ Shape preservation: All activations maintain input dimensions\")\n",
    "    \n",
    "    # Validate mathematical properties\n",
    "    # ReLU properties\n",
    "    assert np.all(relu_output.data >= 0), \"ReLU outputs must be non-negative\"\n",
    "    sparsity = np.count_nonzero(relu_output.data == 0) / relu_output.size\n",
    "    print(f\"✅ ReLU sparsity: {sparsity*100:.1f}% zeros (good for efficiency)\")\n",
    "    \n",
    "    # Sigmoid properties  \n",
    "    assert np.all(sigmoid_output.data > 0), \"Sigmoid outputs must be positive\"\n",
    "    assert np.all(sigmoid_output.data < 1), \"Sigmoid outputs must be less than 1\"\n",
    "    sigmoid_range = [np.min(sigmoid_output.data), np.max(sigmoid_output.data)]\n",
    "    print(f\"✅ Sigmoid range: [{sigmoid_range[0]:.3f}, {sigmoid_range[1]:.3f}] ∈ (0,1)\")\n",
    "    \n",
    "    # Tanh properties\n",
    "    assert np.all(tanh_output.data > -1), \"Tanh outputs must be greater than -1\"  \n",
    "    assert np.all(tanh_output.data < 1), \"Tanh outputs must be less than 1\"\n",
    "    tanh_range = [np.min(tanh_output.data), np.max(tanh_output.data)]\n",
    "    print(f\"✅ Tanh range: [{tanh_range[0]:.3f}, {tanh_range[1]:.3f}] ∈ (-1,1)\")\n",
    "    \n",
    "    # Softmax properties (most important for multi-class classification)\n",
    "    for i in range(softmax_output.shape[0]):  # Check each sample\n",
    "        sample_sum = np.sum(softmax_output.data[i])\n",
    "        assert abs(sample_sum - 1.0) < 1e-6, f\"Softmax row {i} should sum to 1, got {sample_sum}\"\n",
    "    print(\"✅ Softmax probability: Each row sums to 1.0 (valid probability distribution)\")\n",
    "    \n",
    "    # Test activation chaining (realistic neural network scenario)\n",
    "    print(\"\\n🔗 Testing activation chaining (hidden → output layers)...\")\n",
    "    \n",
    "    # Hidden layer: ReLU activation (common choice)\n",
    "    hidden_output = relu(hidden_pre_activation)\n",
    "    \n",
    "    # Simulate output layer weights (3 hidden → 2 output classes)\n",
    "    output_weights = Tensor([\n",
    "        [0.6, -0.4],  # Hidden unit 1 → [class 0, class 1]\n",
    "        [-0.3, 0.8],  # Hidden unit 2 → [class 0, class 1]  \n",
    "        [0.5, 0.2]    # Hidden unit 3 → [class 0, class 1]\n",
    "    ])\n",
    "    \n",
    "    # Output layer pre-activation\n",
    "    output_pre_activation = hidden_output @ output_weights.data\n",
    "    output_pre_activation = Tensor(output_pre_activation) \n",
    "    \n",
    "    # Output layer: Softmax for classification\n",
    "    final_output = softmax(output_pre_activation)\n",
    "    \n",
    "    print(f\"📊 Final classification output shape: {final_output.shape}\")\n",
    "    print(f\"📊 Final probabilities:\\n{final_output.data}\")\n",
    "    \n",
    "    # Validate final output properties\n",
    "    assert final_output.shape == (3, 2), \"Should have 3 samples × 2 classes\"\n",
    "    for i in range(3):\n",
    "        sample_probs = final_output.data[i]\n",
    "        assert abs(np.sum(sample_probs) - 1.0) < 1e-6, f\"Sample {i} probabilities should sum to 1\"\n",
    "        assert np.all(sample_probs > 0), f\"Sample {i} should have positive probabilities\"\n",
    "        predicted_class = np.argmax(sample_probs)\n",
    "        confidence = np.max(sample_probs)\n",
    "        print(f\"✅ Sample {i+1}: Class {predicted_class}, Confidence {confidence:.3f}\")\n",
    "    \n",
    "    # Test tensor arithmetic integration\n",
    "    print(\"\\n🔢 Testing tensor arithmetic with activations...\")\n",
    "    \n",
    "    # Element-wise operations should work seamlessly\n",
    "    combined_output = relu_output + sigmoid_output * 0.5\n",
    "    assert isinstance(combined_output, Tensor), \"Arithmetic should return Tensor\"\n",
    "    assert combined_output.shape == relu_output.shape, \"Arithmetic should preserve shape\"\n",
    "    print(\"✅ Tensor arithmetic integration: Addition and multiplication work\")\n",
    "    \n",
    "    # Test broadcasting with activations\n",
    "    bias = Tensor([0.1, -0.05, 0.2])  # Shape: (3,)\n",
    "    biased_output = sigmoid_output + bias  # Should broadcast\n",
    "    assert biased_output.shape == sigmoid_output.shape, \"Broadcasting should work with activations\"\n",
    "    print(\"✅ Broadcasting integration: Bias addition works\")\n",
    "    \n",
    "    print(\"\\n🎯 Integration Test Results:\")\n",
    "    print(\"✅ Tensor creation and manipulation\")\n",
    "    print(\"✅ Matrix operations (simulated linear layers)\")  \n",
    "    print(\"✅ All activation functions working correctly\")\n",
    "    print(\"✅ Mathematical properties validated\")\n",
    "    print(\"✅ Realistic neural network forward pass\")\n",
    "    print(\"✅ Activation chaining (hidden → output)\")\n",
    "    print(\"✅ Tensor arithmetic with activations\")\n",
    "    print(\"✅ Broadcasting compatibility\")\n",
    "    \n",
    "    print(\"\\n🚀 Ready for real neural networks! Your tensor and activation implementations\")\n",
    "    print(\"   can handle the computational demands of modern deep learning.\")\n",
    "\n",
    "# Run the comprehensive integration test\n",
    "test_tensor_activations_integration()\n",
    "\n",
    "    Your activation functions are the key to neural network intelligence. Now let's build the layers that use them!\n",
    "\"\"\" "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}