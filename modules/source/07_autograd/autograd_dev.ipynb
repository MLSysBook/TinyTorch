{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "745daee0",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Module 7: Autograd - Automatic Differentiation Engine\n",
    "\n",
    "Welcome to the Autograd module! This is where TinyTorch becomes truly powerful. You'll implement the automatic differentiation engine that makes neural network training possible.\n",
    "\n",
    "## Learning Goals\n",
    "- Understand how automatic differentiation works through computational graphs\n",
    "- Implement the Variable class that tracks gradients and operations\n",
    "- Build backward propagation for gradient computation\n",
    "- Create the foundation for neural network training\n",
    "- Master the mathematical concepts behind backpropagation\n",
    "\n",
    "## Build → Use → Analyze\n",
    "1. **Build**: Create the Variable class and gradient computation system\n",
    "2. **Use**: Perform automatic differentiation on complex expressions\n",
    "3. **Analyze**: Understand how gradients flow through computational graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9276c0",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "autograd-imports",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| default_exp core.autograd\n",
    "\n",
    "#| export\n",
    "import numpy as np\n",
    "import sys\n",
    "from typing import Union, List, Tuple, Optional, Any, Callable\n",
    "from collections import defaultdict\n",
    "\n",
    "# Import our existing components\n",
    "from tinytorch.core.tensor import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7523f8e9",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "autograd-setup",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"🔥 TinyTorch Autograd Module\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Python version: {sys.version_info.major}.{sys.version_info.minor}\")\n",
    "print(\"Ready to build automatic differentiation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e699daf9",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 📦 Where This Code Lives in the Final Package\n",
    "\n",
    "**Learning Side:** You work in `modules/source/07_autograd/autograd_dev.py`  \n",
    "**Building Side:** Code exports to `tinytorch.core.autograd`\n",
    "\n",
    "```python\n",
    "# Final package structure:\n",
    "from tinytorch.core.autograd import Variable, backward  # The gradient engine!\n",
    "from tinytorch.core.tensor import Tensor\n",
    "from tinytorch.core.activations import ReLU, Sigmoid, Tanh\n",
    "```\n",
    "\n",
    "**Why this matters:**\n",
    "- **Learning:** Focused module for understanding gradients\n",
    "- **Production:** Proper organization like PyTorch's `torch.autograd`\n",
    "- **Consistency:** All gradient operations live together in `core.autograd`\n",
    "- **Foundation:** Enables training for all neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574c94bc",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 1: What is Automatic Differentiation?\n",
    "\n",
    "### Definition\n",
    "**Automatic differentiation (autograd)** is a technique that automatically computes derivatives of functions represented as computational graphs. It's the magic that makes neural network training possible.\n",
    "\n",
    "### The Fundamental Challenge: Computing Gradients at Scale\n",
    "\n",
    "#### **The Problem**\n",
    "Neural networks have millions or billions of parameters. To train them, we need to compute the gradient of the loss function with respect to every single parameter:\n",
    "\n",
    "```python\n",
    "# For a neural network with parameters θ = [w1, w2, ..., wn, b1, b2, ..., bm]\n",
    "# We need to compute: ∇θ L = [∂L/∂w1, ∂L/∂w2, ..., ∂L/∂wn, ∂L/∂b1, ∂L/∂b2, ..., ∂L/∂bm]\n",
    "```\n",
    "\n",
    "#### **Why Manual Differentiation Fails**\n",
    "- **Complexity**: Neural networks are compositions of thousands of operations\n",
    "- **Error-prone**: Manual computation is extremely difficult and error-prone\n",
    "- **Inflexible**: Every architecture change requires re-deriving gradients\n",
    "- **Inefficient**: Manual computation doesn't exploit computational structure\n",
    "\n",
    "#### **Why Numerical Differentiation is Inadequate**\n",
    "```python\n",
    "# Numerical differentiation: f'(x) ≈ (f(x + h) - f(x)) / h\n",
    "def numerical_gradient(f, x, h=1e-5):\n",
    "    return (f(x + h) - f(x)) / h\n",
    "```\n",
    "\n",
    "Problems:\n",
    "- **Slow**: Requires 2 function evaluations per parameter\n",
    "- **Imprecise**: Numerical errors accumulate\n",
    "- **Unstable**: Sensitive to choice of h\n",
    "- **Expensive**: O(n) cost for n parameters\n",
    "\n",
    "### The Solution: Computational Graphs\n",
    "\n",
    "#### **Key Insight: Every Computation is a Graph**\n",
    "Any mathematical expression can be represented as a directed acyclic graph (DAG):\n",
    "\n",
    "```python\n",
    "# Expression: f(x, y) = (x + y) * (x - y)\n",
    "# Graph representation:\n",
    "#     x ──┐     ┌── add ──┐\n",
    "#         │     │         │\n",
    "#         ├─────┤         ├── multiply ── output\n",
    "#         │     │         │\n",
    "#     y ──┘     └── sub ──┘\n",
    "```\n",
    "\n",
    "#### **Forward Pass: Computing Values**\n",
    "Traverse the graph from inputs to outputs, computing values at each node:\n",
    "\n",
    "```python\n",
    "# Forward pass for f(x, y) = (x + y) * (x - y)\n",
    "x = 3, y = 2\n",
    "add_result = x + y = 5\n",
    "sub_result = x - y = 1\n",
    "output = add_result * sub_result = 5\n",
    "```\n",
    "\n",
    "#### **Backward Pass: Computing Gradients**\n",
    "Traverse the graph from outputs to inputs, computing gradients using the chain rule:\n",
    "\n",
    "```python\n",
    "# Backward pass for f(x, y) = (x + y) * (x - y)\n",
    "# Starting from output gradient = 1\n",
    "∂output/∂multiply = 1\n",
    "∂output/∂add = ∂output/∂multiply * ∂multiply/∂add = 1 * sub_result = 1\n",
    "∂output/∂sub = ∂output/∂multiply * ∂multiply/∂sub = 1 * add_result = 5\n",
    "∂output/∂x = ∂output/∂add * ∂add/∂x + ∂output/∂sub * ∂sub/∂x = 1 * 1 + 5 * 1 = 6\n",
    "∂output/∂y = ∂output/∂add * ∂add/∂y + ∂output/∂sub * ∂sub/∂y = 1 * 1 + 5 * (-1) = -4\n",
    "```\n",
    "\n",
    "### Mathematical Foundation: The Chain Rule\n",
    "\n",
    "#### **Single Variable Chain Rule**\n",
    "For composite functions: If z = f(g(x)), then:\n",
    "```\n",
    "dz/dx = (dz/df) * (df/dx)\n",
    "```\n",
    "\n",
    "#### **Multivariable Chain Rule**\n",
    "For functions of multiple variables: If z = f(x, y) where x = g(t) and y = h(t), then:\n",
    "```\n",
    "dz/dt = (∂z/∂x) * (dx/dt) + (∂z/∂y) * (dy/dt)\n",
    "```\n",
    "\n",
    "#### **Chain Rule in Computational Graphs**\n",
    "For any path from input to output through intermediate nodes:\n",
    "```\n",
    "∂output/∂input = ∏(∂node_{i+1}/∂node_i) for all nodes in the path\n",
    "```\n",
    "\n",
    "### Automatic Differentiation Modes\n",
    "\n",
    "#### **Forward Mode (Forward Accumulation)**\n",
    "- **Process**: Compute derivatives alongside forward pass\n",
    "- **Efficiency**: Efficient when #inputs << #outputs\n",
    "- **Use case**: Jacobian-vector products, sensitivity analysis\n",
    "\n",
    "#### **Reverse Mode (Backpropagation)**\n",
    "- **Process**: Compute derivatives in reverse pass after forward pass\n",
    "- **Efficiency**: Efficient when #outputs << #inputs\n",
    "- **Use case**: Neural network training (many parameters, few outputs)\n",
    "\n",
    "#### **Why Reverse Mode Dominates ML**\n",
    "Neural networks typically have:\n",
    "- **Many inputs**: Millions of parameters\n",
    "- **Few outputs**: Single loss value or small output vector\n",
    "- **Reverse mode**: O(1) cost per parameter vs O(n) for forward mode\n",
    "\n",
    "### The Computational Graph Abstraction\n",
    "\n",
    "#### **Nodes: Operations and Variables**\n",
    "- **Variable nodes**: Store values and gradients\n",
    "- **Operation nodes**: Define how to compute forward and backward passes\n",
    "\n",
    "#### **Edges: Data Dependencies**\n",
    "- **Forward edges**: Data flow from inputs to outputs\n",
    "- **Backward edges**: Gradient flow from outputs to inputs\n",
    "\n",
    "#### **Dynamic vs Static Graphs**\n",
    "- **Static graphs**: Define once, execute many times (TensorFlow 1.x)\n",
    "- **Dynamic graphs**: Build graph during execution (PyTorch, TensorFlow 2.x)\n",
    "\n",
    "### Real-World Impact: What Autograd Enables\n",
    "\n",
    "#### **Deep Learning Revolution**\n",
    "```python\n",
    "# Before autograd: Manual gradient computation\n",
    "def manual_gradient(x, y, w1, w2, b1, b2):\n",
    "    # Forward pass\n",
    "    z1 = w1 * x + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = w2 * a1 + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    loss = (a2 - y) ** 2\n",
    "    \n",
    "    # Backward pass (manual)\n",
    "    dloss_da2 = 2 * (a2 - y)\n",
    "    da2_dz2 = sigmoid_derivative(z2)\n",
    "    dz2_dw2 = a1\n",
    "    dz2_db2 = 1\n",
    "    dz2_da1 = w2\n",
    "    da1_dz1 = sigmoid_derivative(z1)\n",
    "    dz1_dw1 = x\n",
    "    dz1_db1 = 1\n",
    "    \n",
    "    # Chain rule application\n",
    "    dloss_dw2 = dloss_da2 * da2_dz2 * dz2_dw2\n",
    "    dloss_db2 = dloss_da2 * da2_dz2 * dz2_db2\n",
    "    dloss_dw1 = dloss_da2 * da2_dz2 * dz2_da1 * da1_dz1 * dz1_dw1\n",
    "    dloss_db1 = dloss_da2 * da2_dz2 * dz2_da1 * da1_dz1 * dz1_db1\n",
    "    \n",
    "    return dloss_dw1, dloss_db1, dloss_dw2, dloss_db2\n",
    "\n",
    "# With autograd: Automatic gradient computation\n",
    "def autograd_gradient(x, y, w1, w2, b1, b2):\n",
    "    # Forward pass with gradient tracking\n",
    "    z1 = w1 * x + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = w2 * a1 + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    loss = (a2 - y) ** 2\n",
    "    \n",
    "    # Backward pass (automatic)\n",
    "    loss.backward()\n",
    "    \n",
    "    return w1.grad, b1.grad, w2.grad, b2.grad\n",
    "```\n",
    "\n",
    "#### **Scientific Computing**\n",
    "- **Optimization**: Gradient-based optimization algorithms\n",
    "- **Inverse problems**: Parameter estimation from observations\n",
    "- **Sensitivity analysis**: How outputs change with input perturbations\n",
    "\n",
    "#### **Modern AI Applications**\n",
    "- **Neural architecture search**: Differentiable architecture optimization\n",
    "- **Meta-learning**: Learning to learn with gradient-based meta-algorithms\n",
    "- **Differentiable programming**: Entire programs as differentiable functions\n",
    "\n",
    "### Performance Considerations\n",
    "\n",
    "#### **Memory Management**\n",
    "- **Intermediate storage**: Must store forward pass results for backward pass\n",
    "- **Memory optimization**: Checkpointing, gradient accumulation\n",
    "- **Trade-offs**: Memory vs computation time\n",
    "\n",
    "#### **Computational Efficiency**\n",
    "- **Graph optimization**: Fuse operations, eliminate redundancy\n",
    "- **Parallelization**: Compute independent gradients simultaneously\n",
    "- **Hardware acceleration**: Specialized gradient computation on GPUs/TPUs\n",
    "\n",
    "#### **Numerical Stability**\n",
    "- **Gradient clipping**: Prevent exploding gradients\n",
    "- **Numerical precision**: Balance between float16 and float32\n",
    "- **Accumulation order**: Minimize numerical errors\n",
    "\n",
    "### Connection to Neural Network Training\n",
    "\n",
    "#### **The Training Loop**\n",
    "```python\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        # Forward pass\n",
    "        predictions = model(batch.inputs)\n",
    "        loss = criterion(predictions, batch.targets)\n",
    "        \n",
    "        # Backward pass (autograd)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Parameter update\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "```\n",
    "\n",
    "#### **Gradient-Based Optimization**\n",
    "- **Stochastic Gradient Descent**: Use gradients to update parameters\n",
    "- **Adaptive methods**: Adam, RMSprop use gradient statistics\n",
    "- **Second-order methods**: Use gradient and Hessian information\n",
    "\n",
    "### Why Autograd is Revolutionary\n",
    "\n",
    "#### **Democratization of Deep Learning**\n",
    "- **Research acceleration**: Focus on architecture, not gradient computation\n",
    "- **Experimentation**: Easy to try new ideas and architectures\n",
    "- **Accessibility**: Researchers don't need to be differentiation experts\n",
    "\n",
    "#### **Scalability**\n",
    "- **Large models**: Handle millions/billions of parameters automatically\n",
    "- **Complex architectures**: Support arbitrary computational graphs\n",
    "- **Distributed training**: Coordinate gradients across multiple devices\n",
    "\n",
    "Let's implement the Variable class that makes this magic possible!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0425fc",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 2: The Variable Class\n",
    "\n",
    "### Core Concept\n",
    "A **Variable** wraps a Tensor and tracks:\n",
    "- **Data**: The actual values (forward pass)\n",
    "- **Gradient**: The computed gradients (backward pass)\n",
    "- **Computation history**: How this Variable was created\n",
    "- **Backward function**: How to compute gradients\n",
    "\n",
    "### Design Principles\n",
    "- **Transparency**: Works seamlessly with existing Tensor operations\n",
    "- **Efficiency**: Minimal overhead for forward pass\n",
    "- **Flexibility**: Supports any differentiable operation\n",
    "- **Correctness**: Implements the chain rule precisely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2ba760",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "variable-class",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class Variable:\n",
    "    \"\"\"\n",
    "    Variable: Tensor wrapper with automatic differentiation capabilities.\n",
    "    \n",
    "    The fundamental class for gradient computation in TinyTorch.\n",
    "    Wraps Tensor objects and tracks computational history for backpropagation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data: Union[Tensor, np.ndarray, list, float, int], \n",
    "                 requires_grad: bool = True, grad_fn: Optional[Callable] = None):\n",
    "        \"\"\"\n",
    "        Create a Variable with gradient tracking.\n",
    "        \n",
    "        Args:\n",
    "            data: The data to wrap (will be converted to Tensor)\n",
    "            requires_grad: Whether to compute gradients for this Variable\n",
    "            grad_fn: Function to compute gradients (None for leaf nodes)\n",
    "            \n",
    "        TODO: Implement Variable initialization with gradient tracking.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Convert data to Tensor if it's not already\n",
    "        2. Store the tensor data\n",
    "        3. Set gradient tracking flag\n",
    "        4. Initialize gradient to None (will be computed later)\n",
    "        5. Store the gradient function for backward pass\n",
    "        6. Track if this is a leaf node (no grad_fn)\n",
    "        \n",
    "        EXAMPLE:\n",
    "        Variable(5.0) → Variable wrapping Tensor(5.0)\n",
    "        Variable([1, 2, 3]) → Variable wrapping Tensor([1, 2, 3])\n",
    "        \n",
    "        HINTS:\n",
    "        - Use isinstance() to check if data is already a Tensor\n",
    "        - Store requires_grad, grad_fn, and is_leaf flags\n",
    "        - Initialize self.grad to None\n",
    "        - A leaf node has grad_fn=None\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        # Convert data to Tensor if needed\n",
    "        if isinstance(data, Tensor):\n",
    "            self.data = data\n",
    "        else:\n",
    "            self.data = Tensor(data)\n",
    "        \n",
    "        # Set gradient tracking\n",
    "        self.requires_grad = requires_grad\n",
    "        self.grad = None  # Will be initialized when needed\n",
    "        self.grad_fn = grad_fn\n",
    "        self.is_leaf = grad_fn is None\n",
    "        \n",
    "        # For computational graph\n",
    "        self._backward_hooks = []\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    @property\n",
    "    def shape(self) -> Tuple[int, ...]:\n",
    "        \"\"\"Get the shape of the underlying tensor.\"\"\"\n",
    "        return self.data.shape\n",
    "    \n",
    "    @property\n",
    "    def size(self) -> int:\n",
    "        \"\"\"Get the total number of elements.\"\"\"\n",
    "        return self.data.size\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"String representation of the Variable.\"\"\"\n",
    "        grad_str = f\", grad_fn={self.grad_fn.__name__}\" if self.grad_fn else \"\"\n",
    "        return f\"Variable({self.data.data.tolist()}, requires_grad={self.requires_grad}{grad_str})\"\n",
    "    \n",
    "    def backward(self, gradient: Optional['Variable'] = None) -> None:\n",
    "        \"\"\"\n",
    "        Compute gradients using backpropagation.\n",
    "        \n",
    "        Args:\n",
    "            gradient: The gradient to backpropagate (defaults to ones)\n",
    "            \n",
    "        TODO: Implement backward propagation.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. If gradient is None, create a gradient of ones with same shape\n",
    "        2. If this Variable doesn't require gradients, return early\n",
    "        3. If this is a leaf node, accumulate the gradient\n",
    "        4. If this has a grad_fn, call it to propagate gradients\n",
    "        \n",
    "        EXAMPLE:\n",
    "        x = Variable(5.0)\n",
    "        y = x * 2\n",
    "        y.backward()  # Computes x.grad = 2.0\n",
    "        \n",
    "        HINTS:\n",
    "        - Use np.ones_like() to create default gradient\n",
    "        - Accumulate gradients with += for leaf nodes\n",
    "        - Call self.grad_fn(gradient) for non-leaf nodes\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        # Default gradient is ones\n",
    "        if gradient is None:\n",
    "            gradient = Variable(np.ones_like(self.data.data))\n",
    "        \n",
    "        # Skip if gradients not required\n",
    "        if not self.requires_grad:\n",
    "            return\n",
    "        \n",
    "        # Accumulate gradient for leaf nodes\n",
    "        if self.is_leaf:\n",
    "            if self.grad is None:\n",
    "                self.grad = Variable(np.zeros_like(self.data.data))\n",
    "            self.grad.data._data += gradient.data.data\n",
    "        else:\n",
    "            # Propagate gradients through grad_fn\n",
    "            if self.grad_fn is not None:\n",
    "                self.grad_fn(gradient)\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def zero_grad(self) -> None:\n",
    "        \"\"\"Zero out the gradient.\"\"\"\n",
    "        if self.grad is not None:\n",
    "            self.grad.data._data.fill(0)\n",
    "    \n",
    "    # Arithmetic operations with gradient tracking\n",
    "    def __add__(self, other: Union['Variable', float, int]) -> 'Variable':\n",
    "        \"\"\"Addition with gradient tracking.\"\"\"\n",
    "        return add(self, other)\n",
    "    \n",
    "    def __mul__(self, other: Union['Variable', float, int]) -> 'Variable':\n",
    "        \"\"\"Multiplication with gradient tracking.\"\"\"\n",
    "        return multiply(self, other)\n",
    "    \n",
    "    def __sub__(self, other: Union['Variable', float, int]) -> 'Variable':\n",
    "        \"\"\"Subtraction with gradient tracking.\"\"\"\n",
    "        return subtract(self, other)\n",
    "    \n",
    "    def __truediv__(self, other: Union['Variable', float, int]) -> 'Variable':\n",
    "        \"\"\"Division with gradient tracking.\"\"\"\n",
    "        return divide(self, other) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861498f5",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 3: Basic Operations with Gradients\n",
    "\n",
    "### The Pattern\n",
    "Every differentiable operation follows the same pattern:\n",
    "1. **Forward pass**: Compute the result\n",
    "2. **Create grad_fn**: Function that knows how to compute gradients\n",
    "3. **Return Variable**: With the result and grad_fn\n",
    "\n",
    "### Mathematical Rules\n",
    "- **Addition**: `d(x + y)/dx = 1, d(x + y)/dy = 1`\n",
    "- **Multiplication**: `d(x * y)/dx = y, d(x * y)/dy = x`\n",
    "- **Subtraction**: `d(x - y)/dx = 1, d(x - y)/dy = -1`\n",
    "- **Division**: `d(x / y)/dx = 1/y, d(x / y)/dy = -x/y²`\n",
    "\n",
    "### Implementation Strategy\n",
    "Each operation creates a closure that captures the input variables and implements the gradient computation rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27204e0",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "add-operation",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def add(a: Union[Variable, float, int], b: Union[Variable, float, int]) -> Variable:\n",
    "    \"\"\"\n",
    "    Addition operation with gradient tracking.\n",
    "    \n",
    "    Args:\n",
    "        a: First operand\n",
    "        b: Second operand\n",
    "        \n",
    "    Returns:\n",
    "        Variable with sum and gradient function\n",
    "        \n",
    "    TODO: Implement addition with gradient computation.\n",
    "    \n",
    "    APPROACH:\n",
    "    1. Convert inputs to Variables if needed\n",
    "    2. Compute forward pass: result = a + b\n",
    "    3. Create gradient function that distributes gradients\n",
    "    4. Return Variable with result and grad_fn\n",
    "    \n",
    "    MATHEMATICAL RULE:\n",
    "    If z = x + y, then dz/dx = 1, dz/dy = 1\n",
    "    \n",
    "    EXAMPLE:\n",
    "    x = Variable(2.0), y = Variable(3.0)\n",
    "    z = add(x, y)  # z.data = 5.0\n",
    "    z.backward()   # x.grad = 1.0, y.grad = 1.0\n",
    "    \n",
    "    HINTS:\n",
    "    - Use isinstance() to check if inputs are Variables\n",
    "    - Create a closure that captures a and b\n",
    "    - In grad_fn, call a.backward() and b.backward() with appropriate gradients\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    # Convert to Variables if needed\n",
    "    if not isinstance(a, Variable):\n",
    "        a = Variable(a, requires_grad=False)\n",
    "    if not isinstance(b, Variable):\n",
    "        b = Variable(b, requires_grad=False)\n",
    "    \n",
    "    # Forward pass\n",
    "    result_data = a.data + b.data\n",
    "    \n",
    "    # Create gradient function\n",
    "    def grad_fn(grad_output):\n",
    "        # Addition distributes gradients equally\n",
    "        if a.requires_grad:\n",
    "            a.backward(grad_output)\n",
    "        if b.requires_grad:\n",
    "            b.backward(grad_output)\n",
    "    \n",
    "    # Determine if result requires gradients\n",
    "    requires_grad = a.requires_grad or b.requires_grad\n",
    "    \n",
    "    return Variable(result_data, requires_grad=requires_grad, grad_fn=grad_fn)\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb00886",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "multiply-operation",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def multiply(a: Union[Variable, float, int], b: Union[Variable, float, int]) -> Variable:\n",
    "    \"\"\"\n",
    "    Multiplication operation with gradient tracking.\n",
    "    \n",
    "    Args:\n",
    "        a: First operand\n",
    "        b: Second operand\n",
    "        \n",
    "    Returns:\n",
    "        Variable with product and gradient function\n",
    "        \n",
    "    TODO: Implement multiplication with gradient computation.\n",
    "    \n",
    "    APPROACH:\n",
    "    1. Convert inputs to Variables if needed\n",
    "    2. Compute forward pass: result = a * b\n",
    "    3. Create gradient function using product rule\n",
    "    4. Return Variable with result and grad_fn\n",
    "    \n",
    "    MATHEMATICAL RULE:\n",
    "    If z = x * y, then dz/dx = y, dz/dy = x\n",
    "    \n",
    "    EXAMPLE:\n",
    "    x = Variable(2.0), y = Variable(3.0)\n",
    "    z = multiply(x, y)  # z.data = 6.0\n",
    "    z.backward()        # x.grad = 3.0, y.grad = 2.0\n",
    "    \n",
    "    HINTS:\n",
    "    - Store a.data and b.data for gradient computation\n",
    "    - In grad_fn, multiply incoming gradient by the other operand\n",
    "    - Handle broadcasting if shapes are different\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    # Convert to Variables if needed\n",
    "    if not isinstance(a, Variable):\n",
    "        a = Variable(a, requires_grad=False)\n",
    "    if not isinstance(b, Variable):\n",
    "        b = Variable(b, requires_grad=False)\n",
    "    \n",
    "    # Forward pass\n",
    "    result_data = a.data * b.data\n",
    "    \n",
    "    # Create gradient function\n",
    "    def grad_fn(grad_output):\n",
    "        # Product rule: d(xy)/dx = y, d(xy)/dy = x\n",
    "        if a.requires_grad:\n",
    "            a_grad = Variable(grad_output.data * b.data)\n",
    "            a.backward(a_grad)\n",
    "        if b.requires_grad:\n",
    "            b_grad = Variable(grad_output.data * a.data)\n",
    "            b.backward(b_grad)\n",
    "    \n",
    "    # Determine if result requires gradients\n",
    "    requires_grad = a.requires_grad or b.requires_grad\n",
    "    \n",
    "    return Variable(result_data, requires_grad=requires_grad, grad_fn=grad_fn)\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48266396",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "subtract-operation",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def subtract(a: Union[Variable, float, int], b: Union[Variable, float, int]) -> Variable:\n",
    "    \"\"\"\n",
    "    Subtraction operation with gradient tracking.\n",
    "    \n",
    "    Args:\n",
    "        a: First operand (minuend)\n",
    "        b: Second operand (subtrahend)\n",
    "        \n",
    "    Returns:\n",
    "        Variable with difference and gradient function\n",
    "        \n",
    "    TODO: Implement subtraction with gradient computation.\n",
    "    \n",
    "    APPROACH:\n",
    "    1. Convert inputs to Variables if needed\n",
    "    2. Compute forward pass: result = a - b\n",
    "    3. Create gradient function with correct signs\n",
    "    4. Return Variable with result and grad_fn\n",
    "    \n",
    "    MATHEMATICAL RULE:\n",
    "    If z = x - y, then dz/dx = 1, dz/dy = -1\n",
    "    \n",
    "    EXAMPLE:\n",
    "    x = Variable(5.0), y = Variable(3.0)\n",
    "    z = subtract(x, y)  # z.data = 2.0\n",
    "    z.backward()        # x.grad = 1.0, y.grad = -1.0\n",
    "    \n",
    "    HINTS:\n",
    "    - Forward pass is straightforward: a - b\n",
    "    - Gradient for a is positive, for b is negative\n",
    "    - Remember to negate the gradient for b\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    # Convert to Variables if needed\n",
    "    if not isinstance(a, Variable):\n",
    "        a = Variable(a, requires_grad=False)\n",
    "    if not isinstance(b, Variable):\n",
    "        b = Variable(b, requires_grad=False)\n",
    "    \n",
    "    # Forward pass\n",
    "    result_data = a.data - b.data\n",
    "    \n",
    "    # Create gradient function\n",
    "    def grad_fn(grad_output):\n",
    "        # Subtraction rule: d(x-y)/dx = 1, d(x-y)/dy = -1\n",
    "        if a.requires_grad:\n",
    "            a.backward(grad_output)\n",
    "        if b.requires_grad:\n",
    "            b_grad = Variable(-grad_output.data.data)\n",
    "            b.backward(b_grad)\n",
    "    \n",
    "    # Determine if result requires gradients\n",
    "    requires_grad = a.requires_grad or b.requires_grad\n",
    "    \n",
    "    return Variable(result_data, requires_grad=requires_grad, grad_fn=grad_fn)\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f4518c",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "divide-operation",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def divide(a: Union[Variable, float, int], b: Union[Variable, float, int]) -> Variable:\n",
    "    \"\"\"\n",
    "    Division operation with gradient tracking.\n",
    "    \n",
    "    Args:\n",
    "        a: Numerator\n",
    "        b: Denominator\n",
    "        \n",
    "    Returns:\n",
    "        Variable with quotient and gradient function\n",
    "        \n",
    "    TODO: Implement division with gradient computation.\n",
    "    \n",
    "    APPROACH:\n",
    "    1. Convert inputs to Variables if needed\n",
    "    2. Compute forward pass: result = a / b\n",
    "    3. Create gradient function using quotient rule\n",
    "    4. Return Variable with result and grad_fn\n",
    "    \n",
    "    MATHEMATICAL RULE:\n",
    "    If z = x / y, then dz/dx = 1/y, dz/dy = -x/y²\n",
    "    \n",
    "    EXAMPLE:\n",
    "    x = Variable(6.0), y = Variable(2.0)\n",
    "    z = divide(x, y)  # z.data = 3.0\n",
    "    z.backward()      # x.grad = 0.5, y.grad = -1.5\n",
    "    \n",
    "    HINTS:\n",
    "    - Forward pass: a.data / b.data\n",
    "    - Gradient for a: grad_output / b.data\n",
    "    - Gradient for b: -grad_output * a.data / (b.data ** 2)\n",
    "    - Be careful with numerical stability\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    # Convert to Variables if needed\n",
    "    if not isinstance(a, Variable):\n",
    "        a = Variable(a, requires_grad=False)\n",
    "    if not isinstance(b, Variable):\n",
    "        b = Variable(b, requires_grad=False)\n",
    "    \n",
    "    # Forward pass\n",
    "    result_data = a.data / b.data\n",
    "    \n",
    "    # Create gradient function\n",
    "    def grad_fn(grad_output):\n",
    "        # Quotient rule: d(x/y)/dx = 1/y, d(x/y)/dy = -x/y²\n",
    "        if a.requires_grad:\n",
    "            a_grad = Variable(grad_output.data.data / b.data.data)\n",
    "            a.backward(a_grad)\n",
    "        if b.requires_grad:\n",
    "            b_grad = Variable(-grad_output.data.data * a.data.data / (b.data.data ** 2))\n",
    "            b.backward(b_grad)\n",
    "    \n",
    "    # Determine if result requires gradients\n",
    "    requires_grad = a.requires_grad or b.requires_grad\n",
    "    \n",
    "    return Variable(result_data, requires_grad=requires_grad, grad_fn=grad_fn)\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f08b90",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 4: Testing Basic Operations\n",
    "\n",
    "Let's test our basic operations to ensure they compute gradients correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b4d23b",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-basic-operations",
     "locked": true,
     "points": 25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_basic_operations():\n",
    "    \"\"\"Test basic operations with gradient computation.\"\"\"\n",
    "    print(\"🔬 Testing basic operations...\")\n",
    "    \n",
    "    # Test addition\n",
    "    print(\"📊 Testing addition...\")\n",
    "    x = Variable(2.0, requires_grad=True)\n",
    "    y = Variable(3.0, requires_grad=True)\n",
    "    z = add(x, y)\n",
    "    \n",
    "    assert abs(z.data.data.item() - 5.0) < 1e-6, f\"Addition failed: expected 5.0, got {z.data.data.item()}\"\n",
    "    \n",
    "    z.backward()\n",
    "    assert abs(x.grad.data.data.item() - 1.0) < 1e-6, f\"Addition gradient for x failed: expected 1.0, got {x.grad.data.data.item()}\"\n",
    "    assert abs(y.grad.data.data.item() - 1.0) < 1e-6, f\"Addition gradient for y failed: expected 1.0, got {y.grad.data.data.item()}\"\n",
    "    print(\"✅ Addition test passed!\")\n",
    "    \n",
    "    # Test multiplication\n",
    "    print(\"📊 Testing multiplication...\")\n",
    "    x = Variable(2.0, requires_grad=True)\n",
    "    y = Variable(3.0, requires_grad=True)\n",
    "    z = multiply(x, y)\n",
    "    \n",
    "    assert abs(z.data.data.item() - 6.0) < 1e-6, f\"Multiplication failed: expected 6.0, got {z.data.data.item()}\"\n",
    "    \n",
    "    z.backward()\n",
    "    assert abs(x.grad.data.data.item() - 3.0) < 1e-6, f\"Multiplication gradient for x failed: expected 3.0, got {x.grad.data.data.item()}\"\n",
    "    assert abs(y.grad.data.data.item() - 2.0) < 1e-6, f\"Multiplication gradient for y failed: expected 2.0, got {y.grad.data.data.item()}\"\n",
    "    print(\"✅ Multiplication test passed!\")\n",
    "    \n",
    "    # Test subtraction\n",
    "    print(\"📊 Testing subtraction...\")\n",
    "    x = Variable(5.0, requires_grad=True)\n",
    "    y = Variable(3.0, requires_grad=True)\n",
    "    z = subtract(x, y)\n",
    "    \n",
    "    assert abs(z.data.data.item() - 2.0) < 1e-6, f\"Subtraction failed: expected 2.0, got {z.data.data.item()}\"\n",
    "    \n",
    "    z.backward()\n",
    "    assert abs(x.grad.data.data.item() - 1.0) < 1e-6, f\"Subtraction gradient for x failed: expected 1.0, got {x.grad.data.data.item()}\"\n",
    "    assert abs(y.grad.data.data.item() - (-1.0)) < 1e-6, f\"Subtraction gradient for y failed: expected -1.0, got {y.grad.data.data.item()}\"\n",
    "    print(\"✅ Subtraction test passed!\")\n",
    "    \n",
    "    # Test division\n",
    "    print(\"📊 Testing division...\")\n",
    "    x = Variable(6.0, requires_grad=True)\n",
    "    y = Variable(2.0, requires_grad=True)\n",
    "    z = divide(x, y)\n",
    "    \n",
    "    assert abs(z.data.data.item() - 3.0) < 1e-6, f\"Division failed: expected 3.0, got {z.data.data.item()}\"\n",
    "    \n",
    "    z.backward()\n",
    "    assert abs(x.grad.data.data.item() - 0.5) < 1e-6, f\"Division gradient for x failed: expected 0.5, got {x.grad.data.data.item()}\"\n",
    "    assert abs(y.grad.data.data.item() - (-1.5)) < 1e-6, f\"Division gradient for y failed: expected -1.5, got {y.grad.data.data.item()}\"\n",
    "    print(\"✅ Division test passed!\")\n",
    "    \n",
    "    print(\"🎉 All basic operation tests passed!\")\n",
    "    return True\n",
    "\n",
    "# Run the test\n",
    "success = test_basic_operations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f1577c",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 5: Chain Rule Testing\n",
    "\n",
    "Let's test more complex expressions to ensure the chain rule works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f9662c",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-chain-rule",
     "locked": true,
     "points": 25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_chain_rule():\n",
    "    \"\"\"Test chain rule with complex expressions.\"\"\"\n",
    "    print(\"🔬 Testing chain rule...\")\n",
    "    \n",
    "    # Test: f(x, y) = (x + y) * (x - y) = x² - y²\n",
    "    print(\"📊 Testing f(x, y) = (x + y) * (x - y)...\")\n",
    "    x = Variable(3.0, requires_grad=True)\n",
    "    y = Variable(2.0, requires_grad=True)\n",
    "    \n",
    "    # Forward pass\n",
    "    sum_xy = add(x, y)      # x + y = 5\n",
    "    diff_xy = subtract(x, y) # x - y = 1\n",
    "    result = multiply(sum_xy, diff_xy)  # (x + y) * (x - y) = 5\n",
    "    \n",
    "    assert abs(result.data.data.item() - 5.0) < 1e-6, f\"Chain rule forward failed: expected 5.0, got {result.data.data.item()}\"\n",
    "    \n",
    "    # Backward pass\n",
    "    result.backward()\n",
    "    \n",
    "    # Analytical gradients: df/dx = 2x = 6, df/dy = -2y = -4\n",
    "    expected_x_grad = 2 * 3.0  # 6.0\n",
    "    expected_y_grad = -2 * 2.0  # -4.0\n",
    "    \n",
    "    assert abs(x.grad.data.data.item() - expected_x_grad) < 1e-6, f\"Chain rule x gradient failed: expected {expected_x_grad}, got {x.grad.data.data.item()}\"\n",
    "    assert abs(y.grad.data.data.item() - expected_y_grad) < 1e-6, f\"Chain rule y gradient failed: expected {expected_y_grad}, got {y.grad.data.data.item()}\"\n",
    "    print(\"✅ Chain rule test passed!\")\n",
    "    \n",
    "    # Test: f(x) = x * x * x (x³)\n",
    "    print(\"📊 Testing f(x) = x³...\")\n",
    "    x = Variable(2.0, requires_grad=True)\n",
    "    \n",
    "    # Forward pass\n",
    "    x_squared = multiply(x, x)      # x²\n",
    "    x_cubed = multiply(x_squared, x)  # x³\n",
    "    \n",
    "    assert abs(x_cubed.data.data.item() - 8.0) < 1e-6, f\"x³ forward failed: expected 8.0, got {x_cubed.data.data.item()}\"\n",
    "    \n",
    "    # Backward pass\n",
    "    x_cubed.backward()\n",
    "    \n",
    "    # Analytical gradient: df/dx = 3x² = 12\n",
    "    expected_grad = 3 * (2.0 ** 2)  # 12.0\n",
    "    \n",
    "    assert abs(x.grad.data.data.item() - expected_grad) < 1e-6, f\"x³ gradient failed: expected {expected_grad}, got {x.grad.data.data.item()}\"\n",
    "    print(\"✅ x³ test passed!\")\n",
    "    \n",
    "    print(\"🎉 All chain rule tests passed!\")\n",
    "    return True\n",
    "\n",
    "# Run the test\n",
    "success = test_chain_rule()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482c07ae",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 6: Activation Function Gradients\n",
    "\n",
    "Now let's implement gradients for activation functions to integrate with our existing modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d162dc",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "relu-gradient",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def relu_with_grad(x: Variable) -> Variable:\n",
    "    \"\"\"\n",
    "    ReLU activation with gradient tracking.\n",
    "    \n",
    "    Args:\n",
    "        x: Input Variable\n",
    "        \n",
    "    Returns:\n",
    "        Variable with ReLU applied and gradient function\n",
    "        \n",
    "    TODO: Implement ReLU with gradient computation.\n",
    "    \n",
    "    APPROACH:\n",
    "    1. Compute forward pass: max(0, x)\n",
    "    2. Create gradient function using ReLU derivative\n",
    "    3. Return Variable with result and grad_fn\n",
    "    \n",
    "    MATHEMATICAL RULE:\n",
    "    f(x) = max(0, x)\n",
    "    f'(x) = 1 if x > 0, else 0\n",
    "    \n",
    "    EXAMPLE:\n",
    "    x = Variable([-1.0, 0.0, 1.0])\n",
    "    y = relu_with_grad(x)  # y.data = [0.0, 0.0, 1.0]\n",
    "    y.backward()           # x.grad = [0.0, 0.0, 1.0]\n",
    "    \n",
    "    HINTS:\n",
    "    - Use np.maximum(0, x.data.data) for forward pass\n",
    "    - Use (x.data.data > 0) for gradient mask\n",
    "    - Only propagate gradients where input was positive\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    # Forward pass\n",
    "    result_data = Tensor(np.maximum(0, x.data.data))\n",
    "    \n",
    "    # Create gradient function\n",
    "    def grad_fn(grad_output):\n",
    "        if x.requires_grad:\n",
    "            # ReLU derivative: 1 if x > 0, else 0\n",
    "            mask = (x.data.data > 0).astype(np.float32)\n",
    "            x_grad = Variable(grad_output.data.data * mask)\n",
    "            x.backward(x_grad)\n",
    "    \n",
    "    return Variable(result_data, requires_grad=x.requires_grad, grad_fn=grad_fn)\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9228d4",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "sigmoid-gradient",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def sigmoid_with_grad(x: Variable) -> Variable:\n",
    "    \"\"\"\n",
    "    Sigmoid activation with gradient tracking.\n",
    "    \n",
    "    Args:\n",
    "        x: Input Variable\n",
    "        \n",
    "    Returns:\n",
    "        Variable with sigmoid applied and gradient function\n",
    "        \n",
    "    TODO: Implement sigmoid with gradient computation.\n",
    "    \n",
    "    APPROACH:\n",
    "    1. Compute forward pass: 1 / (1 + exp(-x))\n",
    "    2. Create gradient function using sigmoid derivative\n",
    "    3. Return Variable with result and grad_fn\n",
    "    \n",
    "    MATHEMATICAL RULE:\n",
    "    f(x) = 1 / (1 + exp(-x))\n",
    "    f'(x) = f(x) * (1 - f(x))\n",
    "    \n",
    "    EXAMPLE:\n",
    "    x = Variable(0.0)\n",
    "    y = sigmoid_with_grad(x)  # y.data = 0.5\n",
    "    y.backward()              # x.grad = 0.25\n",
    "    \n",
    "    HINTS:\n",
    "    - Use np.clip for numerical stability\n",
    "    - Store sigmoid output for gradient computation\n",
    "    - Gradient is sigmoid * (1 - sigmoid)\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    # Forward pass with numerical stability\n",
    "    clipped = np.clip(x.data.data, -500, 500)\n",
    "    sigmoid_output = 1.0 / (1.0 + np.exp(-clipped))\n",
    "    result_data = Tensor(sigmoid_output)\n",
    "    \n",
    "    # Create gradient function\n",
    "    def grad_fn(grad_output):\n",
    "        if x.requires_grad:\n",
    "            # Sigmoid derivative: sigmoid * (1 - sigmoid)\n",
    "            sigmoid_grad = sigmoid_output * (1.0 - sigmoid_output)\n",
    "            x_grad = Variable(grad_output.data.data * sigmoid_grad)\n",
    "            x.backward(x_grad)\n",
    "    \n",
    "    return Variable(result_data, requires_grad=x.requires_grad, grad_fn=grad_fn)\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d23d230",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 7: Integration Testing\n",
    "\n",
    "Let's test our autograd system with a simple neural network scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b89cce",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-integration",
     "locked": true,
     "points": 25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_integration():\n",
    "    \"\"\"Test autograd integration with neural network scenario.\"\"\"\n",
    "    print(\"🔬 Testing autograd integration...\")\n",
    "    \n",
    "    # Simple neural network: input -> linear -> ReLU -> output\n",
    "    print(\"📊 Testing simple neural network...\")\n",
    "    \n",
    "    # Input\n",
    "    x = Variable(2.0, requires_grad=True)\n",
    "    \n",
    "    # Weights and bias\n",
    "    w1 = Variable(0.5, requires_grad=True)\n",
    "    b1 = Variable(0.1, requires_grad=True)\n",
    "    w2 = Variable(1.5, requires_grad=True)\n",
    "    \n",
    "    # Forward pass\n",
    "    linear1 = add(multiply(x, w1), b1)  # x * w1 + b1 = 2*0.5 + 0.1 = 1.1\n",
    "    activation1 = relu_with_grad(linear1)  # ReLU(1.1) = 1.1\n",
    "    output = multiply(activation1, w2)     # 1.1 * 1.5 = 1.65\n",
    "    \n",
    "    # Check forward pass\n",
    "    expected_output = 1.65\n",
    "    assert abs(output.data.data.item() - expected_output) < 1e-6, f\"Integration forward failed: expected {expected_output}, got {output.data.data.item()}\"\n",
    "    \n",
    "    # Backward pass\n",
    "    output.backward()\n",
    "    \n",
    "    # Check gradients\n",
    "    # dL/dx = dL/doutput * doutput/dactivation1 * dactivation1/dlinear1 * dlinear1/dx\n",
    "    #       = 1 * w2 * 1 * w1 = 1.5 * 0.5 = 0.75\n",
    "    expected_x_grad = 0.75\n",
    "    assert abs(x.grad.data.data.item() - expected_x_grad) < 1e-6, f\"Integration x gradient failed: expected {expected_x_grad}, got {x.grad.data.data.item()}\"\n",
    "    \n",
    "    # dL/dw1 = dL/doutput * doutput/dactivation1 * dactivation1/dlinear1 * dlinear1/dw1\n",
    "    #        = 1 * w2 * 1 * x = 1.5 * 2.0 = 3.0\n",
    "    expected_w1_grad = 3.0\n",
    "    assert abs(w1.grad.data.data.item() - expected_w1_grad) < 1e-6, f\"Integration w1 gradient failed: expected {expected_w1_grad}, got {w1.grad.data.data.item()}\"\n",
    "    \n",
    "    # dL/db1 = dL/doutput * doutput/dactivation1 * dactivation1/dlinear1 * dlinear1/db1\n",
    "    #        = 1 * w2 * 1 * 1 = 1.5\n",
    "    expected_b1_grad = 1.5\n",
    "    assert abs(b1.grad.data.data.item() - expected_b1_grad) < 1e-6, f\"Integration b1 gradient failed: expected {expected_b1_grad}, got {b1.grad.data.data.item()}\"\n",
    "    \n",
    "    # dL/dw2 = dL/doutput * doutput/dw2 = 1 * activation1 = 1.1\n",
    "    expected_w2_grad = 1.1\n",
    "    assert abs(w2.grad.data.data.item() - expected_w2_grad) < 1e-6, f\"Integration w2 gradient failed: expected {expected_w2_grad}, got {w2.grad.data.data.item()}\"\n",
    "    \n",
    "    print(\"✅ Integration test passed!\")\n",
    "    print(\"🎉 All autograd tests passed!\")\n",
    "    return True\n",
    "\n",
    "# Run the test\n",
    "success = test_integration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a04652",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 🎯 Module Summary\n",
    "\n",
    "Congratulations! You've successfully implemented automatic differentiation for TinyTorch:\n",
    "\n",
    "### What You've Accomplished\n",
    "✅ **Variable Class**: Tensor wrapper with gradient tracking and computational graph  \n",
    "✅ **Basic Operations**: Addition, multiplication, subtraction, division with gradients  \n",
    "✅ **Chain Rule**: Automatic gradient computation through complex expressions  \n",
    "✅ **Activation Functions**: ReLU and Sigmoid with proper gradient computation  \n",
    "✅ **Integration**: Works seamlessly with neural network scenarios  \n",
    "\n",
    "### Key Concepts You've Learned\n",
    "- **Computational graphs** represent mathematical expressions as directed graphs\n",
    "- **Forward pass** computes function values following the graph\n",
    "- **Backward pass** computes gradients using the chain rule in reverse\n",
    "- **Gradient functions** capture how to compute gradients for each operation\n",
    "- **Variable tracking** enables automatic differentiation of any expression\n",
    "\n",
    "### Mathematical Foundations\n",
    "- **Chain rule**: The fundamental principle behind backpropagation\n",
    "- **Partial derivatives**: How gradients flow through operations\n",
    "- **Computational efficiency**: Reusing forward pass results in backward pass\n",
    "- **Numerical stability**: Handling edge cases in gradient computation\n",
    "\n",
    "### Real-World Applications\n",
    "- **Neural network training**: Backpropagation through layers\n",
    "- **Optimization**: Gradient descent and advanced optimizers\n",
    "- **Scientific computing**: Sensitivity analysis and inverse problems\n",
    "- **Machine learning**: Any gradient-based learning algorithm\n",
    "\n",
    "### Next Steps\n",
    "1. **Export your code**: `tito package nbdev --export 07_autograd`\n",
    "2. **Test your implementation**: `tito module test 07_autograd`\n",
    "3. **Use your autograd**: \n",
    "   ```python\n",
    "   from tinytorch.core.autograd import Variable\n",
    "   \n",
    "   x = Variable(2.0, requires_grad=True)\n",
    "   y = x**2 + 3*x + 1\n",
    "   y.backward()\n",
    "   print(x.grad)  # Your gradients in action!\n",
    "   ```\n",
    "4. **Move to Module 8**: Start building training loops and optimizers!\n",
    "\n",
    "**Ready for the next challenge?** Let's use your autograd system to build complete training pipelines!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0416534a",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 8: Performance Optimizations and Advanced Features\n",
    "\n",
    "### Memory Management\n",
    "- **Gradient Accumulation**: Efficient in-place gradient updates\n",
    "- **Computational Graph Cleanup**: Release intermediate values when possible\n",
    "- **Lazy Evaluation**: Compute gradients only when needed\n",
    "\n",
    "### Numerical Stability\n",
    "- **Gradient Clipping**: Prevent exploding gradients\n",
    "- **Numerical Precision**: Handle edge cases gracefully\n",
    "- **Overflow Protection**: Clip extreme values\n",
    "\n",
    "### Advanced Features\n",
    "- **Higher-Order Gradients**: Gradients of gradients\n",
    "- **Gradient Checkpointing**: Memory-efficient backpropagation\n",
    "- **Custom Operations**: Framework for user-defined differentiable functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff184aed",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "advanced-features",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def power(base: Variable, exponent: Union[float, int]) -> Variable:\n",
    "    \"\"\"\n",
    "    Power operation with gradient tracking: base^exponent.\n",
    "    \n",
    "    Args:\n",
    "        base: Base Variable\n",
    "        exponent: Exponent (scalar)\n",
    "        \n",
    "    Returns:\n",
    "        Variable with power applied and gradient function\n",
    "        \n",
    "    TODO: Implement power operation with gradient computation.\n",
    "    \n",
    "    APPROACH:\n",
    "    1. Compute forward pass: base^exponent\n",
    "    2. Create gradient function using power rule\n",
    "    3. Return Variable with result and grad_fn\n",
    "    \n",
    "    MATHEMATICAL RULE:\n",
    "    If z = x^n, then dz/dx = n * x^(n-1)\n",
    "    \n",
    "    EXAMPLE:\n",
    "    x = Variable(2.0)\n",
    "    y = power(x, 3)  # y.data = 8.0\n",
    "    y.backward()     # x.grad = 3 * 2^2 = 12.0\n",
    "    \n",
    "    HINTS:\n",
    "    - Use np.power() for forward pass\n",
    "    - Power rule: gradient = exponent * base^(exponent-1)\n",
    "    - Handle edge cases like exponent=0 or base=0\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    # Forward pass\n",
    "    result_data = Tensor(np.power(base.data.data, exponent))\n",
    "    \n",
    "    # Create gradient function\n",
    "    def grad_fn(grad_output):\n",
    "        if base.requires_grad:\n",
    "            # Power rule: d(x^n)/dx = n * x^(n-1)\n",
    "            if exponent == 0:\n",
    "                # Special case: derivative of constant is 0\n",
    "                base_grad = Variable(np.zeros_like(base.data.data))\n",
    "            else:\n",
    "                base_grad_data = exponent * np.power(base.data.data, exponent - 1)\n",
    "                base_grad = Variable(grad_output.data.data * base_grad_data)\n",
    "            base.backward(base_grad)\n",
    "    \n",
    "    return Variable(result_data, requires_grad=base.requires_grad, grad_fn=grad_fn)\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30d36bc",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "exp-operation",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def exp(x: Variable) -> Variable:\n",
    "    \"\"\"\n",
    "    Exponential operation with gradient tracking: e^x.\n",
    "    \n",
    "    Args:\n",
    "        x: Input Variable\n",
    "        \n",
    "    Returns:\n",
    "        Variable with exponential applied and gradient function\n",
    "        \n",
    "    TODO: Implement exponential operation with gradient computation.\n",
    "    \n",
    "    APPROACH:\n",
    "    1. Compute forward pass: e^x\n",
    "    2. Create gradient function using exponential derivative\n",
    "    3. Return Variable with result and grad_fn\n",
    "    \n",
    "    MATHEMATICAL RULE:\n",
    "    If z = e^x, then dz/dx = e^x\n",
    "    \n",
    "    EXAMPLE:\n",
    "    x = Variable(1.0)\n",
    "    y = exp(x)  # y.data = e^1 ≈ 2.718\n",
    "    y.backward()  # x.grad = e^1 ≈ 2.718\n",
    "    \n",
    "    HINTS:\n",
    "    - Use np.exp() for forward pass\n",
    "    - Exponential derivative is itself: d(e^x)/dx = e^x\n",
    "    - Store result for gradient computation\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    # Forward pass\n",
    "    exp_result = np.exp(x.data.data)\n",
    "    result_data = Tensor(exp_result)\n",
    "    \n",
    "    # Create gradient function\n",
    "    def grad_fn(grad_output):\n",
    "        if x.requires_grad:\n",
    "            # Exponential derivative: d(e^x)/dx = e^x\n",
    "            x_grad = Variable(grad_output.data.data * exp_result)\n",
    "            x.backward(x_grad)\n",
    "    \n",
    "    return Variable(result_data, requires_grad=x.requires_grad, grad_fn=grad_fn)\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a63169d",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "log-operation",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def log(x: Variable) -> Variable:\n",
    "    \"\"\"\n",
    "    Natural logarithm operation with gradient tracking: ln(x).\n",
    "    \n",
    "    Args:\n",
    "        x: Input Variable\n",
    "        \n",
    "    Returns:\n",
    "        Variable with logarithm applied and gradient function\n",
    "        \n",
    "    TODO: Implement logarithm operation with gradient computation.\n",
    "    \n",
    "    APPROACH:\n",
    "    1. Compute forward pass: ln(x)\n",
    "    2. Create gradient function using logarithm derivative\n",
    "    3. Return Variable with result and grad_fn\n",
    "    \n",
    "    MATHEMATICAL RULE:\n",
    "    If z = ln(x), then dz/dx = 1/x\n",
    "    \n",
    "    EXAMPLE:\n",
    "    x = Variable(2.0)\n",
    "    y = log(x)  # y.data = ln(2) ≈ 0.693\n",
    "    y.backward()  # x.grad = 1/2 = 0.5\n",
    "    \n",
    "    HINTS:\n",
    "    - Use np.log() for forward pass\n",
    "    - Logarithm derivative: d(ln(x))/dx = 1/x\n",
    "    - Handle numerical stability for small x\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    # Forward pass with numerical stability\n",
    "    clipped_x = np.clip(x.data.data, 1e-8, np.inf)  # Avoid log(0)\n",
    "    result_data = Tensor(np.log(clipped_x))\n",
    "    \n",
    "    # Create gradient function\n",
    "    def grad_fn(grad_output):\n",
    "        if x.requires_grad:\n",
    "            # Logarithm derivative: d(ln(x))/dx = 1/x\n",
    "            x_grad = Variable(grad_output.data.data / clipped_x)\n",
    "            x.backward(x_grad)\n",
    "    \n",
    "    return Variable(result_data, requires_grad=x.requires_grad, grad_fn=grad_fn)\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbb8311",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "sum-operation",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def sum_all(x: Variable) -> Variable:\n",
    "    \"\"\"\n",
    "    Sum all elements operation with gradient tracking.\n",
    "    \n",
    "    Args:\n",
    "        x: Input Variable\n",
    "        \n",
    "    Returns:\n",
    "        Variable with sum and gradient function\n",
    "        \n",
    "    TODO: Implement sum operation with gradient computation.\n",
    "    \n",
    "    APPROACH:\n",
    "    1. Compute forward pass: sum of all elements\n",
    "    2. Create gradient function that broadcasts gradient back\n",
    "    3. Return Variable with result and grad_fn\n",
    "    \n",
    "    MATHEMATICAL RULE:\n",
    "    If z = sum(x), then dz/dx_i = 1 for all i\n",
    "    \n",
    "    EXAMPLE:\n",
    "    x = Variable([[1, 2], [3, 4]])\n",
    "    y = sum_all(x)  # y.data = 10\n",
    "    y.backward()    # x.grad = [[1, 1], [1, 1]]\n",
    "    \n",
    "    HINTS:\n",
    "    - Use np.sum() for forward pass\n",
    "    - Gradient is ones with same shape as input\n",
    "    - This is used for loss computation\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    # Forward pass\n",
    "    result_data = Tensor(np.sum(x.data.data))\n",
    "    \n",
    "    # Create gradient function\n",
    "    def grad_fn(grad_output):\n",
    "        if x.requires_grad:\n",
    "            # Sum gradient: broadcasts to all elements\n",
    "            x_grad = Variable(grad_output.data.data * np.ones_like(x.data.data))\n",
    "            x.backward(x_grad)\n",
    "    \n",
    "    return Variable(result_data, requires_grad=x.requires_grad, grad_fn=grad_fn)\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072982e2",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "mean-operation",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def mean(x: Variable) -> Variable:\n",
    "    \"\"\"\n",
    "    Mean operation with gradient tracking.\n",
    "    \n",
    "    Args:\n",
    "        x: Input Variable\n",
    "        \n",
    "    Returns:\n",
    "        Variable with mean and gradient function\n",
    "        \n",
    "    TODO: Implement mean operation with gradient computation.\n",
    "    \n",
    "    APPROACH:\n",
    "    1. Compute forward pass: mean of all elements\n",
    "    2. Create gradient function that distributes gradient evenly\n",
    "    3. Return Variable with result and grad_fn\n",
    "    \n",
    "    MATHEMATICAL RULE:\n",
    "    If z = mean(x), then dz/dx_i = 1/n for all i (where n is number of elements)\n",
    "    \n",
    "    EXAMPLE:\n",
    "    x = Variable([[1, 2], [3, 4]])\n",
    "    y = mean(x)  # y.data = 2.5\n",
    "    y.backward()  # x.grad = [[0.25, 0.25], [0.25, 0.25]]\n",
    "    \n",
    "    HINTS:\n",
    "    - Use np.mean() for forward pass\n",
    "    - Gradient is 1/n for each element\n",
    "    - This is commonly used for loss computation\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    # Forward pass\n",
    "    result_data = Tensor(np.mean(x.data.data))\n",
    "    \n",
    "    # Create gradient function\n",
    "    def grad_fn(grad_output):\n",
    "        if x.requires_grad:\n",
    "            # Mean gradient: 1/n for each element\n",
    "            n = x.data.size\n",
    "            x_grad = Variable(grad_output.data.data * np.ones_like(x.data.data) / n)\n",
    "            x.backward(x_grad)\n",
    "    \n",
    "    return Variable(result_data, requires_grad=x.requires_grad, grad_fn=grad_fn)\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec3135b",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 9: Gradient Utilities and Helper Functions\n",
    "\n",
    "### Gradient Management\n",
    "- **Gradient Clipping**: Prevent exploding gradients\n",
    "- **Gradient Checking**: Verify gradient correctness\n",
    "- **Parameter Collection**: Gather all parameters for optimization\n",
    "\n",
    "### Debugging Tools\n",
    "- **Gradient Visualization**: Inspect gradient flow\n",
    "- **Computational Graph**: Visualize the computation graph\n",
    "- **Gradient Statistics**: Monitor gradient magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dc3c65",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "gradient-utilities",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def clip_gradients(variables: List[Variable], max_norm: float = 1.0) -> None:\n",
    "    \"\"\"\n",
    "    Clip gradients to prevent exploding gradients.\n",
    "    \n",
    "    Args:\n",
    "        variables: List of Variables to clip gradients for\n",
    "        max_norm: Maximum gradient norm allowed\n",
    "        \n",
    "    TODO: Implement gradient clipping.\n",
    "    \n",
    "    APPROACH:\n",
    "    1. Compute total gradient norm across all variables\n",
    "    2. If norm exceeds max_norm, scale all gradients down\n",
    "    3. Modify gradients in-place\n",
    "    \n",
    "    MATHEMATICAL RULE:\n",
    "    If ||g|| > max_norm, then g := g * (max_norm / ||g||)\n",
    "    \n",
    "    EXAMPLE:\n",
    "    variables = [w1, w2, b1, b2]\n",
    "    clip_gradients(variables, max_norm=1.0)\n",
    "    \n",
    "    HINTS:\n",
    "    - Compute L2 norm of all gradients combined\n",
    "    - Scale factor = max_norm / total_norm\n",
    "    - Only clip if total_norm > max_norm\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    # Compute total gradient norm\n",
    "    total_norm = 0.0\n",
    "    for var in variables:\n",
    "        if var.grad is not None:\n",
    "            total_norm += np.sum(var.grad.data.data ** 2)\n",
    "    total_norm = np.sqrt(total_norm)\n",
    "    \n",
    "    # Clip if necessary\n",
    "    if total_norm > max_norm:\n",
    "        scale_factor = max_norm / total_norm\n",
    "        for var in variables:\n",
    "            if var.grad is not None:\n",
    "                var.grad.data._data *= scale_factor\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b746ae1",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "collect-parameters",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def collect_parameters(*modules) -> List[Variable]:\n",
    "    \"\"\"\n",
    "    Collect all parameters from modules for optimization.\n",
    "    \n",
    "    Args:\n",
    "        *modules: Variable number of modules/objects with parameters\n",
    "        \n",
    "    Returns:\n",
    "        List of all Variables that require gradients\n",
    "        \n",
    "    TODO: Implement parameter collection.\n",
    "    \n",
    "    APPROACH:\n",
    "    1. Iterate through all provided modules\n",
    "    2. Find all Variable attributes that require gradients\n",
    "    3. Return list of all such Variables\n",
    "    \n",
    "    EXAMPLE:\n",
    "    layer1 = SomeLayer()\n",
    "    layer2 = SomeLayer()\n",
    "    params = collect_parameters(layer1, layer2)\n",
    "    \n",
    "    HINTS:\n",
    "    - Use hasattr() and getattr() to find Variable attributes\n",
    "    - Check if attribute is Variable and requires_grad\n",
    "    - Handle different module types gracefully\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    parameters = []\n",
    "    for module in modules:\n",
    "        if hasattr(module, '__dict__'):\n",
    "            for attr_name, attr_value in module.__dict__.items():\n",
    "                if isinstance(attr_value, Variable) and attr_value.requires_grad:\n",
    "                    parameters.append(attr_value)\n",
    "    return parameters\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facdd5be",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "zero-gradients",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def zero_gradients(variables: List[Variable]) -> None:\n",
    "    \"\"\"\n",
    "    Zero out gradients for all variables.\n",
    "    \n",
    "    Args:\n",
    "        variables: List of Variables to zero gradients for\n",
    "        \n",
    "    TODO: Implement gradient zeroing.\n",
    "    \n",
    "    APPROACH:\n",
    "    1. Iterate through all variables\n",
    "    2. Call zero_grad() on each variable\n",
    "    3. Handle None gradients gracefully\n",
    "    \n",
    "    EXAMPLE:\n",
    "    parameters = [w1, w2, b1, b2]\n",
    "    zero_gradients(parameters)\n",
    "    \n",
    "    HINTS:\n",
    "    - Use the zero_grad() method on each Variable\n",
    "    - Check if variable has gradients before zeroing\n",
    "    - This is typically called before each training step\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    for var in variables:\n",
    "        if var.grad is not None:\n",
    "            var.zero_grad()\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9083d782",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 10: Advanced Testing\n",
    "\n",
    "Let's test our advanced features and optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ad0ac0",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": true,
     "grade_id": "test-advanced-operations",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_advanced_operations():\n",
    "    \"\"\"Test advanced mathematical operations.\"\"\"\n",
    "    print(\"🔬 Testing advanced operations...\")\n",
    "    \n",
    "    # Test power operation\n",
    "    print(\"📊 Testing power operation...\")\n",
    "    x = Variable(2.0, requires_grad=True)\n",
    "    y = power(x, 3)  # x^3\n",
    "    \n",
    "    assert abs(y.data.data.item() - 8.0) < 1e-6, f\"Power forward failed: expected 8.0, got {y.data.data.item()}\"\n",
    "    \n",
    "    y.backward()\n",
    "    # Gradient: d(x^3)/dx = 3x^2 = 3 * 4 = 12\n",
    "    assert abs(x.grad.data.data.item() - 12.0) < 1e-6, f\"Power gradient failed: expected 12.0, got {x.grad.data.data.item()}\"\n",
    "    print(\"✅ Power operation test passed!\")\n",
    "    \n",
    "    # Test exponential operation\n",
    "    print(\"📊 Testing exponential operation...\")\n",
    "    x = Variable(1.0, requires_grad=True)\n",
    "    y = exp(x)  # e^x\n",
    "    \n",
    "    expected_exp = np.exp(1.0)\n",
    "    assert abs(y.data.data.item() - expected_exp) < 1e-6, f\"Exp forward failed: expected {expected_exp}, got {y.data.data.item()}\"\n",
    "    \n",
    "    y.backward()\n",
    "    # Gradient: d(e^x)/dx = e^x\n",
    "    assert abs(x.grad.data.data.item() - expected_exp) < 1e-6, f\"Exp gradient failed: expected {expected_exp}, got {x.grad.data.data.item()}\"\n",
    "    print(\"✅ Exponential operation test passed!\")\n",
    "    \n",
    "    # Test logarithm operation\n",
    "    print(\"📊 Testing logarithm operation...\")\n",
    "    x = Variable(2.0, requires_grad=True)\n",
    "    y = log(x)  # ln(x)\n",
    "    \n",
    "    expected_log = np.log(2.0)\n",
    "    assert abs(y.data.data.item() - expected_log) < 1e-6, f\"Log forward failed: expected {expected_log}, got {y.data.data.item()}\"\n",
    "    \n",
    "    y.backward()\n",
    "    # Gradient: d(ln(x))/dx = 1/x = 1/2 = 0.5\n",
    "    assert abs(x.grad.data.data.item() - 0.5) < 1e-6, f\"Log gradient failed: expected 0.5, got {x.grad.data.data.item()}\"\n",
    "    print(\"✅ Logarithm operation test passed!\")\n",
    "    \n",
    "    # Test sum operation\n",
    "    print(\"📊 Testing sum operation...\")\n",
    "    x = Variable([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
    "    y = sum_all(x)  # sum of all elements\n",
    "    \n",
    "    assert abs(y.data.data.item() - 10.0) < 1e-6, f\"Sum forward failed: expected 10.0, got {y.data.data.item()}\"\n",
    "    \n",
    "    y.backward()\n",
    "    # Gradient: all elements should be 1\n",
    "    expected_grad = np.ones((2, 2))\n",
    "    np.testing.assert_array_almost_equal(x.grad.data.data, expected_grad)\n",
    "    print(\"✅ Sum operation test passed!\")\n",
    "    \n",
    "    # Test mean operation\n",
    "    print(\"📊 Testing mean operation...\")\n",
    "    x = Variable([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
    "    y = mean(x)  # mean of all elements\n",
    "    \n",
    "    assert abs(y.data.data.item() - 2.5) < 1e-6, f\"Mean forward failed: expected 2.5, got {y.data.data.item()}\"\n",
    "    \n",
    "    y.backward()\n",
    "    # Gradient: all elements should be 1/4 = 0.25\n",
    "    expected_grad = np.ones((2, 2)) * 0.25\n",
    "    np.testing.assert_array_almost_equal(x.grad.data.data, expected_grad)\n",
    "    print(\"✅ Mean operation test passed!\")\n",
    "    \n",
    "    print(\"🎉 All advanced operation tests passed!\")\n",
    "    return True\n",
    "\n",
    "# Run the test\n",
    "success = test_advanced_operations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce78d18",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-gradient-utilities",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_gradient_utilities():\n",
    "    \"\"\"Test gradient utility functions.\"\"\"\n",
    "    print(\"🔬 Testing gradient utilities...\")\n",
    "    \n",
    "    # Test gradient clipping\n",
    "    print(\"📊 Testing gradient clipping...\")\n",
    "    x = Variable(1.0, requires_grad=True)\n",
    "    y = Variable(1.0, requires_grad=True)\n",
    "    \n",
    "    # Create large gradients\n",
    "    z = multiply(x, 10.0)  # Large gradient for x\n",
    "    w = multiply(y, 10.0)  # Large gradient for y\n",
    "    loss = add(z, w)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Check gradients are large before clipping\n",
    "    assert abs(x.grad.data.data.item() - 10.0) < 1e-6\n",
    "    assert abs(y.grad.data.data.item() - 10.0) < 1e-6\n",
    "    \n",
    "    # Clip gradients\n",
    "    clip_gradients([x, y], max_norm=1.0)\n",
    "    \n",
    "    # Check gradients are clipped\n",
    "    total_norm = np.sqrt(x.grad.data.data.item()**2 + y.grad.data.data.item()**2)\n",
    "    assert abs(total_norm - 1.0) < 1e-6, f\"Gradient clipping failed: total norm {total_norm}, expected 1.0\"\n",
    "    print(\"✅ Gradient clipping test passed!\")\n",
    "    \n",
    "    # Test zero gradients\n",
    "    print(\"📊 Testing zero gradients...\")\n",
    "    # Gradients should be non-zero before zeroing\n",
    "    assert abs(x.grad.data.data.item()) > 1e-6\n",
    "    assert abs(y.grad.data.data.item()) > 1e-6\n",
    "    \n",
    "    # Zero gradients\n",
    "    zero_gradients([x, y])\n",
    "    \n",
    "    # Check gradients are zero\n",
    "    assert abs(x.grad.data.data.item()) < 1e-6\n",
    "    assert abs(y.grad.data.data.item()) < 1e-6\n",
    "    print(\"✅ Zero gradients test passed!\")\n",
    "    \n",
    "    print(\"🎉 All gradient utility tests passed!\")\n",
    "    return True\n",
    "\n",
    "# Run the test\n",
    "success = test_gradient_utilities()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6517e6",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 11: Complete ML Pipeline Example\n",
    "\n",
    "Let's demonstrate a complete machine learning pipeline using our autograd system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b775b615",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-complete-pipeline",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_complete_ml_pipeline():\n",
    "    \"\"\"Test complete ML pipeline with autograd.\"\"\"\n",
    "    print(\"🔬 Testing complete ML pipeline...\")\n",
    "    \n",
    "    # Create a simple regression problem: y = 2x + 1 + noise\n",
    "    print(\"📊 Setting up regression problem...\")\n",
    "    \n",
    "    # Training data\n",
    "    x_data = [1.0, 2.0, 3.0, 4.0, 5.0]\n",
    "    y_data = [3.1, 4.9, 7.2, 9.1, 10.8]  # Approximately 2x + 1 with noise\n",
    "    \n",
    "    # Model parameters\n",
    "    w = Variable(0.1, requires_grad=True)  # Weight\n",
    "    b = Variable(0.0, requires_grad=True)  # Bias\n",
    "    \n",
    "    # Training loop\n",
    "    learning_rate = 0.01\n",
    "    num_epochs = 100\n",
    "    \n",
    "    print(\"📊 Training model...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = Variable(0.0, requires_grad=False)\n",
    "        \n",
    "        # Forward pass for all data points\n",
    "        for x_val, y_val in zip(x_data, y_data):\n",
    "            x = Variable(x_val, requires_grad=False)\n",
    "            y_target = Variable(y_val, requires_grad=False)\n",
    "            \n",
    "            # Prediction: y_pred = w * x + b\n",
    "            y_pred = add(multiply(w, x), b)\n",
    "            \n",
    "            # Loss: MSE = (y_pred - y_target)^2\n",
    "            diff = subtract(y_pred, y_target)\n",
    "            loss = multiply(diff, diff)\n",
    "            \n",
    "            # Accumulate loss\n",
    "            total_loss = add(total_loss, loss)\n",
    "        \n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        w.data._data -= learning_rate * w.grad.data.data\n",
    "        b.data._data -= learning_rate * b.grad.data.data\n",
    "        \n",
    "        # Zero gradients for next iteration\n",
    "        zero_gradients([w, b])\n",
    "        \n",
    "        # Print progress\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"   Epoch {epoch}: Loss = {total_loss.data.data.item():.4f}, w = {w.data.data.item():.4f}, b = {b.data.data.item():.4f}\")\n",
    "    \n",
    "    # Check final parameters\n",
    "    print(\"📊 Checking final parameters...\")\n",
    "    final_w = w.data.data.item()\n",
    "    final_b = b.data.data.item()\n",
    "    \n",
    "    # Should be close to true values: w=2, b=1\n",
    "    assert abs(final_w - 2.0) < 0.5, f\"Weight not learned correctly: expected ~2.0, got {final_w}\"\n",
    "    assert abs(final_b - 1.0) < 0.5, f\"Bias not learned correctly: expected ~1.0, got {final_b}\"\n",
    "    \n",
    "    print(f\"✅ Model learned: w = {final_w:.3f}, b = {final_b:.3f}\")\n",
    "    print(\"✅ Complete ML pipeline test passed!\")\n",
    "    \n",
    "    # Test prediction on new data\n",
    "    print(\"📊 Testing prediction on new data...\")\n",
    "    x_test = Variable(6.0, requires_grad=False)\n",
    "    y_pred = add(multiply(w, x_test), b)\n",
    "    expected_pred = 2.0 * 6.0 + 1.0  # True function value\n",
    "    \n",
    "    print(f\"   Prediction for x=6: {y_pred.data.data.item():.3f} (expected ~{expected_pred})\")\n",
    "    assert abs(y_pred.data.data.item() - expected_pred) < 1.0, \"Prediction accuracy insufficient\"\n",
    "    \n",
    "    print(\"🎉 Complete ML pipeline test passed!\")\n",
    "    return True\n",
    "\n",
    "# Run the test\n",
    "success = test_complete_ml_pipeline() "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
