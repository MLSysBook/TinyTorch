# TinyTorch Module Metadata
# Essential system information for CLI tools and build systems

name: "attention"
title: "Attention"
description: "Core attention mechanism and masking utilities"

# Dependencies - Used by CLI for module ordering and prerequisites
dependencies:
  prerequisites: ["setup", "tensor", "activations", "layers", "dense", "spatial"]
  enables: ["training", "transformers", "nlp", "multimodal"]

# Package Export - What gets built into tinytorch package
exports_to: "tinytorch.core.attention"

# File Structure - What files exist in this module
files:
  dev_file: "attention_dev.py"
  readme: "README.md"
  tests: "inline"

# Educational Metadata
difficulty: "⭐⭐⭐"
time_estimate: "4-5 hours"

# Components - What's implemented in this module
components:
  - "scaled_dot_product_attention"
  - "SelfAttention"
  - "create_causal_mask"
  - "create_padding_mask"
  - "create_bidirectional_mask" 