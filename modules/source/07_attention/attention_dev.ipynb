{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2085540",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Attention - The Foundation of Modern AI\n",
    "\n",
    "Welcome to the Attention module! This is where you'll implement the revolutionary mechanism that powers ChatGPT, BERT, GPT-4, and virtually all state-of-the-art AI systems.\n",
    "\n",
    "## Learning Goals\n",
    "- Understand attention as dynamic pattern matching with Query, Key, Value projections\n",
    "- Implement scaled dot-product attention from mathematical foundations\n",
    "- Master the attention formula that powers all transformer models\n",
    "- Create masking utilities for different attention patterns\n",
    "- Build the foundation for understanding modern AI architectures\n",
    "\n",
    "## Build → Use → Understand\n",
    "1. **Build**: Implement the core attention mechanism from scratch using mathematical principles\n",
    "2. **Use**: Apply attention to sequence tasks and visualize attention patterns\n",
    "3. **Understand**: How attention revolutionized AI by enabling global context modeling\n",
    "\n",
    "## What You'll Learn\n",
    "By the end of this module, you'll understand:\n",
    "- How attention enables dynamic focus on relevant input parts\n",
    "- The mathematical foundation behind all transformer models\n",
    "- Why attention is more powerful than fixed convolution kernels\n",
    "- How masking enables different attention patterns (causal, padding)\n",
    "- The building block that powers ChatGPT, BERT, and modern AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f68c38",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "attention-imports",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| default_exp core.attention\n",
    "\n",
    "#| export\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "from typing import List, Union, Optional, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import our building blocks - try package first, then local modules\n",
    "try:\n",
    "    from tinytorch.core.tensor import Tensor\n",
    "except ImportError:\n",
    "    # For development, import from local modules\n",
    "    sys.path.append(os.path.join(os.path.dirname(__file__), '..', '02_tensor'))\n",
    "    from tensor_dev import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eab632",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "attention-setup",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "def _should_show_plots():\n",
    "    \"\"\"Check if we should show plots (disable during testing)\"\"\"\n",
    "    # Check multiple conditions that indicate we're in test mode\n",
    "    is_pytest = (\n",
    "        'pytest' in sys.modules or\n",
    "        'test' in sys.argv or\n",
    "        os.environ.get('PYTEST_CURRENT_TEST') is not None or\n",
    "        any('test' in arg for arg in sys.argv) or\n",
    "        any('pytest' in arg for arg in sys.argv)\n",
    "    )\n",
    "    \n",
    "    # Show plots in development mode (when not in test mode)\n",
    "    return not is_pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47a3e00",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "attention-welcome",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"🔥 TinyTorch Attention Module\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Python version: {sys.version_info.major}.{sys.version_info.minor}\")\n",
    "print(\"Ready to build attention mechanisms that power modern AI!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e97deab",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 📦 Where This Code Lives in the Final Package\n",
    "\n",
    "**Learning Side:** You work in `modules/source/06_attention/attention_dev.py`  \n",
    "**Building Side:** Code exports to `tinytorch.core.attention`\n",
    "\n",
    "```python\n",
    "# Final package structure:\n",
    "from tinytorch.core.attention import (\n",
    "    scaled_dot_product_attention,  # Core attention function\n",
    "    SelfAttention,                 # Self-attention wrapper\n",
    "    create_causal_mask,           # Masking utilities\n",
    "    create_padding_mask\n",
    ")\n",
    "from tinytorch.core.tensor import Tensor  # Foundation\n",
    "```\n",
    "\n",
    "**Why this matters:**\n",
    "- **Learning:** Focused module for deep understanding of core attention\n",
    "- **Production:** Proper organization like PyTorch's attention functions\n",
    "- **Consistency:** All attention mechanisms live together in `core.attention`\n",
    "- **Foundation:** Building block for future transformer modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cb4273",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 1: Understanding Attention - The Revolutionary Mechanism\n",
    "\n",
    "### What is Attention?\n",
    "**Attention** is a mechanism that allows models to dynamically focus on relevant parts of the input. It's like having a spotlight that can shine on different parts of a sequence based on what's most important for the current task.\n",
    "\n",
    "### The Fundamental Insight: Query, Key, Value\n",
    "Attention works through three projections:\n",
    "- **Query (Q)**: \"What am I looking for?\"\n",
    "- **Key (K)**: \"What information is available?\"\n",
    "- **Value (V)**: \"What is the actual content?\"\n",
    "\n",
    "### Real-World Analogy: Library Search\n",
    "Imagine searching in a library:\n",
    "```\n",
    "Query: \"machine learning books\"     ← What you're looking for\n",
    "Keys: [\"AI\", \"ML\", \"physics\", ...] ← Book category labels  \n",
    "Values: [book1, book2, book3, ...]  ← Actual book contents\n",
    "\n",
    "Attention: Look at all keys, find matches with query, \n",
    "          return weighted combination of corresponding values\n",
    "```\n",
    "\n",
    "### The Attention Formula\n",
    "```\n",
    "Attention(Q,K,V) = softmax(QK^T/√d_k)V\n",
    "```\n",
    "\n",
    "**Step by step:**\n",
    "1. **Compute scores**: `QK^T` measures similarity between queries and keys\n",
    "2. **Scale**: Divide by `√d_k` to prevent extremely large values\n",
    "3. **Normalize**: `softmax` converts scores to probabilities\n",
    "4. **Combine**: Weight the values by attention probabilities\n",
    "\n",
    "### Why This Is Revolutionary\n",
    "- **Dynamic weights**: Unlike fixed convolution kernels, attention adapts to input\n",
    "- **Global connectivity**: Any position can attend to any other position directly\n",
    "- **Interpretability**: Attention weights show what the model focuses on\n",
    "- **Scalability**: Works for sequences of varying lengths\n",
    "\n",
    "### Attention vs Convolution\n",
    "| Aspect | Convolution | Attention |\n",
    "|--------|-------------|-----------|\n",
    "| **Receptive field** | Local, grows with depth | Global from layer 1 |\n",
    "| **Computation** | O(n) with kernel size | O(n²) with sequence length |\n",
    "| **Weights** | Fixed learned kernels | Dynamic input-dependent |\n",
    "| **Best for** | Spatial data (images) | Sequential data (text) |\n",
    "\n",
    "Let's implement this step by step!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22047a2e",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 2: Implementing Scaled Dot-Product Attention\n",
    "\n",
    "### The Core Attention Operation\n",
    "This is the mathematical heart of all modern AI systems. Every transformer model (GPT, BERT, etc.) uses this exact operation.\n",
    "\n",
    "### Mathematical Foundation\n",
    "```\n",
    "scores = QK^T / √d_k\n",
    "attention_weights = softmax(scores)\n",
    "output = attention_weights @ V\n",
    "```\n",
    "\n",
    "### Why Scale by √d_k?\n",
    "- **Prevents saturation**: Large dot products → extreme softmax values → vanishing gradients\n",
    "- **Stable training**: Keeps attention weights in a reasonable range\n",
    "- **Mathematical insight**: Compensates for variance growth with dimension\n",
    "\n",
    "Let's build the fundamental attention function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b0186d",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "scaled-dot-product-attention",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def scaled_dot_product_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, \n",
    "                                mask: Optional[np.ndarray] = None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention - The foundation of all transformer models.\n",
    "    \n",
    "    This is the exact mechanism used in GPT, BERT, and all modern language models.\n",
    "    \n",
    "    TODO: Implement the core attention mechanism.\n",
    "    \n",
    "    STEP-BY-STEP IMPLEMENTATION:\n",
    "    1. Get d_k (dimension of keys) from Q.shape[-1]\n",
    "    2. Compute attention scores: Q @ K^T (matrix multiplication)\n",
    "    3. Scale by √d_k: scores / sqrt(d_k)\n",
    "    4. Apply mask if provided: set masked positions to -1e9\n",
    "    5. Apply softmax to get attention weights (probabilities)\n",
    "    6. Apply attention weights to values: weights @ V\n",
    "    7. Return (output, attention_weights)\n",
    "    \n",
    "    MATHEMATICAL OPERATION:\n",
    "        Attention(Q,K,V) = softmax(QK^T/√d_k)V\n",
    "    \n",
    "    IMPLEMENTATION HINTS:\n",
    "    - Use np.matmul() for matrix multiplication\n",
    "    - Use np.swapaxes(K, -2, -1) to transpose last two dimensions\n",
    "    - Use math.sqrt() for square root\n",
    "    - Use np.where() for masking: np.where(mask == 0, -1e9, scores)\n",
    "    - Implement softmax manually: exp(x) / sum(exp(x))\n",
    "    - Use keepdims=True for broadcasting\n",
    "    \n",
    "    LEARNING CONNECTIONS:\n",
    "    - This exact function powers ChatGPT, BERT, GPT-4\n",
    "    - The scaling prevents gradient vanishing in deep networks\n",
    "    - Masking enables causal (GPT) and bidirectional (BERT) models\n",
    "    - Attention weights are interpretable - you can visualize them!\n",
    "    \n",
    "    Args:\n",
    "        Q: Query matrix of shape (..., seq_len_q, d_k)\n",
    "        K: Key matrix of shape (..., seq_len_k, d_k)  \n",
    "        V: Value matrix of shape (..., seq_len_v, d_v)\n",
    "        mask: Optional mask of shape (..., seq_len_q, seq_len_k)\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output (..., seq_len_q, d_v)\n",
    "        attention_weights: Attention probabilities (..., seq_len_q, seq_len_k)\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    # Get the dimension for scaling\n",
    "    d_k = Q.shape[-1]\n",
    "    \n",
    "    # Step 1: Compute attention scores (QK^T)\n",
    "    # This measures similarity between each query and each key\n",
    "    scores = np.matmul(Q, np.swapaxes(K, -2, -1))  # (..., seq_len_q, seq_len_k)\n",
    "    \n",
    "    # Step 2: Scale by √d_k to prevent exploding gradients\n",
    "    scores = scores / math.sqrt(d_k)\n",
    "    \n",
    "    # Step 3: Apply mask if provided (for padding or causality)\n",
    "    if mask is not None:\n",
    "        # Replace masked positions with large negative values\n",
    "        # This makes softmax output ~0 for these positions\n",
    "        scores = np.where(mask == 0, -1e9, scores)\n",
    "    \n",
    "    # Step 4: Apply softmax to get attention probabilities\n",
    "    # Each row sums to 1, representing where to focus attention\n",
    "    # Using numerically stable softmax\n",
    "    scores_max = np.max(scores, axis=-1, keepdims=True)\n",
    "    scores_exp = np.exp(scores - scores_max)\n",
    "    attention_weights = scores_exp / np.sum(scores_exp, axis=-1, keepdims=True)\n",
    "    \n",
    "    # Step 5: Apply attention weights to values\n",
    "    # This gives us the weighted combination of values\n",
    "    output = np.matmul(attention_weights, V)  # (..., seq_len_q, d_v)\n",
    "    \n",
    "    return output, attention_weights\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b80a093",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### 🧪 Test Your Attention Implementation\n",
    "\n",
    "Once you implement the `scaled_dot_product_attention` function above, run this cell to test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d25f9f4",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-attention-immediate",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_scaled_dot_product_attention():\n",
    "    \"\"\"Test scaled dot-product attention implementation\"\"\"\n",
    "    print(\"🔬 Unit Test: Scaled Dot-Product Attention...\")\n",
    "\n",
    "    # Create simple test data\n",
    "    seq_len, d_model = 4, 6\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Create Q, K, V matrices\n",
    "    Q = np.random.randn(seq_len, d_model) * 0.1\n",
    "    K = np.random.randn(seq_len, d_model) * 0.1  \n",
    "    V = np.random.randn(seq_len, d_model) * 0.1\n",
    "\n",
    "    print(f\"📊 Input shapes: Q{Q.shape}, K{K.shape}, V{V.shape}\")\n",
    "\n",
    "    # Test attention\n",
    "    output, weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "    print(f\"📊 Output shapes: output{output.shape}, weights{weights.shape}\")\n",
    "\n",
    "    # Verify properties\n",
    "    weights_sum = np.sum(weights, axis=-1)\n",
    "    assert np.allclose(weights_sum, 1.0), f\"Attention weights should sum to 1, got {weights_sum}\"\n",
    "    assert output.shape == (seq_len, d_model), f\"Output shape should be {(seq_len, d_model)}, got {output.shape}\"\n",
    "    assert np.all(weights >= 0), \"All attention weights should be non-negative\"\n",
    "\n",
    "    # Test with mask\n",
    "    mask = np.array([\n",
    "        [1, 1, 0, 0],\n",
    "        [1, 1, 1, 0], \n",
    "        [1, 1, 1, 1],\n",
    "        [1, 1, 1, 1]\n",
    "    ])\n",
    "    output_masked, weights_masked = scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "    # Check that masked positions have near-zero attention\n",
    "    masked_positions = (mask == 0)\n",
    "    masked_weights = weights_masked[masked_positions]\n",
    "    assert np.all(masked_weights < 1e-6), \"Masked positions should have near-zero weights\"\n",
    "\n",
    "    print(\"✅ Attention weights sum to 1: True\")\n",
    "    print(\"✅ Output has correct shape: True\")\n",
    "    print(\"✅ All weights are non-negative: True\")\n",
    "    print(\"✅ Masked positions have near-zero weights: True\")\n",
    "    print(\"📈 Progress: Scaled Dot-Product Attention ✓\")\n",
    "\n",
    "# Run the test\n",
    "test_scaled_dot_product_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdf778d",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 3: Self-Attention - The Most Common Case\n",
    "\n",
    "### What is Self-Attention?\n",
    "**Self-Attention** is the most common use of attention where Q, K, and V all come from the same input sequence. This is what enables models like GPT to understand relationships between words in a sentence.\n",
    "\n",
    "### Why Self-Attention Matters\n",
    "- **Context understanding**: Each word can attend to every other word\n",
    "- **Long-range dependencies**: Connect distant related concepts\n",
    "- **Parallel processing**: Unlike RNNs, all positions computed simultaneously\n",
    "- **Foundation of GPT**: How language models understand context\n",
    "\n",
    "Let's create a convenient wrapper for self-attention!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba64fa66",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "self-attention",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class SelfAttention:\n",
    "    \"\"\"\n",
    "    Self-Attention wrapper - Convenience class for self-attention where Q=K=V.\n",
    "    \n",
    "    This is the most common use case in transformer models where each position\n",
    "    attends to all positions in the same sequence.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int):\n",
    "        \"\"\"\n",
    "        Initialize Self-Attention.\n",
    "        \n",
    "        TODO: Store the model dimension for this self-attention layer.\n",
    "        \n",
    "        STEP-BY-STEP IMPLEMENTATION:\n",
    "        1. Store d_model as an instance variable (self.d_model)\n",
    "        2. Print initialization message for debugging\n",
    "        \n",
    "        EXAMPLE USAGE:\n",
    "        ```python\n",
    "        self_attn = SelfAttention(d_model=64)\n",
    "        output, weights = self_attn(input_sequence)\n",
    "        ```\n",
    "        \n",
    "        IMPLEMENTATION HINTS:\n",
    "        - Simply store d_model parameter: self.d_model = d_model\n",
    "        - Print message: print(f\"🔧 SelfAttention: d_model={d_model}\")\n",
    "        \n",
    "        LEARNING CONNECTIONS:\n",
    "        - This is like nn.MultiheadAttention in PyTorch (but simpler)\n",
    "        - Used in every transformer layer for self-attention\n",
    "        - Foundation for understanding GPT, BERT architectures\n",
    "        \n",
    "        Args:\n",
    "            d_model: Model dimension\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        self.d_model = d_model\n",
    "        print(f\"🔧 SelfAttention: d_model={d_model}\")\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def forward(self, x: np.ndarray, mask: Optional[np.ndarray] = None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Forward pass of self-attention.\n",
    "        \n",
    "        TODO: Apply self-attention where Q=K=V=x.\n",
    "        \n",
    "        STEP-BY-STEP IMPLEMENTATION:\n",
    "        1. Call scaled_dot_product_attention with Q=K=V=x\n",
    "        2. Pass the mask parameter through\n",
    "        3. Return the output and attention weights\n",
    "        \n",
    "        EXAMPLE USAGE:\n",
    "        ```python\n",
    "        x = np.random.randn(seq_len, d_model)  # Input sequence\n",
    "        output, weights = self_attn.forward(x)\n",
    "        # weights[i,j] = how much position i attends to position j\n",
    "        ```\n",
    "        \n",
    "        IMPLEMENTATION HINTS:\n",
    "        - Use the function you implemented above\n",
    "        - Self-attention means: Q = K = V = x\n",
    "        - Return: scaled_dot_product_attention(x, x, x, mask)\n",
    "        \n",
    "        LEARNING CONNECTIONS:\n",
    "        - This is how transformers process sequences\n",
    "        - Each position can attend to any other position\n",
    "        - Enables understanding of long-range dependencies\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor (..., seq_len, d_model)\n",
    "            mask: Optional attention mask\n",
    "            \n",
    "        Returns:\n",
    "            output: Self-attention output (..., seq_len, d_model)\n",
    "            attention_weights: Attention weights\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        # Self-attention: Q = K = V = x\n",
    "        return scaled_dot_product_attention(x, x, x, mask)\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def __call__(self, x: np.ndarray, mask: Optional[np.ndarray] = None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Make the class callable.\"\"\"\n",
    "        return self.forward(x, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77435888",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### 🧪 Test Your Self-Attention Implementation\n",
    "\n",
    "Once you implement the SelfAttention class above, run this cell to test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decc3f0c",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-self-attention-immediate",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_self_attention():\n",
    "    \"\"\"Test self-attention wrapper\"\"\"\n",
    "    print(\"🔬 Unit Test: Self-Attention...\")\n",
    "\n",
    "    # Test parameters\n",
    "    d_model = 32\n",
    "    seq_len = 8\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Create test data (like word embeddings)\n",
    "    x = np.random.randn(seq_len, d_model) * 0.1\n",
    "\n",
    "    print(f\"📊 Test setup: d_model={d_model}, seq_len={seq_len}\")\n",
    "\n",
    "    # Create self-attention\n",
    "    self_attn = SelfAttention(d_model)\n",
    "\n",
    "    # Test forward pass\n",
    "    output, weights = self_attn(x)\n",
    "\n",
    "    print(f\"📊 Output shapes: output{output.shape}, weights{weights.shape}\")\n",
    "\n",
    "    # Verify properties\n",
    "    assert output.shape == x.shape, f\"Output shape should match input shape {x.shape}, got {output.shape}\"\n",
    "    assert weights.shape == (seq_len, seq_len), f\"Attention weights shape should be {(seq_len, seq_len)}, got {weights.shape}\"\n",
    "    assert np.allclose(np.sum(weights, axis=-1), 1.0), \"Attention weights should sum to 1\"\n",
    "    assert weights.shape[0] == weights.shape[1], \"Self-attention weights should be square matrix\"\n",
    "\n",
    "    print(\"✅ Output shape preserved: True\")\n",
    "    print(\"✅ Attention weights correct shape: True\")\n",
    "    print(\"✅ Attention weights sum to 1: True\")\n",
    "    print(\"✅ Self-attention is symmetric operation: True\")\n",
    "    print(\"📈 Progress: Self-Attention ✓\")\n",
    "\n",
    "# Run the test\n",
    "test_self_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9452d069",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 4: Attention Masking - Controlling Information Flow\n",
    "\n",
    "### Why Masking Matters\n",
    "Masking allows us to control which positions can attend to which other positions:\n",
    "\n",
    "1. **Causal Masking**: For autoregressive models (like GPT) - can't see future tokens\n",
    "2. **Padding Masking**: Ignore padding tokens in variable-length sequences\n",
    "3. **Custom Masking**: Application-specific attention patterns\n",
    "\n",
    "### Types of Masks\n",
    "- **Causal (Lower Triangular)**: Position i can only attend to positions ≤ i\n",
    "- **Padding**: Mask out padding tokens so they don't affect attention\n",
    "- **Bidirectional**: All positions can attend to all positions (like BERT)\n",
    "\n",
    "Let's implement these essential masking utilities!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0859ba6",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "attention-masking",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_causal_mask(seq_len: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create a causal (lower triangular) mask for autoregressive models.\n",
    "    \n",
    "    Used in models like GPT where each position can only attend to \n",
    "    previous positions, not future ones.\n",
    "    \n",
    "    TODO: Create a lower triangular matrix of ones.\n",
    "    \n",
    "    STEP-BY-STEP IMPLEMENTATION:\n",
    "    1. Use np.tril() to create lower triangular matrix\n",
    "    2. Create matrix of ones with shape (seq_len, seq_len)\n",
    "    3. Return the lower triangular part\n",
    "    \n",
    "    EXAMPLE USAGE:\n",
    "    ```python\n",
    "    mask = create_causal_mask(4)\n",
    "    # mask = [[1, 0, 0, 0],\n",
    "    #         [1, 1, 0, 0], \n",
    "    #         [1, 1, 1, 0],\n",
    "    #         [1, 1, 1, 1]]\n",
    "    ```\n",
    "    \n",
    "    IMPLEMENTATION HINTS:\n",
    "    - Use np.ones((seq_len, seq_len)) to create matrix of ones\n",
    "    - Use np.tril() to get lower triangular part\n",
    "    - Or combine: np.tril(np.ones((seq_len, seq_len)))\n",
    "    \n",
    "    LEARNING CONNECTIONS:\n",
    "    - Used in GPT for autoregressive generation\n",
    "    - Prevents looking into the future during training\n",
    "    - Essential for language modeling tasks\n",
    "    \n",
    "    Args:\n",
    "        seq_len: Sequence length\n",
    "        \n",
    "    Returns:\n",
    "        mask: Causal mask (seq_len, seq_len) with 1s for allowed positions, 0s for blocked\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    return np.tril(np.ones((seq_len, seq_len)))\n",
    "    ### END SOLUTION\n",
    "\n",
    "#| export  \n",
    "def create_padding_mask(lengths: List[int], max_length: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create padding mask for variable-length sequences.\n",
    "    \n",
    "    TODO: Create mask that ignores padding tokens.\n",
    "    \n",
    "    STEP-BY-STEP IMPLEMENTATION:\n",
    "    1. Initialize zero array with shape (batch_size, max_length, max_length)\n",
    "    2. For each sequence in the batch, set valid positions to 1\n",
    "    3. Valid positions are [:length, :length] for each sequence\n",
    "    4. Return the mask array\n",
    "    \n",
    "    EXAMPLE USAGE:\n",
    "    ```python\n",
    "    lengths = [3, 2, 4]  # Actual sequence lengths\n",
    "    mask = create_padding_mask(lengths, max_length=4)\n",
    "    # For sequence 0 (length=3): positions [0,1,2] can attend to [0,1,2]\n",
    "    # For sequence 1 (length=2): positions [0,1] can attend to [0,1] \n",
    "    ```\n",
    "    \n",
    "    IMPLEMENTATION HINTS:\n",
    "    - batch_size = len(lengths)\n",
    "    - Use np.zeros((batch_size, max_length, max_length))\n",
    "    - Loop through lengths: for i, length in enumerate(lengths)\n",
    "    - Set valid region: mask[i, :length, :length] = 1\n",
    "    \n",
    "    LEARNING CONNECTIONS:\n",
    "    - Used when sequences have different lengths\n",
    "    - Prevents attention to padding tokens\n",
    "    - Essential for efficient batch processing\n",
    "    \n",
    "    Args:\n",
    "        lengths: List of actual sequence lengths\n",
    "        max_length: Maximum sequence length (padded length)\n",
    "        \n",
    "    Returns:\n",
    "        mask: Padding mask (batch_size, max_length, max_length)\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    batch_size = len(lengths)\n",
    "    mask = np.zeros((batch_size, max_length, max_length))\n",
    "    \n",
    "    for i, length in enumerate(lengths):\n",
    "        mask[i, :length, :length] = 1\n",
    "    \n",
    "    return mask\n",
    "    ### END SOLUTION\n",
    "\n",
    "#| export\n",
    "def create_bidirectional_mask(seq_len: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create a bidirectional mask where all positions can attend to all positions.\n",
    "    \n",
    "    Used in models like BERT for bidirectional context understanding.\n",
    "    \n",
    "    TODO: Create a matrix of all ones.\n",
    "    \n",
    "    STEP-BY-STEP IMPLEMENTATION:\n",
    "    1. Use np.ones() to create matrix of all ones\n",
    "    2. Shape should be (seq_len, seq_len)\n",
    "    3. Return the matrix\n",
    "    \n",
    "    EXAMPLE USAGE:\n",
    "    ```python\n",
    "    mask = create_bidirectional_mask(3)\n",
    "    # mask = [[1, 1, 1],\n",
    "    #         [1, 1, 1],\n",
    "    #         [1, 1, 1]]\n",
    "    ```\n",
    "    \n",
    "    IMPLEMENTATION HINTS:\n",
    "    - Very simple: np.ones((seq_len, seq_len))\n",
    "    - All positions can attend to all positions\n",
    "    \n",
    "    LEARNING CONNECTIONS:\n",
    "    - Used in BERT for bidirectional understanding\n",
    "    - Allows looking at past and future context\n",
    "    - Good for understanding tasks, not generation\n",
    "    \n",
    "    Args:\n",
    "        seq_len: Sequence length\n",
    "        \n",
    "    Returns:\n",
    "        mask: All-ones mask (seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    return np.ones((seq_len, seq_len))\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73de5db9",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### 🧪 Test Your Masking Functions\n",
    "\n",
    "Once you implement the masking functions above, run this cell to test them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e4efdf",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-masking-immediate",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_attention_masking():\n",
    "    \"\"\"Test attention masking utilities\"\"\"\n",
    "    print(\"🔬 Unit Test: Attention Masking...\")\n",
    "\n",
    "    # Test causal mask\n",
    "    seq_len = 5\n",
    "    causal_mask = create_causal_mask(seq_len)\n",
    "\n",
    "    print(f\"📊 Causal mask for seq_len={seq_len}:\")\n",
    "    print(causal_mask)\n",
    "\n",
    "    # Verify causal mask properties\n",
    "    assert np.allclose(causal_mask, np.tril(causal_mask)), \"Causal mask should be lower triangular\"\n",
    "    assert causal_mask.shape == (seq_len, seq_len), f\"Causal mask should have shape {(seq_len, seq_len)}\"\n",
    "    assert np.all(np.triu(causal_mask, k=1) == 0), \"Causal mask upper triangle should be zeros\"\n",
    "\n",
    "    # Test padding mask\n",
    "    lengths = [5, 3, 4]\n",
    "    max_length = 5\n",
    "    padding_mask = create_padding_mask(lengths, max_length)\n",
    "\n",
    "    print(f\"📊 Padding mask for lengths {lengths}, max_length={max_length}:\")\n",
    "    print(\"Mask for sequence 0 (length 5):\")\n",
    "    print(padding_mask[0])\n",
    "    print(\"Mask for sequence 1 (length 3):\")\n",
    "    print(padding_mask[1])\n",
    "\n",
    "    # Verify padding mask properties\n",
    "    assert padding_mask.shape == (3, max_length, max_length), f\"Padding mask should have shape {(3, max_length, max_length)}\"\n",
    "    assert np.all(padding_mask[0] == 1), \"Full-length sequence should be all ones\"\n",
    "    assert np.all(padding_mask[1, 3:, :] == 0), \"Short sequence should have zeros in padding area\"\n",
    "\n",
    "    # Test bidirectional mask\n",
    "    bidirectional_mask = create_bidirectional_mask(seq_len)\n",
    "    assert np.all(bidirectional_mask == 1), \"Bidirectional mask should be all ones\"\n",
    "    assert bidirectional_mask.shape == (seq_len, seq_len), f\"Bidirectional mask should have shape {(seq_len, seq_len)}\"\n",
    "\n",
    "    print(\"✅ Causal mask is lower triangular: True\")\n",
    "    print(\"✅ Causal mask has correct shape: True\")\n",
    "    print(\"✅ Causal mask upper triangle is zeros: True\")\n",
    "    print(\"✅ Padding mask has correct shape: True\")\n",
    "    print(\"✅ Full-length sequence is all ones: True\")\n",
    "    print(\"✅ Short sequence has zeros in padding area: True\")\n",
    "    print(\"✅ Bidirectional mask is all ones: True\")\n",
    "    print(\"✅ Bidirectional mask has correct shape: True\")\n",
    "    print(\"📈 Progress: Attention Masking ✓\")\n",
    "\n",
    "# Run the test\n",
    "test_attention_masking()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0186421",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 5: Complete System Integration Test\n",
    "\n",
    "### Bringing It All Together\n",
    "Let's test all components working together in a realistic scenario similar to how they would be used in actual transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884802a7",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "test-integration-final",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_complete_attention_system():\n",
    "    \"\"\"Test the complete attention system working together\"\"\"\n",
    "    print(\"🔬 Unit Test: Complete Attention System Integration...\")\n",
    "\n",
    "    # Test parameters\n",
    "    d_model = 64\n",
    "    seq_len = 16\n",
    "    batch_size = 2\n",
    "    np.random.seed(42)\n",
    "\n",
    "    print(f\"📊 Integration test: d_model={d_model}, seq_len={seq_len}, batch_size={batch_size}\")\n",
    "\n",
    "    # Step 1: Create input embeddings (simulating word embeddings)\n",
    "    embeddings = np.random.randn(batch_size, seq_len, d_model) * 0.1\n",
    "    print(f\"📊 Input embeddings: {embeddings.shape}\")\n",
    "\n",
    "    # Step 2: Test basic attention\n",
    "    output, attention_weights = scaled_dot_product_attention(embeddings, embeddings, embeddings)\n",
    "    assert output.shape == embeddings.shape, \"Basic attention should preserve shape\"\n",
    "    print(f\"✅ Basic attention works: {output.shape}\")\n",
    "\n",
    "    # Step 3: Test self-attention wrapper\n",
    "    self_attn = SelfAttention(d_model)\n",
    "    self_output, self_weights = self_attn(embeddings[0])  # Single batch item\n",
    "    assert self_output.shape == (seq_len, d_model), \"Self-attention should preserve shape\"\n",
    "    print(f\"✅ Self-attention output: {self_output.shape}\")\n",
    "\n",
    "    # Step 4: Test with causal mask (like GPT)\n",
    "    causal_mask = create_causal_mask(seq_len)\n",
    "    causal_output, causal_weights = scaled_dot_product_attention(\n",
    "        embeddings[0], embeddings[0], embeddings[0], causal_mask\n",
    "    )\n",
    "    assert causal_output.shape == (seq_len, d_model), \"Causal attention should preserve shape\"\n",
    "    print(f\"✅ Causal attention works: {causal_output.shape}\")\n",
    "\n",
    "    # Step 5: Test with padding mask (variable lengths)\n",
    "    lengths = [seq_len, seq_len-3]  # Different sequence lengths\n",
    "    padding_mask = create_padding_mask(lengths, seq_len)\n",
    "    padded_output, padded_weights = scaled_dot_product_attention(\n",
    "        embeddings[0], embeddings[0], embeddings[0], padding_mask[0]\n",
    "    )\n",
    "    assert padded_output.shape == (seq_len, d_model), \"Padding attention should preserve shape\"\n",
    "    print(f\"✅ Padding mask works: {padded_output.shape}\")\n",
    "\n",
    "    # Step 6: Verify all outputs have correct properties\n",
    "    assert np.allclose(np.sum(attention_weights, axis=-1), 1.0), \"All attention weights should sum to 1\"\n",
    "    assert output.shape == embeddings.shape, \"All outputs should preserve input shape\"\n",
    "    assert np.all(np.triu(causal_weights, k=1) < 1e-6), \"Causal masking should work\"\n",
    "\n",
    "    print(\"✅ All attention weights sum to 1: True\")\n",
    "    print(\"✅ All outputs preserve input shape: True\")\n",
    "    print(\"✅ Causal masking works: True\")\n",
    "    print(\"📈 Progress: Complete Attention System ✓\")\n",
    "\n",
    "# Run the test\n",
    "test_complete_attention_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe63253",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 🎯 Attention Behavior Analysis\n",
    "\n",
    "Let's create a simple example to see what attention patterns emerge and understand the behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967e42ed",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "attention-analysis",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"🎯 Attention behavior analysis:\")\n",
    "\n",
    "# Create a simple sequence with clear patterns\n",
    "simple_seq = np.array([\n",
    "    [1, 0, 0, 0],  # Position 0: [1, 0, 0, 0]\n",
    "    [0, 1, 0, 0],  # Position 1: [0, 1, 0, 0]  \n",
    "    [0, 0, 1, 0],  # Position 2: [0, 0, 1, 0]\n",
    "    [1, 0, 0, 0],  # Position 3: [1, 0, 0, 0] (same as position 0)\n",
    "])\n",
    "\n",
    "print(f\"🎯 Simple test sequence shape: {simple_seq.shape}\")\n",
    "\n",
    "# Apply attention\n",
    "output, weights = scaled_dot_product_attention(simple_seq, simple_seq, simple_seq)\n",
    "\n",
    "print(f\"🎯 Attention pattern analysis:\")\n",
    "print(f\"Position 0 attends most to position: {np.argmax(weights[0])}\")\n",
    "print(f\"Position 3 attends most to position: {np.argmax(weights[3])}\")\n",
    "print(f\"✅ Positions with same content should attend to each other!\")\n",
    "\n",
    "# Test with causal masking\n",
    "causal_mask = create_causal_mask(4)\n",
    "output_causal, weights_causal = scaled_dot_product_attention(simple_seq, simple_seq, simple_seq, causal_mask)\n",
    "\n",
    "print(f\"🎯 With causal masking:\")\n",
    "print(f\"Position 3 can only attend to positions 0-3: {np.sum(weights_causal[3, :]) > 0.99}\")\n",
    "\n",
    "if _should_show_plots():\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(weights, cmap='Blues')\n",
    "    plt.title('Full Attention Weights\\n(Darker = Higher Attention)')\n",
    "    plt.xlabel('Key Position')\n",
    "    plt.ylabel('Query Position')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            plt.text(j, i, f'{weights[i,j]:.2f}', \n",
    "                    ha='center', va='center', \n",
    "                    color='white' if weights[i,j] > 0.5 else 'black')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(weights_causal, cmap='Blues')\n",
    "    plt.title('Causal Attention Weights\\n(Upper triangle masked)')\n",
    "    plt.xlabel('Key Position')\n",
    "    plt.ylabel('Query Position')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(weights[0], 'o-', label='Position 0 attention')\n",
    "    plt.plot(weights[3], 's-', label='Position 3 attention')\n",
    "    plt.xlabel('Attending to Position')\n",
    "    plt.ylabel('Attention Weight')\n",
    "    plt.title('Attention Distribution')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"🎯 Attention learns to focus on similar content!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🔥 ATTENTION MODULE COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "print(\"✅ Scaled dot-product attention\")\n",
    "print(\"✅ Self-attention wrapper\") \n",
    "print(\"✅ Causal masking\")\n",
    "print(\"✅ Padding masking\")\n",
    "print(\"✅ Bidirectional masking\")\n",
    "print(\"✅ Attention visualization\")\n",
    "print(\"✅ Complete integration tests\")\n",
    "print(\"\\nYou now understand the core mechanism powering modern AI! 🚀\")\n",
    "print(\"Next: Learn how to build complete transformer models using this foundation.\")\n",
    "\n",
    "def test_attention_mechanism():\n",
    "    \"\"\"Test attention mechanism implementation.\"\"\"\n",
    "    print(\"🔬 Unit Test: Attention Mechanism...\")\n",
    "    \n",
    "    # Test basic attention\n",
    "    Q = np.random.randn(4, 6) * 0.1\n",
    "    K = np.random.randn(4, 6) * 0.1  \n",
    "    V = np.random.randn(4, 6) * 0.1\n",
    "    output, weights = scaled_dot_product_attention(Q, K, V)\n",
    "    \n",
    "    assert output.shape == (4, 6), \"Attention should produce correct output shape\"\n",
    "    assert weights.shape == (4, 4), \"Attention weights should be square matrix\"\n",
    "    assert np.allclose(np.sum(weights, axis=-1), 1.0), \"Attention weights should sum to 1\"\n",
    "    \n",
    "    print(\"✅ Attention mechanism works correctly\")\n",
    "\n",
    "def test_self_attention_wrapper():\n",
    "    \"\"\"Test self-attention wrapper implementation.\"\"\"\n",
    "    print(\"🔬 Unit Test: Self-Attention Wrapper...\")\n",
    "    \n",
    "    # Test self-attention\n",
    "    self_attn = SelfAttention(d_model=32)\n",
    "    x = np.random.randn(8, 32) * 0.1\n",
    "    output, weights = self_attn(x)\n",
    "    \n",
    "    assert output.shape == x.shape, \"Self-attention should preserve input shape\"\n",
    "    assert weights.shape == (8, 8), \"Self-attention weights should be square\"\n",
    "    assert np.allclose(np.sum(weights, axis=-1), 1.0), \"Weights should sum to 1\"\n",
    "    \n",
    "    print(\"✅ Self-attention wrapper works correctly\")\n",
    "\n",
    "def test_masking_utilities():\n",
    "    \"\"\"Test attention masking utilities.\"\"\"\n",
    "    print(\"🔬 Unit Test: Masking Utilities...\")\n",
    "    \n",
    "    # Test causal mask\n",
    "    causal_mask = create_causal_mask(4)\n",
    "    assert np.allclose(causal_mask, np.tril(causal_mask)), \"Causal mask should be lower triangular\"\n",
    "    \n",
    "    # Test padding mask  \n",
    "    padding_mask = create_padding_mask([3, 2], 4)\n",
    "    assert padding_mask.shape == (2, 4, 4), \"Padding mask should have correct shape\"\n",
    "    \n",
    "    # Test bidirectional mask\n",
    "    bidirectional_mask = create_bidirectional_mask(3)\n",
    "    assert np.all(bidirectional_mask == 1), \"Bidirectional mask should be all ones\"\n",
    "    \n",
    "    print(\"✅ Masking utilities work correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf6ec9a",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 🧪 Module Testing\n",
    "\n",
    "Time to test your implementation! This section uses TinyTorch's standardized testing framework to ensure your implementation works correctly.\n",
    "\n",
    "**This testing section is locked** - it provides consistent feedback across all modules and cannot be modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a48a5e",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "standardized-testing",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STANDARDIZED MODULE TESTING - DO NOT MODIFY\n",
    "# This cell is locked to ensure consistent testing across all TinyTorch modules\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from tito.tools.testing import run_module_tests_auto\n",
    "    \n",
    "    # Automatically discover and run all tests in this module\n",
    "    success = run_module_tests_auto(\"Attention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926deae2",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 🎯 Module Summary\n",
    "\n",
    "Congratulations! You've successfully implemented the revolutionary attention mechanism that powers all modern AI systems:\n",
    "\n",
    "### What You've Accomplished\n",
    "✅ **Scaled Dot-Product Attention**: Implemented the mathematical core of all transformer models  \n",
    "✅ **Self-Attention Wrapper**: Built the mechanism that enables sequence understanding  \n",
    "✅ **Attention Masking**: Created causal, padding, and bidirectional attention patterns  \n",
    "✅ **Complete Integration**: Tested all components working together seamlessly  \n",
    "✅ **Real Applications**: Applied attention to sequence processing and pattern matching\n",
    "\n",
    "### Key Concepts You've Learned\n",
    "- **Attention as dynamic pattern matching**: Query-Key-Value projections enable adaptive focus\n",
    "- **Mathematical foundation**: Attention(Q,K,V) = softmax(QK^T/√d_k)V powers all modern AI\n",
    "- **Global connectivity**: Unlike convolution, attention connects all positions directly\n",
    "- **Interpretability**: Attention weights reveal what the model focuses on\n",
    "- **Masking mechanisms**: Control information flow for different model architectures\n",
    "\n",
    "### Mathematical Foundations\n",
    "- **Attention formula**: The exact operation used in ChatGPT, BERT, GPT-4\n",
    "- **Scaling factor**: √d_k prevents gradient vanishing in deep networks\n",
    "- **Softmax normalization**: Converts similarity scores to probability distributions\n",
    "- **Matrix operations**: Efficient parallel computation of all attention heads\n",
    "\n",
    "### Real-World Applications\n",
    "- **Language models**: ChatGPT, GPT-4, BERT use this exact mechanism\n",
    "- **Machine translation**: Google Translate's transformer architecture\n",
    "- **Computer vision**: Vision Transformers (ViTs) for image classification\n",
    "- **Multimodal AI**: DALL-E, CLIP combining text and image understanding\n",
    "\n",
    "### Attention vs. Convolution Insights\n",
    "- **Receptive field**: Attention is global from layer 1, convolution is local\n",
    "- **Computation**: Attention is O(n²), convolution is O(n) with kernel size\n",
    "- **Weights**: Attention weights are dynamic and input-dependent\n",
    "- **Best applications**: Attention excels at sequential/relational data\n",
    "\n",
    "### Architecture Design Patterns\n",
    "- **Self-attention**: Most common pattern where Q=K=V=input\n",
    "- **Causal masking**: Enables autoregressive generation (GPT-style models)\n",
    "- **Bidirectional**: Allows full context access (BERT-style models)\n",
    "- **Padding masks**: Handle variable-length sequences efficiently\n",
    "\n",
    "### Performance Characteristics\n",
    "- **Quadratic scaling**: Memory and computation grow with sequence length squared\n",
    "- **Parallelization**: All positions computed simultaneously (unlike RNNs)\n",
    "- **Memory efficiency**: Attention weights require careful management\n",
    "- **Gradient flow**: Direct connections enable training very deep networks\n",
    "\n",
    "### Transformer Building Blocks\n",
    "Your attention implementation is the foundation for:\n",
    "- **Multi-head attention**: Multiple attention heads in parallel\n",
    "- **Transformer blocks**: Attention + feedforward + residual connections\n",
    "- **Positional encoding**: Adding sequence position information\n",
    "- **Complete transformers**: Full encoder-decoder architectures\n",
    "\n",
    "### Next Steps\n",
    "1. **Export your code**: Use NBDev to export to the `tinytorch` package\n",
    "2. **Test your implementation**: Run the complete test suite\n",
    "3. **Build transformer architectures**: \n",
    "   ```python\n",
    "   from tinytorch.core.attention import scaled_dot_product_attention, SelfAttention\n",
    "   from tinytorch.core.attention import create_causal_mask, create_padding_mask\n",
    "   \n",
    "   # Create self-attention\n",
    "   self_attn = SelfAttention(d_model=512)\n",
    "   \n",
    "   # Process sequence with causal masking (GPT-style)\n",
    "   mask = create_causal_mask(seq_len)\n",
    "   output, weights = self_attn(embeddings, mask)\n",
    "   \n",
    "   # Visualize attention patterns\n",
    "   plt.imshow(weights, cmap='Blues')\n",
    "   plt.title('Attention Patterns')\n",
    "   ```\n",
    "4. **Explore advanced transformers**: Multi-head attention, positional encoding, full transformer blocks!\n",
    "\n",
    "### The Revolutionary Impact\n",
    "You've implemented the mechanism that:\n",
    "- **Revolutionized NLP**: Enabled ChatGPT, GPT-4, BERT breakthrough performance\n",
    "- **Transformed computer vision**: Vision Transformers (ViTs) now compete with CNNs\n",
    "- **Powers modern AI**: Almost every state-of-the-art model uses attention\n",
    "- **Enables interpretability**: Attention weights show what AI models focus on\n",
    "\n",
    "**Ready for the next challenge?** Let's build complete transformer architectures using your attention foundation!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
