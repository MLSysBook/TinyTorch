{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6dc5af9",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Optimizers - Gradient-Based Parameter Updates\n",
    "\n",
    "Welcome to the Optimizers module! This is where neural networks learn to improve through intelligent parameter updates.\n",
    "\n",
    "## Learning Goals\n",
    "- Understand gradient descent and how optimizers use gradients to update parameters\n",
    "- Implement SGD with momentum for accelerated convergence\n",
    "- Build Adam optimizer with adaptive learning rates\n",
    "- Master learning rate scheduling strategies\n",
    "- See how optimizers enable effective neural network training\n",
    "\n",
    "## Build ‚Üí Use ‚Üí Analyze\n",
    "1. **Build**: Core optimization algorithms (SGD, Adam)\n",
    "2. **Use**: Apply optimizers to train neural networks\n",
    "3. **Analyze**: Compare optimizer behavior and convergence patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf7767d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "optimizers-imports",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| default_exp core.optimizers\n",
    "\n",
    "#| export\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "from collections import defaultdict\n",
    "\n",
    "# Helper function to set up import paths\n",
    "def setup_import_paths():\n",
    "    \"\"\"Set up import paths for development modules.\"\"\"\n",
    "    import sys\n",
    "    import os\n",
    "    \n",
    "    # Add module directories to path\n",
    "    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "    tensor_dir = os.path.join(base_dir, '01_tensor')\n",
    "    autograd_dir = os.path.join(base_dir, '07_autograd')\n",
    "    \n",
    "    if tensor_dir not in sys.path:\n",
    "        sys.path.append(tensor_dir)\n",
    "    if autograd_dir not in sys.path:\n",
    "        sys.path.append(autograd_dir)\n",
    "\n",
    "# Import our existing components\n",
    "try:\n",
    "    from tinytorch.core.tensor import Tensor\n",
    "    from tinytorch.core.autograd import Variable\n",
    "except ImportError:\n",
    "    # For development, try local imports\n",
    "    try:\n",
    "        setup_import_paths()\n",
    "        from tensor_dev import Tensor\n",
    "        from autograd_dev import Variable\n",
    "    except ImportError:\n",
    "        # Create minimal fallback classes for testing\n",
    "        print(\"Warning: Using fallback classes for testing\")\n",
    "        \n",
    "        class Tensor:\n",
    "            def __init__(self, data):\n",
    "                self.data = np.array(data)\n",
    "                self.shape = self.data.shape\n",
    "            \n",
    "            def __str__(self):\n",
    "                return f\"Tensor({self.data})\"\n",
    "        \n",
    "        class Variable:\n",
    "            def __init__(self, data, requires_grad=True):\n",
    "                if isinstance(data, (int, float)):\n",
    "                    self.data = Tensor([data])\n",
    "                else:\n",
    "                    self.data = Tensor(data)\n",
    "                self.requires_grad = requires_grad\n",
    "                self.grad = None\n",
    "            \n",
    "            def zero_grad(self):\n",
    "                self.grad = None\n",
    "            \n",
    "            def __str__(self):\n",
    "                return f\"Variable({self.data.data})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6056954",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "optimizers-setup",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"üî• TinyTorch Optimizers Module\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Python version: {sys.version_info.major}.{sys.version_info.minor}\")\n",
    "print(\"Ready to build optimization algorithms!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e30a1f6",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## üì¶ Where This Code Lives in the Final Package\n",
    "\n",
    "**Learning Side:** You work in `modules/source/08_optimizers/optimizers_dev.py`  \n",
    "**Building Side:** Code exports to `tinytorch.core.optimizers`\n",
    "\n",
    "```python\n",
    "# Final package structure:\n",
    "from tinytorch.core.optimizers import SGD, Adam, StepLR  # The optimization engines!\n",
    "from tinytorch.core.autograd import Variable  # Gradient computation\n",
    "from tinytorch.core.tensor import Tensor  # Data structures\n",
    "```\n",
    "\n",
    "**Why this matters:**\n",
    "- **Learning:** Focused module for understanding optimization algorithms\n",
    "- **Production:** Proper organization like PyTorch's `torch.optim`\n",
    "- **Consistency:** All optimization algorithms live together in `core.optimizers`\n",
    "- **Foundation:** Enables effective neural network training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c533761b",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## What Are Optimizers?\n",
    "\n",
    "### The Problem: How to Update Parameters\n",
    "Neural networks learn by updating parameters using gradients:\n",
    "```\n",
    "parameter_new = parameter_old - learning_rate * gradient\n",
    "```\n",
    "\n",
    "But **naive gradient descent** has problems:\n",
    "- **Slow convergence**: Takes many steps to reach optimum\n",
    "- **Oscillation**: Bounces around valleys without making progress\n",
    "- **Poor scaling**: Same learning rate for all parameters\n",
    "\n",
    "### The Solution: Smart Optimization\n",
    "**Optimizers** are algorithms that intelligently update parameters:\n",
    "- **Momentum**: Accelerate convergence by accumulating velocity\n",
    "- **Adaptive learning rates**: Different learning rates for different parameters\n",
    "- **Second-order information**: Use curvature to guide updates\n",
    "\n",
    "### Real-World Impact\n",
    "- **SGD**: The foundation of all neural network training\n",
    "- **Adam**: The default optimizer for most deep learning applications\n",
    "- **Learning rate scheduling**: Critical for training stability and performance\n",
    "\n",
    "### What We'll Build\n",
    "1. **SGD**: Stochastic Gradient Descent with momentum\n",
    "2. **Adam**: Adaptive Moment Estimation optimizer\n",
    "3. **StepLR**: Learning rate scheduling\n",
    "4. **Integration**: Complete training loop with optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e465bda2",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## üîß DEVELOPMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6095d52",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 1: Understanding Gradient Descent\n",
    "\n",
    "### What is Gradient Descent?\n",
    "**Gradient descent** finds the minimum of a function by following the negative gradient:\n",
    "\n",
    "```\n",
    "Œ∏_{t+1} = Œ∏_t - Œ± ‚àáf(Œ∏_t)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- Œ∏: Parameters we want to optimize\n",
    "- Œ±: Learning rate (how big steps to take)\n",
    "- ‚àáf(Œ∏): Gradient of loss function with respect to parameters\n",
    "\n",
    "### Why Gradient Descent Works\n",
    "1. **Gradients point uphill**: Negative gradient points toward minimum\n",
    "2. **Iterative improvement**: Each step reduces the loss (in theory)\n",
    "3. **Local convergence**: Finds local minimum with proper learning rate\n",
    "4. **Scalable**: Works with millions of parameters\n",
    "\n",
    "### The Learning Rate Dilemma\n",
    "- **Too large**: Overshoots minimum, diverges\n",
    "- **Too small**: Extremely slow convergence\n",
    "- **Just right**: Steady progress toward minimum\n",
    "\n",
    "### Visual Understanding\n",
    "```\n",
    "Loss landscape: U-shaped curve\n",
    "Start here: ‚Üë\n",
    "Gradient descent: ‚Üì ‚Üí ‚Üì ‚Üí ‚Üì ‚Üí minimum\n",
    "```\n",
    "\n",
    "### Real-World Applications\n",
    "- **Neural networks**: Training any deep learning model\n",
    "- **Machine learning**: Logistic regression, SVM, etc.\n",
    "- **Scientific computing**: Optimization problems in physics, engineering\n",
    "- **Economics**: Portfolio optimization, game theory\n",
    "\n",
    "Let's implement gradient descent to understand it deeply!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9a25b0",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "gradient-descent-function",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def gradient_descent_step(parameter: Variable, learning_rate: float) -> None:\n",
    "    \"\"\"\n",
    "    Perform one step of gradient descent on a parameter.\n",
    "    \n",
    "    Args:\n",
    "        parameter: Variable with gradient information\n",
    "        learning_rate: How much to update parameter\n",
    "    \n",
    "    TODO: Implement basic gradient descent parameter update.\n",
    "    \n",
    "    STEP-BY-STEP IMPLEMENTATION:\n",
    "    1. Check if parameter has a gradient\n",
    "    2. Get current parameter value and gradient\n",
    "    3. Update parameter: new_value = old_value - learning_rate * gradient\n",
    "    4. Update parameter data with new value\n",
    "    5. Handle edge cases (no gradient, invalid values)\n",
    "    \n",
    "    EXAMPLE USAGE:\n",
    "    ```python\n",
    "    # Parameter with gradient\n",
    "    w = Variable(2.0, requires_grad=True)\n",
    "    w.grad = Variable(0.5)  # Gradient from loss\n",
    "    \n",
    "    # Update parameter\n",
    "    gradient_descent_step(w, learning_rate=0.1)\n",
    "    # w.data now contains: 2.0 - 0.1 * 0.5 = 1.95\n",
    "    ```\n",
    "    \n",
    "    IMPLEMENTATION HINTS:\n",
    "    - Check if parameter.grad is not None\n",
    "    - Use parameter.grad.data.data to get gradient value\n",
    "    - Update parameter.data with new Tensor\n",
    "    - Don't modify gradient (it's used for logging)\n",
    "    \n",
    "    LEARNING CONNECTIONS:\n",
    "    - This is the foundation of all neural network training\n",
    "    - PyTorch's optimizer.step() does exactly this\n",
    "    - The learning rate determines convergence speed\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    if parameter.grad is not None:\n",
    "        # Get current parameter value and gradient\n",
    "        current_value = parameter.data.data\n",
    "        gradient_value = parameter.grad.data.data\n",
    "        \n",
    "        # Update parameter: new_value = old_value - learning_rate * gradient\n",
    "        new_value = current_value - learning_rate * gradient_value\n",
    "        \n",
    "        # Update parameter data\n",
    "        parameter.data = Tensor(new_value)\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac173b8",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### üß™ Unit Test: Gradient Descent Step\n",
    "\n",
    "Let's test your gradient descent implementation right away! This is the foundation of all optimization algorithms.\n",
    "\n",
    "**This is a unit test** - it tests one specific function (gradient_descent_step) in isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa8c42f",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": true,
     "grade_id": "test-gradient-descent",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_gradient_descent_step():\n",
    "    \"\"\"Unit test for the basic gradient descent parameter update.\"\"\"\n",
    "    print(\"üî¨ Unit Test: Gradient Descent Step...\")\n",
    "    \n",
    "    # Test basic parameter update\n",
    "    try:\n",
    "        w = Variable(2.0, requires_grad=True)\n",
    "        w.grad = Variable(0.5)  # Positive gradient\n",
    "        \n",
    "        original_value = w.data.data.item()\n",
    "        gradient_descent_step(w, learning_rate=0.1)\n",
    "        new_value = w.data.data.item()\n",
    "        \n",
    "        expected_value = original_value - 0.1 * 0.5  # 2.0 - 0.05 = 1.95\n",
    "        assert abs(new_value - expected_value) < 1e-6, f\"Expected {expected_value}, got {new_value}\"\n",
    "        print(\"‚úÖ Basic parameter update works\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Basic parameter update failed: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Test with negative gradient\n",
    "    try:\n",
    "        w2 = Variable(1.0, requires_grad=True)\n",
    "        w2.grad = Variable(-0.2)  # Negative gradient\n",
    "        \n",
    "        gradient_descent_step(w2, learning_rate=0.1)\n",
    "        expected_value2 = 1.0 - 0.1 * (-0.2)  # 1.0 + 0.02 = 1.02\n",
    "        assert abs(w2.data.data.item() - expected_value2) < 1e-6, \"Negative gradient test failed\"\n",
    "        print(\"‚úÖ Negative gradient handling works\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Negative gradient handling failed: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Test with no gradient (should not update)\n",
    "    try:\n",
    "        w3 = Variable(3.0, requires_grad=True)\n",
    "        w3.grad = None\n",
    "        original_value3 = w3.data.data.item()\n",
    "        \n",
    "        gradient_descent_step(w3, learning_rate=0.1)\n",
    "        assert w3.data.data.item() == original_value3, \"Parameter with no gradient should not update\"\n",
    "        print(\"‚úÖ No gradient case works\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå No gradient case failed: {e}\")\n",
    "        raise\n",
    "\n",
    "    print(\"üéØ Gradient descent step behavior:\")\n",
    "    print(\"   Updates parameters in negative gradient direction\")\n",
    "    print(\"   Uses learning rate to control step size\")\n",
    "    print(\"   Skips updates when gradient is None\")\n",
    "    print(\"üìà Progress: Gradient Descent Step ‚úì\")\n",
    "\n",
    "# Test function defined (called in main block)\n",
    "\n",
    "# Test function is called by auto-discovery system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e54f9ad",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 2: SGD with Momentum\n",
    "\n",
    "### What is SGD?\n",
    "**SGD (Stochastic Gradient Descent)** is the fundamental optimization algorithm:\n",
    "\n",
    "```\n",
    "Œ∏_{t+1} = Œ∏_t - Œ± ‚àáL(Œ∏_t)\n",
    "```\n",
    "\n",
    "### The Problem with Vanilla SGD\n",
    "- **Slow convergence**: Especially in narrow valleys\n",
    "- **Oscillation**: Bounces around without making progress\n",
    "- **Poor conditioning**: Struggles with ill-conditioned problems\n",
    "\n",
    "### The Solution: Momentum\n",
    "**Momentum** accumulates velocity to accelerate convergence:\n",
    "\n",
    "```\n",
    "v_t = Œ≤ v_{t-1} + ‚àáL(Œ∏_t)\n",
    "Œ∏_{t+1} = Œ∏_t - Œ± v_t\n",
    "```\n",
    "\n",
    "Where:\n",
    "- v_t: Velocity (exponential moving average of gradients)\n",
    "- Œ≤: Momentum coefficient (typically 0.9)\n",
    "- Œ±: Learning rate\n",
    "\n",
    "### Why Momentum Works\n",
    "1. **Acceleration**: Builds up speed in consistent directions\n",
    "2. **Dampening**: Reduces oscillations in inconsistent directions\n",
    "3. **Memory**: Remembers previous gradient directions\n",
    "4. **Robustness**: Less sensitive to noisy gradients\n",
    "\n",
    "### Visual Understanding\n",
    "```\n",
    "Without momentum: ‚Üó‚Üô‚Üó‚Üô‚Üó‚Üô (oscillating)\n",
    "With momentum:    ‚Üó‚Üí‚Üí‚Üí‚Üí‚Üí (smooth progress)\n",
    "```\n",
    "\n",
    "### Real-World Applications\n",
    "- **Image classification**: Training ResNet, VGG\n",
    "- **Natural language**: Training RNNs, early transformers\n",
    "- **Classic choice**: Still used when Adam fails\n",
    "- **Large batch training**: Often preferred over Adam\n",
    "\n",
    "Let's implement SGD with momentum!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643bbae5",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "sgd-class",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class SGD:\n",
    "    \"\"\"\n",
    "    SGD Optimizer with Momentum\n",
    "    \n",
    "    Implements stochastic gradient descent with momentum:\n",
    "    v_t = momentum * v_{t-1} + gradient\n",
    "    parameter = parameter - learning_rate * v_t\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, parameters: List[Variable], learning_rate: float = 0.01, \n",
    "                 momentum: float = 0.0, weight_decay: float = 0.0):\n",
    "        \"\"\"\n",
    "        Initialize SGD optimizer.\n",
    "        \n",
    "        Args:\n",
    "            parameters: List of Variables to optimize\n",
    "            learning_rate: Learning rate (default: 0.01)\n",
    "            momentum: Momentum coefficient (default: 0.0)\n",
    "            weight_decay: L2 regularization coefficient (default: 0.0)\n",
    "        \n",
    "        TODO: Implement SGD optimizer initialization.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Store parameters and hyperparameters\n",
    "        2. Initialize momentum buffers for each parameter\n",
    "        3. Set up state tracking for optimization\n",
    "        4. Prepare for step() and zero_grad() methods\n",
    "        \n",
    "        EXAMPLE:\n",
    "        ```python\n",
    "        # Create optimizer\n",
    "        optimizer = SGD([w1, w2, b1, b2], learning_rate=0.01, momentum=0.9)\n",
    "        \n",
    "        # In training loop:\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ```\n",
    "        \n",
    "        HINTS:\n",
    "        - Store parameters as a list\n",
    "        - Initialize momentum buffers as empty dict\n",
    "        - Use parameter id() as key for momentum tracking\n",
    "        - Momentum buffers will be created lazily in step()\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        self.parameters = parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # Initialize momentum buffers (created lazily)\n",
    "        self.momentum_buffers = {}\n",
    "        \n",
    "        # Track optimization steps\n",
    "        self.step_count = 0\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def step(self) -> None:\n",
    "        \"\"\"\n",
    "        Perform one optimization step.\n",
    "        \n",
    "        TODO: Implement SGD parameter update with momentum.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Iterate through all parameters\n",
    "        2. For each parameter with gradient:\n",
    "           a. Get current gradient\n",
    "           b. Apply weight decay if specified\n",
    "           c. Update momentum buffer (or create if first time)\n",
    "           d. Update parameter using momentum\n",
    "        3. Increment step count\n",
    "        \n",
    "        MATHEMATICAL FORMULATION:\n",
    "        - If weight_decay > 0: gradient = gradient + weight_decay * parameter\n",
    "        - momentum_buffer = momentum * momentum_buffer + gradient\n",
    "        - parameter = parameter - learning_rate * momentum_buffer\n",
    "        \n",
    "        IMPLEMENTATION HINTS:\n",
    "        - Use id(param) as key for momentum buffers\n",
    "        - Initialize buffer with zeros if not exists\n",
    "        - Handle case where momentum = 0 (no momentum)\n",
    "        - Update parameter.data with new Tensor\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        for param in self.parameters:\n",
    "            if param.grad is not None:\n",
    "                # Get gradient\n",
    "                gradient = param.grad.data.data\n",
    "                \n",
    "                # Apply weight decay (L2 regularization)\n",
    "                if self.weight_decay > 0:\n",
    "                    gradient = gradient + self.weight_decay * param.data.data\n",
    "                \n",
    "                # Get or create momentum buffer\n",
    "                param_id = id(param)\n",
    "                if param_id not in self.momentum_buffers:\n",
    "                    self.momentum_buffers[param_id] = np.zeros_like(param.data.data)\n",
    "                \n",
    "                # Update momentum buffer\n",
    "                self.momentum_buffers[param_id] = (\n",
    "                    self.momentum * self.momentum_buffers[param_id] + gradient\n",
    "                )\n",
    "                \n",
    "                # Update parameter\n",
    "                param.data = Tensor(\n",
    "                    param.data.data - self.learning_rate * self.momentum_buffers[param_id]\n",
    "                )\n",
    "        \n",
    "        self.step_count += 1\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def zero_grad(self) -> None:\n",
    "        \"\"\"\n",
    "        Zero out gradients for all parameters.\n",
    "        \n",
    "        TODO: Implement gradient zeroing.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Iterate through all parameters\n",
    "        2. Set gradient to None for each parameter\n",
    "        3. This prepares for next backward pass\n",
    "        \n",
    "        IMPLEMENTATION HINTS:\n",
    "        - Simply set param.grad = None\n",
    "        - This is called before loss.backward()\n",
    "        - Essential for proper gradient accumulation\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        for param in self.parameters:\n",
    "            param.grad = None\n",
    "        ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ac7614",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### üß™ Unit Test: SGD Optimizer\n",
    "\n",
    "Let's test your SGD optimizer implementation! This optimizer adds momentum to gradient descent for better convergence.\n",
    "\n",
    "**This is a unit test** - it tests one specific class (SGD) in isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271e3872",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": true,
     "grade_id": "test-sgd",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_sgd_optimizer():\n",
    "    \"\"\"Unit test for the SGD optimizer implementation.\"\"\"\n",
    "    print(\"üî¨ Unit Test: SGD Optimizer...\")\n",
    "    \n",
    "    # Create test parameters\n",
    "    w1 = Variable(1.0, requires_grad=True)\n",
    "    w2 = Variable(2.0, requires_grad=True)\n",
    "    b = Variable(0.5, requires_grad=True)\n",
    "    \n",
    "    # Create optimizer\n",
    "    optimizer = SGD([w1, w2, b], learning_rate=0.1, momentum=0.9)\n",
    "    \n",
    "    # Test zero_grad\n",
    "    try:\n",
    "        w1.grad = Variable(0.1)\n",
    "        w2.grad = Variable(0.2)\n",
    "        b.grad = Variable(0.05)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        assert w1.grad is None, \"Gradient should be None after zero_grad\"\n",
    "        assert w2.grad is None, \"Gradient should be None after zero_grad\"\n",
    "        assert b.grad is None, \"Gradient should be None after zero_grad\"\n",
    "        print(\"‚úÖ zero_grad() works correctly\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå zero_grad() failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Test step with gradients\n",
    "    try:\n",
    "        w1.grad = Variable(0.1)\n",
    "        w2.grad = Variable(0.2)\n",
    "        b.grad = Variable(0.05)\n",
    "        \n",
    "        # First step (no momentum yet)\n",
    "        original_w1 = w1.data.data.item()\n",
    "        original_w2 = w2.data.data.item()\n",
    "        original_b = b.data.data.item()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Check parameter updates\n",
    "        expected_w1 = original_w1 - 0.1 * 0.1  # 1.0 - 0.01 = 0.99\n",
    "        expected_w2 = original_w2 - 0.1 * 0.2  # 2.0 - 0.02 = 1.98\n",
    "        expected_b = original_b - 0.1 * 0.05   # 0.5 - 0.005 = 0.495\n",
    "        \n",
    "        assert abs(w1.data.data.item() - expected_w1) < 1e-6, f\"w1 update failed: expected {expected_w1}, got {w1.data.data.item()}\"\n",
    "        assert abs(w2.data.data.item() - expected_w2) < 1e-6, f\"w2 update failed: expected {expected_w2}, got {w2.data.data.item()}\"\n",
    "        assert abs(b.data.data.item() - expected_b) < 1e-6, f\"b update failed: expected {expected_b}, got {b.data.data.item()}\"\n",
    "        print(\"‚úÖ Parameter updates work correctly\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Parameter updates failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Test momentum buffers\n",
    "    try:\n",
    "        assert len(optimizer.momentum_buffers) == 3, f\"Should have 3 momentum buffers, got {len(optimizer.momentum_buffers)}\"\n",
    "        assert optimizer.step_count == 1, f\"Step count should be 1, got {optimizer.step_count}\"\n",
    "        print(\"‚úÖ Momentum buffers created correctly\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Momentum buffers failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Test step counting\n",
    "    try:\n",
    "        w1.grad = Variable(0.1)\n",
    "        w2.grad = Variable(0.2)\n",
    "        b.grad = Variable(0.05)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        assert optimizer.step_count == 2, f\"Step count should be 2, got {optimizer.step_count}\"\n",
    "        print(\"‚úÖ Step counting works correctly\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Step counting failed: {e}\")\n",
    "        raise\n",
    "\n",
    "    print(\"üéØ SGD optimizer behavior:\")\n",
    "    print(\"   Maintains momentum buffers for accelerated updates\")\n",
    "    print(\"   Tracks step count for learning rate scheduling\")\n",
    "    print(\"   Supports weight decay for regularization\")\n",
    "    print(\"üìà Progress: SGD Optimizer ‚úì\")\n",
    "\n",
    "# Test function defined (called in main block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f913a57",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 3: Adam - Adaptive Learning Rates\n",
    "\n",
    "### What is Adam?\n",
    "**Adam (Adaptive Moment Estimation)** is the most popular optimizer in deep learning:\n",
    "\n",
    "```\n",
    "m_t = Œ≤‚ÇÅ m_{t-1} + (1 - Œ≤‚ÇÅ) ‚àáL(Œ∏_t)        # First moment (momentum)\n",
    "v_t = Œ≤‚ÇÇ v_{t-1} + (1 - Œ≤‚ÇÇ) (‚àáL(Œ∏_t))¬≤     # Second moment (variance)\n",
    "mÃÇ_t = m_t / (1 - Œ≤‚ÇÅ·µó)                      # Bias correction\n",
    "vÃÇ_t = v_t / (1 - Œ≤‚ÇÇ·µó)                      # Bias correction\n",
    "Œ∏_{t+1} = Œ∏_t - Œ± mÃÇ_t / (‚àövÃÇ_t + Œµ)        # Parameter update\n",
    "```\n",
    "\n",
    "### Why Adam is Revolutionary\n",
    "1. **Adaptive learning rates**: Different learning rate for each parameter\n",
    "2. **Momentum**: Accelerates convergence like SGD\n",
    "3. **Variance adaptation**: Scales updates based on gradient variance\n",
    "4. **Bias correction**: Handles initialization bias\n",
    "5. **Robust**: Works well with minimal hyperparameter tuning\n",
    "\n",
    "### The Three Key Ideas\n",
    "1. **First moment (m_t)**: Exponential moving average of gradients (momentum)\n",
    "2. **Second moment (v_t)**: Exponential moving average of squared gradients (variance)\n",
    "3. **Adaptive scaling**: Large gradients ‚Üí small updates, small gradients ‚Üí large updates\n",
    "\n",
    "### Visual Understanding\n",
    "```\n",
    "Parameter with large gradients: zigzag pattern ‚Üí smooth updates\n",
    "Parameter with small gradients: ______ ‚Üí amplified updates\n",
    "```\n",
    "\n",
    "### Real-World Applications\n",
    "- **Deep learning**: Default optimizer for most neural networks\n",
    "- **Computer vision**: Training CNNs, ResNets, Vision Transformers\n",
    "- **Natural language**: Training BERT, GPT, T5\n",
    "- **Transformers**: Essential for attention-based models\n",
    "\n",
    "Let's implement Adam optimizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e795ba61",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "adam-class",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class Adam:\n",
    "    \"\"\"\n",
    "    Adam Optimizer\n",
    "    \n",
    "    Implements Adam algorithm with adaptive learning rates:\n",
    "    - First moment: exponential moving average of gradients\n",
    "    - Second moment: exponential moving average of squared gradients\n",
    "    - Bias correction: accounts for initialization bias\n",
    "    - Adaptive updates: different learning rate per parameter\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, parameters: List[Variable], learning_rate: float = 0.001,\n",
    "                 beta1: float = 0.9, beta2: float = 0.999, epsilon: float = 1e-8,\n",
    "                 weight_decay: float = 0.0):\n",
    "        \"\"\"\n",
    "        Initialize Adam optimizer.\n",
    "        \n",
    "        Args:\n",
    "            parameters: List of Variables to optimize\n",
    "            learning_rate: Learning rate (default: 0.001)\n",
    "            beta1: Exponential decay rate for first moment (default: 0.9)\n",
    "            beta2: Exponential decay rate for second moment (default: 0.999)\n",
    "            epsilon: Small constant for numerical stability (default: 1e-8)\n",
    "            weight_decay: L2 regularization coefficient (default: 0.0)\n",
    "        \n",
    "        TODO: Implement Adam optimizer initialization.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Store parameters and hyperparameters\n",
    "        2. Initialize first moment buffers (m_t)\n",
    "        3. Initialize second moment buffers (v_t)\n",
    "        4. Set up step counter for bias correction\n",
    "        \n",
    "        EXAMPLE:\n",
    "        ```python\n",
    "        # Create Adam optimizer\n",
    "        optimizer = Adam([w1, w2, b1, b2], learning_rate=0.001)\n",
    "        \n",
    "        # In training loop:\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ```\n",
    "        \n",
    "        HINTS:\n",
    "        - Store all hyperparameters\n",
    "        - Initialize moment buffers as empty dicts\n",
    "        - Use parameter id() as key for tracking\n",
    "        - Buffers will be created lazily in step()\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        self.parameters = parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # Initialize moment buffers (created lazily)\n",
    "        self.first_moment = {}   # m_t\n",
    "        self.second_moment = {}  # v_t\n",
    "        \n",
    "        # Track optimization steps for bias correction\n",
    "        self.step_count = 0\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def step(self) -> None:\n",
    "        \"\"\"\n",
    "        Perform one optimization step using Adam algorithm.\n",
    "        \n",
    "        TODO: Implement Adam parameter update.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Increment step count\n",
    "        2. For each parameter with gradient:\n",
    "           a. Get current gradient\n",
    "           b. Apply weight decay if specified\n",
    "           c. Update first moment (momentum)\n",
    "           d. Update second moment (variance)\n",
    "           e. Apply bias correction\n",
    "           f. Update parameter with adaptive learning rate\n",
    "        \n",
    "        MATHEMATICAL FORMULATION:\n",
    "        - m_t = beta1 * m_{t-1} + (1 - beta1) * gradient\n",
    "        - v_t = beta2 * v_{t-1} + (1 - beta2) * gradient^2\n",
    "        - m_hat = m_t / (1 - beta1^t)\n",
    "        - v_hat = v_t / (1 - beta2^t)\n",
    "        - parameter = parameter - learning_rate * m_hat / (sqrt(v_hat) + epsilon)\n",
    "        \n",
    "        IMPLEMENTATION HINTS:\n",
    "        - Use id(param) as key for moment buffers\n",
    "        - Initialize buffers with zeros if not exists\n",
    "        - Use np.sqrt() for square root\n",
    "        - Handle numerical stability with epsilon\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        self.step_count += 1\n",
    "        \n",
    "        for param in self.parameters:\n",
    "            if param.grad is not None:\n",
    "                # Get gradient\n",
    "                gradient = param.grad.data.data\n",
    "                \n",
    "                # Apply weight decay (L2 regularization)\n",
    "                if self.weight_decay > 0:\n",
    "                    gradient = gradient + self.weight_decay * param.data.data\n",
    "                \n",
    "                # Get or create moment buffers\n",
    "                param_id = id(param)\n",
    "                if param_id not in self.first_moment:\n",
    "                    self.first_moment[param_id] = np.zeros_like(param.data.data)\n",
    "                    self.second_moment[param_id] = np.zeros_like(param.data.data)\n",
    "                \n",
    "                # Update first moment (momentum)\n",
    "                self.first_moment[param_id] = (\n",
    "                    self.beta1 * self.first_moment[param_id] + \n",
    "                    (1 - self.beta1) * gradient\n",
    "                )\n",
    "                \n",
    "                # Update second moment (variance)\n",
    "                self.second_moment[param_id] = (\n",
    "                    self.beta2 * self.second_moment[param_id] + \n",
    "                    (1 - self.beta2) * gradient * gradient\n",
    "                )\n",
    "                \n",
    "                # Bias correction\n",
    "                first_moment_corrected = (\n",
    "                    self.first_moment[param_id] / (1 - self.beta1 ** self.step_count)\n",
    "                )\n",
    "                second_moment_corrected = (\n",
    "                    self.second_moment[param_id] / (1 - self.beta2 ** self.step_count)\n",
    "                )\n",
    "                \n",
    "                # Update parameter with adaptive learning rate\n",
    "                param.data = Tensor(\n",
    "                    param.data.data - self.learning_rate * first_moment_corrected / \n",
    "                    (np.sqrt(second_moment_corrected) + self.epsilon)\n",
    "                )\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def zero_grad(self) -> None:\n",
    "        \"\"\"\n",
    "        Zero out gradients for all parameters.\n",
    "        \n",
    "        TODO: Implement gradient zeroing (same as SGD).\n",
    "        \n",
    "        IMPLEMENTATION HINTS:\n",
    "        - Set param.grad = None for all parameters\n",
    "        - This is identical to SGD implementation\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        for param in self.parameters:\n",
    "            param.grad = None\n",
    "        ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62105b5",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### üß™ Test Your Adam Implementation\n",
    "\n",
    "Let's test the Adam optimizer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faee55a",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### üß™ Unit Test: Adam Optimizer\n",
    "\n",
    "Let's test your Adam optimizer implementation! This is a state-of-the-art adaptive optimization algorithm.\n",
    "\n",
    "**This is a unit test** - it tests one specific class (Adam) in isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77da7374",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": true,
     "grade_id": "test-adam",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_adam_optimizer():\n",
    "    \"\"\"Unit test for the Adam optimizer implementation.\"\"\"\n",
    "    print(\"üî¨ Unit Test: Adam Optimizer...\")\n",
    "    \n",
    "    # Create test parameters\n",
    "    w1 = Variable(1.0, requires_grad=True)\n",
    "    w2 = Variable(2.0, requires_grad=True)\n",
    "    b = Variable(0.5, requires_grad=True)\n",
    "    \n",
    "    # Create optimizer\n",
    "    optimizer = Adam([w1, w2, b], learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8)\n",
    "    \n",
    "    # Test zero_grad\n",
    "    try:\n",
    "        w1.grad = Variable(0.1)\n",
    "        w2.grad = Variable(0.2)\n",
    "        b.grad = Variable(0.05)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        assert w1.grad is None, \"Gradient should be None after zero_grad\"\n",
    "        assert w2.grad is None, \"Gradient should be None after zero_grad\"\n",
    "        assert b.grad is None, \"Gradient should be None after zero_grad\"\n",
    "        print(\"‚úÖ zero_grad() works correctly\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå zero_grad() failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Test step with gradients\n",
    "    try:\n",
    "        w1.grad = Variable(0.1)\n",
    "        w2.grad = Variable(0.2)\n",
    "        b.grad = Variable(0.05)\n",
    "        \n",
    "        # First step\n",
    "        original_w1 = w1.data.data.item()\n",
    "        original_w2 = w2.data.data.item()\n",
    "        original_b = b.data.data.item()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Check that parameters were updated (Adam uses adaptive learning rates)\n",
    "        assert w1.data.data.item() != original_w1, \"w1 should have been updated\"\n",
    "        assert w2.data.data.item() != original_w2, \"w2 should have been updated\"\n",
    "        assert b.data.data.item() != original_b, \"b should have been updated\"\n",
    "        print(\"‚úÖ Parameter updates work correctly\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Parameter updates failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Test moment buffers\n",
    "    try:\n",
    "        assert len(optimizer.first_moment) == 3, f\"Should have 3 first moment buffers, got {len(optimizer.first_moment)}\"\n",
    "        assert len(optimizer.second_moment) == 3, f\"Should have 3 second moment buffers, got {len(optimizer.second_moment)}\"\n",
    "        print(\"‚úÖ Moment buffers created correctly\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Moment buffers failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Test step counting and bias correction\n",
    "    try:\n",
    "        assert optimizer.step_count == 1, f\"Step count should be 1, got {optimizer.step_count}\"\n",
    "        \n",
    "        # Take another step\n",
    "        w1.grad = Variable(0.1)\n",
    "        w2.grad = Variable(0.2)\n",
    "        b.grad = Variable(0.05)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        assert optimizer.step_count == 2, f\"Step count should be 2, got {optimizer.step_count}\"\n",
    "        print(\"‚úÖ Step counting and bias correction work correctly\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Step counting and bias correction failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Test adaptive learning rates\n",
    "    try:\n",
    "        # Adam should have different effective learning rates for different parameters\n",
    "        # This is tested implicitly by the parameter updates above\n",
    "        print(\"‚úÖ Adaptive learning rates work correctly\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Adaptive learning rates failed: {e}\")\n",
    "        raise\n",
    "\n",
    "    print(\"üéØ Adam optimizer behavior:\")\n",
    "    print(\"   Maintains first and second moment estimates\")\n",
    "    print(\"   Applies bias correction for early training\")\n",
    "    print(\"   Uses adaptive learning rates per parameter\")\n",
    "    print(\"   Combines benefits of momentum and RMSprop\")\n",
    "    print(\"üìà Progress: Adam Optimizer ‚úì\")\n",
    "\n",
    "# Test function defined (called in main block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd778078",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 4: Learning Rate Scheduling\n",
    "\n",
    "### What is Learning Rate Scheduling?\n",
    "**Learning rate scheduling** adjusts the learning rate during training:\n",
    "\n",
    "```\n",
    "Initial: learning_rate = 0.1\n",
    "After 10 epochs: learning_rate = 0.01\n",
    "After 20 epochs: learning_rate = 0.001\n",
    "```\n",
    "\n",
    "### Why Scheduling Matters\n",
    "1. **Fine-tuning**: Start with large steps, then refine with small steps\n",
    "2. **Convergence**: Prevents overshooting near optimum\n",
    "3. **Stability**: Reduces oscillations in later training\n",
    "4. **Performance**: Often improves final accuracy\n",
    "\n",
    "### Common Scheduling Strategies\n",
    "1. **Step decay**: Reduce by factor every N epochs\n",
    "2. **Exponential decay**: Gradual exponential reduction\n",
    "3. **Cosine annealing**: Smooth cosine curve reduction\n",
    "4. **Warm-up**: Start small, increase, then decrease\n",
    "\n",
    "### Visual Understanding\n",
    "```\n",
    "Step decay:     ----‚Üì----‚Üì----‚Üì\n",
    "Exponential:    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "Cosine:         ‚à©‚à©‚à©‚à©‚à©‚à©‚à©‚à©‚à©‚à©‚à©‚à©‚à©\n",
    "```\n",
    "\n",
    "### Real-World Applications\n",
    "- **ImageNet training**: Essential for achieving state-of-the-art results\n",
    "- **Language models**: Critical for training large transformers\n",
    "- **Fine-tuning**: Prevents catastrophic forgetting\n",
    "- **Transfer learning**: Adapts pre-trained models\n",
    "\n",
    "Let's implement step learning rate scheduling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c23ae10",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "steplr-class",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class StepLR:\n",
    "    \"\"\"\n",
    "    Step Learning Rate Scheduler\n",
    "    \n",
    "    Decays learning rate by gamma every step_size epochs:\n",
    "    learning_rate = initial_lr * (gamma ^ (epoch // step_size))\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, optimizer: Union[SGD, Adam], step_size: int, gamma: float = 0.1):\n",
    "        \"\"\"\n",
    "        Initialize step learning rate scheduler.\n",
    "        \n",
    "        Args:\n",
    "            optimizer: Optimizer to schedule\n",
    "            step_size: Number of epochs between decreases\n",
    "            gamma: Multiplicative factor for learning rate decay\n",
    "        \n",
    "        TODO: Implement learning rate scheduler initialization.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Store optimizer reference\n",
    "        2. Store scheduling parameters\n",
    "        3. Save initial learning rate\n",
    "        4. Initialize step counter\n",
    "        \n",
    "        EXAMPLE:\n",
    "        ```python\n",
    "        optimizer = SGD([w1, w2], learning_rate=0.1)\n",
    "        scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "        \n",
    "        # In training loop:\n",
    "        for epoch in range(100):\n",
    "            train_one_epoch()\n",
    "            scheduler.step()  # Update learning rate\n",
    "        ```\n",
    "        \n",
    "        HINTS:\n",
    "        - Store optimizer reference\n",
    "        - Save initial learning rate from optimizer\n",
    "        - Initialize step counter to 0\n",
    "        - gamma is the decay factor (0.1 = 10x reduction)\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        self.optimizer = optimizer\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "        self.initial_lr = optimizer.learning_rate\n",
    "        self.step_count = 0\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def step(self) -> None:\n",
    "        \"\"\"\n",
    "        Update learning rate based on current step.\n",
    "        \n",
    "        TODO: Implement learning rate update.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Increment step counter\n",
    "        2. Calculate new learning rate using step decay formula\n",
    "        3. Update optimizer's learning rate\n",
    "        \n",
    "        MATHEMATICAL FORMULATION:\n",
    "        new_lr = initial_lr * (gamma ^ ((step_count - 1) // step_size))\n",
    "        \n",
    "        IMPLEMENTATION HINTS:\n",
    "        - Use // for integer division\n",
    "        - Use ** for exponentiation\n",
    "        - Update optimizer.learning_rate directly\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        self.step_count += 1\n",
    "        \n",
    "        # Calculate new learning rate\n",
    "        decay_factor = self.gamma ** ((self.step_count - 1) // self.step_size)\n",
    "        new_lr = self.initial_lr * decay_factor\n",
    "        \n",
    "        # Update optimizer's learning rate\n",
    "        self.optimizer.learning_rate = new_lr\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def get_lr(self) -> float:\n",
    "        \"\"\"\n",
    "        Get current learning rate.\n",
    "        \n",
    "        TODO: Return current learning rate.\n",
    "        \n",
    "        IMPLEMENTATION HINTS:\n",
    "        - Return optimizer.learning_rate\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        return self.optimizer.learning_rate\n",
    "        ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e286f41",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### üß™ Unit Test: Step Learning Rate Scheduler\n",
    "\n",
    "Let's test your step learning rate scheduler implementation! This scheduler reduces learning rate at regular intervals.\n",
    "\n",
    "**This is a unit test** - it tests one specific class (StepLR) in isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b09c5aa",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": true,
     "grade_id": "test-step-scheduler",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_step_scheduler():\n",
    "    \"\"\"Unit test for the StepLR scheduler implementation.\"\"\"\n",
    "    print(\"üî¨ Unit Test: Step Learning Rate Scheduler...\")\n",
    "    \n",
    "    # Create test parameters and optimizer\n",
    "    w = Variable(1.0, requires_grad=True)\n",
    "    optimizer = SGD([w], learning_rate=0.1)\n",
    "    \n",
    "    # Test scheduler initialization\n",
    "    try:\n",
    "        scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "        \n",
    "        # Test initial learning rate\n",
    "        assert scheduler.get_lr() == 0.1, f\"Initial learning rate should be 0.1, got {scheduler.get_lr()}\"\n",
    "        print(\"‚úÖ Initial learning rate is correct\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Initial learning rate failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Test step-based decay\n",
    "    try:\n",
    "        # Steps 1-10: no decay (decay happens after step 10)\n",
    "        for i in range(10):\n",
    "            scheduler.step()\n",
    "        \n",
    "        assert scheduler.get_lr() == 0.1, f\"Learning rate should still be 0.1 after 10 steps, got {scheduler.get_lr()}\"\n",
    "        \n",
    "        # Step 11: decay should occur\n",
    "        scheduler.step()\n",
    "        expected_lr = 0.1 * 0.1  # 0.01\n",
    "        assert abs(scheduler.get_lr() - expected_lr) < 1e-6, f\"Learning rate should be {expected_lr} after 11 steps, got {scheduler.get_lr()}\"\n",
    "        print(\"‚úÖ Step-based decay works correctly\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Step-based decay failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Test multiple decay levels\n",
    "    try:\n",
    "        # Steps 12-20: should stay at 0.01\n",
    "        for i in range(9):\n",
    "            scheduler.step()\n",
    "        \n",
    "        assert abs(scheduler.get_lr() - 0.01) < 1e-6, f\"Learning rate should be 0.01 after 20 steps, got {scheduler.get_lr()}\"\n",
    "        \n",
    "        # Step 21: another decay\n",
    "        scheduler.step()\n",
    "        expected_lr = 0.01 * 0.1  # 0.001\n",
    "        assert abs(scheduler.get_lr() - expected_lr) < 1e-6, f\"Learning rate should be {expected_lr} after 21 steps, got {scheduler.get_lr()}\"\n",
    "        print(\"‚úÖ Multiple decay levels work correctly\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Multiple decay levels failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Test with different optimizer\n",
    "    try:\n",
    "        w2 = Variable(2.0, requires_grad=True)\n",
    "        adam_optimizer = Adam([w2], learning_rate=0.001)\n",
    "        adam_scheduler = StepLR(adam_optimizer, step_size=5, gamma=0.5)\n",
    "        \n",
    "        # Test initial learning rate\n",
    "        assert adam_scheduler.get_lr() == 0.001, f\"Initial Adam learning rate should be 0.001, got {adam_scheduler.get_lr()}\"\n",
    "        \n",
    "        # Test decay after 5 steps\n",
    "        for i in range(5):\n",
    "            adam_scheduler.step()\n",
    "        \n",
    "        # Learning rate should still be 0.001 after 5 steps\n",
    "        assert adam_scheduler.get_lr() == 0.001, f\"Adam learning rate should still be 0.001 after 5 steps, got {adam_scheduler.get_lr()}\"\n",
    "        \n",
    "        # Step 6: decay should occur\n",
    "        adam_scheduler.step()\n",
    "        expected_lr = 0.001 * 0.5  # 0.0005\n",
    "        assert abs(adam_scheduler.get_lr() - expected_lr) < 1e-6, f\"Adam learning rate should be {expected_lr} after 6 steps, got {adam_scheduler.get_lr()}\"\n",
    "        print(\"‚úÖ Works with different optimizers\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Different optimizers failed: {e}\")\n",
    "        raise\n",
    "\n",
    "    print(\"üéØ Step learning rate scheduler behavior:\")\n",
    "    print(\"   Reduces learning rate at regular intervals\")\n",
    "    print(\"   Multiplies current rate by gamma factor\")\n",
    "    print(\"   Works with any optimizer (SGD, Adam, etc.)\")\n",
    "    print(\"üìà Progress: Step Learning Rate Scheduler ‚úì\")\n",
    "\n",
    "# Test function defined (called in main block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465a5a61",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 5: Integration - Complete Training Example\n",
    "\n",
    "### Putting It All Together\n",
    "Let's see how optimizers enable complete neural network training:\n",
    "\n",
    "1. **Forward pass**: Compute predictions\n",
    "2. **Loss computation**: Compare with targets\n",
    "3. **Backward pass**: Compute gradients\n",
    "4. **Optimizer step**: Update parameters\n",
    "5. **Learning rate scheduling**: Adjust learning rate\n",
    "\n",
    "### The Modern Training Loop\n",
    "```python\n",
    "# Setup\n",
    "optimizer = Adam(model.parameters(), learning_rate=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        # Forward pass\n",
    "        predictions = model(batch.inputs)\n",
    "        loss = criterion(predictions, batch.targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "```\n",
    "\n",
    "Let's implement a complete training example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10135b0",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "training-integration",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_simple_model():\n",
    "    \"\"\"\n",
    "    Complete training example using optimizers.\n",
    "    \n",
    "    TODO: Implement a complete training loop.\n",
    "    \n",
    "    APPROACH:\n",
    "    1. Create a simple model (linear regression)\n",
    "    2. Generate training data\n",
    "    3. Set up optimizer and scheduler\n",
    "    4. Train for several epochs\n",
    "    5. Show convergence\n",
    "    \n",
    "    LEARNING OBJECTIVE:\n",
    "    - See how optimizers enable real learning\n",
    "    - Compare SGD vs Adam performance\n",
    "    - Understand the complete training workflow\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    print(\"Training simple linear regression model...\")\n",
    "    \n",
    "    # Create simple model: y = w*x + b\n",
    "    w = Variable(0.1, requires_grad=True)  # Initialize near zero\n",
    "    b = Variable(0.0, requires_grad=True)\n",
    "    \n",
    "    # Training data: y = 2*x + 1\n",
    "    x_data = [1.0, 2.0, 3.0, 4.0, 5.0]\n",
    "    y_data = [3.0, 5.0, 7.0, 9.0, 11.0]\n",
    "    \n",
    "    # Try SGD first\n",
    "    print(\"\\nüîç Training with SGD...\")\n",
    "    optimizer_sgd = SGD([w, b], learning_rate=0.01, momentum=0.9)\n",
    "    \n",
    "    for epoch in range(60):\n",
    "        total_loss = 0\n",
    "        \n",
    "        for x_val, y_val in zip(x_data, y_data):\n",
    "            # Forward pass\n",
    "            x = Variable(x_val, requires_grad=False)\n",
    "            y_target = Variable(y_val, requires_grad=False)\n",
    "            \n",
    "            # Prediction: y = w*x + b\n",
    "            try:\n",
    "                from tinytorch.core.autograd import add, multiply, subtract\n",
    "            except ImportError:\n",
    "                setup_import_paths()\n",
    "                from autograd_dev import add, multiply, subtract\n",
    "            \n",
    "            prediction = add(multiply(w, x), b)\n",
    "            \n",
    "            # Loss: (prediction - target)^2\n",
    "            error = subtract(prediction, y_target)\n",
    "            loss = multiply(error, error)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer_sgd.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_sgd.step()\n",
    "            \n",
    "            total_loss += loss.data.data.item()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}: Loss = {total_loss:.4f}, w = {w.data.data.item():.3f}, b = {b.data.data.item():.3f}\")\n",
    "    \n",
    "    sgd_final_w = w.data.data.item()\n",
    "    sgd_final_b = b.data.data.item()\n",
    "    \n",
    "    # Reset parameters and try Adam\n",
    "    print(\"\\nüîç Training with Adam...\")\n",
    "    w.data = Tensor(0.1)\n",
    "    b.data = Tensor(0.0)\n",
    "    \n",
    "    optimizer_adam = Adam([w, b], learning_rate=0.01)\n",
    "    \n",
    "    for epoch in range(60):\n",
    "        total_loss = 0\n",
    "        \n",
    "        for x_val, y_val in zip(x_data, y_data):\n",
    "            # Forward pass\n",
    "            x = Variable(x_val, requires_grad=False)\n",
    "            y_target = Variable(y_val, requires_grad=False)\n",
    "            \n",
    "            # Prediction: y = w*x + b\n",
    "            prediction = add(multiply(w, x), b)\n",
    "            \n",
    "            # Loss: (prediction - target)^2\n",
    "            error = subtract(prediction, y_target)\n",
    "            loss = multiply(error, error)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer_adam.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_adam.step()\n",
    "            \n",
    "            total_loss += loss.data.data.item()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}: Loss = {total_loss:.4f}, w = {w.data.data.item():.3f}, b = {b.data.data.item():.3f}\")\n",
    "    \n",
    "    adam_final_w = w.data.data.item()\n",
    "    adam_final_b = b.data.data.item()\n",
    "    \n",
    "    print(f\"\\nüìä Results:\")\n",
    "    print(f\"Target: w = 2.0, b = 1.0\")\n",
    "    print(f\"SGD:    w = {sgd_final_w:.3f}, b = {sgd_final_b:.3f}\")\n",
    "    print(f\"Adam:   w = {adam_final_w:.3f}, b = {adam_final_b:.3f}\")\n",
    "    \n",
    "    return sgd_final_w, sgd_final_b, adam_final_w, adam_final_b\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a703160",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### üß™ Unit Test: Complete Training Integration\n",
    "\n",
    "Let's test your complete training integration! This demonstrates optimizers working together in a realistic training scenario.\n",
    "\n",
    "**This is a unit test** - it tests the complete training workflow with optimizers in isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b706ab",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": true,
     "grade_id": "test-training-integration",
     "locked": true,
     "points": 25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_module_unit_training():\n",
    "    \"\"\"Comprehensive unit test for complete training integration with optimizers.\"\"\"\n",
    "    print(\"üî¨ Unit Test: Complete Training Integration...\")\n",
    "    \n",
    "    # Test training with SGD and Adam\n",
    "    try:\n",
    "        sgd_w, sgd_b, adam_w, adam_b = train_simple_model()\n",
    "        \n",
    "        # Test SGD convergence\n",
    "        assert abs(sgd_w - 2.0) < 0.1, f\"SGD should converge close to w=2.0, got {sgd_w}\"\n",
    "        assert abs(sgd_b - 1.0) < 0.1, f\"SGD should converge close to b=1.0, got {sgd_b}\"\n",
    "        print(\"‚úÖ SGD convergence works\")\n",
    "        \n",
    "        # Test Adam convergence (may be different due to adaptive learning rates)\n",
    "        assert abs(adam_w - 2.0) < 1.0, f\"Adam should converge reasonably close to w=2.0, got {adam_w}\"\n",
    "        assert abs(adam_b - 1.0) < 1.0, f\"Adam should converge reasonably close to b=1.0, got {adam_b}\"\n",
    "        print(\"‚úÖ Adam convergence works\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Training integration failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Test optimizer comparison\n",
    "    try:\n",
    "        # Both optimizers should achieve reasonable results\n",
    "        sgd_error = (sgd_w - 2.0)**2 + (sgd_b - 1.0)**2\n",
    "        adam_error = (adam_w - 2.0)**2 + (adam_b - 1.0)**2\n",
    "        \n",
    "        # Both should have low error (< 0.1)\n",
    "        assert sgd_error < 0.1, f\"SGD error should be < 0.1, got {sgd_error}\"\n",
    "        assert adam_error < 1.0, f\"Adam error should be < 1.0, got {adam_error}\"\n",
    "        print(\"‚úÖ Optimizer comparison works\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Optimizer comparison failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Test gradient flow\n",
    "    try:\n",
    "        # Create a simple test to verify gradients flow correctly\n",
    "        w = Variable(1.0, requires_grad=True)\n",
    "        b = Variable(0.0, requires_grad=True)\n",
    "        \n",
    "        # Set up simple gradients\n",
    "        w.grad = Variable(0.1)\n",
    "        b.grad = Variable(0.05)\n",
    "        \n",
    "        # Test SGD step\n",
    "        sgd_optimizer = SGD([w, b], learning_rate=0.1)\n",
    "        original_w = w.data.data.item()\n",
    "        original_b = b.data.data.item()\n",
    "        \n",
    "        sgd_optimizer.step()\n",
    "        \n",
    "        # Check updates\n",
    "        assert w.data.data.item() != original_w, \"SGD should update w\"\n",
    "        assert b.data.data.item() != original_b, \"SGD should update b\"\n",
    "        print(\"‚úÖ Gradient flow works correctly\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Gradient flow failed: {e}\")\n",
    "        raise\n",
    "\n",
    "    print(\"üéØ Training integration behavior:\")\n",
    "    print(\"   Optimizers successfully minimize loss functions\")\n",
    "    print(\"   SGD and Adam both converge to target values\")\n",
    "    print(\"   Gradient computation and updates work correctly\")\n",
    "    print(\"   Ready for real neural network training\")\n",
    "    print(\"üìà Progress: Complete Training Integration ‚úì\")\n",
    "\n",
    "# Test function defined (called in main block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8314e14",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 6: ML Systems - Optimizer Performance Analysis\n",
    "\n",
    "### Real-World Challenge: Optimizer Selection and Tuning\n",
    "\n",
    "In production ML systems, choosing the right optimizer and hyperparameters can make the difference between:\n",
    "- **Success**: Model converges to good performance in reasonable time\n",
    "- **Failure**: Model doesn't converge, explodes, or takes too long to train\n",
    "\n",
    "### The Production Reality\n",
    "When training large models (millions or billions of parameters):\n",
    "- **Wrong optimizer**: Can waste weeks of expensive GPU time\n",
    "- **Wrong learning rate**: Can cause gradient explosion or extremely slow convergence\n",
    "- **Wrong scheduling**: Can prevent models from reaching optimal performance\n",
    "- **Memory constraints**: Some optimizers use significantly more memory than others\n",
    "\n",
    "### What We'll Build\n",
    "An **OptimizerConvergenceProfiler** that analyzes:\n",
    "1. **Convergence patterns** across different optimizers\n",
    "2. **Learning rate sensitivity** and optimal hyperparameters\n",
    "3. **Computational cost vs convergence speed** trade-offs\n",
    "4. **Gradient statistics** and update patterns\n",
    "5. **Memory usage patterns** for different optimizers\n",
    "\n",
    "This mirrors tools used in production for optimizer selection and hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017016a6",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "convergence-profiler",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class OptimizerConvergenceProfiler:\n",
    "    \"\"\"\n",
    "    ML Systems Tool: Optimizer Performance and Convergence Analysis\n",
    "    \n",
    "    Profiles convergence patterns, learning rate sensitivity, and computational costs\n",
    "    across different optimizers to guide production optimizer selection.\n",
    "    \n",
    "    This is 60% implementation focusing on core analysis capabilities:\n",
    "    - Convergence rate comparison across optimizers\n",
    "    - Learning rate sensitivity analysis\n",
    "    - Gradient statistics tracking\n",
    "    - Memory usage estimation\n",
    "    - Performance recommendations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize optimizer convergence profiler.\n",
    "        \n",
    "        TODO: Implement profiler initialization.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Initialize tracking dictionaries for different metrics\n",
    "        2. Set up convergence analysis parameters\n",
    "        3. Prepare memory and performance tracking\n",
    "        4. Initialize recommendation engine components\n",
    "        \n",
    "        PRODUCTION CONTEXT:\n",
    "        In production, this profiler would run on representative tasks to:\n",
    "        - Select optimal optimizers for new models\n",
    "        - Tune hyperparameters before expensive training runs\n",
    "        - Predict training time and resource requirements\n",
    "        - Monitor training stability and convergence\n",
    "        \n",
    "        IMPLEMENTATION HINTS:\n",
    "        - Track convergence history per optimizer\n",
    "        - Store gradient statistics over time\n",
    "        - Monitor memory usage patterns\n",
    "        - Prepare for comparative analysis\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        # Convergence tracking\n",
    "        self.convergence_history = defaultdict(list)  # {optimizer_name: [losses]}\n",
    "        self.gradient_norms = defaultdict(list)       # {optimizer_name: [grad_norms]}\n",
    "        self.learning_rates = defaultdict(list)       # {optimizer_name: [lr_values]}\n",
    "        self.step_times = defaultdict(list)           # {optimizer_name: [step_durations]}\n",
    "        \n",
    "        # Performance metrics\n",
    "        self.memory_usage = defaultdict(list)         # {optimizer_name: [memory_estimates]}\n",
    "        self.convergence_rates = {}                   # {optimizer_name: convergence_rate}\n",
    "        self.stability_scores = {}                    # {optimizer_name: stability_score}\n",
    "        \n",
    "        # Analysis parameters\n",
    "        self.convergence_threshold = 1e-6\n",
    "        self.stability_window = 10\n",
    "        self.gradient_explosion_threshold = 1e6\n",
    "        \n",
    "        # Recommendations\n",
    "        self.optimizer_rankings = {}\n",
    "        self.hyperparameter_suggestions = {}\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def profile_optimizer_convergence(self, optimizer_name: str, optimizer: Union[SGD, Adam], \n",
    "                                    training_function, initial_loss: float, \n",
    "                                    max_steps: int = 100) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Profile convergence behavior of an optimizer on a specific task.\n",
    "        \n",
    "        Args:\n",
    "            optimizer_name: Name identifier for the optimizer\n",
    "            optimizer: Optimizer instance to profile\n",
    "            training_function: Function that performs one training step and returns loss\n",
    "            initial_loss: Starting loss value\n",
    "            max_steps: Maximum training steps to profile\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary containing convergence analysis results\n",
    "        \n",
    "        TODO: Implement optimizer convergence profiling.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Run training loop with the optimizer\n",
    "        2. Track loss, gradients, learning rates at each step\n",
    "        3. Measure step execution time\n",
    "        4. Estimate memory usage\n",
    "        5. Analyze convergence patterns and stability\n",
    "        6. Generate performance metrics\n",
    "        \n",
    "        CONVERGENCE ANALYSIS:\n",
    "        - Track loss reduction over time\n",
    "        - Measure convergence rate (loss reduction per step)\n",
    "        - Detect convergence plateaus\n",
    "        - Identify gradient explosion or vanishing\n",
    "        - Assess training stability\n",
    "        \n",
    "        PRODUCTION INSIGHTS:\n",
    "        This analysis helps determine:\n",
    "        - Which optimizers converge fastest for specific model types\n",
    "        - Optimal learning rates for different optimizers\n",
    "        - Memory vs performance trade-offs\n",
    "        - Training stability and robustness\n",
    "        \n",
    "        IMPLEMENTATION HINTS:\n",
    "        - Use time.time() to measure step duration\n",
    "        - Calculate gradient norms across all parameters\n",
    "        - Track learning rate changes (for schedulers)\n",
    "        - Estimate memory from optimizer state size\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        import time\n",
    "        \n",
    "        print(f\"üîç Profiling {optimizer_name} convergence...\")\n",
    "        \n",
    "        # Initialize tracking\n",
    "        losses = []\n",
    "        grad_norms = []\n",
    "        step_durations = []\n",
    "        lr_values = []\n",
    "        \n",
    "        previous_loss = initial_loss\n",
    "        convergence_step = None\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            step_start = time.time()\n",
    "            \n",
    "            # Perform training step\n",
    "            try:\n",
    "                current_loss = training_function()\n",
    "                losses.append(current_loss)\n",
    "                \n",
    "                # Calculate gradient norm\n",
    "                total_grad_norm = 0.0\n",
    "                param_count = 0\n",
    "                for param in optimizer.parameters:\n",
    "                    if param.grad is not None:\n",
    "                        grad_data = param.grad.data.data\n",
    "                        if hasattr(grad_data, 'flatten'):\n",
    "                            grad_norm = np.linalg.norm(grad_data.flatten())\n",
    "                        else:\n",
    "                            grad_norm = abs(float(grad_data))\n",
    "                        total_grad_norm += grad_norm ** 2\n",
    "                        param_count += 1\n",
    "                \n",
    "                if param_count > 0:\n",
    "                    total_grad_norm = (total_grad_norm / param_count) ** 0.5\n",
    "                grad_norms.append(total_grad_norm)\n",
    "                \n",
    "                # Track learning rate\n",
    "                lr_values.append(optimizer.learning_rate)\n",
    "                \n",
    "                # Check convergence\n",
    "                if convergence_step is None and abs(current_loss - previous_loss) < self.convergence_threshold:\n",
    "                    convergence_step = step\n",
    "                \n",
    "                previous_loss = current_loss\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Training step {step} failed: {e}\")\n",
    "                break\n",
    "            \n",
    "            step_end = time.time()\n",
    "            step_durations.append(step_end - step_start)\n",
    "            \n",
    "            # Early stopping for exploded gradients\n",
    "            if total_grad_norm > self.gradient_explosion_threshold:\n",
    "                print(f\"‚ö†Ô∏è Gradient explosion detected at step {step}\")\n",
    "                break\n",
    "        \n",
    "        # Store results\n",
    "        self.convergence_history[optimizer_name] = losses\n",
    "        self.gradient_norms[optimizer_name] = grad_norms\n",
    "        self.learning_rates[optimizer_name] = lr_values\n",
    "        self.step_times[optimizer_name] = step_durations\n",
    "        \n",
    "        # Analyze results\n",
    "        analysis = self._analyze_convergence_profile(optimizer_name, losses, grad_norms, \n",
    "                                                   step_durations, convergence_step)\n",
    "        \n",
    "        return analysis\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def compare_optimizers(self, profiles: Dict[str, Dict]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Compare multiple optimizer profiles and generate recommendations.\n",
    "        \n",
    "        Args:\n",
    "            profiles: Dictionary mapping optimizer names to their profile results\n",
    "        \n",
    "        Returns:\n",
    "            Comprehensive comparison analysis with recommendations\n",
    "        \n",
    "        TODO: Implement optimizer comparison and ranking.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Analyze convergence speed across optimizers\n",
    "        2. Compare final performance and stability\n",
    "        3. Assess computational efficiency\n",
    "        4. Generate rankings and recommendations\n",
    "        5. Identify optimal hyperparameters\n",
    "        \n",
    "        COMPARISON METRICS:\n",
    "        - Steps to convergence\n",
    "        - Final loss achieved\n",
    "        - Training stability (loss variance)\n",
    "        - Computational cost per step\n",
    "        - Memory efficiency\n",
    "        - Gradient explosion resistance\n",
    "        \n",
    "        PRODUCTION VALUE:\n",
    "        This comparison guides:\n",
    "        - Optimizer selection for new projects\n",
    "        - Hyperparameter optimization strategies\n",
    "        - Resource allocation decisions\n",
    "        - Training pipeline design\n",
    "        \n",
    "        IMPLEMENTATION HINTS:\n",
    "        - Normalize metrics for fair comparison\n",
    "        - Weight different factors based on importance\n",
    "        - Generate actionable recommendations\n",
    "        - Consider trade-offs between speed and stability\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        comparison = {\n",
    "            'convergence_speed': {},\n",
    "            'final_performance': {},\n",
    "            'stability': {},\n",
    "            'efficiency': {},\n",
    "            'rankings': {},\n",
    "            'recommendations': {}\n",
    "        }\n",
    "        \n",
    "        print(\"üìä Comparing optimizer performance...\")\n",
    "        \n",
    "        # Analyze each optimizer\n",
    "        for opt_name, profile in profiles.items():\n",
    "            # Convergence speed\n",
    "            convergence_step = profile.get('convergence_step', len(self.convergence_history[opt_name]))\n",
    "            comparison['convergence_speed'][opt_name] = convergence_step\n",
    "            \n",
    "            # Final performance\n",
    "            losses = self.convergence_history[opt_name]\n",
    "            if losses:\n",
    "                final_loss = losses[-1]\n",
    "                comparison['final_performance'][opt_name] = final_loss\n",
    "            \n",
    "            # Stability (coefficient of variation in last 10 steps)\n",
    "            if len(losses) >= self.stability_window:\n",
    "                recent_losses = losses[-self.stability_window:]\n",
    "                stability = 1.0 / (1.0 + np.std(recent_losses) / (np.mean(recent_losses) + 1e-8))\n",
    "                comparison['stability'][opt_name] = stability\n",
    "            \n",
    "            # Efficiency (loss reduction per unit time)\n",
    "            step_times = self.step_times[opt_name]\n",
    "            if losses and step_times:\n",
    "                initial_loss = losses[0]\n",
    "                final_loss = losses[-1]\n",
    "                total_time = sum(step_times)\n",
    "                efficiency = (initial_loss - final_loss) / (total_time + 1e-8)\n",
    "                comparison['efficiency'][opt_name] = efficiency\n",
    "        \n",
    "        # Generate rankings\n",
    "        metrics = ['convergence_speed', 'final_performance', 'stability', 'efficiency']\n",
    "        for metric in metrics:\n",
    "            if comparison[metric]:\n",
    "                if metric == 'convergence_speed':\n",
    "                    # Lower is better for convergence speed\n",
    "                    sorted_opts = sorted(comparison[metric].items(), key=lambda x: x[1])\n",
    "                elif metric == 'final_performance':\n",
    "                    # Lower is better for final loss\n",
    "                    sorted_opts = sorted(comparison[metric].items(), key=lambda x: x[1])\n",
    "                else:\n",
    "                    # Higher is better for stability and efficiency\n",
    "                    sorted_opts = sorted(comparison[metric].items(), key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "                comparison['rankings'][metric] = [opt for opt, _ in sorted_opts]\n",
    "        \n",
    "        # Generate recommendations\n",
    "        recommendations = []\n",
    "        \n",
    "        # Best overall optimizer\n",
    "        if comparison['rankings']:\n",
    "            # Simple scoring: rank position across metrics\n",
    "            scores = defaultdict(float)\n",
    "            for metric, ranking in comparison['rankings'].items():\n",
    "                for i, opt_name in enumerate(ranking):\n",
    "                    scores[opt_name] += len(ranking) - i\n",
    "            \n",
    "            best_optimizer = max(scores.items(), key=lambda x: x[1])[0]\n",
    "            recommendations.append(f\"üèÜ Best overall optimizer: {best_optimizer}\")\n",
    "        \n",
    "        # Specific recommendations\n",
    "        if 'convergence_speed' in comparison['rankings']:\n",
    "            fastest = comparison['rankings']['convergence_speed'][0]\n",
    "            recommendations.append(f\"‚ö° Fastest convergence: {fastest}\")\n",
    "        \n",
    "        if 'stability' in comparison['rankings']:\n",
    "            most_stable = comparison['rankings']['stability'][0]\n",
    "            recommendations.append(f\"üéØ Most stable training: {most_stable}\")\n",
    "        \n",
    "        if 'efficiency' in comparison['rankings']:\n",
    "            most_efficient = comparison['rankings']['efficiency'][0]\n",
    "            recommendations.append(f\"üí∞ Most compute-efficient: {most_efficient}\")\n",
    "        \n",
    "        comparison['recommendations']['summary'] = recommendations\n",
    "        \n",
    "        return comparison\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def analyze_learning_rate_sensitivity(self, optimizer_class, learning_rates: List[float],\n",
    "                                        training_function, steps: int = 50) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Analyze optimizer sensitivity to different learning rates.\n",
    "        \n",
    "        Args:\n",
    "            optimizer_class: Optimizer class (SGD or Adam)\n",
    "            learning_rates: List of learning rates to test\n",
    "            training_function: Function that creates and runs training\n",
    "            steps: Number of training steps per learning rate\n",
    "        \n",
    "        Returns:\n",
    "            Learning rate sensitivity analysis\n",
    "        \n",
    "        TODO: Implement learning rate sensitivity analysis.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Test optimizer with different learning rates\n",
    "        2. Measure convergence performance for each rate\n",
    "        3. Identify optimal learning rate range\n",
    "        4. Detect learning rate instability regions\n",
    "        5. Generate learning rate recommendations\n",
    "        \n",
    "        SENSITIVITY ANALYSIS:\n",
    "        - Plot loss curves for different learning rates\n",
    "        - Identify optimal learning rate range\n",
    "        - Detect gradient explosion thresholds\n",
    "        - Measure convergence robustness\n",
    "        - Generate adaptive scheduling suggestions\n",
    "        \n",
    "        PRODUCTION INSIGHTS:\n",
    "        This analysis enables:\n",
    "        - Automatic learning rate tuning\n",
    "        - Learning rate scheduling optimization\n",
    "        - Gradient explosion prevention\n",
    "        - Training stability improvement\n",
    "        \n",
    "        IMPLEMENTATION HINTS:\n",
    "        - Reset model state for each learning rate test\n",
    "        - Track convergence metrics consistently\n",
    "        - Identify learning rate sweet spots\n",
    "        - Flag unstable learning rate regions\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        print(\"üîç Analyzing learning rate sensitivity...\")\n",
    "        \n",
    "        lr_analysis = {\n",
    "            'learning_rates': learning_rates,\n",
    "            'final_losses': [],\n",
    "            'convergence_steps': [],\n",
    "            'stability_scores': [],\n",
    "            'gradient_explosions': [],\n",
    "            'optimal_range': None,\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        # Test each learning rate\n",
    "        for lr in learning_rates:\n",
    "            print(f\"  Testing learning rate: {lr}\")\n",
    "            \n",
    "            try:\n",
    "                # Create optimizer with current learning rate\n",
    "                # This is a simplified test - in production, would reset model state\n",
    "                losses, grad_norms = training_function(lr, steps)\n",
    "                \n",
    "                if losses:\n",
    "                    final_loss = losses[-1]\n",
    "                    lr_analysis['final_losses'].append(final_loss)\n",
    "                    \n",
    "                    # Find convergence step\n",
    "                    convergence_step = steps\n",
    "                    for i in range(1, len(losses)):\n",
    "                        if abs(losses[i] - losses[i-1]) < self.convergence_threshold:\n",
    "                            convergence_step = i\n",
    "                            break\n",
    "                    lr_analysis['convergence_steps'].append(convergence_step)\n",
    "                    \n",
    "                    # Calculate stability\n",
    "                    if len(losses) >= 10:\n",
    "                        recent_losses = losses[-10:]\n",
    "                        stability = 1.0 / (1.0 + np.std(recent_losses) / (np.mean(recent_losses) + 1e-8))\n",
    "                        lr_analysis['stability_scores'].append(stability)\n",
    "                    else:\n",
    "                        lr_analysis['stability_scores'].append(0.0)\n",
    "                    \n",
    "                    # Check for gradient explosion\n",
    "                    max_grad_norm = max(grad_norms) if grad_norms else 0.0\n",
    "                    explosion = max_grad_norm > self.gradient_explosion_threshold\n",
    "                    lr_analysis['gradient_explosions'].append(explosion)\n",
    "                    \n",
    "                else:\n",
    "                    # Failed to get losses\n",
    "                    lr_analysis['final_losses'].append(float('inf'))\n",
    "                    lr_analysis['convergence_steps'].append(steps)\n",
    "                    lr_analysis['stability_scores'].append(0.0)\n",
    "                    lr_analysis['gradient_explosions'].append(True)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"    ‚ö†Ô∏è Failed with lr={lr}: {e}\")\n",
    "                lr_analysis['final_losses'].append(float('inf'))\n",
    "                lr_analysis['convergence_steps'].append(steps)\n",
    "                lr_analysis['stability_scores'].append(0.0)\n",
    "                lr_analysis['gradient_explosions'].append(True)\n",
    "        \n",
    "        # Find optimal learning rate range\n",
    "        valid_indices = [i for i, (loss, explosion) in \n",
    "                        enumerate(zip(lr_analysis['final_losses'], lr_analysis['gradient_explosions']))\n",
    "                        if not explosion and loss != float('inf')]\n",
    "        \n",
    "        if valid_indices:\n",
    "            # Find learning rate with best final loss among stable ones\n",
    "            stable_losses = [(i, lr_analysis['final_losses'][i]) for i in valid_indices]\n",
    "            best_idx = min(stable_losses, key=lambda x: x[1])[0]\n",
    "            \n",
    "            # Define optimal range around best learning rate\n",
    "            best_lr = learning_rates[best_idx]\n",
    "            lr_analysis['optimal_range'] = (best_lr * 0.1, best_lr * 10.0)\n",
    "            \n",
    "            # Generate recommendations\n",
    "            recommendations = []\n",
    "            recommendations.append(f\"üéØ Optimal learning rate: {best_lr:.2e}\")\n",
    "            recommendations.append(f\"üìà Safe range: {lr_analysis['optimal_range'][0]:.2e} - {lr_analysis['optimal_range'][1]:.2e}\")\n",
    "            \n",
    "            # Learning rate scheduling suggestions\n",
    "            if best_idx > 0:\n",
    "                recommendations.append(\"üí° Consider starting with higher LR and decaying\")\n",
    "            if any(lr_analysis['gradient_explosions']):\n",
    "                max_safe_lr = max([learning_rates[i] for i in valid_indices])\n",
    "                recommendations.append(f\"‚ö†Ô∏è Avoid learning rates above {max_safe_lr:.2e}\")\n",
    "            \n",
    "            lr_analysis['recommendations'] = recommendations\n",
    "        else:\n",
    "            lr_analysis['recommendations'] = [\"‚ö†Ô∏è No stable learning rates found - try lower values\"]\n",
    "        \n",
    "        return lr_analysis\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def estimate_memory_usage(self, optimizer: Union[SGD, Adam], num_parameters: int) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Estimate memory usage for different optimizers.\n",
    "        \n",
    "        Args:\n",
    "            optimizer: Optimizer instance\n",
    "            num_parameters: Number of model parameters\n",
    "        \n",
    "        Returns:\n",
    "            Memory usage estimates in MB\n",
    "        \n",
    "        TODO: Implement memory usage estimation.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Calculate parameter memory requirements\n",
    "        2. Estimate optimizer state memory\n",
    "        3. Account for gradient storage\n",
    "        4. Include temporary computation memory\n",
    "        5. Provide memory scaling predictions\n",
    "        \n",
    "        MEMORY ANALYSIS:\n",
    "        - Parameter storage: num_params * 4 bytes (float32)\n",
    "        - Gradient storage: num_params * 4 bytes\n",
    "        - Optimizer state: varies by optimizer type\n",
    "        - SGD momentum: num_params * 4 bytes\n",
    "        - Adam: num_params * 8 bytes (first + second moments)\n",
    "        \n",
    "        PRODUCTION VALUE:\n",
    "        Memory estimation helps:\n",
    "        - Select optimizers for memory-constrained environments\n",
    "        - Plan GPU memory allocation\n",
    "        - Scale to larger models\n",
    "        - Optimize batch sizes\n",
    "        \n",
    "        IMPLEMENTATION HINTS:\n",
    "        - Use typical float32 size (4 bytes)\n",
    "        - Account for optimizer-specific state\n",
    "        - Include gradient accumulation overhead\n",
    "        - Provide scaling estimates\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        # Base memory requirements\n",
    "        bytes_per_param = 4  # float32\n",
    "        \n",
    "        memory_breakdown = {\n",
    "            'parameters_mb': num_parameters * bytes_per_param / (1024 * 1024),\n",
    "            'gradients_mb': num_parameters * bytes_per_param / (1024 * 1024),\n",
    "            'optimizer_state_mb': 0.0,\n",
    "            'total_mb': 0.0\n",
    "        }\n",
    "        \n",
    "        # Optimizer-specific state memory\n",
    "        if isinstance(optimizer, SGD):\n",
    "            if optimizer.momentum > 0:\n",
    "                # Momentum buffers\n",
    "                memory_breakdown['optimizer_state_mb'] = num_parameters * bytes_per_param / (1024 * 1024)\n",
    "            else:\n",
    "                memory_breakdown['optimizer_state_mb'] = 0.0\n",
    "        elif isinstance(optimizer, Adam):\n",
    "            # First and second moment estimates\n",
    "            memory_breakdown['optimizer_state_mb'] = num_parameters * 2 * bytes_per_param / (1024 * 1024)\n",
    "        \n",
    "        # Calculate total\n",
    "        memory_breakdown['total_mb'] = (\n",
    "            memory_breakdown['parameters_mb'] + \n",
    "            memory_breakdown['gradients_mb'] + \n",
    "            memory_breakdown['optimizer_state_mb']\n",
    "        )\n",
    "        \n",
    "        # Add efficiency estimates\n",
    "        memory_breakdown['memory_efficiency'] = memory_breakdown['parameters_mb'] / memory_breakdown['total_mb']\n",
    "        memory_breakdown['overhead_ratio'] = memory_breakdown['optimizer_state_mb'] / memory_breakdown['parameters_mb']\n",
    "        \n",
    "        return memory_breakdown\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def generate_production_recommendations(self, analysis_results: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate actionable recommendations for production optimizer usage.\n",
    "        \n",
    "        Args:\n",
    "            analysis_results: Combined results from convergence and sensitivity analysis\n",
    "        \n",
    "        Returns:\n",
    "            List of production recommendations\n",
    "        \n",
    "        TODO: Implement production recommendation generation.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Analyze convergence patterns and stability\n",
    "        2. Consider computational efficiency requirements\n",
    "        3. Account for memory constraints\n",
    "        4. Generate optimizer selection guidance\n",
    "        5. Provide hyperparameter tuning suggestions\n",
    "        \n",
    "        RECOMMENDATION CATEGORIES:\n",
    "        - Optimizer selection for different scenarios\n",
    "        - Learning rate and scheduling strategies\n",
    "        - Memory optimization techniques\n",
    "        - Training stability improvements\n",
    "        - Production deployment considerations\n",
    "        \n",
    "        PRODUCTION CONTEXT:\n",
    "        These recommendations guide:\n",
    "        - ML engineer optimizer selection\n",
    "        - DevOps resource allocation\n",
    "        - Training pipeline optimization\n",
    "        - Cost reduction strategies\n",
    "        \n",
    "        IMPLEMENTATION HINTS:\n",
    "        - Provide specific, actionable advice\n",
    "        - Consider different deployment scenarios\n",
    "        - Include quantitative guidelines\n",
    "        - Address common production challenges\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        recommendations = []\n",
    "        \n",
    "        # Optimizer selection recommendations\n",
    "        recommendations.append(\"üîß OPTIMIZER SELECTION GUIDE:\")\n",
    "        recommendations.append(\"  ‚Ä¢ SGD + Momentum: Best for large batch training, proven stability\")\n",
    "        recommendations.append(\"  ‚Ä¢ Adam: Best for rapid prototyping, adaptive learning rates\")\n",
    "        recommendations.append(\"  ‚Ä¢ Consider memory constraints: SGD uses ~50% less memory than Adam\")\n",
    "        \n",
    "        # Learning rate recommendations\n",
    "        if 'learning_rate_analysis' in analysis_results:\n",
    "            lr_analysis = analysis_results['learning_rate_analysis']\n",
    "            if lr_analysis.get('optimal_range'):\n",
    "                opt_range = lr_analysis['optimal_range']\n",
    "                recommendations.append(f\"üìà LEARNING RATE GUIDANCE:\")\n",
    "                recommendations.append(f\"  ‚Ä¢ Start with: {opt_range[0]:.2e}\")\n",
    "                recommendations.append(f\"  ‚Ä¢ Safe upper bound: {opt_range[1]:.2e}\")\n",
    "                recommendations.append(\"  ‚Ä¢ Use learning rate scheduling for best results\")\n",
    "        \n",
    "        # Convergence recommendations\n",
    "        if 'convergence_comparison' in analysis_results:\n",
    "            comparison = analysis_results['convergence_comparison']\n",
    "            if 'recommendations' in comparison and 'summary' in comparison['recommendations']:\n",
    "                recommendations.append(\"üéØ CONVERGENCE OPTIMIZATION:\")\n",
    "                for rec in comparison['recommendations']['summary']:\n",
    "                    recommendations.append(f\"  ‚Ä¢ {rec}\")\n",
    "        \n",
    "        # Production deployment recommendations\n",
    "        recommendations.append(\"üöÄ PRODUCTION DEPLOYMENT:\")\n",
    "        recommendations.append(\"  ‚Ä¢ Monitor gradient norms to detect training instability\")\n",
    "        recommendations.append(\"  ‚Ä¢ Implement gradient clipping for large models\")\n",
    "        recommendations.append(\"  ‚Ä¢ Use learning rate warmup for transformer architectures\")\n",
    "        recommendations.append(\"  ‚Ä¢ Consider mixed precision training to reduce memory usage\")\n",
    "        \n",
    "        # Scaling recommendations\n",
    "        recommendations.append(\"üìä SCALING CONSIDERATIONS:\")\n",
    "        recommendations.append(\"  ‚Ä¢ Large batch training: Prefer SGD with linear learning rate scaling\")\n",
    "        recommendations.append(\"  ‚Ä¢ Distributed training: Use synchronized optimizers\")\n",
    "        recommendations.append(\"  ‚Ä¢ Memory-constrained: Choose SGD or use gradient accumulation\")\n",
    "        recommendations.append(\"  ‚Ä¢ Fine-tuning: Use lower learning rates (10x-100x smaller)\")\n",
    "        \n",
    "        # Monitoring recommendations\n",
    "        recommendations.append(\"üìà MONITORING & DEBUGGING:\")\n",
    "        recommendations.append(\"  ‚Ä¢ Track loss smoothness to detect learning rate issues\")\n",
    "        recommendations.append(\"  ‚Ä¢ Monitor gradient norms for explosion/vanishing detection\")\n",
    "        recommendations.append(\"  ‚Ä¢ Log learning rate schedules for reproducibility\")\n",
    "        recommendations.append(\"  ‚Ä¢ Profile memory usage to optimize batch sizes\")\n",
    "        \n",
    "        return recommendations\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def _analyze_convergence_profile(self, optimizer_name: str, losses: List[float], \n",
    "                                   grad_norms: List[float], step_durations: List[float],\n",
    "                                   convergence_step: Optional[int]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Internal helper to analyze convergence profile data.\n",
    "        \n",
    "        Args:\n",
    "            optimizer_name: Name of the optimizer\n",
    "            losses: List of loss values over training\n",
    "            grad_norms: List of gradient norms over training\n",
    "            step_durations: List of step execution times\n",
    "            convergence_step: Step where convergence was detected (if any)\n",
    "        \n",
    "        Returns:\n",
    "            Analysis results dictionary\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        analysis = {\n",
    "            'optimizer_name': optimizer_name,\n",
    "            'total_steps': len(losses),\n",
    "            'convergence_step': convergence_step,\n",
    "            'final_loss': losses[-1] if losses else float('inf'),\n",
    "            'initial_loss': losses[0] if losses else float('inf'),\n",
    "            'loss_reduction': 0.0,\n",
    "            'convergence_rate': 0.0,\n",
    "            'stability_score': 0.0,\n",
    "            'average_step_time': 0.0,\n",
    "            'gradient_health': 'unknown'\n",
    "        }\n",
    "        \n",
    "        if losses:\n",
    "            # Calculate loss reduction\n",
    "            initial_loss = losses[0]\n",
    "            final_loss = losses[-1]\n",
    "            analysis['loss_reduction'] = initial_loss - final_loss\n",
    "            \n",
    "            # Calculate convergence rate (loss reduction per step)\n",
    "            if len(losses) > 1:\n",
    "                analysis['convergence_rate'] = analysis['loss_reduction'] / len(losses)\n",
    "            \n",
    "            # Calculate stability (inverse of coefficient of variation)\n",
    "            if len(losses) >= self.stability_window:\n",
    "                recent_losses = losses[-self.stability_window:]\n",
    "                mean_loss = np.mean(recent_losses)\n",
    "                std_loss = np.std(recent_losses)\n",
    "                analysis['stability_score'] = 1.0 / (1.0 + std_loss / (mean_loss + 1e-8))\n",
    "        \n",
    "        # Average step time\n",
    "        if step_durations:\n",
    "            analysis['average_step_time'] = np.mean(step_durations)\n",
    "        \n",
    "        # Gradient health assessment\n",
    "        if grad_norms:\n",
    "            max_grad_norm = max(grad_norms)\n",
    "            avg_grad_norm = np.mean(grad_norms)\n",
    "            \n",
    "            if max_grad_norm > self.gradient_explosion_threshold:\n",
    "                analysis['gradient_health'] = 'exploding'\n",
    "            elif avg_grad_norm < 1e-8:\n",
    "                analysis['gradient_health'] = 'vanishing'\n",
    "            elif np.std(grad_norms) / (avg_grad_norm + 1e-8) > 2.0:\n",
    "                analysis['gradient_health'] = 'unstable'\n",
    "            else:\n",
    "                analysis['gradient_health'] = 'healthy'\n",
    "        \n",
    "        return analysis\n",
    "        ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21b66f1",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### üß™ Unit Test: OptimizerConvergenceProfiler\n",
    "\n",
    "Let's test your ML systems optimizer profiler! This tool helps analyze and compare optimizer performance in production scenarios.\n",
    "\n",
    "**This is a unit test** - it tests the OptimizerConvergenceProfiler class functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400c914c",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": true,
     "grade_id": "test-convergence-profiler",
     "locked": true,
     "points": 30,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_convergence_profiler():\n",
    "    \"\"\"Unit test for the OptimizerConvergenceProfiler implementation.\"\"\"\n",
    "    print(\"üî¨ Unit Test: Optimizer Convergence Profiler...\")\n",
    "    \n",
    "    # Test profiler initialization\n",
    "    try:\n",
    "        profiler = OptimizerConvergenceProfiler()\n",
    "        \n",
    "        assert hasattr(profiler, 'convergence_history'), \"Should have convergence_history tracking\"\n",
    "        assert hasattr(profiler, 'gradient_norms'), \"Should have gradient_norms tracking\"\n",
    "        assert hasattr(profiler, 'learning_rates'), \"Should have learning_rates tracking\"\n",
    "        assert hasattr(profiler, 'step_times'), \"Should have step_times tracking\"\n",
    "        print(\"‚úÖ Profiler initialization works\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Profiler initialization failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Test memory usage estimation\n",
    "    try:\n",
    "        # Test SGD memory estimation\n",
    "        w = Variable(1.0, requires_grad=True)\n",
    "        sgd_optimizer = SGD([w], learning_rate=0.01, momentum=0.9)\n",
    "        \n",
    "        memory_estimate = profiler.estimate_memory_usage(sgd_optimizer, num_parameters=1000000)\n",
    "        \n",
    "        assert 'parameters_mb' in memory_estimate, \"Should estimate parameter memory\"\n",
    "        assert 'gradients_mb' in memory_estimate, \"Should estimate gradient memory\"\n",
    "        assert 'optimizer_state_mb' in memory_estimate, \"Should estimate optimizer state memory\"\n",
    "        assert 'total_mb' in memory_estimate, \"Should provide total memory estimate\"\n",
    "        \n",
    "        # SGD with momentum should have optimizer state\n",
    "        assert memory_estimate['optimizer_state_mb'] > 0, \"SGD with momentum should have state memory\"\n",
    "        print(\"‚úÖ Memory usage estimation works\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Memory usage estimation failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Test simple convergence analysis\n",
    "    try:\n",
    "        # Create a simple training function for testing\n",
    "        def simple_training_function():\n",
    "            # Simulate decreasing loss\n",
    "            losses = [10.0 - i * 0.5 for i in range(20)]\n",
    "            return losses[-1]  # Return final loss\n",
    "        \n",
    "        # Create test optimizer\n",
    "        w = Variable(1.0, requires_grad=True)\n",
    "        w.grad = Variable(0.1)  # Set gradient for testing\n",
    "        test_optimizer = SGD([w], learning_rate=0.01)\n",
    "        \n",
    "        # Profile convergence (simplified test)\n",
    "        analysis = profiler.profile_optimizer_convergence(\n",
    "            optimizer_name=\"test_sgd\",\n",
    "            optimizer=test_optimizer,\n",
    "            training_function=simple_training_function,\n",
    "            initial_loss=10.0,\n",
    "            max_steps=10\n",
    "        )\n",
    "        \n",
    "        assert 'optimizer_name' in analysis, \"Should return optimizer name\"\n",
    "        assert 'total_steps' in analysis, \"Should track total steps\"\n",
    "        assert 'final_loss' in analysis, \"Should track final loss\"\n",
    "        print(\"‚úÖ Basic convergence profiling works\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Convergence profiling failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Test production recommendations\n",
    "    try:\n",
    "        # Create mock analysis results\n",
    "        mock_results = {\n",
    "            'learning_rate_analysis': {\n",
    "                'optimal_range': (0.001, 0.1)\n",
    "            },\n",
    "            'convergence_comparison': {\n",
    "                'recommendations': {\n",
    "                    'summary': ['Best overall: Adam', 'Fastest: SGD']\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        recommendations = profiler.generate_production_recommendations(mock_results)\n",
    "        \n",
    "        assert isinstance(recommendations, list), \"Should return list of recommendations\"\n",
    "        assert len(recommendations) > 0, \"Should provide recommendations\"\n",
    "        \n",
    "        # Check for key recommendation categories\n",
    "        rec_text = ' '.join(recommendations)\n",
    "        assert 'OPTIMIZER SELECTION' in rec_text, \"Should include optimizer selection guidance\"\n",
    "        assert 'PRODUCTION DEPLOYMENT' in rec_text, \"Should include production deployment advice\"\n",
    "        print(\"‚úÖ Production recommendations work\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Production recommendations failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Test optimizer comparison framework\n",
    "    try:\n",
    "        # Create mock profiles for comparison\n",
    "        mock_profiles = {\n",
    "            'sgd': {'convergence_step': 50, 'final_loss': 0.1},\n",
    "            'adam': {'convergence_step': 30, 'final_loss': 0.05}\n",
    "        }\n",
    "        \n",
    "        # Add some mock data to profiler\n",
    "        profiler.convergence_history['sgd'] = [1.0, 0.5, 0.2, 0.1]\n",
    "        profiler.convergence_history['adam'] = [1.0, 0.3, 0.1, 0.05]\n",
    "        profiler.step_times['sgd'] = [0.01, 0.01, 0.01, 0.01]\n",
    "        profiler.step_times['adam'] = [0.02, 0.02, 0.02, 0.02]\n",
    "        \n",
    "        comparison = profiler.compare_optimizers(mock_profiles)\n",
    "        \n",
    "        assert 'convergence_speed' in comparison, \"Should compare convergence speed\"\n",
    "        assert 'final_performance' in comparison, \"Should compare final performance\"\n",
    "        assert 'stability' in comparison, \"Should compare stability\"\n",
    "        assert 'recommendations' in comparison, \"Should provide recommendations\"\n",
    "        print(\"‚úÖ Optimizer comparison works\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Optimizer comparison failed: {e}\")\n",
    "        raise\n",
    "\n",
    "    print(\"üéØ Optimizer Convergence Profiler behavior:\")\n",
    "    print(\"   Profiles convergence patterns across different optimizers\")\n",
    "    print(\"   Estimates memory usage for production planning\")\n",
    "    print(\"   Provides actionable recommendations for ML systems\")\n",
    "    print(\"   Enables data-driven optimizer selection\")\n",
    "    print(\"üìà Progress: ML Systems Optimizer Analysis ‚úì\")\n",
    "\n",
    "# Test function defined (called in main block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fee550e",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 7: Advanced Optimizer Features\n",
    "\n",
    "### Production Optimizer Patterns\n",
    "\n",
    "Real ML systems need more than basic optimizers. They need:\n",
    "\n",
    "1. **Gradient Clipping**: Prevents gradient explosion in large models\n",
    "2. **Learning Rate Warmup**: Gradually increases learning rate at start\n",
    "3. **Gradient Accumulation**: Simulates large batch training\n",
    "4. **Mixed Precision**: Reduces memory usage with FP16\n",
    "5. **Distributed Synchronization**: Coordinates optimizer across GPUs\n",
    "\n",
    "Let's implement these production patterns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cae95e1",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "advanced-optimizer-features",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class AdvancedOptimizerFeatures:\n",
    "    \"\"\"\n",
    "    Advanced optimizer features for production ML systems.\n",
    "    \n",
    "    Implements production-ready optimizer enhancements:\n",
    "    - Gradient clipping for stability\n",
    "    - Learning rate warmup strategies\n",
    "    - Gradient accumulation for large batches\n",
    "    - Mixed precision optimization patterns\n",
    "    - Distributed optimizer synchronization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize advanced optimizer features.\n",
    "        \n",
    "        TODO: Implement advanced features initialization.\n",
    "        \n",
    "        PRODUCTION CONTEXT:\n",
    "        These features are essential for:\n",
    "        - Training large language models (GPT, BERT)\n",
    "        - Computer vision at scale (ImageNet, COCO)\n",
    "        - Distributed training across multiple GPUs\n",
    "        - Memory-efficient training with limited resources\n",
    "        \n",
    "        IMPLEMENTATION HINTS:\n",
    "        - Initialize gradient clipping parameters\n",
    "        - Set up warmup scheduling state\n",
    "        - Prepare accumulation buffers\n",
    "        - Configure synchronization patterns\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        # Gradient clipping\n",
    "        self.max_grad_norm = 1.0\n",
    "        self.clip_enabled = False\n",
    "        \n",
    "        # Learning rate warmup\n",
    "        self.warmup_steps = 0\n",
    "        self.warmup_factor = 0.1\n",
    "        self.base_lr = 0.001\n",
    "        \n",
    "        # Gradient accumulation\n",
    "        self.accumulation_steps = 1\n",
    "        self.accumulated_gradients = {}\n",
    "        self.accumulation_count = 0\n",
    "        \n",
    "        # Mixed precision simulation\n",
    "        self.use_fp16 = False\n",
    "        self.loss_scale = 1.0\n",
    "        self.dynamic_loss_scaling = False\n",
    "        \n",
    "        # Distributed training simulation\n",
    "        self.world_size = 1\n",
    "        self.rank = 0\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def apply_gradient_clipping(self, optimizer: Union[SGD, Adam], max_norm: float = 1.0) -> float:\n",
    "        \"\"\"\n",
    "        Apply gradient clipping to prevent gradient explosion.\n",
    "        \n",
    "        Args:\n",
    "            optimizer: Optimizer with parameters to clip\n",
    "            max_norm: Maximum allowed gradient norm\n",
    "        \n",
    "        Returns:\n",
    "            Actual gradient norm before clipping\n",
    "        \n",
    "        TODO: Implement gradient clipping.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Calculate total gradient norm across all parameters\n",
    "        2. If norm exceeds max_norm, scale all gradients down\n",
    "        3. Apply scaling factor to maintain gradient direction\n",
    "        4. Return original norm for monitoring\n",
    "        \n",
    "        MATHEMATICAL FORMULATION:\n",
    "        total_norm = sqrt(sum(param_grad_norm^2 for all params))\n",
    "        if total_norm > max_norm:\n",
    "            clip_factor = max_norm / total_norm\n",
    "            for each param: param.grad *= clip_factor\n",
    "        \n",
    "        PRODUCTION VALUE:\n",
    "        Gradient clipping is essential for:\n",
    "        - Training RNNs and Transformers\n",
    "        - Preventing training instability\n",
    "        - Enabling higher learning rates\n",
    "        - Improving convergence reliability\n",
    "        \n",
    "        IMPLEMENTATION HINTS:\n",
    "        - Calculate global gradient norm\n",
    "        - Apply uniform scaling to all gradients\n",
    "        - Preserve gradient directions\n",
    "        - Return unclipped norm for logging\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        # Calculate total gradient norm\n",
    "        total_norm = 0.0\n",
    "        param_count = 0\n",
    "        \n",
    "        for param in optimizer.parameters:\n",
    "            if param.grad is not None:\n",
    "                grad_data = param.grad.data.data\n",
    "                if hasattr(grad_data, 'flatten'):\n",
    "                    param_norm = np.linalg.norm(grad_data.flatten())\n",
    "                else:\n",
    "                    param_norm = abs(float(grad_data))\n",
    "                total_norm += param_norm ** 2\n",
    "                param_count += 1\n",
    "        \n",
    "        if param_count > 0:\n",
    "            total_norm = total_norm ** 0.5\n",
    "        else:\n",
    "            return 0.0\n",
    "        \n",
    "        # Apply clipping if necessary\n",
    "        if total_norm > max_norm:\n",
    "            clip_factor = max_norm / total_norm\n",
    "            \n",
    "            for param in optimizer.parameters:\n",
    "                if param.grad is not None:\n",
    "                    grad_data = param.grad.data.data\n",
    "                    clipped_grad = grad_data * clip_factor\n",
    "                    param.grad.data = Tensor(clipped_grad)\n",
    "        \n",
    "        return total_norm\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def apply_warmup_schedule(self, optimizer: Union[SGD, Adam], step: int, \n",
    "                            warmup_steps: int, base_lr: float) -> float:\n",
    "        \"\"\"\n",
    "        Apply learning rate warmup schedule.\n",
    "        \n",
    "        Args:\n",
    "            optimizer: Optimizer to apply warmup to\n",
    "            step: Current training step\n",
    "            warmup_steps: Number of warmup steps\n",
    "            base_lr: Target learning rate after warmup\n",
    "        \n",
    "        Returns:\n",
    "            Current learning rate\n",
    "        \n",
    "        TODO: Implement learning rate warmup.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. If step < warmup_steps: gradually increase learning rate\n",
    "        2. Use linear or polynomial warmup schedule\n",
    "        3. Update optimizer's learning rate\n",
    "        4. Return current learning rate for logging\n",
    "        \n",
    "        WARMUP STRATEGIES:\n",
    "        - Linear: lr = base_lr * (step / warmup_steps)\n",
    "        - Polynomial: lr = base_lr * ((step / warmup_steps) ^ power)\n",
    "        - Constant: lr = base_lr * warmup_factor for warmup_steps\n",
    "        \n",
    "        PRODUCTION VALUE:\n",
    "        Warmup prevents:\n",
    "        - Early training instability\n",
    "        - Poor initialization effects\n",
    "        - Gradient explosion at start\n",
    "        - Suboptimal convergence paths\n",
    "        \n",
    "        IMPLEMENTATION HINTS:\n",
    "        - Handle step=0 case (avoid division by zero)\n",
    "        - Use linear warmup for simplicity\n",
    "        - Update optimizer.learning_rate directly\n",
    "        - Smoothly transition to base learning rate\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        if step < warmup_steps and warmup_steps > 0:\n",
    "            # Linear warmup\n",
    "            warmup_factor = step / warmup_steps\n",
    "            current_lr = base_lr * warmup_factor\n",
    "        else:\n",
    "            # After warmup, use base learning rate\n",
    "            current_lr = base_lr\n",
    "        \n",
    "        # Update optimizer learning rate\n",
    "        optimizer.learning_rate = current_lr\n",
    "        \n",
    "        return current_lr\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def accumulate_gradients(self, optimizer: Union[SGD, Adam], accumulation_steps: int) -> bool:\n",
    "        \"\"\"\n",
    "        Accumulate gradients to simulate larger batch sizes.\n",
    "        \n",
    "        Args:\n",
    "            optimizer: Optimizer with parameters to accumulate\n",
    "            accumulation_steps: Number of steps to accumulate before update\n",
    "        \n",
    "        Returns:\n",
    "            True if ready to perform optimizer step, False otherwise\n",
    "        \n",
    "        TODO: Implement gradient accumulation.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Add current gradients to accumulated gradient buffers\n",
    "        2. Increment accumulation counter\n",
    "        3. If counter reaches accumulation_steps:\n",
    "           a. Average accumulated gradients\n",
    "           b. Set as current gradients\n",
    "           c. Return True (ready for optimizer step)\n",
    "           d. Reset accumulation\n",
    "        4. Otherwise return False (continue accumulating)\n",
    "        \n",
    "        MATHEMATICAL FORMULATION:\n",
    "        accumulated_grad += current_grad\n",
    "        if accumulation_count == accumulation_steps:\n",
    "            final_grad = accumulated_grad / accumulation_steps\n",
    "            reset accumulation\n",
    "            return True\n",
    "        \n",
    "        PRODUCTION VALUE:\n",
    "        Gradient accumulation enables:\n",
    "        - Large effective batch sizes on limited memory\n",
    "        - Training large models on small GPUs\n",
    "        - Consistent training across different hardware\n",
    "        - Memory-efficient distributed training\n",
    "        \n",
    "        IMPLEMENTATION HINTS:\n",
    "        - Store accumulated gradients per parameter\n",
    "        - Use parameter id() as key for tracking\n",
    "        - Average gradients before optimizer step\n",
    "        - Reset accumulation after each update\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        # Initialize accumulation if first time\n",
    "        if not hasattr(self, 'accumulation_count'):\n",
    "            self.accumulation_count = 0\n",
    "            self.accumulated_gradients = {}\n",
    "        \n",
    "        # Accumulate gradients\n",
    "        for param in optimizer.parameters:\n",
    "            if param.grad is not None:\n",
    "                param_id = id(param)\n",
    "                grad_data = param.grad.data.data\n",
    "                \n",
    "                if param_id not in self.accumulated_gradients:\n",
    "                    self.accumulated_gradients[param_id] = np.zeros_like(grad_data)\n",
    "                \n",
    "                self.accumulated_gradients[param_id] += grad_data\n",
    "        \n",
    "        self.accumulation_count += 1\n",
    "        \n",
    "        # Check if ready to update\n",
    "        if self.accumulation_count >= accumulation_steps:\n",
    "            # Average accumulated gradients and set as current gradients\n",
    "            for param in optimizer.parameters:\n",
    "                if param.grad is not None:\n",
    "                    param_id = id(param)\n",
    "                    if param_id in self.accumulated_gradients:\n",
    "                        averaged_grad = self.accumulated_gradients[param_id] / accumulation_steps\n",
    "                        param.grad.data = Tensor(averaged_grad)\n",
    "            \n",
    "            # Reset accumulation\n",
    "            self.accumulation_count = 0\n",
    "            self.accumulated_gradients = {}\n",
    "            \n",
    "            return True  # Ready for optimizer step\n",
    "        \n",
    "        return False  # Continue accumulating\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def simulate_mixed_precision(self, optimizer: Union[SGD, Adam], loss_scale: float = 1.0) -> bool:\n",
    "        \"\"\"\n",
    "        Simulate mixed precision training effects.\n",
    "        \n",
    "        Args:\n",
    "            optimizer: Optimizer to apply mixed precision to\n",
    "            loss_scale: Loss scaling factor for gradient preservation\n",
    "        \n",
    "        Returns:\n",
    "            True if gradients are valid (no overflow), False if overflow detected\n",
    "        \n",
    "        TODO: Implement mixed precision simulation.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Scale gradients by loss_scale factor\n",
    "        2. Check for gradient overflow (inf or nan values)\n",
    "        3. If overflow detected, skip optimizer step\n",
    "        4. If valid, descale gradients before optimizer step\n",
    "        5. Return overflow status\n",
    "        \n",
    "        MIXED PRECISION CONCEPTS:\n",
    "        - Use FP16 for forward pass (memory savings)\n",
    "        - Use FP32 for backward pass (numerical stability)\n",
    "        - Scale loss to prevent gradient underflow\n",
    "        - Check for overflow before optimization\n",
    "        \n",
    "        PRODUCTION VALUE:\n",
    "        Mixed precision provides:\n",
    "        - 50% memory reduction\n",
    "        - Faster training on modern GPUs\n",
    "        - Maintained numerical stability\n",
    "        - Automatic overflow detection\n",
    "        \n",
    "        IMPLEMENTATION HINTS:\n",
    "        - Scale gradients by loss_scale\n",
    "        - Check for inf/nan in gradients\n",
    "        - Descale before optimizer step\n",
    "        - Return overflow status for dynamic scaling\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        # Check for gradient overflow before scaling\n",
    "        has_overflow = False\n",
    "        \n",
    "        for param in optimizer.parameters:\n",
    "            if param.grad is not None:\n",
    "                grad_data = param.grad.data.data\n",
    "                if hasattr(grad_data, 'flatten'):\n",
    "                    grad_flat = grad_data.flatten()\n",
    "                    if np.any(np.isinf(grad_flat)) or np.any(np.isnan(grad_flat)):\n",
    "                        has_overflow = True\n",
    "                        break\n",
    "                else:\n",
    "                    if np.isinf(grad_data) or np.isnan(grad_data):\n",
    "                        has_overflow = True\n",
    "                        break\n",
    "        \n",
    "        if has_overflow:\n",
    "            # Zero gradients to prevent corruption\n",
    "            for param in optimizer.parameters:\n",
    "                if param.grad is not None:\n",
    "                    param.grad = None\n",
    "            return False  # Overflow detected\n",
    "        \n",
    "        # Descale gradients (simulate unscaling from FP16)\n",
    "        if loss_scale > 1.0:\n",
    "            for param in optimizer.parameters:\n",
    "                if param.grad is not None:\n",
    "                    grad_data = param.grad.data.data\n",
    "                    descaled_grad = grad_data / loss_scale\n",
    "                    param.grad.data = Tensor(descaled_grad)\n",
    "        \n",
    "        return True  # No overflow, safe to proceed\n",
    "        ### END SOLUTION\n",
    "    \n",
    "    def simulate_distributed_sync(self, optimizer: Union[SGD, Adam], world_size: int = 1) -> None:\n",
    "        \"\"\"\n",
    "        Simulate distributed training gradient synchronization.\n",
    "        \n",
    "        Args:\n",
    "            optimizer: Optimizer with gradients to synchronize\n",
    "            world_size: Number of distributed processes\n",
    "        \n",
    "        TODO: Implement distributed gradient synchronization simulation.\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Simulate all-reduce operation on gradients\n",
    "        2. Average gradients across all processes\n",
    "        3. Update local gradients with synchronized values\n",
    "        4. Handle communication overhead simulation\n",
    "        \n",
    "        DISTRIBUTED CONCEPTS:\n",
    "        - All-reduce: Combine gradients from all GPUs\n",
    "        - Averaging: Divide by world_size for consistency\n",
    "        - Synchronization: Ensure all GPUs have same gradients\n",
    "        - Communication: Network overhead for gradient sharing\n",
    "        \n",
    "        PRODUCTION VALUE:\n",
    "        Distributed training enables:\n",
    "        - Scaling to multiple GPUs/nodes\n",
    "        - Training large models efficiently\n",
    "        - Reduced training time\n",
    "        - Consistent convergence across devices\n",
    "        \n",
    "        IMPLEMENTATION HINTS:\n",
    "        - Simulate averaging by keeping gradients unchanged\n",
    "        - Add small noise to simulate communication variance\n",
    "        - Scale learning rate by world_size if needed\n",
    "        - Log synchronization overhead\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        if world_size <= 1:\n",
    "            return  # No synchronization needed for single process\n",
    "        \n",
    "        # Simulate all-reduce operation (averaging gradients)\n",
    "        for param in optimizer.parameters:\n",
    "            if param.grad is not None:\n",
    "                grad_data = param.grad.data.data\n",
    "                \n",
    "                # In real distributed training, gradients would be averaged across all processes\n",
    "                # Here we simulate this by keeping gradients unchanged (already \"averaged\")\n",
    "                # In practice, this would involve MPI/NCCL communication\n",
    "                \n",
    "                # Simulate communication noise (very small)\n",
    "                if hasattr(grad_data, 'shape'):\n",
    "                    noise = np.random.normal(0, 1e-10, grad_data.shape)\n",
    "                    synchronized_grad = grad_data + noise\n",
    "                else:\n",
    "                    noise = np.random.normal(0, 1e-10)\n",
    "                    synchronized_grad = grad_data + noise\n",
    "                \n",
    "                param.grad.data = Tensor(synchronized_grad)\n",
    "        \n",
    "        # In distributed training, learning rate is often scaled by world_size\n",
    "        # to maintain effective learning rate with larger batch sizes\n",
    "        if hasattr(optimizer, 'base_learning_rate'):\n",
    "            optimizer.learning_rate = optimizer.base_learning_rate * world_size\n",
    "        ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed2e46b",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### üß™ Unit Test: Advanced Optimizer Features\n",
    "\n",
    "Let's test your advanced optimizer features! These are production-ready enhancements used in real ML systems.\n",
    "\n",
    "**This is a unit test** - it tests the AdvancedOptimizerFeatures class functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32abb52a",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": true,
     "grade_id": "test-advanced-features",
     "locked": true,
     "points": 25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_advanced_optimizer_features():\n",
    "    \"\"\"Unit test for advanced optimizer features implementation.\"\"\"\n",
    "    print(\"üî¨ Unit Test: Advanced Optimizer Features...\")\n",
    "    \n",
    "    # Test advanced features initialization\n",
    "    try:\n",
    "        features = AdvancedOptimizerFeatures()\n",
    "        \n",
    "        assert hasattr(features, 'max_grad_norm'), \"Should have gradient clipping parameters\"\n",
    "        assert hasattr(features, 'warmup_steps'), \"Should have warmup parameters\"\n",
    "        assert hasattr(features, 'accumulation_steps'), \"Should have accumulation parameters\"\n",
    "        print(\"‚úÖ Advanced features initialization works\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Advanced features initialization failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Test gradient clipping\n",
    "    try:\n",
    "        # Create optimizer with large gradients\n",
    "        w = Variable(1.0, requires_grad=True)\n",
    "        w.grad = Variable(10.0)  # Large gradient\n",
    "        optimizer = SGD([w], learning_rate=0.01)\n",
    "        \n",
    "        # Apply gradient clipping\n",
    "        original_norm = features.apply_gradient_clipping(optimizer, max_norm=1.0)\n",
    "        \n",
    "        # Check that gradient was clipped\n",
    "        clipped_grad = w.grad.data.data.item()\n",
    "        assert abs(clipped_grad) <= 1.0, f\"Gradient should be clipped to <= 1.0, got {clipped_grad}\"\n",
    "        assert original_norm > 1.0, f\"Original norm should be > 1.0, got {original_norm}\"\n",
    "        print(\"‚úÖ Gradient clipping works\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Gradient clipping failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Test learning rate warmup\n",
    "    try:\n",
    "        w2 = Variable(1.0, requires_grad=True)\n",
    "        optimizer2 = SGD([w2], learning_rate=0.01)\n",
    "        \n",
    "        # Test warmup schedule\n",
    "        lr_step_0 = features.apply_warmup_schedule(optimizer2, step=0, warmup_steps=10, base_lr=0.1)\n",
    "        lr_step_5 = features.apply_warmup_schedule(optimizer2, step=5, warmup_steps=10, base_lr=0.1)\n",
    "        lr_step_10 = features.apply_warmup_schedule(optimizer2, step=10, warmup_steps=10, base_lr=0.1)\n",
    "        \n",
    "        # Check warmup progression\n",
    "        assert lr_step_0 == 0.0, f\"Step 0 should have lr=0.0, got {lr_step_0}\"\n",
    "        assert 0.0 < lr_step_5 < 0.1, f\"Step 5 should have 0 < lr < 0.1, got {lr_step_5}\"\n",
    "        assert lr_step_10 == 0.1, f\"Step 10 should have lr=0.1, got {lr_step_10}\"\n",
    "        print(\"‚úÖ Learning rate warmup works\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Learning rate warmup failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Test gradient accumulation\n",
    "    try:\n",
    "        w3 = Variable(1.0, requires_grad=True)\n",
    "        w3.grad = Variable(0.1)\n",
    "        optimizer3 = SGD([w3], learning_rate=0.01)\n",
    "        \n",
    "        # Test accumulation over multiple steps\n",
    "        ready_step_1 = features.accumulate_gradients(optimizer3, accumulation_steps=3)\n",
    "        ready_step_2 = features.accumulate_gradients(optimizer3, accumulation_steps=3)\n",
    "        ready_step_3 = features.accumulate_gradients(optimizer3, accumulation_steps=3)\n",
    "        \n",
    "        # Check accumulation behavior\n",
    "        assert not ready_step_1, \"Should not be ready after step 1\"\n",
    "        assert not ready_step_2, \"Should not be ready after step 2\"\n",
    "        assert ready_step_3, \"Should be ready after step 3\"\n",
    "        print(\"‚úÖ Gradient accumulation works\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Gradient accumulation failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Test mixed precision simulation\n",
    "    try:\n",
    "        w4 = Variable(1.0, requires_grad=True)\n",
    "        w4.grad = Variable(0.1)\n",
    "        optimizer4 = SGD([w4], learning_rate=0.01)\n",
    "        \n",
    "        # Test normal case (no overflow)\n",
    "        no_overflow = features.simulate_mixed_precision(optimizer4, loss_scale=1.0)\n",
    "        assert no_overflow, \"Should not detect overflow with normal gradients\"\n",
    "        \n",
    "        # Test overflow case\n",
    "        w4.grad = Variable(float('inf'))\n",
    "        overflow = features.simulate_mixed_precision(optimizer4, loss_scale=1.0)\n",
    "        assert not overflow, \"Should detect overflow with inf gradients\"\n",
    "        print(\"‚úÖ Mixed precision simulation works\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Mixed precision simulation failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Test distributed synchronization\n",
    "    try:\n",
    "        w5 = Variable(1.0, requires_grad=True)\n",
    "        w5.grad = Variable(0.1)\n",
    "        optimizer5 = SGD([w5], learning_rate=0.01)\n",
    "        \n",
    "        original_grad = w5.grad.data.data.item()\n",
    "        \n",
    "        # Simulate distributed sync\n",
    "        features.simulate_distributed_sync(optimizer5, world_size=4)\n",
    "        \n",
    "        # Gradient should be slightly modified (due to simulated communication noise)\n",
    "        # but still close to original\n",
    "        synced_grad = w5.grad.data.data.item()\n",
    "        assert abs(synced_grad - original_grad) < 0.01, \"Synchronized gradient should be close to original\"\n",
    "        print(\"‚úÖ Distributed synchronization simulation works\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Distributed synchronization failed: {e}\")\n",
    "        raise\n",
    "\n",
    "    print(\"üéØ Advanced Optimizer Features behavior:\")\n",
    "    print(\"   Implements gradient clipping for training stability\")\n",
    "    print(\"   Provides learning rate warmup for better convergence\")\n",
    "    print(\"   Enables gradient accumulation for large effective batches\")\n",
    "    print(\"   Simulates mixed precision training patterns\")\n",
    "    print(\"   Handles distributed training synchronization\")\n",
    "    print(\"üìà Progress: Advanced Production Optimizer Features ‚úì\")\n",
    "\n",
    "# Test function defined (called in main block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d611a2",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 8: Comprehensive Testing - ML Systems Integration\n",
    "\n",
    "### Real-World Optimizer Performance Testing\n",
    "\n",
    "Let's test our optimizers in realistic scenarios that mirror production ML systems:\n",
    "\n",
    "1. **Convergence Race**: Compare optimizers on the same task\n",
    "2. **Learning Rate Sensitivity**: Find optimal hyperparameters\n",
    "3. **Memory Analysis**: Compare resource usage\n",
    "4. **Production Recommendations**: Get actionable guidance\n",
    "\n",
    "This integration test demonstrates how our ML systems tools work together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9d2f21",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": true,
     "grade_id": "test-ml-systems-integration",
     "locked": true,
     "points": 35,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_comprehensive_ml_systems_integration():\n",
    "    \"\"\"Comprehensive integration test demonstrating ML systems optimizer analysis.\"\"\"\n",
    "    print(\"üî¨ Comprehensive Test: ML Systems Integration...\")\n",
    "    \n",
    "    # Initialize ML systems tools\n",
    "    try:\n",
    "        profiler = OptimizerConvergenceProfiler()\n",
    "        advanced_features = AdvancedOptimizerFeatures()\n",
    "        print(\"‚úÖ ML systems tools initialized\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ML systems tools initialization failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Test convergence profiling with multiple optimizers\n",
    "    try:\n",
    "        print(\"\\nüìä Running optimizer convergence comparison...\")\n",
    "        \n",
    "        # Create simple training scenario\n",
    "        def create_training_function(optimizer_instance):\n",
    "            def training_step():\n",
    "                # Simulate a quadratic loss function: loss = (x - target)^2\n",
    "                # where we're trying to minimize x towards target = 2.0\n",
    "                current_x = optimizer_instance.parameters[0].data.data.item()\n",
    "                target = 2.0\n",
    "                loss = (current_x - target) ** 2\n",
    "                \n",
    "                # Compute gradient: d/dx (x - target)^2 = 2 * (x - target)\n",
    "                gradient = 2 * (current_x - target)\n",
    "                optimizer_instance.parameters[0].grad = Variable(gradient)\n",
    "                \n",
    "                # Perform optimizer step\n",
    "                optimizer_instance.step()\n",
    "                \n",
    "                return loss\n",
    "            return training_step\n",
    "        \n",
    "        # Test SGD\n",
    "        w_sgd = Variable(0.0, requires_grad=True)  # Start at x=0, target=2\n",
    "        sgd_optimizer = SGD([w_sgd], learning_rate=0.1, momentum=0.9)\n",
    "        sgd_training = create_training_function(sgd_optimizer)\n",
    "        \n",
    "        sgd_profile = profiler.profile_optimizer_convergence(\n",
    "            optimizer_name=\"SGD_momentum\",\n",
    "            optimizer=sgd_optimizer,\n",
    "            training_function=sgd_training,\n",
    "            initial_loss=4.0,  # (0-2)^2 = 4\n",
    "            max_steps=30\n",
    "        )\n",
    "        \n",
    "        # Test Adam\n",
    "        w_adam = Variable(0.0, requires_grad=True)  # Start at x=0, target=2\n",
    "        adam_optimizer = Adam([w_adam], learning_rate=0.1)\n",
    "        adam_training = create_training_function(adam_optimizer)\n",
    "        \n",
    "        adam_profile = profiler.profile_optimizer_convergence(\n",
    "            optimizer_name=\"Adam\",\n",
    "            optimizer=adam_optimizer,\n",
    "            training_function=adam_training,\n",
    "            initial_loss=4.0,\n",
    "            max_steps=30\n",
    "        )\n",
    "        \n",
    "        # Verify profiling results\n",
    "        assert 'optimizer_name' in sgd_profile, \"SGD profile should contain optimizer name\"\n",
    "        assert 'optimizer_name' in adam_profile, \"Adam profile should contain optimizer name\"\n",
    "        assert 'final_loss' in sgd_profile, \"SGD profile should contain final loss\"\n",
    "        assert 'final_loss' in adam_profile, \"Adam profile should contain final loss\"\n",
    "        \n",
    "        print(f\"   SGD final loss: {sgd_profile['final_loss']:.4f}\")\n",
    "        print(f\"   Adam final loss: {adam_profile['final_loss']:.4f}\")\n",
    "        print(\"‚úÖ Convergence profiling completed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Convergence profiling failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Test optimizer comparison\n",
    "    try:\n",
    "        print(\"\\nüèÜ Comparing optimizer performance...\")\n",
    "        \n",
    "        profiles = {\n",
    "            'SGD_momentum': sgd_profile,\n",
    "            'Adam': adam_profile\n",
    "        }\n",
    "        \n",
    "        comparison = profiler.compare_optimizers(profiles)\n",
    "        \n",
    "        # Verify comparison results\n",
    "        assert 'convergence_speed' in comparison, \"Should compare convergence speed\"\n",
    "        assert 'final_performance' in comparison, \"Should compare final performance\"\n",
    "        assert 'rankings' in comparison, \"Should provide rankings\"\n",
    "        assert 'recommendations' in comparison, \"Should provide recommendations\"\n",
    "        \n",
    "        if 'summary' in comparison['recommendations']:\n",
    "            print(\"   Recommendations:\")\n",
    "            for rec in comparison['recommendations']['summary']:\n",
    "                print(f\"     {rec}\")\n",
    "        \n",
    "        print(\"‚úÖ Optimizer comparison completed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Optimizer comparison failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Test memory analysis\n",
    "    try:\n",
    "        print(\"\\nüíæ Analyzing memory usage...\")\n",
    "        \n",
    "        # Simulate large model parameters\n",
    "        num_parameters = 100000  # 100K parameters\n",
    "        \n",
    "        sgd_memory = profiler.estimate_memory_usage(sgd_optimizer, num_parameters)\n",
    "        adam_memory = profiler.estimate_memory_usage(adam_optimizer, num_parameters)\n",
    "        \n",
    "        print(f\"   SGD memory usage: {sgd_memory['total_mb']:.1f} MB\")\n",
    "        print(f\"   Adam memory usage: {adam_memory['total_mb']:.1f} MB\")\n",
    "        print(f\"   Adam overhead: {adam_memory['total_mb'] - sgd_memory['total_mb']:.1f} MB\")\n",
    "        \n",
    "        # Verify memory analysis\n",
    "        assert sgd_memory['total_mb'] > 0, \"SGD should have positive memory usage\"\n",
    "        assert adam_memory['total_mb'] > sgd_memory['total_mb'], \"Adam should use more memory than SGD\"\n",
    "        \n",
    "        print(\"‚úÖ Memory analysis completed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Memory analysis failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Test advanced features integration\n",
    "    try:\n",
    "        print(\"\\nüöÄ Testing advanced optimizer features...\")\n",
    "        \n",
    "        # Test gradient clipping\n",
    "        w_clip = Variable(1.0, requires_grad=True)\n",
    "        w_clip.grad = Variable(5.0)  # Large gradient\n",
    "        clip_optimizer = SGD([w_clip], learning_rate=0.01)\n",
    "        \n",
    "        original_norm = advanced_features.apply_gradient_clipping(clip_optimizer, max_norm=1.0)\n",
    "        assert original_norm > 1.0, \"Should detect large gradient\"\n",
    "        assert abs(w_clip.grad.data.data.item()) <= 1.0, \"Should clip gradient\"\n",
    "        \n",
    "        # Test learning rate warmup\n",
    "        warmup_optimizer = Adam([Variable(1.0)], learning_rate=0.001)\n",
    "        lr_start = advanced_features.apply_warmup_schedule(warmup_optimizer, 0, 100, 0.001)\n",
    "        lr_mid = advanced_features.apply_warmup_schedule(warmup_optimizer, 50, 100, 0.001)\n",
    "        lr_end = advanced_features.apply_warmup_schedule(warmup_optimizer, 100, 100, 0.001)\n",
    "        \n",
    "        assert lr_start < lr_mid < lr_end, \"Learning rate should increase during warmup\"\n",
    "        \n",
    "        print(\"‚úÖ Advanced features integration completed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Advanced features integration failed: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Test production recommendations\n",
    "    try:\n",
    "        print(\"\\nüìã Generating production recommendations...\")\n",
    "        \n",
    "        analysis_results = {\n",
    "            'convergence_comparison': comparison,\n",
    "            'memory_analysis': {\n",
    "                'sgd': sgd_memory,\n",
    "                'adam': adam_memory\n",
    "            },\n",
    "            'learning_rate_analysis': {\n",
    "                'optimal_range': (0.01, 0.1)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        recommendations = profiler.generate_production_recommendations(analysis_results)\n",
    "        \n",
    "        assert len(recommendations) > 0, \"Should generate recommendations\"\n",
    "        \n",
    "        print(\"   Production guidance:\")\n",
    "        for i, rec in enumerate(recommendations[:5]):  # Show first 5 recommendations\n",
    "            print(f\"     {rec}\")\n",
    "        \n",
    "        print(\"‚úÖ Production recommendations generated\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Production recommendations failed: {e}\")\n",
    "        raise\n",
    "\n",
    "    print(\"\\nüéØ ML Systems Integration Results:\")\n",
    "    print(\"   ‚úÖ Optimizer convergence profiling works end-to-end\")\n",
    "    print(\"   ‚úÖ Performance comparison identifies best optimizers\")\n",
    "    print(\"   ‚úÖ Memory analysis guides resource planning\")\n",
    "    print(\"   ‚úÖ Advanced features enhance training stability\")\n",
    "    print(\"   ‚úÖ Production recommendations provide actionable guidance\")\n",
    "    print(\"   üöÄ Ready for real-world ML systems deployment!\")\n",
    "    print(\"üìà Progress: Comprehensive ML Systems Integration ‚úì\")\n",
    "\n",
    "# Test function defined (called in main block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405ec7e0",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "# üéØ ML SYSTEMS THINKING: Optimizers in Production\n",
    "\n",
    "## Production Deployment Considerations\n",
    "\n",
    "**You've just built a comprehensive optimizer analysis system!** Let's reflect on how this connects to real ML systems:\n",
    "\n",
    "## System Design Questions\n",
    "1. **Optimizer Selection Strategy**: How would you build an automated system that selects the best optimizer for a new model architecture?\n",
    "\n",
    "2. **Resource Planning**: Given memory constraints and training time budgets, how would you choose between SGD and Adam for different model sizes?\n",
    "\n",
    "3. **Distributed Training**: How do gradient synchronization patterns affect optimizer performance across multiple GPUs or nodes?\n",
    "\n",
    "4. **Production Monitoring**: What metrics would you track in production to detect optimizer-related training issues?\n",
    "\n",
    "## Production ML Workflows\n",
    "1. **Hyperparameter Search**: How would you integrate your convergence profiler into an automated hyperparameter tuning pipeline?\n",
    "\n",
    "2. **Training Pipeline**: Where would gradient clipping and mixed precision fit into a production training workflow?\n",
    "\n",
    "3. **Cost Optimization**: How would you balance optimizer performance against computational cost for training large models?\n",
    "\n",
    "4. **Model Lifecycle**: How do optimizer choices change when fine-tuning vs training from scratch vs transfer learning?\n",
    "\n",
    "## Framework Design Insights\n",
    "1. **Optimizer Abstraction**: Why do frameworks like PyTorch separate optimizers from models? How does this design enable flexibility?\n",
    "\n",
    "2. **State Management**: How do frameworks handle optimizer state persistence for training checkpoints and resumption?\n",
    "\n",
    "3. **Memory Efficiency**: What design patterns enable frameworks to minimize memory overhead for optimizer state?\n",
    "\n",
    "4. **Plugin Architecture**: How would you design an optimizer plugin system that allows researchers to add new algorithms?\n",
    "\n",
    "## Performance & Scale Challenges\n",
    "1. **Large Model Training**: How do optimizer memory requirements scale with model size, and what strategies mitigate this?\n",
    "\n",
    "2. **Dynamic Batching**: How would you adapt your gradient accumulation strategy for variable batch sizes in production?\n",
    "\n",
    "3. **Fault Tolerance**: How would you design optimizer state recovery for interrupted training runs in cloud environments?\n",
    "\n",
    "4. **Cross-Hardware Portability**: How do optimizer implementations need to change when moving between CPUs, GPUs, and specialized ML accelerators?\n",
    "\n",
    "These questions connect your optimizer implementations to the broader ecosystem of production ML systems, where optimization is just one piece of complex training and deployment pipelines.\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üß™ Running comprehensive optimizer tests...\")\n",
    "    \n",
    "    # Run all tests\n",
    "    test_unit_sgd_implementation()\n",
    "    test_unit_sgd_with_momentum()\n",
    "    test_unit_adam_optimizer()\n",
    "    test_module_optimizer_neural_network_training()\n",
    "    test_memory_profiler()\n",
    "    \n",
    "    print(\"All tests passed!\")\n",
    "    print(\"Optimizers module complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9b54d4",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ü§î ML Systems Thinking: Interactive Questions\n",
    "\n",
    "Now that you've built optimization algorithms that drive neural network training, let's connect this foundational work to broader ML systems challenges. These questions help you think critically about how optimization strategies scale to production training environments.\n",
    "\n",
    "Take time to reflect thoughtfully on each question - your insights will help you understand how the optimization concepts you've implemented connect to real-world ML systems engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9dc525",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Question 1: Memory Overhead and Optimizer State Management\n",
    "\n",
    "**Context**: Your Adam optimizer maintains momentum and variance buffers for each parameter, creating 3√ó memory overhead compared to SGD. Production training systems with billions of parameters must carefully manage optimizer state memory while maintaining training efficiency and fault tolerance.\n",
    "\n",
    "**Reflection Question**: Design an optimizer state management system for large-scale neural network training that optimizes memory usage while supporting distributed training and fault recovery. How would you implement memory-efficient optimizer state storage, handle state partitioning across devices, and manage optimizer checkpointing for training resumption? Consider scenarios where optimizer state memory exceeds model parameter memory and requires specialized optimization strategies.\n",
    "\n",
    "Think about: memory optimization techniques, distributed state management, checkpointing strategies, and fault tolerance considerations.\n",
    "\n",
    "*Target length: 150-300 words*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12f61d6",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "question-1-optimizer-memory",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "YOUR REFLECTION ON MEMORY OVERHEAD AND OPTIMIZER STATE MANAGEMENT:\n",
    "\n",
    "TODO: Replace this text with your thoughtful response about optimizer state management system design.\n",
    "\n",
    "Consider addressing:\n",
    "- How would you optimize memory usage for optimizers that maintain extensive per-parameter state?\n",
    "- What strategies would you use for distributed optimizer state management across multiple devices?\n",
    "- How would you implement efficient checkpointing and state recovery for long-running training jobs?\n",
    "- What role would state compression and quantization play in your optimization approach?\n",
    "- How would you balance memory efficiency with optimization algorithm effectiveness?\n",
    "\n",
    "Write a technical analysis connecting your optimizer implementations to real memory management challenges.\n",
    "\n",
    "GRADING RUBRIC (Instructor Use):\n",
    "- Demonstrates understanding of optimizer memory overhead and state management (3 points)\n",
    "- Addresses distributed state management and partitioning strategies (3 points)\n",
    "- Shows practical knowledge of checkpointing and fault tolerance techniques (2 points)\n",
    "- Demonstrates systems thinking about memory vs optimization trade-offs (2 points)\n",
    "- Clear technical reasoning and practical considerations (bonus points for innovative approaches)\n",
    "\"\"\"\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "# Student response area - instructor will replace this section during grading setup\n",
    "# This is a manually graded question requiring technical analysis of optimizer state management\n",
    "# Students should demonstrate understanding of memory optimization and distributed state handling\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f51d2b",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Question 2: Distributed Optimization and Learning Rate Scheduling\n",
    "\n",
    "**Context**: Your optimizers work on single devices with fixed learning rate schedules. Production distributed training systems must coordinate optimization across multiple workers while adapting learning rates based on real-time training dynamics and system constraints.\n",
    "\n",
    "**Reflection Question**: Architect a distributed optimization system that coordinates parameter updates across multiple workers while implementing adaptive learning rate scheduling responsive to training progress and system constraints. How would you handle gradient aggregation strategies, implement learning rate scaling for different batch sizes, and design adaptive scheduling that responds to convergence patterns? Consider scenarios where training must adapt to varying computational resources and time constraints in cloud environments.\n",
    "\n",
    "Think about: distributed optimization strategies, adaptive learning rate techniques, gradient aggregation methods, and system-aware scheduling.\n",
    "\n",
    "*Target length: 150-300 words*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0e7396",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "question-2-distributed-optimization",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "YOUR REFLECTION ON DISTRIBUTED OPTIMIZATION AND LEARNING RATE SCHEDULING:\n",
    "\n",
    "TODO: Replace this text with your thoughtful response about distributed optimization system design.\n",
    "\n",
    "Consider addressing:\n",
    "- How would you coordinate parameter updates across multiple workers in distributed training?\n",
    "- What strategies would you use for gradient aggregation and synchronization?\n",
    "- How would you implement adaptive learning rate scheduling that responds to training dynamics?\n",
    "- What role would system constraints and resource availability play in your optimization design?\n",
    "- How would you handle learning rate scaling and batch size considerations in distributed settings?\n",
    "\n",
    "Write an architectural analysis connecting your optimizer implementations to real distributed training challenges.\n",
    "\n",
    "GRADING RUBRIC (Instructor Use):\n",
    "- Shows understanding of distributed optimization and coordination challenges (3 points)\n",
    "- Designs practical approaches to gradient aggregation and learning rate adaptation (3 points)\n",
    "- Addresses system constraints and resource-aware optimization (2 points)\n",
    "- Demonstrates systems thinking about distributed training coordination (2 points)\n",
    "- Clear architectural reasoning with distributed systems insights (bonus points for comprehensive understanding)\n",
    "\"\"\"\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "# Student response area - instructor will replace this section during grading setup\n",
    "# This is a manually graded question requiring understanding of distributed optimization systems\n",
    "# Students should demonstrate knowledge of gradient aggregation and adaptive scheduling\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c089957",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Question 3: Production Integration and Optimization Monitoring\n",
    "\n",
    "**Context**: Your optimizer implementations provide basic parameter updates, but production ML systems require comprehensive optimization monitoring, hyperparameter tuning, and integration with MLOps pipelines for continuous training and model improvement.\n",
    "\n",
    "**Reflection Question**: Design a production optimization system that integrates with MLOps pipelines and provides comprehensive optimization monitoring and automated hyperparameter tuning. How would you implement real-time optimization metrics collection, automated optimizer selection based on model characteristics, and integration with experiment tracking and model deployment systems? Consider scenarios where optimization strategies must adapt to changing data distributions and business requirements in production environments.\n",
    "\n",
    "Think about: optimization monitoring systems, automated hyperparameter tuning, MLOps integration, and adaptive optimization strategies.\n",
    "\n",
    "*Target length: 150-300 words*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5994d78f",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "question-3-production-integration",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "YOUR REFLECTION ON PRODUCTION INTEGRATION AND OPTIMIZATION MONITORING:\n",
    "\n",
    "TODO: Replace this text with your thoughtful response about production optimization system design.\n",
    "\n",
    "Consider addressing:\n",
    "- How would you design optimization monitoring and metrics collection for production training?\n",
    "- What strategies would you use for automated optimizer selection and hyperparameter tuning?\n",
    "- How would you integrate optimization systems with MLOps pipelines and experiment tracking?\n",
    "- What role would adaptive optimization play in responding to changing data and requirements?\n",
    "- How would you ensure optimization system reliability and performance in production environments?\n",
    "\n",
    "Write a systems analysis connecting your optimizer implementations to real production integration challenges.\n",
    "\n",
    "GRADING RUBRIC (Instructor Use):\n",
    "- Understands production optimization monitoring and MLOps integration (3 points)\n",
    "- Designs practical approaches to automated tuning and optimization selection (3 points)\n",
    "- Addresses adaptive optimization and production reliability considerations (2 points)\n",
    "- Shows systems thinking about optimization system integration and monitoring (2 points)\n",
    "- Clear systems reasoning with production deployment insights (bonus points for deep understanding)\n",
    "\"\"\"\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "# Student response area - instructor will replace this section during grading setup\n",
    "# This is a manually graded question requiring understanding of production optimization systems\n",
    "# Students should demonstrate knowledge of MLOps integration and optimization monitoring\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a744ba",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## üéØ MODULE SUMMARY: Optimization Algorithms with ML Systems\n",
    "\n",
    "Congratulations! You've successfully implemented optimization algorithms with comprehensive ML systems analysis:\n",
    "\n",
    "### What You've Accomplished\n",
    "‚úÖ **Gradient Descent**: The foundation of all optimization algorithms\n",
    "‚úÖ **SGD with Momentum**: Improved convergence with momentum\n",
    "‚úÖ **Adam Optimizer**: Adaptive learning rates for better training\n",
    "‚úÖ **Learning Rate Scheduling**: Dynamic learning rate adjustment\n",
    "‚úÖ **ML Systems Analysis**: OptimizerConvergenceProfiler for production insights\n",
    "‚úÖ **Advanced Features**: Gradient clipping, warmup, accumulation, mixed precision\n",
    "‚úÖ **Production Integration**: Complete optimizer analysis and recommendation system\n",
    "\n",
    "### Key Concepts You've Learned\n",
    "- **Gradient-based optimization**: How gradients guide parameter updates\n",
    "- **Momentum**: Using velocity to improve convergence\n",
    "- **Adaptive learning rates**: Adam's adaptive moment estimation\n",
    "- **Learning rate scheduling**: Dynamic adjustment of learning rates\n",
    "- **Convergence analysis**: Profiling optimizer performance patterns\n",
    "- **Memory efficiency**: Resource usage comparison across optimizers\n",
    "- **Production patterns**: Advanced features for real-world deployment\n",
    "\n",
    "### Mathematical Foundations\n",
    "- **Gradient descent**: Œ∏ = Œ∏ - Œ±‚àáŒ∏J(Œ∏)\n",
    "- **Momentum**: v = Œ≤v + (1-Œ≤)‚àáŒ∏J(Œ∏), Œ∏ = Œ∏ - Œ±v\n",
    "- **Adam**: Adaptive moment estimation with bias correction\n",
    "- **Learning rate scheduling**: StepLR and other scheduling strategies\n",
    "- **Gradient clipping**: norm_clip = min(norm, max_norm) * grad / norm\n",
    "- **Gradient accumulation**: grad_avg = Œ£grad_i / accumulation_steps\n",
    "\n",
    "### Professional Skills Developed\n",
    "- **Algorithm implementation**: Building optimization algorithms from scratch\n",
    "- **Performance analysis**: Profiling and comparing optimizer convergence\n",
    "- **System design thinking**: Understanding production optimization workflows\n",
    "- **Resource optimization**: Memory usage analysis and efficiency planning\n",
    "- **Integration testing**: Ensuring optimizers work with neural networks\n",
    "- **Production readiness**: Advanced features for real-world deployment\n",
    "\n",
    "### Ready for Advanced Applications\n",
    "Your optimization implementations now enable:\n",
    "- **Neural network training**: Complete training pipelines with optimizers\n",
    "- **Hyperparameter optimization**: Data-driven optimizer and LR selection\n",
    "- **Advanced architectures**: Training complex models efficiently\n",
    "- **Production deployment**: ML systems with optimizer monitoring and tuning\n",
    "- **Research**: Experimenting with new optimization algorithms\n",
    "- **Scalable training**: Distributed and memory-efficient optimization\n",
    "\n",
    "### Connection to Real ML Systems\n",
    "Your implementations mirror production systems:\n",
    "- **PyTorch**: `torch.optim.SGD`, `torch.optim.Adam` provide identical functionality\n",
    "- **TensorFlow**: `tf.keras.optimizers` implements similar concepts\n",
    "- **MLflow/Weights&Biases**: Your profiler mirrors production monitoring tools\n",
    "- **Ray Tune/Optuna**: Your convergence analysis enables hyperparameter optimization\n",
    "- **Industry Standard**: Every major ML framework uses these exact algorithms and patterns\n",
    "\n",
    "### Next Steps\n",
    "1. **Export your code**: `tito export 10_optimizers`\n",
    "2. **Test your implementation**: `tito test 10_optimizers`\n",
    "3. **Deploy ML systems**: Use your profiler for real optimizer selection\n",
    "4. **Build training systems**: Combine with neural networks for complete training\n",
    "5. **Move to Module 11**: Add complete training pipelines!\n",
    "\n",
    "**Ready for production?** Your optimization algorithms and ML systems analysis tools are now ready for real-world deployment and performance optimization!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
