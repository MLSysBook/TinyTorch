{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7023e2cc",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Setup - TinyTorch Development Environment Configuration\n",
    "\n",
    "Welcome to the Setup module! You'll configure your development environment and master the foundation of professional ML systems development.\n",
    "\n",
    "## Learning Goals\n",
    "- Systems understanding: How environment configuration affects ML system reproducibility and performance\n",
    "- Core implementation skill: Build system configuration and introspection capabilities\n",
    "- Pattern recognition: Understand how professional ML teams manage development environments\n",
    "- Framework connection: See how PyTorch handles environment detection and hardware optimization\n",
    "- Performance insight: Learn why proper environment setup is critical for ML system performance\n",
    "\n",
    "## Build â†’ Use â†’ Reflect\n",
    "1. **Build**: System configuration and environment detection functions\n",
    "2. **Use**: Configure your personal TinyTorch installation with environment-aware settings\n",
    "3. **Reflect**: Why do ML systems fail when environments differ between development and production?\n",
    "\n",
    "## What You'll Achieve\n",
    "By the end of this module, you'll understand:\n",
    "- Deep technical understanding of how ML systems detect and adapt to their runtime environment\n",
    "- Practical capability to build robust configuration systems that work across different platforms\n",
    "- Systems insight into why environment reproducibility is critical for ML system reliability\n",
    "- Performance consideration of how hardware detection enables automatic optimization choices\n",
    "- Connection to production ML systems and how frameworks like PyTorch handle cross-platform deployment\n",
    "\n",
    "## Systems Reality Check\n",
    "ðŸ’¡ **Production Context**: PyTorch automatically detects CUDA availability and optimizes operations based on hardware - your configuration system enables similar adaptability\n",
    "âš¡ **Performance Note**: Environment detection happens once at startup, but configuration choices affect every operation - design for minimal runtime overhead\n",
    "\n",
    "Let's build the foundation of your ML systems engineering skills!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ff455b",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "setup-imports",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| default_exp core.setup\n",
    "\n",
    "#| export\n",
    "import sys\n",
    "import platform\n",
    "import psutil\n",
    "from typing import Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b599384",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "setup-verification",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"ðŸ”¥ TinyTorch Setup Module\")\n",
    "print(f\"Python version: {sys.version_info.major}.{sys.version_info.minor}\")\n",
    "print(f\"Platform: {platform.system()}\")\n",
    "print(\"Ready to configure your TinyTorch installation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69084c61",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ðŸ—ï¸ The Architecture of ML Systems Configuration\n",
    "\n",
    "### Configuration Layers in Production ML\n",
    "Real ML systems have multiple configuration layers:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚        Application Config           â”‚  â† Your personal info\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚        System Environment           â”‚  â† Hardware specs\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚        Runtime Configuration        â”‚  â† Python, libraries\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚        Infrastructure Config        â”‚  â† Cloud, containers\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Why Each Layer Matters\n",
    "- **Application**: Identifies who built what and when\n",
    "- **System**: Determines performance characteristics and limitations\n",
    "- **Runtime**: Affects compatibility and feature availability\n",
    "- **Infrastructure**: Enables scaling and deployment strategies\n",
    "\n",
    "### Connection to Real ML Frameworks\n",
    "Every major ML framework has configuration:\n",
    "- **PyTorch**: `torch.cuda.is_available()`, `torch.get_num_threads()`\n",
    "- **TensorFlow**: `tf.config.list_physical_devices()`, `tf.sysconfig.get_build_info()`\n",
    "- **Hugging Face**: Model cards with system requirements and performance metrics\n",
    "- **MLflow**: Experiment tracking with system context and reproducibility\n",
    "\n",
    "### TinyTorch's Approach\n",
    "We'll build configuration that's:\n",
    "- **Educational**: Teaches system awareness\n",
    "- **Practical**: Actually useful for debugging\n",
    "- **Professional**: Follows industry standards\n",
    "- **Extensible**: Ready for future ML systems features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fa8bd3",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 2
   },
   "source": [
    "## Step 1: What is System Configuration?\n",
    "\n",
    "### Definition\n",
    "**System configuration** is the process of setting up your development environment with personalized information and system diagnostics. In TinyTorch, this means:\n",
    "\n",
    "- **Personal Information**: Your name, email, institution for identification\n",
    "- **System Information**: Hardware specs, Python version, platform details\n",
    "- **Customization**: Making your TinyTorch installation uniquely yours\n",
    "\n",
    "### Why Configuration Matters in ML Systems\n",
    "Proper system configuration is crucial because:\n",
    "\n",
    "#### 1. **Reproducibility** \n",
    "Your setup can be documented and shared:\n",
    "```python\n",
    "# Someone else can recreate your environment\n",
    "config = {\n",
    "    'developer': 'Your Name',\n",
    "    'python_version': '3.9.7',\n",
    "    'platform': 'Darwin',\n",
    "    'memory_gb': 16.0\n",
    "}\n",
    "```\n",
    "\n",
    "#### 2. **Debugging**\n",
    "System info helps troubleshoot ML performance issues:\n",
    "- **Memory errors**: \"Do I have enough RAM for this model?\"\n",
    "- **Performance issues**: \"How many CPU cores can I use?\"\n",
    "- **Compatibility problems**: \"What Python version am I running?\"\n",
    "\n",
    "#### 3. **Professional Development**\n",
    "Shows proper engineering practices:\n",
    "- **Attribution**: Your work is properly credited\n",
    "- **Collaboration**: Others can contact you about your code\n",
    "- **Documentation**: System context is preserved\n",
    "\n",
    "#### 4. **ML Systems Integration**\n",
    "Connects to broader ML engineering:\n",
    "- **Model cards**: Document system requirements\n",
    "- **Experiment tracking**: Record hardware context\n",
    "- **Deployment**: Match development to production environments\n",
    "\n",
    "### Real-World Examples\n",
    "- **Google Colab**: Shows GPU type, RAM, disk space\n",
    "- **Kaggle**: Displays system specs for reproducibility\n",
    "- **MLflow**: Tracks system context with experiments\n",
    "- **Docker**: Containerizes entire system configuration\n",
    "\n",
    "Let's start configuring your TinyTorch system!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64cb15f",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 2: Personal Information Configuration\n",
    "\n",
    "### The Concept: Identity in ML Systems\n",
    "Your **personal information** identifies you as the developer and configures your TinyTorch installation. This isn't just administrative - it's foundational to professional ML development.\n",
    "\n",
    "### Why Personal Info Matters in ML Engineering\n",
    "\n",
    "#### 1. **Attribution and Accountability**\n",
    "- **Model ownership**: Who built this model?\n",
    "- **Responsibility**: Who should be contacted about issues?\n",
    "- **Credit**: Proper recognition for your work\n",
    "\n",
    "#### 2. **Collaboration and Communication**\n",
    "- **Team coordination**: Multiple developers on ML projects\n",
    "- **Knowledge sharing**: Others can learn from your work\n",
    "- **Bug reports**: Contact info for issues and improvements\n",
    "\n",
    "#### 3. **Professional Standards**\n",
    "- **Industry practice**: All professional software has attribution\n",
    "- **Open source**: Proper credit in shared code\n",
    "- **Academic integrity**: Clear authorship in research\n",
    "\n",
    "#### 4. **System Customization**\n",
    "- **Personalized experience**: Your TinyTorch installation\n",
    "- **Unique identification**: Distinguish your work from others\n",
    "- **Development tracking**: Link code to developer\n",
    "\n",
    "### Real-World Parallels\n",
    "- **Git commits**: Author name and email in every commit\n",
    "- **Docker images**: Maintainer information in container metadata\n",
    "- **Python packages**: Author info in `setup.py` and `pyproject.toml`\n",
    "- **Model cards**: Creator information for ML models\n",
    "\n",
    "### Best Practices for Personal Configuration\n",
    "- **Use real information**: Not placeholders or fake data\n",
    "- **Professional email**: Accessible and appropriate\n",
    "- **Descriptive system name**: Unique and meaningful\n",
    "- **Consistent formatting**: Follow established conventions\n",
    "\n",
    "Now let's implement your personal configuration!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5880f9a8",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### Before We Code: The 5 C's\n",
    "\n",
    "```python\n",
    "# CONCEPT: What is Personal Information Configuration?\n",
    "# Developer identity configuration that identifies you as the creator and\n",
    "# configures your TinyTorch installation. Think Git commit attribution -\n",
    "# every professional system needs to know who built it.\n",
    "\n",
    "# CODE STRUCTURE: What We're Building  \n",
    "def personal_info() -> Dict[str, str]:     # Returns developer identity\n",
    "    return {                               # Dictionary with required fields\n",
    "        'developer': 'Your Name',         # Your actual name\n",
    "        'email': 'your@domain.com',       # Contact information\n",
    "        'institution': 'Your Place',      # Affiliation\n",
    "        'system_name': 'YourName-Dev',    # Unique system identifier\n",
    "        'version': '1.0.0'                # Configuration version\n",
    "    }\n",
    "\n",
    "# CONNECTIONS: Real-World Equivalents\n",
    "# Git commits - author name and email in every commit\n",
    "# Docker images - maintainer information in container metadata\n",
    "# Python packages - author info in setup.py and pyproject.toml\n",
    "# Model cards - creator information for ML models\n",
    "\n",
    "# CONSTRAINTS: Key Implementation Requirements\n",
    "# - Use actual information (not placeholder text)\n",
    "# - Email must be valid format (contains @ and domain)\n",
    "# - System name should be unique and descriptive\n",
    "# - All values must be strings, version stays '1.0.0'\n",
    "\n",
    "# CONTEXT: Why This Matters in ML Systems\n",
    "# Professional ML development requires attribution:\n",
    "# - Model ownership: Who built this neural network?\n",
    "# - Collaboration: Others can contact you about issues\n",
    "# - Professional standards: Industry practice for all software\n",
    "# - System customization: Makes your TinyTorch installation unique\n",
    "```\n",
    "\n",
    "**You're establishing your identity in the ML systems world.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d3408b",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "personal-info",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def personal_info() -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Return personal information for this TinyTorch installation.\n",
    "    \n",
    "    This function configures your personal TinyTorch installation with your identity.\n",
    "    It's the foundation of proper ML engineering practices - every system needs\n",
    "    to know who built it and how to contact them.\n",
    "    \n",
    "    TODO: Implement personal information configuration.\n",
    "    \n",
    "    STEP-BY-STEP IMPLEMENTATION:\n",
    "    1. Create a dictionary with your personal details\n",
    "    2. Include all required keys: developer, email, institution, system_name, version\n",
    "    3. Use your actual information (not placeholder text)\n",
    "    4. Make system_name unique and descriptive\n",
    "    5. Keep version as '1.0.0' for now\n",
    "    \n",
    "    EXAMPLE USAGE:\n",
    "    ```python\n",
    "    # Get your personal configuration\n",
    "    info = personal_info()\n",
    "    print(info['developer'])     # Expected: \"Your Name\" (not placeholder)\n",
    "    print(info['email'])         # Expected: \"you@domain.com\" (valid email)\n",
    "    print(info['system_name'])   # Expected: \"YourName-Dev\" (unique identifier)\n",
    "    print(info)                  # Expected: Complete dict with 5 fields\n",
    "    # Output: {\n",
    "    #     'developer': 'Your Name',\n",
    "    #     'email': 'you@domain.com',\n",
    "    #     'institution': 'Your Institution',\n",
    "    #     'system_name': 'YourName-TinyTorch-Dev',\n",
    "    #     'version': '1.0.0'\n",
    "    # }\n",
    "    ```\n",
    "    \n",
    "    IMPLEMENTATION HINTS:\n",
    "    - Replace the example with your real information\n",
    "    - Use a descriptive system_name (e.g., 'YourName-TinyTorch-Dev')\n",
    "    - Keep email format valid (contains @ and domain)\n",
    "    - Make sure all values are strings\n",
    "    - Consider how this info will be used in debugging and collaboration\n",
    "    \n",
    "    LEARNING CONNECTIONS:\n",
    "    - This is like the 'author' field in Git commits\n",
    "    - Similar to maintainer info in Docker images\n",
    "    - Parallels author info in Python packages\n",
    "    - Foundation for professional ML development\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    return {\n",
    "        'developer': 'Student Name',\n",
    "        'email': 'student@university.edu',\n",
    "        'institution': 'University Name',\n",
    "        'system_name': 'StudentName-TinyTorch-Dev',\n",
    "        'version': '1.0.0'\n",
    "    }\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c0982a",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### ðŸ§ª Unit Test: Personal Information Configuration\n",
    "\n",
    "This test validates your `personal_info()` function implementation, ensuring it returns properly formatted developer information for system attribution and collaboration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39228303",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": true,
     "grade_id": "test-personal-info-immediate",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_personal_info_basic():\n",
    "    \"\"\"Test personal_info function implementation.\"\"\"\n",
    "    print(\"ðŸ”¬ Unit Test: Personal Information...\")\n",
    "    \n",
    "    # Test personal_info function\n",
    "    personal = personal_info()\n",
    "    \n",
    "    # Test return type\n",
    "    assert isinstance(personal, dict), \"personal_info should return a dictionary\"\n",
    "    \n",
    "    # Test required keys\n",
    "    required_keys = ['developer', 'email', 'institution', 'system_name', 'version']\n",
    "    for key in required_keys:\n",
    "        assert key in personal, f\"Dictionary should have '{key}' key\"\n",
    "    \n",
    "    # Test non-empty values\n",
    "    for key, value in personal.items():\n",
    "        assert isinstance(value, str), f\"Value for '{key}' should be a string\"\n",
    "        assert len(value) > 0, f\"Value for '{key}' cannot be empty\"\n",
    "    \n",
    "    # Test email format\n",
    "    assert '@' in personal['email'], \"Email should contain @ symbol\"\n",
    "    assert '.' in personal['email'], \"Email should contain domain\"\n",
    "    \n",
    "    # Test version format\n",
    "    assert personal['version'] == '1.0.0', \"Version should be '1.0.0'\"\n",
    "    \n",
    "    # Test system name (should be unique/personalized)\n",
    "    assert len(personal['system_name']) > 5, \"System name should be descriptive\"\n",
    "    \n",
    "    print(\"âœ… Personal info function tests passed!\")\n",
    "    print(f\"âœ… TinyTorch configured for: {personal['developer']}\")\n",
    "\n",
    "# Test function defined (called in main block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73694f24",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 3: System Information Queries\n",
    "\n",
    "### The Concept: Hardware-Aware ML Systems\n",
    "**System information** provides details about your hardware and software environment. This is crucial for ML development because machine learning is fundamentally about computation, and computation depends on hardware.\n",
    "\n",
    "### Why System Information Matters in ML Engineering\n",
    "\n",
    "#### 1. **Performance Optimization**\n",
    "- **CPU cores**: Determines parallelization strategies\n",
    "- **Memory**: Limits batch size and model size\n",
    "- **Architecture**: Affects numerical precision and optimization\n",
    "\n",
    "#### 2. **Compatibility and Debugging**\n",
    "- **Python version**: Determines available features and libraries\n",
    "- **Platform**: Affects file paths, process management, and system calls\n",
    "- **Architecture**: Influences numerical behavior and optimization\n",
    "\n",
    "#### 3. **Resource Planning**\n",
    "- **Training time estimation**: More cores = faster training\n",
    "- **Memory requirements**: Avoid out-of-memory errors\n",
    "- **Deployment matching**: Development should match production\n",
    "\n",
    "#### 4. **Reproducibility**\n",
    "- **Environment documentation**: Exact system specifications\n",
    "- **Performance comparison**: Same code, different hardware\n",
    "- **Bug reproduction**: System-specific issues\n",
    "\n",
    "### The Python System Query Toolkit\n",
    "You'll learn to use these essential Python modules:\n",
    "\n",
    "#### `sys.version_info` - Python Version\n",
    "```python\n",
    "version_info = sys.version_info\n",
    "python_version = f\"{version_info.major}.{version_info.minor}.{version_info.micro}\"\n",
    "# Example: \"3.9.7\"\n",
    "```\n",
    "\n",
    "#### `platform.system()` - Operating System\n",
    "```python\n",
    "platform_name = platform.system()\n",
    "# Examples: \"Darwin\" (macOS), \"Linux\", \"Windows\"\n",
    "```\n",
    "\n",
    "#### `platform.machine()` - CPU Architecture\n",
    "```python\n",
    "architecture = platform.machine()\n",
    "# Examples: \"x86_64\", \"arm64\", \"aarch64\"\n",
    "```\n",
    "\n",
    "#### `psutil.cpu_count()` - CPU Cores\n",
    "```python\n",
    "cpu_count = psutil.cpu_count()\n",
    "# Example: 8 (cores available for parallel processing)\n",
    "```\n",
    "\n",
    "#### `psutil.virtual_memory().total` - Total RAM\n",
    "```python\n",
    "memory_bytes = psutil.virtual_memory().total\n",
    "memory_gb = round(memory_bytes / (1024**3), 1)\n",
    "# Example: 16.0 GB\n",
    "```\n",
    "\n",
    "### Real-World Applications\n",
    "- **PyTorch**: `torch.get_num_threads()` uses CPU count\n",
    "- **TensorFlow**: `tf.config.list_physical_devices()` queries hardware\n",
    "- **Scikit-learn**: `n_jobs=-1` uses all available cores\n",
    "- **Dask**: Automatically configures workers based on CPU count\n",
    "\n",
    "### ML Systems Performance Considerations\n",
    "- **Memory-bound operations**: Matrix multiplication, large model loading\n",
    "- **CPU-bound operations**: Data preprocessing, feature engineering\n",
    "- **I/O-bound operations**: Data loading, model saving\n",
    "- **Platform-specific optimizations**: SIMD instructions, memory management\n",
    "\n",
    "Now let's implement system information queries!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9915b650",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### Before We Code: The 5 C's\n",
    "\n",
    "```python\n",
    "# CONCEPT: What is System Information?\n",
    "# Hardware and software environment detection for ML systems.\n",
    "# Think computer specifications for gaming - ML needs to know what\n",
    "# resources are available for optimal performance.\n",
    "\n",
    "# CODE STRUCTURE: What We're Building  \n",
    "def system_info() -> Dict[str, Any]:       # Queries system specs\n",
    "    return {                               # Hardware/software details\n",
    "        'python_version': '3.9.7',        # Python compatibility\n",
    "        'platform': 'Darwin',             # Operating system\n",
    "        'architecture': 'arm64',          # CPU architecture\n",
    "        'cpu_count': 8,                   # Parallel processing cores\n",
    "        'memory_gb': 16.0                 # Available RAM\n",
    "    }\n",
    "\n",
    "# CONNECTIONS: Real-World Equivalents\n",
    "# torch.get_num_threads() (PyTorch) - uses CPU count for optimization\n",
    "# tf.config.list_physical_devices() (TensorFlow) - queries hardware\n",
    "# psutil.cpu_count() (System monitoring) - same underlying queries\n",
    "# MLflow system tracking - documents environment for reproducibility\n",
    "\n",
    "# CONSTRAINTS: Key Implementation Requirements\n",
    "# - Use actual system queries (not hardcoded values)\n",
    "# - Convert memory from bytes to GB for readability\n",
    "# - Round memory to 1 decimal place for clean output\n",
    "# - Return proper data types (strings, int, float)\n",
    "\n",
    "# CONTEXT: Why This Matters in ML Systems\n",
    "# Hardware awareness enables performance optimization:\n",
    "# - Training: More CPU cores = faster data processing\n",
    "# - Memory: Determines maximum model and batch sizes\n",
    "# - Debugging: System specs help troubleshoot performance issues\n",
    "# - Reproducibility: Document exact environment for experiment tracking\n",
    "```\n",
    "\n",
    "**You're building hardware-aware ML systems that adapt to their environment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb29328",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": false,
     "grade_id": "system-info",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def system_info() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Query and return system information for this TinyTorch installation.\n",
    "    \n",
    "    This function gathers crucial hardware and software information that affects\n",
    "    ML performance, compatibility, and debugging. It's the foundation of \n",
    "    hardware-aware ML systems.\n",
    "    \n",
    "    TODO: Implement system information queries.\n",
    "    \n",
    "    STEP-BY-STEP IMPLEMENTATION:\n",
    "    1. Get Python version using sys.version_info\n",
    "    2. Get platform using platform.system()\n",
    "    3. Get architecture using platform.machine()\n",
    "    4. Get CPU count using psutil.cpu_count()\n",
    "    5. Get memory using psutil.virtual_memory().total\n",
    "    6. Convert memory from bytes to GB (divide by 1024^3)\n",
    "    7. Return all information in a dictionary\n",
    "    \n",
    "    EXAMPLE USAGE:\n",
    "    ```python\n",
    "    # Query system information\n",
    "    sys_info = system_info()\n",
    "    print(f\"Python: {sys_info['python_version']}\")  # Expected: \"3.x.x\"\n",
    "    print(f\"Platform: {sys_info['platform']}\")      # Expected: \"Darwin\"/\"Linux\"/\"Windows\"\n",
    "    print(f\"CPUs: {sys_info['cpu_count']}\")         # Expected: 4, 8, 16, etc.\n",
    "    print(f\"Memory: {sys_info['memory_gb']} GB\")    # Expected: 8.0, 16.0, 32.0, etc.\n",
    "    \n",
    "    # Full output example:\n",
    "    print(sys_info)\n",
    "    # Expected: {\n",
    "    #     'python_version': '3.9.7',\n",
    "    #     'platform': 'Darwin',\n",
    "    #     'architecture': 'arm64', \n",
    "    #     'cpu_count': 8,\n",
    "    #     'memory_gb': 16.0\n",
    "    # }\n",
    "    ```\n",
    "    \n",
    "    IMPLEMENTATION HINTS:\n",
    "    - Use f-string formatting for Python version: f\"{major}.{minor}.{micro}\"\n",
    "    - Memory conversion: bytes / (1024^3) = GB\n",
    "    - Round memory to 1 decimal place for readability\n",
    "    - Make sure data types are correct (strings for text, int for cpu_count, float for memory_gb)\n",
    "    \n",
    "    LEARNING CONNECTIONS:\n",
    "    - This is like `torch.cuda.is_available()` in PyTorch\n",
    "    - Similar to system info in MLflow experiment tracking\n",
    "    - Parallels hardware detection in TensorFlow\n",
    "    - Foundation for performance optimization in ML systems\n",
    "    \n",
    "    PERFORMANCE IMPLICATIONS:\n",
    "    - cpu_count affects parallel processing capabilities\n",
    "    - memory_gb determines maximum model and batch sizes\n",
    "    - platform affects file system and process management\n",
    "    - architecture influences numerical precision and optimization\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION\n",
    "    # Get Python version\n",
    "    version_info = sys.version_info\n",
    "    python_version = f\"{version_info.major}.{version_info.minor}.{version_info.micro}\"\n",
    "    \n",
    "    # Get platform information\n",
    "    platform_name = platform.system()\n",
    "    architecture = platform.machine()\n",
    "    \n",
    "    # Get CPU information\n",
    "    cpu_count = psutil.cpu_count()\n",
    "    \n",
    "    # Get memory information (convert bytes to GB)\n",
    "    memory_bytes = psutil.virtual_memory().total\n",
    "    memory_gb = round(memory_bytes / (1024**3), 1)\n",
    "    \n",
    "    return {\n",
    "        'python_version': python_version,\n",
    "        'platform': platform_name,\n",
    "        'architecture': architecture,\n",
    "        'cpu_count': cpu_count,\n",
    "        'memory_gb': memory_gb\n",
    "    }\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3a0594",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### ðŸ§ª Unit Test: System Information Query\n",
    "\n",
    "This test validates your `system_info()` function implementation, ensuring it accurately detects and reports hardware and software specifications for performance optimization and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f54c21",
   "metadata": {
    "lines_to_next_cell": 1,
    "nbgrader": {
     "grade": true,
     "grade_id": "test-system-info-immediate",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test_unit_system_info_basic():\n",
    "    \"\"\"Test system_info function implementation.\"\"\"\n",
    "    print(\"ðŸ”¬ Unit Test: System Information...\")\n",
    "    \n",
    "    # Test system_info function\n",
    "    sys_info = system_info()\n",
    "    \n",
    "    # Test return type\n",
    "    assert isinstance(sys_info, dict), \"system_info should return a dictionary\"\n",
    "    \n",
    "    # Test required keys\n",
    "    required_keys = ['python_version', 'platform', 'architecture', 'cpu_count', 'memory_gb']\n",
    "    for key in required_keys:\n",
    "        assert key in sys_info, f\"Dictionary should have '{key}' key\"\n",
    "    \n",
    "    # Test data types\n",
    "    assert isinstance(sys_info['python_version'], str), \"python_version should be string\"\n",
    "    assert isinstance(sys_info['platform'], str), \"platform should be string\"\n",
    "    assert isinstance(sys_info['architecture'], str), \"architecture should be string\"\n",
    "    assert isinstance(sys_info['cpu_count'], int), \"cpu_count should be integer\"\n",
    "    assert isinstance(sys_info['memory_gb'], (int, float)), \"memory_gb should be number\"\n",
    "    \n",
    "    # Test reasonable values\n",
    "    assert sys_info['cpu_count'] > 0, \"CPU count should be positive\"\n",
    "    assert sys_info['memory_gb'] > 0, \"Memory should be positive\"\n",
    "    assert len(sys_info['python_version']) > 0, \"Python version should not be empty\"\n",
    "    \n",
    "    # Test that values are actually queried (not hardcoded)\n",
    "    actual_version = f\"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\"\n",
    "    assert sys_info['python_version'] == actual_version, \"Python version should match actual system\"\n",
    "    \n",
    "    print(\"âœ… System info function tests passed!\")\n",
    "    print(f\"âœ… Python: {sys_info['python_version']} on {sys_info['platform']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730beb32",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "## ðŸŽ¯ Additional Comprehensive Tests\n",
    "\n",
    "These comprehensive tests validate that your configuration functions work together and integrate properly with the TinyTorch system.\n",
    "\"\"\"\n",
    "\n",
    "Test function defined (called in main block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12fa04e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\"\"\"\n",
    "# ðŸ§ª Testing Your Configuration Functions\n",
    "\n",
    "## The Importance of Testing in ML Systems\n",
    "Before we test your implementation, let's understand why testing is crucial in ML systems:\n",
    "\n",
    "### 1. **Reliability**\n",
    "- **Function correctness**: Does your code do what it's supposed to?\n",
    "- **Edge case handling**: What happens with unexpected inputs?\n",
    "- **Error detection**: Catch bugs before they cause problems\n",
    "\n",
    "### 2. **Reproducibility**\n",
    "- **Consistent behavior**: Same inputs always produce same outputs\n",
    "- **Environment validation**: Ensure setup works across different systems\n",
    "- **Regression prevention**: New changes don't break existing functionality\n",
    "\n",
    "### 3. **Professional Development**\n",
    "- **Code quality**: Well-tested code is maintainable code\n",
    "- **Collaboration**: Others can trust and extend your work\n",
    "- **Documentation**: Tests serve as executable documentation\n",
    "\n",
    "### 4. **ML-Specific Concerns**\n",
    "- **Data validation**: Ensure data types and shapes are correct\n",
    "- **Performance verification**: Check that optimizations work\n",
    "- **System compatibility**: Verify cross-platform behavior\n",
    "\n",
    "## Testing Strategy\n",
    "We'll use comprehensive testing that checks:\n",
    "- **Return types**: Are outputs the correct data types?\n",
    "- **Required fields**: Are all expected keys present?\n",
    "- **Data validation**: Are values reasonable and properly formatted?\n",
    "- **System accuracy**: Do queries match actual system state?\n",
    "\n",
    "Now let's test your configuration functions!\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the unit tests\n",
    "    test_unit_personal_info_basic()\n",
    "    test_unit_system_info_basic()\n",
    "    \n",
    "    print(\"All tests passed!\")\n",
    "    print(\"Setup module complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995aef75",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ðŸ¤” ML Systems Thinking: Interactive Questions\n",
    "\n",
    "Now that you've built configuration management for your TinyTorch system, let's connect this foundational work to broader ML systems challenges. These questions help you think critically about how basic setup principles scale to production ML environments.\n",
    "\n",
    "Take time to reflect thoughtfully on each question - your insights will help you understand how the configuration concepts you've implemented connect to real-world ML systems engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54236e9a",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Question 1: Team Development Configuration\n",
    "\n",
    "**Context**: You've implemented personal configuration for individual TinyTorch development. In real ML teams, multiple developers, data scientists, and engineers collaborate on shared models and infrastructure.\n",
    "\n",
    "**Reflection Question**: How would you design a configuration system that balances individual developer identity with team coordination needs in a production ML environment? Consider scenarios where teams share GPU clusters, model registries, and deployment pipelines while maintaining individual accountability and debugging capabilities.\n",
    "\n",
    "Think about: individual vs shared resources, conflict resolution, attribution tracking, and environment standardization.\n",
    "\n",
    "*Target length: 150-300 words*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c99286f",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "question-1-team-config",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "YOUR REFLECTION ON TEAM DEVELOPMENT CONFIGURATION:\n",
    "\n",
    "TODO: Replace this text with your thoughtful response about designing configuration systems for team-based ML development.\n",
    "\n",
    "Consider addressing:\n",
    "- How would you maintain individual identity while enabling team coordination?\n",
    "- What information would be shared vs. personal in a team configuration system?\n",
    "- How would you handle resource conflicts when multiple developers need GPU access?\n",
    "- What role would configuration play in debugging issues across team environments?\n",
    "- How might team configuration differ from individual configuration?\n",
    "\n",
    "Write a thoughtful analysis connecting your setup module experience to real team challenges.\n",
    "\n",
    "GRADING RUBRIC (Instructor Use):\n",
    "- Demonstrates understanding of individual vs team configuration needs (3 points)\n",
    "- Addresses resource sharing and conflict resolution challenges (3 points)  \n",
    "- Connects setup module concepts to real team scenarios (2 points)\n",
    "- Shows systems thinking about scalability and coordination (2 points)\n",
    "- Clear writing and practical insights (bonus points for exceptional responses)\n",
    "\"\"\"\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "# Student response area - instructor will replace this section during grading setup\n",
    "# This is a manually graded question requiring thoughtful analysis of team configuration challenges\n",
    "# Students should demonstrate understanding of balancing individual accountability with team coordination\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff6205a",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Question 2: Hardware-Aware System Design\n",
    "\n",
    "**Context**: Your system_info() function detects CPU cores, memory, and architecture. In production ML systems, this hardware awareness becomes critical for performance optimization and automated resource allocation.\n",
    "\n",
    "**Reflection Question**: Design a hardware-aware configuration system that automatically adapts ML training strategies based on detected resources. How would your system handle the transition from development (8GB laptop) to training (64GB GPU server) to inference (edge device with 2GB)? What configuration decisions would be automated vs. manual?\n",
    "\n",
    "Think about: automatic batch size scaling, parallelization strategies, memory management, and deployment target optimization.\n",
    "\n",
    "*Target length: 150-300 words*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2caf2d",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "question-2-hardware-aware",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "YOUR REFLECTION ON HARDWARE-AWARE SYSTEM DESIGN:\n",
    "\n",
    "TODO: Replace this text with your thoughtful response about hardware-aware configuration systems.\n",
    "\n",
    "Consider addressing:\n",
    "- How would your system automatically adapt training parameters based on detected hardware?\n",
    "- What would you do differently for development vs. training vs. inference environments?\n",
    "- How would you handle memory constraints and batch size optimization automatically?\n",
    "- What configuration decisions should be automated vs. left to manual tuning?\n",
    "- How would your system gracefully handle resource limitations?\n",
    "\n",
    "Write a practical design connecting your system_info() implementation to real performance challenges.\n",
    "\n",
    "GRADING RUBRIC (Instructor Use):\n",
    "- Shows understanding of hardware constraints impact on ML performance (3 points)\n",
    "- Designs practical automated adaptation strategies (3 points)\n",
    "- Addresses different deployment environments appropriately (2 points)\n",
    "- Demonstrates systems thinking about resource optimization (2 points)\n",
    "- Clear design reasoning and practical considerations (bonus points for innovative approaches)\n",
    "\"\"\"\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "# Student response area - instructor will replace this section during grading setup  \n",
    "# This is a manually graded question requiring design thinking about hardware-aware systems\n",
    "# Students should demonstrate understanding of automatic adaptation based on hardware detection\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09dd37a",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### Question 3: Configuration in Production ML Pipelines\n",
    "\n",
    "**Context**: Your configuration functions provide system information and personal attribution. In production ML pipelines, configuration becomes part of model lineage, experiment tracking, and compliance requirements.\n",
    "\n",
    "**Reflection Question**: How would configuration management integrate with modern ML operations (MLOps) tools for model deployment and monitoring? Consider a scenario where your model needs to be deployed across multiple environments with different compliance requirements, monitoring needs, and performance targets. What configuration information becomes part of the model artifact vs. environment-specific?\n",
    "\n",
    "Think about: model lineage tracking, compliance auditing, A/B testing configuration, and production monitoring requirements.\n",
    "\n",
    "*Target length: 150-300 words*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e37d1f9",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "question-3-production-config",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "YOUR REFLECTION ON PRODUCTION ML PIPELINE CONFIGURATION:\n",
    "\n",
    "TODO: Replace this text with your thoughtful response about configuration in production ML systems.\n",
    "\n",
    "Consider addressing:\n",
    "- How would configuration information become part of model lineage and audit trails?\n",
    "- What configuration would be bundled with the model vs. environment-specific?\n",
    "- How would your configuration system support A/B testing and deployment strategies?\n",
    "- What role would configuration play in monitoring and debugging production issues?\n",
    "- How would compliance requirements (data governance, model validation) affect configuration design?\n",
    "\n",
    "Write an analysis connecting your setup module concepts to MLOps and production deployment challenges.\n",
    "\n",
    "GRADING RUBRIC (Instructor Use):\n",
    "- Understands configuration role in model lineage and compliance (3 points)\n",
    "- Addresses model artifact vs. environment configuration separation (3 points)\n",
    "- Shows awareness of MLOps integration challenges (2 points)\n",
    "- Demonstrates production systems thinking (2 points)\n",
    "- Clear analysis with practical MLOps insights (bonus points for deep understanding)\n",
    "\"\"\"\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "# Student response area - instructor will replace this section during grading setup\n",
    "# This is a manually graded question requiring understanding of production ML configuration challenges  \n",
    "# Students should demonstrate understanding of configuration in MLOps and production deployment contexts\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8894d9eb",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ðŸŽ¯ MODULE SUMMARY: Setup Configuration\n",
    "\n",
    "You've successfully configured your TinyTorch installation and learned the foundations of ML systems engineering:\n",
    "\n",
    "### What You've Accomplished\n",
    "âœ… **Personal Configuration**: Set up your identity and custom system name  \n",
    "âœ… **System Queries**: Learned to gather hardware and software information  \n",
    "âœ… **NBGrader Workflow**: Mastered solution blocks and automated testing  \n",
    "âœ… **Code Export**: Created functions that become part of your tinytorch package  \n",
    "âœ… **Professional Setup**: Established proper development practices  \n",
    "\n",
    "### Key Concepts You've Learned\n",
    "\n",
    "#### 1. **System Awareness**\n",
    "- **Hardware constraints**: Understanding CPU, memory, and architecture limitations\n",
    "- **Software dependencies**: Python version and platform compatibility\n",
    "- **Performance implications**: How system specs affect ML workloads\n",
    "\n",
    "#### 2. **Configuration Management**\n",
    "- **Personal identification**: Professional attribution and contact information\n",
    "- **Environment documentation**: Reproducible system specifications\n",
    "- **Professional standards**: Industry-standard development practices\n",
    "\n",
    "#### 3. **ML Systems Foundations**\n",
    "- **Reproducibility**: System context for experiment tracking\n",
    "- **Debugging**: Hardware info for performance troubleshooting\n",
    "- **Collaboration**: Proper attribution and contact information\n",
    "\n",
    "#### 4. **Development Workflow**\n",
    "- **NBGrader integration**: Automated testing and grading\n",
    "- **Code export**: Functions become part of production package\n",
    "- **Testing practices**: Comprehensive validation of functionality\n",
    "\n",
    "### Next Steps in Your ML Systems Journey\n",
    "\n",
    "#### **Immediate Actions**\n",
    "1. **Export your code**: `tito module export 01_setup`\n",
    "2. **Test your installation**: \n",
    "   ```python\n",
    "   from tinytorch.core.setup import personal_info, system_info\n",
    "   print(personal_info())  # Your personal details\n",
    "   print(system_info())    # System information\n",
    "   ```\n",
    "3. **Verify package integration**: Ensure your functions work in the tinytorch package\n",
    "\n",
    "#### **Looking Ahead**\n",
    "- **Module 1 (Tensor)**: Build the fundamental data structure for ML\n",
    "- **Module 2 (Activations)**: Add nonlinearity for complex learning\n",
    "- **Module 3 (Layers)**: Create the building blocks of neural networks\n",
    "- **Module 4 (Networks)**: Compose layers into powerful architectures\n",
    "\n",
    "#### **Course Progression**\n",
    "You're now ready to build a complete ML system from scratch:\n",
    "```\n",
    "Setup â†’ Tensor â†’ Activations â†’ Layers â†’ Networks â†’ CNN â†’ DataLoader â†’ \n",
    "Autograd â†’ Optimizers â†’ Training â†’ Compression â†’ Kernels â†’ Benchmarking â†’ MLOps\n",
    "```\n",
    "\n",
    "### Professional Development Milestone\n",
    "\n",
    "You've taken your first step in ML systems engineering! This module taught you:\n",
    "- **System thinking**: Understanding hardware and software constraints\n",
    "- **Professional practices**: Proper attribution, testing, and documentation\n",
    "- **Tool mastery**: NBGrader workflow and package development\n",
    "- **Foundation building**: Creating reusable, tested, documented code\n",
    "\n",
    "**Ready for the next challenge?** Let's build the foundation of ML systems with tensors!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
