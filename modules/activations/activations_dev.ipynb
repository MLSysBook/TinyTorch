{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "jupyter:\n",
        "  jupytext:\n",
        "    text_representation:\n",
        "      extension: .py\n",
        "      format_name: percent\n",
        "      format_version: '1.3'\n",
        "      jupytext_version: 1.17.1\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# \ud83d\udd25 TinyTorch Activations Module\n",
        "\n",
        "Welcome to the **Activations** module! This is where you'll implement the mathematical functions that give neural networks their power.\n",
        "\n",
        "## \ud83c\udfaf Learning Objectives\n",
        "\n",
        "By the end of this module, you will:\n",
        "1. **Understand** why activation functions are essential for neural networks\n",
        "2. **Implement** the three most important activation functions: ReLU, Sigmoid, and Tanh\n",
        "3. **Test** your functions with various inputs to understand their behavior\n",
        "4. **Use** these functions as building blocks for neural networks\n",
        "\n",
        "## \ud83e\udde0 Why Activation Functions Matter\n",
        "\n",
        "**Without activation functions, neural networks are just linear transformations!**\n",
        "\n",
        "```\n",
        "Linear \u2192 Linear \u2192 Linear = Still just Linear\n",
        "Linear \u2192 Activation \u2192 Linear = Can learn complex patterns!\n",
        "```\n",
        "\n",
        "**Key insight**: Activation functions add **nonlinearity**, allowing networks to learn complex patterns that linear functions cannot capture.\n",
        "\n",
        "## \ud83d\udcda What You'll Build\n",
        "\n",
        "- **ReLU**: `f(x) = max(0, x)` - The workhorse of deep learning\n",
        "- **Sigmoid**: `f(x) = 1 / (1 + e^(-x))` - Squashes to (0, 1)\n",
        "- **Tanh**: `f(x) = tanh(x)` - Squashes to (-1, 1)\n",
        "\n",
        "Each function serves different purposes and has different mathematical properties.\n",
        "\n",
        "---\n",
        "\n",
        "Let's start building! \ud83d\ude80\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| default_exp core.activations\n",
        "\n",
        "# Standard library imports\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# TinyTorch imports\n",
        "from tinytorch.core.tensor import Tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function to detect if we're in a testing environment\n",
        "def _should_show_plots():\n",
        "    \"\"\"\n",
        "    Determine if we should show plots based on the execution context.\n",
        "    \n",
        "    Returns False if:\n",
        "    - Running in pytest (detected by 'pytest' in sys.modules)\n",
        "    - Running in test environment (detected by environment variables)\n",
        "    - Running from command line test runner\n",
        "    \n",
        "    Returns True if:\n",
        "    - Running in Jupyter notebook\n",
        "    - Running interactively in Python\n",
        "    \"\"\"\n",
        "    # Check if we're running in pytest\n",
        "    if 'pytest' in sys.modules:\n",
        "        return False\n",
        "    \n",
        "    # Check if we're in a test environment\n",
        "    if os.environ.get('PYTEST_CURRENT_TEST'):\n",
        "        return False\n",
        "    \n",
        "    # Check if we're running from a test file (more specific check)\n",
        "    if any(arg.endswith('.py') and 'test_' in os.path.basename(arg) and 'tests/' in arg for arg in sys.argv):\n",
        "        return False\n",
        "    \n",
        "    # Check if we're running from the tito CLI test command\n",
        "    if len(sys.argv) > 0 and 'tito.py' in sys.argv[0] and 'test' in sys.argv:\n",
        "        return False\n",
        "    \n",
        "    # Default to showing plots (notebook/interactive environment)\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "## Step 1: ReLU Activation Function\n",
        "\n",
        "**ReLU** (Rectified Linear Unit) is the most popular activation function in deep learning.\n",
        "\n",
        "**Formula**: `f(x) = max(0, x)`\n",
        "\n",
        "**Properties**:\n",
        "- **Simple**: Easy to compute and understand\n",
        "- **Sparse**: Outputs exactly zero for negative inputs\n",
        "- **Unbounded**: No upper limit on positive outputs\n",
        "- **Non-saturating**: Doesn't suffer from vanishing gradients\n",
        "\n",
        "**When to use**: Almost everywhere! It's the default choice for hidden layers.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| export\n",
        "class ReLU:\n",
        "    \"\"\"\n",
        "    ReLU Activation: f(x) = max(0, x)\n",
        "    \n",
        "    The most popular activation function in deep learning.\n",
        "    Simple, effective, and computationally efficient.\n",
        "    \n",
        "    TODO: Implement ReLU activation function.\n",
        "    \"\"\"\n",
        "    \n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Apply ReLU: f(x) = max(0, x)\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor\n",
        "            \n",
        "        Returns:\n",
        "            Output tensor with ReLU applied element-wise\n",
        "            \n",
        "        TODO: Implement element-wise max(0, x) operation\n",
        "        Hint: Use np.maximum(0, x.data)\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Student implementation required\")\n",
        "    \n",
        "    def __call__(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"Make activation callable: relu(x) same as relu.forward(x)\"\"\"\n",
        "        return self.forward(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| hide\n",
        "#| export\n",
        "class ReLU:\n",
        "    \"\"\"ReLU Activation: f(x) = max(0, x)\"\"\"\n",
        "    \n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"Apply ReLU: f(x) = max(0, x)\"\"\"\n",
        "        return Tensor(np.maximum(0, x.data))\n",
        "    \n",
        "    def __call__(self, x: Tensor) -> Tensor:\n",
        "        return self.forward(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "### \ud83e\uddea Test Your ReLU Function\n",
        "\n",
        "Once you implement ReLU above, run this cell to test it:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test ReLU function\n",
        "try:\n",
        "    print(\"=== Testing ReLU Function ===\")\n",
        "    \n",
        "    # Test data: mix of positive, negative, and zero\n",
        "    x = Tensor([[-3.0, -1.0, 0.0, 1.0, 3.0]])\n",
        "    print(f\"Input: {x.data}\")\n",
        "    \n",
        "    # Test ReLU\n",
        "    relu = ReLU()\n",
        "    y = relu(x)\n",
        "    print(f\"ReLU output: {y.data}\")\n",
        "    print(f\"Expected: [[0. 0. 0. 1. 3.]]\")\n",
        "    \n",
        "    # Test with different shapes\n",
        "    x_2d = Tensor([[-2.0, 1.0], [0.5, -0.5]])\n",
        "    y_2d = relu(x_2d)\n",
        "    print(f\"\\n2D Input: {x_2d.data}\")\n",
        "    print(f\"2D ReLU output: {y_2d.data}\")\n",
        "    \n",
        "    print(\"\u2705 ReLU working!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\u274c Error: {e}\")\n",
        "    print(\"Make sure to implement the ReLU function above!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "### \ud83d\udcca Visualize ReLU Function\n",
        "\n",
        "Let's plot the ReLU function to see how it transforms inputs:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot ReLU function\n",
        "try:\n",
        "    print(\"=== Plotting ReLU Function ===\")\n",
        "    \n",
        "    # Create a range of input values\n",
        "    x_range = np.linspace(-5, 5, 100)\n",
        "    x_tensor = Tensor([x_range])\n",
        "    \n",
        "    # Apply ReLU (student implementation)\n",
        "    relu = ReLU()\n",
        "    y_tensor = relu(x_tensor)\n",
        "    y_range = y_tensor.data[0]\n",
        "    \n",
        "    # Create ideal ReLU for comparison\n",
        "    y_ideal = np.maximum(0, x_range)\n",
        "    \n",
        "    # Only show plots if we're not in a testing environment\n",
        "    if _should_show_plots():\n",
        "        # Create the plot\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        \n",
        "        # Plot both student implementation and ideal\n",
        "        plt.subplot(2, 2, 1)\n",
        "        plt.plot(x_range, y_range, 'b-', linewidth=3, label='Your ReLU Implementation')\n",
        "        plt.plot(x_range, y_ideal, 'r--', linewidth=2, alpha=0.7, label='Ideal ReLU')\n",
        "        plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
        "        plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
        "        plt.xlabel('Input (x)')\n",
        "        plt.ylabel('Output')\n",
        "        plt.title('ReLU: Your Implementation vs Ideal')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.legend()\n",
        "        plt.xlim(-5, 5)\n",
        "        plt.ylim(-1, 5)\n",
        "        \n",
        "        # Mathematical explanation plot\n",
        "        plt.subplot(2, 2, 2)\n",
        "        # Show the mathematical definition\n",
        "        x_math = np.array([-3, -2, -1, 0, 1, 2, 3])\n",
        "        y_math = np.maximum(0, x_math)\n",
        "        plt.stem(x_math, y_math, basefmt=' ', linefmt='g-', markerfmt='go')\n",
        "        plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
        "        plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
        "        plt.xlabel('Input (x)')\n",
        "        plt.ylabel('max(0, x)')\n",
        "        plt.title('Mathematical Definition: max(0, x)')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.xlim(-4, 4)\n",
        "        plt.ylim(-0.5, 3.5)\n",
        "        \n",
        "        # Show the piecewise nature\n",
        "        plt.subplot(2, 2, 3)\n",
        "        x_left = np.linspace(-5, 0, 50)\n",
        "        x_right = np.linspace(0, 5, 50)\n",
        "        plt.plot(x_left, np.zeros_like(x_left), 'r-', linewidth=3, label='f(x) = 0 for x < 0')\n",
        "        plt.plot(x_right, x_right, 'b-', linewidth=3, label='f(x) = x for x \u2265 0')\n",
        "        plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
        "        plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
        "        plt.xlabel('Input (x)')\n",
        "        plt.ylabel('Output')\n",
        "        plt.title('Piecewise Function Definition')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.legend()\n",
        "        plt.xlim(-5, 5)\n",
        "        plt.ylim(-1, 5)\n",
        "        \n",
        "        # Error analysis\n",
        "        plt.subplot(2, 2, 4)\n",
        "        difference = np.abs(y_range - y_ideal)\n",
        "        max_error = np.max(difference)\n",
        "        plt.plot(x_range, difference, 'purple', linewidth=2)\n",
        "        plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
        "        plt.xlabel('Input (x)')\n",
        "        plt.ylabel('|Your Output - Ideal Output|')\n",
        "        plt.title(f'Implementation Error (Max: {max_error:.6f})')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.xlim(-5, 5)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Print analysis\n",
        "        print(f\"\\n\ud83d\udcca Analysis:\")\n",
        "        print(f\"\u2705 Maximum error: {max_error:.10f}\")\n",
        "        if max_error < 1e-10:\n",
        "            print(\"\ud83c\udf89 Perfect implementation!\")\n",
        "        elif max_error < 1e-6:\n",
        "            print(\"\ud83c\udf1f Excellent implementation!\")\n",
        "        elif max_error < 1e-3:\n",
        "            print(\"\ud83d\udc4d Good implementation!\")\n",
        "        else:\n",
        "            print(\"\ud83d\udd27 Implementation needs work.\")\n",
        "            \n",
        "        print(f\"\ud83d\udcc8 Function properties:\")\n",
        "        print(f\"   \u2022 Range: [0, \u221e)\")\n",
        "        print(f\"   \u2022 Piecewise: f(x) = 0 for x < 0, f(x) = x for x \u2265 0\")\n",
        "        print(f\"   \u2022 Monotonic: Always increasing for x \u2265 0\")\n",
        "        print(f\"   \u2022 Sparse: Exactly zero for negative inputs\")\n",
        "    else:\n",
        "        print(\"\ud83d\udcca Plots disabled during testing - this is normal!\")\n",
        "        \n",
        "    # Always show the mathematical analysis\n",
        "    difference = np.abs(y_range - y_ideal)\n",
        "    max_error = np.max(difference)\n",
        "    print(f\"\\n\ud83d\udcca Mathematical Analysis:\")\n",
        "    print(f\"\u2705 Maximum error: {max_error:.10f}\")\n",
        "    if max_error < 1e-10:\n",
        "        print(\"\ud83c\udf89 Perfect implementation!\")\n",
        "    elif max_error < 1e-6:\n",
        "        print(\"\ud83c\udf1f Excellent implementation!\")\n",
        "    elif max_error < 1e-3:\n",
        "        print(\"\ud83d\udc4d Good implementation!\")\n",
        "    else:\n",
        "        print(\"\ud83d\udd27 Implementation needs work.\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"\u274c Error in plotting: {e}\")\n",
        "    print(\"Make sure to implement the ReLU function above!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "## Step 2: Sigmoid Activation Function\n",
        "\n",
        "**Sigmoid** squashes any input to the range (0, 1), making it useful for probabilities.\n",
        "\n",
        "**Formula**: `f(x) = 1 / (1 + e^(-x))`\n",
        "\n",
        "**Properties**:\n",
        "- **Bounded**: Always outputs between 0 and 1\n",
        "- **Smooth**: Differentiable everywhere\n",
        "- **S-shaped**: Smooth transition from 0 to 1\n",
        "- **Saturating**: Can suffer from vanishing gradients\n",
        "\n",
        "**When to use**: Binary classification (final layer), gates in RNNs/LSTMs.\n",
        "\n",
        "**\u26a0\ufe0f Numerical Stability**: Be careful with large inputs to avoid overflow!\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| export\n",
        "class Sigmoid:\n",
        "    \"\"\"\n",
        "    Sigmoid Activation: f(x) = 1 / (1 + e^(-x))\n",
        "    \n",
        "    Squashes input to range (0, 1). Often used for binary classification.\n",
        "    \n",
        "    TODO: Implement Sigmoid activation function.\n",
        "    \"\"\"\n",
        "    \n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Apply Sigmoid: f(x) = 1 / (1 + e^(-x))\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor\n",
        "            \n",
        "        Returns:\n",
        "            Output tensor with Sigmoid applied element-wise\n",
        "            \n",
        "        TODO: Implement sigmoid function (be careful with numerical stability!)\n",
        "        \n",
        "        Hint: For numerical stability, use:\n",
        "        - For x >= 0: sigmoid(x) = 1 / (1 + exp(-x))\n",
        "        - For x < 0: sigmoid(x) = exp(x) / (1 + exp(x))\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Student implementation required\")\n",
        "    \n",
        "    def __call__(self, x: Tensor) -> Tensor:\n",
        "        return self.forward(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| hide\n",
        "#| export\n",
        "class Sigmoid:\n",
        "    \"\"\"Sigmoid Activation: f(x) = 1 / (1 + e^(-x))\"\"\"\n",
        "    \n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"Apply Sigmoid with numerical stability\"\"\"\n",
        "        # Use the numerically stable version to avoid overflow\n",
        "        # For x >= 0: sigmoid(x) = 1 / (1 + exp(-x))\n",
        "        # For x < 0: sigmoid(x) = exp(x) / (1 + exp(x))\n",
        "        x_data = x.data\n",
        "        result = np.zeros_like(x_data)\n",
        "        \n",
        "        # Stable computation\n",
        "        positive_mask = x_data >= 0\n",
        "        result[positive_mask] = 1.0 / (1.0 + np.exp(-x_data[positive_mask]))\n",
        "        result[~positive_mask] = np.exp(x_data[~positive_mask]) / (1.0 + np.exp(x_data[~positive_mask]))\n",
        "        \n",
        "        return Tensor(result)\n",
        "    \n",
        "    def __call__(self, x: Tensor) -> Tensor:\n",
        "        return self.forward(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "### \ud83e\uddea Test Your Sigmoid Function\n",
        "\n",
        "Once you implement Sigmoid above, run this cell to test it:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Sigmoid function\n",
        "try:\n",
        "    print(\"=== Testing Sigmoid Function ===\")\n",
        "    \n",
        "    # Test data: mix of positive, negative, and zero\n",
        "    x = Tensor([[-5.0, -1.0, 0.0, 1.0, 5.0]])\n",
        "    print(f\"Input: {x.data}\")\n",
        "    \n",
        "    # Test Sigmoid\n",
        "    sigmoid = Sigmoid()\n",
        "    y = sigmoid(x)\n",
        "    print(f\"Sigmoid output: {y.data}\")\n",
        "    print(\"Expected: values between 0 and 1\")\n",
        "    print(f\"All values in (0,1)? {np.all((y.data > 0) & (y.data < 1))}\")\n",
        "    \n",
        "    # Test specific values\n",
        "    x_zero = Tensor([[0.0]])\n",
        "    y_zero = sigmoid(x_zero)\n",
        "    print(f\"\\nSigmoid(0) = {y_zero.data[0, 0]:.4f} (should be 0.5)\")\n",
        "    \n",
        "    # Test extreme values (numerical stability)\n",
        "    x_extreme = Tensor([[-100.0, 100.0]])\n",
        "    y_extreme = sigmoid(x_extreme)\n",
        "    print(f\"Sigmoid([-100, 100]) = {y_extreme.data}\")\n",
        "    print(\"Should be close to [0, 1] without overflow errors\")\n",
        "    \n",
        "    print(\"\u2705 Sigmoid working!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\u274c Error: {e}\")\n",
        "    print(\"Make sure to implement the Sigmoid function above!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "### \ud83d\udcca Visualize Sigmoid Function\n",
        "\n",
        "Let's plot the Sigmoid function to see its S-shaped curve:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot Sigmoid function\n",
        "try:\n",
        "    print(\"=== Plotting Sigmoid Function ===\")\n",
        "    \n",
        "    # Create a range of input values\n",
        "    x_range = np.linspace(-10, 10, 100)\n",
        "    x_tensor = Tensor([x_range])\n",
        "    \n",
        "    # Apply Sigmoid (student implementation)\n",
        "    sigmoid = Sigmoid()\n",
        "    y_tensor = sigmoid(x_tensor)\n",
        "    y_range = y_tensor.data[0]\n",
        "    \n",
        "    # Create ideal Sigmoid for comparison\n",
        "    y_ideal = 1.0 / (1.0 + np.exp(-x_range))\n",
        "    \n",
        "    # Only show plots if we're not in a testing environment\n",
        "    if _should_show_plots():\n",
        "        # Create the plot\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        \n",
        "        # Plot both student implementation and ideal\n",
        "        plt.subplot(2, 2, 1)\n",
        "        plt.plot(x_range, y_range, 'g-', linewidth=3, label='Your Sigmoid Implementation')\n",
        "        plt.plot(x_range, y_ideal, 'r--', linewidth=2, alpha=0.7, label='Ideal Sigmoid')\n",
        "        plt.axhline(y=0.5, color='orange', linestyle='--', alpha=0.5, label='y = 0.5')\n",
        "        plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
        "        plt.axhline(y=1, color='k', linestyle='-', alpha=0.3)\n",
        "        plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
        "        plt.xlabel('Input (x)')\n",
        "        plt.ylabel('Output')\n",
        "        plt.title('Sigmoid: Your Implementation vs Ideal')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.legend()\n",
        "        plt.xlim(-10, 10)\n",
        "        plt.ylim(-0.1, 1.1)\n",
        "        \n",
        "        # Mathematical explanation plot\n",
        "        plt.subplot(2, 2, 2)\n",
        "        # Show key points\n",
        "        x_key = np.array([-5, -2, -1, 0, 1, 2, 5])\n",
        "        y_key = 1.0 / (1.0 + np.exp(-x_key))\n",
        "        plt.stem(x_key, y_key, basefmt=' ', linefmt='orange', markerfmt='o')\n",
        "        plt.axhline(y=0.5, color='orange', linestyle='--', alpha=0.5)\n",
        "        plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
        "        plt.axhline(y=1, color='k', linestyle='-', alpha=0.3)\n",
        "        plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
        "        plt.xlabel('Input (x)')\n",
        "        plt.ylabel('1/(1+e^(-x))')\n",
        "        plt.title('Mathematical Definition: 1/(1+e^(-x))')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.xlim(-6, 6)\n",
        "        plt.ylim(-0.1, 1.1)\n",
        "        \n",
        "        # Show the S-curve properties\n",
        "        plt.subplot(2, 2, 3)\n",
        "        x_detailed = np.linspace(-8, 8, 200)\n",
        "        y_detailed = 1.0 / (1.0 + np.exp(-x_detailed))\n",
        "        plt.plot(x_detailed, y_detailed, 'g-', linewidth=3)\n",
        "        # Add asymptotes\n",
        "        plt.axhline(y=0, color='r', linestyle='--', alpha=0.7, label='Lower asymptote: y = 0')\n",
        "        plt.axhline(y=1, color='r', linestyle='--', alpha=0.7, label='Upper asymptote: y = 1')\n",
        "        plt.axhline(y=0.5, color='orange', linestyle='--', alpha=0.7, label='Midpoint: y = 0.5')\n",
        "        plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
        "        plt.xlabel('Input (x)')\n",
        "        plt.ylabel('Output')\n",
        "        plt.title('S-Curve Properties')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.legend()\n",
        "        plt.xlim(-8, 8)\n",
        "        plt.ylim(-0.1, 1.1)\n",
        "        \n",
        "        # Error analysis\n",
        "        plt.subplot(2, 2, 4)\n",
        "        difference = np.abs(y_range - y_ideal)\n",
        "        max_error = np.max(difference)\n",
        "        plt.plot(x_range, difference, 'purple', linewidth=2)\n",
        "        plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
        "        plt.xlabel('Input (x)')\n",
        "        plt.ylabel('|Your Output - Ideal Output|')\n",
        "        plt.title(f'Implementation Error (Max: {max_error:.6f})')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.xlim(-10, 10)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Print analysis\n",
        "        print(f\"\\n\ud83d\udcca Analysis:\")\n",
        "        print(f\"\u2705 Maximum error: {max_error:.10f}\")\n",
        "        if max_error < 1e-10:\n",
        "            print(\"\ud83c\udf89 Perfect implementation!\")\n",
        "        elif max_error < 1e-6:\n",
        "            print(\"\ud83c\udf1f Excellent implementation!\")\n",
        "        elif max_error < 1e-3:\n",
        "            print(\"\ud83d\udc4d Good implementation!\")\n",
        "        else:\n",
        "            print(\"\ud83d\udd27 Implementation needs work.\")\n",
        "            \n",
        "        print(f\"\ud83d\udcc8 Function properties:\")\n",
        "        print(f\"   \u2022 Range: (0, 1)\")\n",
        "        print(f\"   \u2022 Symmetric around (0, 0.5)\")\n",
        "        print(f\"   \u2022 Smooth and differentiable everywhere\")\n",
        "        print(f\"   \u2022 Saturates for large |x| (vanishing gradient problem)\")\n",
        "        print(f\"   \u2022 Useful for binary classification (outputs probabilities)\")\n",
        "    else:\n",
        "        print(\"\ud83d\udcca Plots disabled during testing - this is normal!\")\n",
        "        \n",
        "    # Always show the mathematical analysis\n",
        "    difference = np.abs(y_range - y_ideal)\n",
        "    max_error = np.max(difference)\n",
        "    print(f\"\\n\ud83d\udcca Mathematical Analysis:\")\n",
        "    print(f\"\u2705 Maximum error: {max_error:.10f}\")\n",
        "    if max_error < 1e-10:\n",
        "        print(\"\ud83c\udf89 Perfect implementation!\")\n",
        "    elif max_error < 1e-6:\n",
        "        print(\"\ud83c\udf1f Excellent implementation!\")\n",
        "    elif max_error < 1e-3:\n",
        "        print(\"\ud83d\udc4d Good implementation!\")\n",
        "    else:\n",
        "        print(\"\ud83d\udd27 Implementation needs work.\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"\u274c Error in plotting: {e}\")\n",
        "    print(\"Make sure to implement the Sigmoid function above!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "## Step 3: Tanh Activation Function\n",
        "\n",
        "**Tanh** (Hyperbolic Tangent) squashes inputs to the range (-1, 1).\n",
        "\n",
        "**Formula**: `f(x) = tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))`\n",
        "\n",
        "**Properties**:\n",
        "- **Bounded**: Always outputs between -1 and 1\n",
        "- **Zero-centered**: Output is centered around 0\n",
        "- **Smooth**: Differentiable everywhere\n",
        "- **Stronger gradients**: Than sigmoid around zero\n",
        "\n",
        "**When to use**: Hidden layers when you want zero-centered outputs, RNNs.\n",
        "\n",
        "**Advantage over Sigmoid**: Zero-centered outputs help with gradient flow.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| export\n",
        "class Tanh:\n",
        "    \"\"\"\n",
        "    Tanh Activation: f(x) = tanh(x)\n",
        "    \n",
        "    Squashes input to range (-1, 1). Zero-centered output.\n",
        "    \n",
        "    TODO: Implement Tanh activation function.\n",
        "    \"\"\"\n",
        "    \n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Apply Tanh: f(x) = tanh(x)\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor\n",
        "            \n",
        "        Returns:\n",
        "            Output tensor with Tanh applied element-wise\n",
        "            \n",
        "        TODO: Implement tanh function\n",
        "        Hint: Use np.tanh(x.data)\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Student implementation required\")\n",
        "    \n",
        "    def __call__(self, x: Tensor) -> Tensor:\n",
        "        return self.forward(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| hide\n",
        "#| export\n",
        "class Tanh:\n",
        "    \"\"\"Tanh Activation: f(x) = tanh(x)\"\"\"\n",
        "    \n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"Apply Tanh\"\"\"\n",
        "        return Tensor(np.tanh(x.data))\n",
        "    \n",
        "    def __call__(self, x: Tensor) -> Tensor:\n",
        "        return self.forward(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "### \ud83e\uddea Test Your Tanh Function\n",
        "\n",
        "Once you implement Tanh above, run this cell to test it:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Tanh function\n",
        "try:\n",
        "    print(\"=== Testing Tanh Function ===\")\n",
        "    \n",
        "    # Test data: mix of positive, negative, and zero\n",
        "    x = Tensor([[-3.0, -1.0, 0.0, 1.0, 3.0]])\n",
        "    print(f\"Input: {x.data}\")\n",
        "    \n",
        "    # Test Tanh\n",
        "    tanh = Tanh()\n",
        "    y = tanh(x)\n",
        "    print(f\"Tanh output: {y.data}\")\n",
        "    print(\"Expected: values between -1 and 1\")\n",
        "    print(f\"All values in (-1,1)? {np.all((y.data > -1) & (y.data < 1))}\")\n",
        "    \n",
        "    # Test specific values\n",
        "    x_zero = Tensor([[0.0]])\n",
        "    y_zero = tanh(x_zero)\n",
        "    print(f\"\\nTanh(0) = {y_zero.data[0, 0]:.4f} (should be 0.0)\")\n",
        "    \n",
        "    # Test extreme values\n",
        "    x_extreme = Tensor([[-10.0, 10.0]])\n",
        "    y_extreme = tanh(x_extreme)\n",
        "    print(f\"Tanh([-10, 10]) = {y_extreme.data}\")\n",
        "    print(\"Should be close to [-1, 1]\")\n",
        "    \n",
        "    print(\"\u2705 Tanh working!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\u274c Error: {e}\")\n",
        "    print(\"Make sure to implement the Tanh function above!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "### \ud83d\udcca Visualize Tanh Function\n",
        "\n",
        "Let's plot the Tanh function to see its zero-centered S-shaped curve:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot Tanh function\n",
        "try:\n",
        "    print(\"=== Plotting Tanh Function ===\")\n",
        "    \n",
        "    # Create a range of input values\n",
        "    x_range = np.linspace(-5, 5, 100)\n",
        "    x_tensor = Tensor([x_range])\n",
        "    \n",
        "    # Apply Tanh (student implementation)\n",
        "    tanh = Tanh()\n",
        "    y_tensor = tanh(x_tensor)\n",
        "    y_range = y_tensor.data[0]\n",
        "    \n",
        "    # Create ideal Tanh for comparison\n",
        "    y_ideal = np.tanh(x_range)\n",
        "    \n",
        "    # Only show plots if we're not in a testing environment\n",
        "    if _should_show_plots():\n",
        "        # Create the plot\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        \n",
        "        # Plot both student implementation and ideal\n",
        "        plt.subplot(2, 2, 1)\n",
        "        plt.plot(x_range, y_range, 'orange', linewidth=3, label='Your Tanh Implementation')\n",
        "        plt.plot(x_range, y_ideal, 'r--', linewidth=2, alpha=0.7, label='Ideal Tanh')\n",
        "        plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
        "        plt.axhline(y=1, color='k', linestyle='--', alpha=0.3)\n",
        "        plt.axhline(y=-1, color='k', linestyle='--', alpha=0.3)\n",
        "        plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
        "        plt.xlabel('Input (x)')\n",
        "        plt.ylabel('Output')\n",
        "        plt.title('Tanh: Your Implementation vs Ideal')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.legend()\n",
        "        plt.xlim(-5, 5)\n",
        "        plt.ylim(-1.2, 1.2)\n",
        "        \n",
        "        # Mathematical explanation plot\n",
        "        plt.subplot(2, 2, 2)\n",
        "        # Show key points\n",
        "        x_key = np.array([-3, -2, -1, 0, 1, 2, 3])\n",
        "        y_key = np.tanh(x_key)\n",
        "        plt.stem(x_key, y_key, basefmt=' ', linefmt='purple', markerfmt='o')\n",
        "        plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
        "        plt.axhline(y=1, color='k', linestyle='--', alpha=0.3)\n",
        "        plt.axhline(y=-1, color='k', linestyle='--', alpha=0.3)\n",
        "        plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
        "        plt.xlabel('Input (x)')\n",
        "        plt.ylabel('tanh(x)')\n",
        "        plt.title('Mathematical Definition: tanh(x)')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.xlim(-4, 4)\n",
        "        plt.ylim(-1.2, 1.2)\n",
        "        \n",
        "        # Show symmetry property\n",
        "        plt.subplot(2, 2, 3)\n",
        "        x_sym = np.linspace(-4, 4, 100)\n",
        "        y_sym = np.tanh(x_sym)\n",
        "        plt.plot(x_sym, y_sym, 'orange', linewidth=3, label='tanh(x)')\n",
        "        plt.plot(-x_sym, -y_sym, 'b--', linewidth=2, alpha=0.7, label='-tanh(-x)')\n",
        "        plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
        "        plt.axhline(y=1, color='r', linestyle='--', alpha=0.7, label='Upper asymptote: y = 1')\n",
        "        plt.axhline(y=-1, color='r', linestyle='--', alpha=0.7, label='Lower asymptote: y = -1')\n",
        "        plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
        "        plt.xlabel('Input (x)')\n",
        "        plt.ylabel('Output')\n",
        "        plt.title('Symmetry: tanh(-x) = -tanh(x)')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.legend()\n",
        "        plt.xlim(-4, 4)\n",
        "        plt.ylim(-1.2, 1.2)\n",
        "        \n",
        "        # Error analysis\n",
        "        plt.subplot(2, 2, 4)\n",
        "        difference = np.abs(y_range - y_ideal)\n",
        "        max_error = np.max(difference)\n",
        "        plt.plot(x_range, difference, 'purple', linewidth=2)\n",
        "        plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
        "        plt.xlabel('Input (x)')\n",
        "        plt.ylabel('|Your Output - Ideal Output|')\n",
        "        plt.title(f'Implementation Error (Max: {max_error:.6f})')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.xlim(-5, 5)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Print analysis\n",
        "        print(f\"\\n\ud83d\udcca Analysis:\")\n",
        "        print(f\"\u2705 Maximum error: {max_error:.10f}\")\n",
        "        if max_error < 1e-10:\n",
        "            print(\"\ud83c\udf89 Perfect implementation!\")\n",
        "        elif max_error < 1e-6:\n",
        "            print(\"\ud83c\udf1f Excellent implementation!\")\n",
        "        elif max_error < 1e-3:\n",
        "            print(\"\ud83d\udc4d Good implementation!\")\n",
        "        else:\n",
        "            print(\"\ud83d\udd27 Implementation needs work.\")\n",
        "            \n",
        "        print(f\"\ud83d\udcc8 Function properties:\")\n",
        "        print(f\"   \u2022 Range: (-1, 1)\")\n",
        "        print(f\"   \u2022 Odd function: tanh(-x) = -tanh(x)\")\n",
        "        print(f\"   \u2022 Symmetric around origin (0, 0)\")\n",
        "        print(f\"   \u2022 Smooth and differentiable everywhere\")\n",
        "        print(f\"   \u2022 Stronger gradients than sigmoid around zero\")\n",
        "        print(f\"   \u2022 Related to sigmoid: tanh(x) = 2*sigmoid(2x) - 1\")\n",
        "    else:\n",
        "        print(\"\ud83d\udcca Plots disabled during testing - this is normal!\")\n",
        "        \n",
        "    # Always show the mathematical analysis\n",
        "    difference = np.abs(y_range - y_ideal)\n",
        "    max_error = np.max(difference)\n",
        "    print(f\"\\n\ud83d\udcca Mathematical Analysis:\")\n",
        "    print(f\"\u2705 Maximum error: {max_error:.10f}\")\n",
        "    if max_error < 1e-10:\n",
        "        print(\"\ud83c\udf89 Perfect implementation!\")\n",
        "    elif max_error < 1e-6:\n",
        "        print(\"\ud83c\udf1f Excellent implementation!\")\n",
        "    elif max_error < 1e-3:\n",
        "        print(\"\ud83d\udc4d Good implementation!\")\n",
        "    else:\n",
        "        print(\"\ud83d\udd27 Implementation needs work.\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"\u274c Error in plotting: {e}\")\n",
        "    print(\"Make sure to implement the Tanh function above!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "## Step 4: Compare All Activation Functions\n",
        "\n",
        "Let's see how all three functions behave on the same input:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare all activation functions\n",
        "try:\n",
        "    print(\"=== Comparing All Activation Functions ===\")\n",
        "    \n",
        "    # Test data: range from -5 to 5\n",
        "    x = Tensor([[-5.0, -2.0, -1.0, 0.0, 1.0, 2.0, 5.0]])\n",
        "    print(f\"Input: {x.data}\")\n",
        "    \n",
        "    # Apply all activations\n",
        "    relu = ReLU()\n",
        "    sigmoid = Sigmoid()\n",
        "    tanh = Tanh()\n",
        "    \n",
        "    y_relu = relu(x)\n",
        "    y_sigmoid = sigmoid(x)\n",
        "    y_tanh = tanh(x)\n",
        "    \n",
        "    print(f\"\\nReLU:    {y_relu.data}\")\n",
        "    print(f\"Sigmoid: {y_sigmoid.data}\")\n",
        "    print(f\"Tanh:    {y_tanh.data}\")\n",
        "    \n",
        "    print(\"\\n\ud83d\udcca Key Differences:\")\n",
        "    print(\"- ReLU: Zeros out negative values, unbounded positive\")\n",
        "    print(\"- Sigmoid: Squashes to (0, 1), always positive\")\n",
        "    print(\"- Tanh: Squashes to (-1, 1), zero-centered\")\n",
        "    \n",
        "    print(\"\\n\u2705 All activation functions working!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\u274c Error: {e}\")\n",
        "    print(\"Make sure to implement all activation functions above!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "### \ud83d\udcca Comprehensive Activation Function Comparison\n",
        "\n",
        "Let's plot all three functions together to see their differences:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot all activation functions together\n",
        "try:\n",
        "    print(\"=== Plotting All Activation Functions Together ===\")\n",
        "    \n",
        "    # Create a range of input values\n",
        "    x_range = np.linspace(-5, 5, 100)\n",
        "    x_tensor = Tensor([x_range])\n",
        "    \n",
        "    # Apply all activations (student implementations)\n",
        "    relu = ReLU()\n",
        "    sigmoid = Sigmoid()\n",
        "    tanh = Tanh()\n",
        "    \n",
        "    y_relu = relu(x_tensor).data[0]\n",
        "    y_sigmoid = sigmoid(x_tensor).data[0]\n",
        "    y_tanh = tanh(x_tensor).data[0]\n",
        "    \n",
        "    # Create ideal functions for comparison\n",
        "    y_relu_ideal = np.maximum(0, x_range)\n",
        "    y_sigmoid_ideal = 1.0 / (1.0 + np.exp(-x_range))\n",
        "    y_tanh_ideal = np.tanh(x_range)\n",
        "    \n",
        "    # Only show plots if we're not in a testing environment\n",
        "    if _should_show_plots():\n",
        "        # Create the comprehensive plot\n",
        "        plt.figure(figsize=(15, 10))\n",
        "        \n",
        "        # Main comparison plot\n",
        "        plt.subplot(2, 3, (1, 2))\n",
        "        plt.plot(x_range, y_relu, 'b-', linewidth=3, label='Your ReLU')\n",
        "        plt.plot(x_range, y_sigmoid, 'g-', linewidth=3, label='Your Sigmoid')\n",
        "        plt.plot(x_range, y_tanh, 'orange', linewidth=3, label='Your Tanh')\n",
        "        \n",
        "        # Add ideal functions as dashed lines\n",
        "        plt.plot(x_range, y_relu_ideal, 'b--', linewidth=1, alpha=0.7, label='Ideal ReLU')\n",
        "        plt.plot(x_range, y_sigmoid_ideal, 'g--', linewidth=1, alpha=0.7, label='Ideal Sigmoid')\n",
        "        plt.plot(x_range, y_tanh_ideal, '--', color='orange', linewidth=1, alpha=0.7, label='Ideal Tanh')\n",
        "        \n",
        "        # Add reference lines\n",
        "        plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
        "        plt.axhline(y=1, color='k', linestyle='--', alpha=0.3)\n",
        "        plt.axhline(y=-1, color='k', linestyle='--', alpha=0.3)\n",
        "        plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
        "        \n",
        "        # Formatting\n",
        "        plt.xlabel('Input (x)', fontsize=12)\n",
        "        plt.ylabel('Output f(x)', fontsize=12)\n",
        "        plt.title('Activation Functions: Your Implementation vs Ideal', fontsize=14, fontweight='bold')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.legend(fontsize=10, loc='upper left')\n",
        "        plt.xlim(-5, 5)\n",
        "        plt.ylim(-1.5, 5)\n",
        "        \n",
        "        # Mathematical definitions\n",
        "        plt.subplot(2, 3, 3)\n",
        "        plt.text(0.05, 0.95, 'Mathematical Definitions:', fontsize=12, fontweight='bold', \n",
        "                 transform=plt.gca().transAxes, verticalalignment='top')\n",
        "        plt.text(0.05, 0.85, 'ReLU:', fontsize=11, fontweight='bold', color='blue',\n",
        "                 transform=plt.gca().transAxes, verticalalignment='top')\n",
        "        plt.text(0.05, 0.80, 'f(x) = max(0, x)', fontsize=10, fontfamily='monospace',\n",
        "                 transform=plt.gca().transAxes, verticalalignment='top')\n",
        "        plt.text(0.05, 0.70, 'Sigmoid:', fontsize=11, fontweight='bold', color='green',\n",
        "                 transform=plt.gca().transAxes, verticalalignment='top')\n",
        "        plt.text(0.05, 0.65, 'f(x) = 1/(1+e^(-x))', fontsize=10, fontfamily='monospace',\n",
        "                 transform=plt.gca().transAxes, verticalalignment='top')\n",
        "        plt.text(0.05, 0.55, 'Tanh:', fontsize=11, fontweight='bold', color='orange',\n",
        "                 transform=plt.gca().transAxes, verticalalignment='top')\n",
        "        plt.text(0.05, 0.50, 'f(x) = tanh(x)', fontsize=10, fontfamily='monospace',\n",
        "                 transform=plt.gca().transAxes, verticalalignment='top')\n",
        "        plt.text(0.05, 0.45, '     = (e^x-e^(-x))/(e^x+e^(-x))', fontsize=10, fontfamily='monospace',\n",
        "                 transform=plt.gca().transAxes, verticalalignment='top')\n",
        "        \n",
        "        plt.text(0.05, 0.30, 'Key Properties:', fontsize=12, fontweight='bold',\n",
        "                 transform=plt.gca().transAxes, verticalalignment='top')\n",
        "        plt.text(0.05, 0.25, '\u2022 ReLU: Sparse, unbounded', fontsize=10, color='blue',\n",
        "                 transform=plt.gca().transAxes, verticalalignment='top')\n",
        "        plt.text(0.05, 0.20, '\u2022 Sigmoid: Bounded (0,1)', fontsize=10, color='green',\n",
        "                 transform=plt.gca().transAxes, verticalalignment='top')\n",
        "        plt.text(0.05, 0.15, '\u2022 Tanh: Zero-centered (-1,1)', fontsize=10, color='orange',\n",
        "                 transform=plt.gca().transAxes, verticalalignment='top')\n",
        "        plt.axis('off')\n",
        "        \n",
        "        # Error analysis for ReLU\n",
        "        plt.subplot(2, 3, 4)\n",
        "        error_relu = np.abs(y_relu - y_relu_ideal)\n",
        "        plt.plot(x_range, error_relu, 'b-', linewidth=2)\n",
        "        plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
        "        plt.xlabel('Input (x)')\n",
        "        plt.ylabel('Error')\n",
        "        plt.title(f'ReLU Error (Max: {np.max(error_relu):.2e})')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.xlim(-5, 5)\n",
        "        \n",
        "        # Error analysis for Sigmoid\n",
        "        plt.subplot(2, 3, 5)\n",
        "        error_sigmoid = np.abs(y_sigmoid - y_sigmoid_ideal)\n",
        "        plt.plot(x_range, error_sigmoid, 'g-', linewidth=2)\n",
        "        plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
        "        plt.xlabel('Input (x)')\n",
        "        plt.ylabel('Error')\n",
        "        plt.title(f'Sigmoid Error (Max: {np.max(error_sigmoid):.2e})')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.xlim(-5, 5)\n",
        "        \n",
        "        # Error analysis for Tanh\n",
        "        plt.subplot(2, 3, 6)\n",
        "        error_tanh = np.abs(y_tanh - y_tanh_ideal)\n",
        "        plt.plot(x_range, error_tanh, 'orange', linewidth=2)\n",
        "        plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
        "        plt.xlabel('Input (x)')\n",
        "        plt.ylabel('Error')\n",
        "        plt.title(f'Tanh Error (Max: {np.max(error_tanh):.2e})')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.xlim(-5, 5)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Comprehensive analysis\n",
        "        print(\"\\n\ud83d\udcca Comprehensive Analysis:\")\n",
        "        print(\"=\" * 60)\n",
        "        \n",
        "        # Function ranges\n",
        "        print(\"\ud83d\udcc8 Output Ranges:\")\n",
        "        print(f\"  ReLU:    [{np.min(y_relu):.3f}, {np.max(y_relu):.3f}]\")\n",
        "        print(f\"  Sigmoid: [{np.min(y_sigmoid):.3f}, {np.max(y_sigmoid):.3f}]\")\n",
        "        print(f\"  Tanh:    [{np.min(y_tanh):.3f}, {np.max(y_tanh):.3f}]\")\n",
        "        \n",
        "        # Implementation accuracy\n",
        "        print(\"\\n\ud83c\udfaf Implementation Accuracy:\")\n",
        "        max_errors = [np.max(error_relu), np.max(error_sigmoid), np.max(error_tanh)]\n",
        "        functions = ['ReLU', 'Sigmoid', 'Tanh']\n",
        "        \n",
        "        for func, error in zip(functions, max_errors):\n",
        "            if error < 1e-10:\n",
        "                status = \"\u2705 PERFECT\"\n",
        "            elif error < 1e-6:\n",
        "                status = \"\u2705 EXCELLENT\"\n",
        "            elif error < 1e-3:\n",
        "                status = \"\u26a0\ufe0f  GOOD\"\n",
        "            else:\n",
        "                status = \"\u274c NEEDS WORK\"\n",
        "            print(f\"  {func:8s}: {status:12s} (error: {error:.2e})\")\n",
        "        \n",
        "        # Mathematical properties verification\n",
        "        print(\"\\n\ud83d\udd0d Mathematical Properties:\")\n",
        "        \n",
        "        # Zero-centered test\n",
        "        x_zero = Tensor([[0.0]])\n",
        "        print(\"  Zero-centered test (f(0) should be 0):\")\n",
        "        for name, func in [(\"ReLU\", relu), (\"Sigmoid\", sigmoid), (\"Tanh\", tanh)]:\n",
        "            output = func(x_zero).data[0, 0]\n",
        "            is_zero = abs(output) < 1e-6\n",
        "            expected = 0.0 if name != \"Sigmoid\" else 0.5\n",
        "            print(f\"    {name:8s}: f(0) = {output:.4f} {'\u2705' if abs(output - expected) < 1e-6 else '\u274c'}\")\n",
        "        \n",
        "        # Monotonicity test\n",
        "        print(\"  Monotonicity test (should be increasing):\")\n",
        "        test_vals = np.array([-2, -1, 0, 1, 2])\n",
        "        x_test = Tensor([test_vals])\n",
        "        for name, func in [(\"ReLU\", relu), (\"Sigmoid\", sigmoid), (\"Tanh\", tanh)]:\n",
        "            outputs = func(x_test).data[0]\n",
        "            is_monotonic = np.all(outputs[1:] >= outputs[:-1])\n",
        "            print(f\"    {name:8s}: {'\u2705 Monotonic' if is_monotonic else '\u274c Not monotonic'}\")\n",
        "        \n",
        "        print(\"\\n\ud83c\udf89 Comparison complete! Use these insights to understand each function's role in neural networks.\")\n",
        "    else:\n",
        "        print(\"\ud83d\udcca Plots disabled during testing - this is normal!\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"\u274c Error in plotting: {e}\")\n",
        "    print(\"Make sure matplotlib is installed and all functions are implemented!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "## Step 5: Understanding Activation Function Properties\n",
        "\n",
        "Let's explore the mathematical properties of each function:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explore activation function properties\n",
        "try:\n",
        "    print(\"=== Activation Function Properties ===\")\n",
        "    \n",
        "    # Create test functions\n",
        "    relu = ReLU()\n",
        "    sigmoid = Sigmoid()\n",
        "    tanh = Tanh()\n",
        "    \n",
        "    # Test with a range of values\n",
        "    test_values = np.linspace(-5, 5, 11)\n",
        "    x = Tensor([test_values])\n",
        "    \n",
        "    print(f\"Input range: {test_values}\")\n",
        "    print(f\"ReLU range: [{np.min(relu(x).data):.2f}, {np.max(relu(x).data):.2f}]\")\n",
        "    print(f\"Sigmoid range: [{np.min(sigmoid(x).data):.2f}, {np.max(sigmoid(x).data):.2f}]\")\n",
        "    print(f\"Tanh range: [{np.min(tanh(x).data):.2f}, {np.max(tanh(x).data):.2f}]\")\n",
        "    \n",
        "    # Test monotonicity (should all be increasing functions)\n",
        "    print(f\"\\n\ud83d\udcc8 Monotonicity Test:\")\n",
        "    for name, func in [(\"ReLU\", relu), (\"Sigmoid\", sigmoid), (\"Tanh\", tanh)]:\n",
        "        outputs = func(x).data[0]\n",
        "        is_monotonic = np.all(outputs[1:] >= outputs[:-1])\n",
        "        print(f\"{name}: {'\u2705 Monotonic' if is_monotonic else '\u274c Not monotonic'}\")\n",
        "    \n",
        "    # Test zero-centered property\n",
        "    print(f\"\\n\ud83c\udfaf Zero-Centered Test (f(0) = 0):\")\n",
        "    x_zero = Tensor([[0.0]])\n",
        "    for name, func in [(\"ReLU\", relu), (\"Sigmoid\", sigmoid), (\"Tanh\", tanh)]:\n",
        "        output = func(x_zero).data[0, 0]\n",
        "        is_zero_centered = abs(output) < 1e-6\n",
        "        print(f\"{name}: f(0) = {output:.4f} {'\u2705 Zero-centered' if is_zero_centered else '\u274c Not zero-centered'}\")\n",
        "    \n",
        "    print(\"\\n\ud83c\udf89 Property analysis complete!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\u274c Error: {e}\")\n",
        "    print(\"Check your activation function implementations!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "## Step 6: Practical Usage Examples\n",
        "\n",
        "Let's see how these functions would be used in practice:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Practical usage examples\n",
        "try:\n",
        "    print(\"=== Practical Usage Examples ===\")\n",
        "    \n",
        "    # Example 1: Binary classification with sigmoid\n",
        "    print(\"1. Binary Classification (Sigmoid):\")\n",
        "    logits = Tensor([[2.5, -1.2, 0.8, -0.3]])  # Raw network outputs\n",
        "    sigmoid = Sigmoid()\n",
        "    probabilities = sigmoid(logits)\n",
        "    print(f\"   Logits: {logits.data}\")\n",
        "    print(f\"   Probabilities: {probabilities.data}\")\n",
        "    print(f\"   Predictions: {(probabilities.data > 0.5).astype(int)}\")\n",
        "    \n",
        "    # Example 2: Feature processing with ReLU\n",
        "    print(\"\\n2. Feature Processing (ReLU):\")\n",
        "    features = Tensor([[-0.5, 1.2, -2.1, 0.8, -0.1]])  # Mixed positive/negative\n",
        "    relu = ReLU()\n",
        "    processed = relu(features)\n",
        "    print(f\"   Raw features: {features.data}\")\n",
        "    print(f\"   After ReLU: {processed.data}\")\n",
        "    print(f\"   Sparsity: {np.mean(processed.data == 0):.1%} zeros\")\n",
        "    \n",
        "    # Example 3: Normalized features with Tanh\n",
        "    print(\"\\n3. Normalized Features (Tanh):\")\n",
        "    raw_features = Tensor([[3.2, -1.8, 0.5, -2.4, 1.1]])\n",
        "    tanh = Tanh()\n",
        "    normalized = tanh(raw_features)\n",
        "    print(f\"   Raw features: {raw_features.data}\")\n",
        "    print(f\"   Normalized: {normalized.data}\")\n",
        "    print(f\"   Mean: {np.mean(normalized.data):.3f} (close to 0)\")\n",
        "    \n",
        "    print(\"\\n\u2705 Practical examples complete!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\u274c Error: {e}\")\n",
        "    print(\"Check your activation function implementations!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "## \ud83c\udf89 Congratulations!\n",
        "\n",
        "You've successfully implemented the three most important activation functions in deep learning!\n",
        "\n",
        "### \ud83e\uddf1 What You Built\n",
        "1. **ReLU**: The workhorse activation that enables deep networks\n",
        "2. **Sigmoid**: The probability activation for binary classification\n",
        "3. **Tanh**: The zero-centered activation for better gradient flow\n",
        "\n",
        "### \ud83c\udfaf Key Insights\n",
        "- **Nonlinearity is essential**: Without activations, neural networks are just linear transformations\n",
        "- **Different functions serve different purposes**: ReLU for hidden layers, Sigmoid for probabilities, Tanh for zero-centered outputs\n",
        "- **Mathematical properties matter**: Monotonicity, boundedness, and zero-centering affect learning\n",
        "\n",
        "### \ud83d\ude80 What's Next\n",
        "These activation functions will be used in:\n",
        "- **Layers Module**: Building neural network layers\n",
        "- **Loss Functions**: Computing training objectives\n",
        "- **Advanced Architectures**: CNNs, RNNs, and more\n",
        "\n",
        "### \ud83d\udd27 Export to Package\n",
        "Run this to export your activations to the TinyTorch package:\n",
        "```bash\n",
        "python bin/tito.py sync\n",
        "```\n",
        "\n",
        "Then test your implementation:\n",
        "```bash\n",
        "python bin/tito.py test --module activations\n",
        "```\n",
        "\n",
        "**Excellent work! You've mastered the mathematical foundations of neural networks!** \ud83c\udf89\n",
        "\n",
        "---\n",
        "\n",
        "## \ud83d\udcda Further Reading\n",
        "\n",
        "**Want to learn more about activation functions?**\n",
        "- **ReLU variants**: Leaky ReLU, ELU, Swish\n",
        "- **Advanced activations**: GELU, Mish, SiLU\n",
        "- **Activation choice**: When to use which function\n",
        "- **Gradient flow**: How activations affect training\n",
        "\n",
        "**Next modules**: Layers, Loss Functions, Optimization\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}