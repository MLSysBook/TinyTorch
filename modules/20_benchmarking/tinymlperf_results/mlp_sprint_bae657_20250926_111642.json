{
  "submission_id": "mlp_sprint_bae657_20250926_111642",
  "timestamp": "2025-09-26T11:16:42.089046",
  "team_name": "Quantized Team",
  "event_name": "mlp_sprint",
  "optimization_description": "INT8 quantization with custom kernels",
  "github_url": "https://github.com/quantized-team/mlp-opt",
  "performance_metrics": {
    "event": "MLP Sprint",
    "model_type": "FastMLPModel",
    "input_shape": [
      100,
      784
    ],
    "benchmark_timestamp": "2025-09-26T11:16:41.998418",
    "mean_inference_time": 0.00031560000090394166,
    "std_inference_time": 1.0198304784279281e-05,
    "min_inference_time": 0.0002964917017379776,
    "max_inference_time": 0.00032719150185585023,
    "p95_inference_time": 0.00032558736158534886,
    "mean_cpu_time": 0.00031560000090394166,
    "cpu_efficiency": 0.85,
    "profiling_method": "TinyTorch Module 15 Profiler",
    "memory_delta_mb": 0.004241943359375,
    "peak_memory_mb": 0.07464599609375,
    "result_size_mb": 0.1,
    "speedup_vs_baseline": 1.2604323855745976
  },
  "speedup_score": 1.2604323855745976,
  "baseline_time_ms": 0.3977924620267003,
  "submission_time_ms": 0.31560000090394164
}