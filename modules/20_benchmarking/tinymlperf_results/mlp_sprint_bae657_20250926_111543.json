{
  "submission_id": "mlp_sprint_bae657_20250926_111543",
  "timestamp": "2025-09-26T11:15:43.873485",
  "team_name": "Quantized Team",
  "event_name": "mlp_sprint",
  "optimization_description": "INT8 quantization with custom kernels",
  "github_url": "https://github.com/quantized-team/mlp-opt",
  "performance_metrics": {
    "event": "MLP Sprint",
    "model_type": "FastMLPModel",
    "input_shape": [
      100,
      784
    ],
    "benchmark_timestamp": "2025-09-26T11:15:43.778773",
    "mean_inference_time": 0.00031336736108642074,
    "std_inference_time": 1.2971096019885512e-05,
    "min_inference_time": 0.0003005249018315226,
    "max_inference_time": 0.0003353498963406309,
    "p95_inference_time": 0.0003320948575856164,
    "mean_cpu_time": 0.00031336736108642074,
    "cpu_efficiency": 0.85,
    "profiling_method": "TinyTorch Module 15 Profiler",
    "memory_delta_mb": 0.004241943359375,
    "peak_memory_mb": 0.07464599609375,
    "result_size_mb": 0.1,
    "speedup_vs_baseline": 1.3386496158831345
  },
  "speedup_score": 1.3386496158831345,
  "baseline_time_ms": 0.4194890975486486,
  "submission_time_ms": 0.31336736108642077
}