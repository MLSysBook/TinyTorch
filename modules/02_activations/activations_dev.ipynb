{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "836ef696",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Module 3: Activation Functions - The Spark of Intelligence\n",
    "\n",
    "**Learning Goals:**\n",
    "- Understand why activation functions are essential for neural networks\n",
    "- Implement four fundamental activation functions from scratch\n",
    "- Learn the mathematical properties and use cases of each activation\n",
    "- Visualize activation function behavior and understand their impact\n",
    "\n",
    "**Why This Matters:**\n",
    "Without activation functions, neural networks would just be linear transformations - no matter how many layers you stack, you'd only get linear relationships. Activation functions introduce the nonlinearity that allows neural networks to learn complex patterns and approximate any function.\n",
    "\n",
    "**Real-World Context:**\n",
    "Every neural network you've heard of - from image recognition to language models - relies on activation functions. Understanding them deeply is crucial for designing effective architectures and debugging training issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd818131",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core.activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3300cf9a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from typing import Union, List\n",
    "\n",
    "# Import our Tensor class from the main package (rock solid foundation)\n",
    "from tinytorch.core.tensor import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3adf3e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "def _should_show_plots():\n",
    "    \"\"\"Check if we should show plots (disable during testing)\"\"\"\n",
    "    # Check multiple conditions that indicate we're in test mode\n",
    "    is_pytest = (\n",
    "        'pytest' in sys.modules or\n",
    "        'test' in sys.argv or\n",
    "        os.environ.get('PYTEST_CURRENT_TEST') is not None or\n",
    "        any('test' in arg for arg in sys.argv) or\n",
    "        any('pytest' in arg for arg in sys.argv)\n",
    "    )\n",
    "    \n",
    "    # Show plots in development mode (when not in test mode)\n",
    "    return not is_pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2131f76a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "def visualize_activation_function(activation_fn, name: str, x_range: tuple = (-5, 5), num_points: int = 100):\n",
    "    \"\"\"Visualize an activation function's behavior\"\"\"\n",
    "    if not _should_show_plots():\n",
    "        return\n",
    "        \n",
    "    try:\n",
    "        \n",
    "        # Generate input values\n",
    "        x_vals = np.linspace(x_range[0], x_range[1], num_points)\n",
    "        \n",
    "        # Apply activation function\n",
    "        y_vals = []\n",
    "        for x in x_vals:\n",
    "            input_tensor = Tensor([[x]])\n",
    "            output = activation_fn(input_tensor)\n",
    "            y_vals.append(output.data.item())\n",
    "        \n",
    "        # Create plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(x_vals, y_vals, 'b-', linewidth=2, label=f'{name} Activation')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.xlabel('Input (x)')\n",
    "        plt.ylabel(f'{name}(x)')\n",
    "        plt.title(f'{name} Activation Function')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"   üìä Matplotlib not available - skipping visualization\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Visualization error: {e}\")\n",
    "\n",
    "def visualize_activation_on_data(activation_fn, name: str, data: Tensor):\n",
    "    \"\"\"Show activation function applied to sample data\"\"\"\n",
    "    if not _should_show_plots():\n",
    "        return\n",
    "        \n",
    "    try:\n",
    "        output = activation_fn(data)\n",
    "        print(f\"   üìä {name} Example:\")\n",
    "        print(f\"      Input:  {data.data.flatten()}\")\n",
    "        print(f\"      Output: {output.data.flatten()}\")\n",
    "        print(f\"      Range:  [{output.data.min():.3f}, {output.data.max():.3f}]\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Data visualization error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7107d23e",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 1: What is an Activation Function?\n",
    "\n",
    "### Definition\n",
    "An **activation function** is a mathematical function that adds nonlinearity to neural networks. It transforms the output of a layer before passing it to the next layer.\n",
    "\n",
    "### Why Activation Functions Matter\n",
    "**Without activation functions, neural networks are just linear transformations!**\n",
    "\n",
    "```\n",
    "Linear ‚Üí Linear ‚Üí Linear = Still Linear\n",
    "```\n",
    "\n",
    "No matter how many layers you stack, without activation functions, you can only learn linear relationships. Activation functions introduce the nonlinearity that allows neural networks to:\n",
    "- Learn complex patterns\n",
    "- Approximate any continuous function\n",
    "- Solve non-linear problems\n",
    "\n",
    "### Visual Analogy\n",
    "Think of activation functions as **decision makers** at each neuron:\n",
    "- **ReLU**: \"If positive, pass it through; if negative, block it\"\n",
    "- **Sigmoid**: \"Squash everything between 0 and 1\"\n",
    "- **Tanh**: \"Squash everything between -1 and 1\"\n",
    "- **Softmax**: \"Convert to probabilities that sum to 1\"\n",
    "\n",
    "### Connection to Previous Modules\n",
    "In Module 2 (Layers), we learned how to transform data through linear operations (matrix multiplication + bias). Now we add the nonlinear activation functions that make neural networks powerful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3452616c",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 2: ReLU - The Workhorse of Deep Learning\n",
    "\n",
    "### What is ReLU?\n",
    "**ReLU (Rectified Linear Unit)** is the most popular activation function in deep learning.\n",
    "\n",
    "**Mathematical Definition:**\n",
    "```\n",
    "f(x) = max(0, x)\n",
    "```\n",
    "\n",
    "**In Plain English:**\n",
    "- If input is positive ‚Üí pass it through unchanged\n",
    "- If input is negative ‚Üí output zero\n",
    "\n",
    "### Why ReLU is Popular\n",
    "1. **Simple**: Easy to compute and understand\n",
    "2. **Fast**: No expensive operations (no exponentials)\n",
    "3. **Sparse**: Outputs many zeros, creating sparse representations\n",
    "4. **Gradient-friendly**: Gradient is either 0 or 1 (no vanishing gradient for positive inputs)\n",
    "\n",
    "### Real-World Analogy\n",
    "ReLU is like a **one-way valve** - it only lets positive \"pressure\" through, blocking negative values completely.\n",
    "\n",
    "### When to Use ReLU\n",
    "- **Hidden layers** in most neural networks\n",
    "- **Convolutional layers** in image processing\n",
    "- **When you want sparse activations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7885061",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class ReLU:\n",
    "    \"\"\"\n",
    "    ReLU Activation Function: f(x) = max(0, x)\n",
    "    \n",
    "    The most popular activation function in deep learning.\n",
    "    Simple, fast, and effective for most applications.\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply ReLU activation: f(x) = max(0, x)\n",
    "        \n",
    "        TODO: Implement ReLU activation\n",
    "        \n",
    "        APPROACH:\n",
    "        1. For each element in the input tensor, apply max(0, element)\n",
    "        2. Return a new Tensor with the results\n",
    "        \n",
    "        EXAMPLE:\n",
    "        Input: Tensor([[-1, 0, 1, 2, -3]])\n",
    "        Expected: Tensor([[0, 0, 1, 2, 0]])\n",
    "        \n",
    "        HINTS:\n",
    "        - Use np.maximum(0, x.data) for element-wise max\n",
    "        - Remember to return a new Tensor object\n",
    "        - The shape should remain the same as input\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Student implementation required\")\n",
    "    \n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Allow calling the activation like a function: relu(x)\"\"\"\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8337a5d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "class ReLU:\n",
    "    \"\"\"ReLU Activation: f(x) = max(0, x)\"\"\"\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        result = np.maximum(0, x.data)\n",
    "        return Tensor(result)\n",
    "        \n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5aec6b",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### üß™ Test Your ReLU Implementation\n",
    "\n",
    "Let's test your ReLU implementation right away to make sure it's working correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0e4569",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Create ReLU activation\n",
    "    relu = ReLU()\n",
    "    \n",
    "    # Test 1: Basic functionality\n",
    "    print(\"üîß Testing ReLU Implementation\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Test with mixed positive/negative values\n",
    "    test_input = Tensor([[-2, -1, 0, 1, 2]])\n",
    "    expected = Tensor([[0, 0, 0, 1, 2]])\n",
    "    \n",
    "    result = relu(test_input)\n",
    "    print(f\"Input:    {test_input.data.flatten()}\")\n",
    "    print(f\"Output:   {result.data.flatten()}\")\n",
    "    print(f\"Expected: {expected.data.flatten()}\")\n",
    "    \n",
    "    # Verify correctness\n",
    "    if np.allclose(result.data, expected.data):\n",
    "        print(\"‚úÖ Basic ReLU test passed!\")\n",
    "    else:\n",
    "        print(\"‚ùå Basic ReLU test failed!\")\n",
    "        print(\"   Check your max(0, x) implementation\")\n",
    "    \n",
    "    # Test 2: Edge cases\n",
    "    edge_cases = Tensor([[-100, -0.1, 0, 0.1, 100]])\n",
    "    edge_result = relu(edge_cases)\n",
    "    expected_edge = np.array([[0, 0, 0, 0.1, 100]])\n",
    "    \n",
    "    print(f\"\\nEdge cases: {edge_cases.data.flatten()}\")\n",
    "    print(f\"Output:     {edge_result.data.flatten()}\")\n",
    "    \n",
    "    if np.allclose(edge_result.data, expected_edge):\n",
    "        print(\"‚úÖ Edge case test passed!\")\n",
    "    else:\n",
    "        print(\"‚ùå Edge case test failed!\")\n",
    "    \n",
    "    # Test 3: Shape preservation\n",
    "    multi_dim = Tensor([[1, -1], [2, -2], [0, 3]])\n",
    "    multi_result = relu(multi_dim)\n",
    "    \n",
    "    if multi_result.data.shape == multi_dim.data.shape:\n",
    "        print(\"‚úÖ Shape preservation test passed!\")\n",
    "    else:\n",
    "        print(\"‚ùå Shape preservation test failed!\")\n",
    "        print(f\"   Expected shape: {multi_dim.data.shape}, got: {multi_result.data.shape}\")\n",
    "    \n",
    "    print(\"‚úÖ ReLU tests complete!\")\n",
    "    \n",
    "except NotImplementedError:\n",
    "    print(\"‚ö†Ô∏è  ReLU not implemented yet - complete the forward method above!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in ReLU: {e}\")\n",
    "    print(\"   Check your implementation in the forward method\")\n",
    "\n",
    "print()  # Add spacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f73603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üé® ReLU Visualization (development only - not exported)\n",
    "if _should_show_plots():\n",
    "    try:\n",
    "        relu = ReLU()\n",
    "        print(\"üé® Visualizing ReLU behavior...\")\n",
    "        visualize_activation_function(relu, \"ReLU\", x_range=(-3, 3))\n",
    "        \n",
    "        # Show ReLU with real data\n",
    "        sample_data = Tensor([[-2.5, -1.0, -0.5, 0.0, 0.5, 1.0, 2.5]])\n",
    "        visualize_activation_on_data(relu, \"ReLU\", sample_data)\n",
    "    except:\n",
    "        pass  # Skip if ReLU not implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235b8ea2",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 3: Sigmoid - The Smooth Classifier\n",
    "\n",
    "### What is Sigmoid?\n",
    "**Sigmoid** is a smooth, S-shaped activation function that squashes inputs to the range (0, 1).\n",
    "\n",
    "**Mathematical Definition:**\n",
    "```\n",
    "f(x) = 1 / (1 + e^(-x))\n",
    "```\n",
    "\n",
    "**Key Properties:**\n",
    "- **Range**: (0, 1) - never exactly 0 or 1\n",
    "- **Smooth**: Differentiable everywhere\n",
    "- **Monotonic**: Always increasing\n",
    "- **Symmetric**: Around the point (0, 0.5)\n",
    "\n",
    "### Why Sigmoid is Useful\n",
    "1. **Probability interpretation**: Output can be interpreted as probability\n",
    "2. **Smooth gradients**: Nice for optimization\n",
    "3. **Bounded output**: Prevents extreme values\n",
    "\n",
    "### Real-World Analogy\n",
    "Sigmoid is like a **smooth dimmer switch** - it gradually transitions from \"off\" (near 0) to \"on\" (near 1), unlike ReLU's sharp cutoff.\n",
    "\n",
    "### When to Use Sigmoid\n",
    "- **Binary classification** (output layer)\n",
    "- **Gate mechanisms** (in LSTMs)\n",
    "- **When you need probabilities**\n",
    "\n",
    "### Numerical Stability Note\n",
    "For very large positive or negative inputs, sigmoid can cause numerical issues. We'll handle this with clipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a7f3a1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class Sigmoid:\n",
    "    \"\"\"\n",
    "    Sigmoid Activation Function: f(x) = 1 / (1 + e^(-x))\n",
    "    \n",
    "    Squashes inputs to the range (0, 1), useful for binary classification\n",
    "    and probability interpretation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply Sigmoid activation: f(x) = 1 / (1 + e^(-x))\n",
    "        \n",
    "        TODO: Implement Sigmoid activation\n",
    "        \n",
    "        APPROACH:\n",
    "        1. For numerical stability, clip x to reasonable range (e.g., -500 to 500)\n",
    "        2. Compute 1 / (1 + exp(-x)) for each element\n",
    "        3. Return a new Tensor with the results\n",
    "        \n",
    "        EXAMPLE:\n",
    "        Input: Tensor([[-2, -1, 0, 1, 2]])\n",
    "        Expected: Tensor([[0.119, 0.269, 0.5, 0.731, 0.881]]) (approximately)\n",
    "        \n",
    "        HINTS:\n",
    "        - Use np.clip(x.data, -500, 500) for numerical stability\n",
    "        - Use np.exp(-clipped_x) for the exponential\n",
    "        - Formula: 1 / (1 + np.exp(-clipped_x))\n",
    "        - Remember to return a new Tensor object\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Student implementation required\")\n",
    "    \n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Allow calling the activation like a function: sigmoid(x)\"\"\"\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2254ff20",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "class Sigmoid:\n",
    "    \"\"\"Sigmoid Activation: f(x) = 1 / (1 + e^(-x))\"\"\"\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # Clip for numerical stability\n",
    "        clipped = np.clip(x.data, -500, 500)\n",
    "        result = 1 / (1 + np.exp(-clipped))\n",
    "        return Tensor(result)\n",
    "        \n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80afbe84",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### üß™ Test Your Sigmoid Implementation\n",
    "\n",
    "Let's test your Sigmoid implementation to ensure it's working correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ed51d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Create Sigmoid activation\n",
    "    sigmoid = Sigmoid()\n",
    "    \n",
    "    print(\"üîß Testing Sigmoid Implementation\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Test 1: Basic functionality\n",
    "    test_input = Tensor([[-2, -1, 0, 1, 2]])\n",
    "    result = sigmoid(test_input)\n",
    "    \n",
    "    print(f\"Input:  {test_input.data.flatten()}\")\n",
    "    print(f\"Output: {result.data.flatten()}\")\n",
    "    \n",
    "    # Check properties\n",
    "    # 1. All outputs should be between 0 and 1\n",
    "    if np.all(result.data >= 0) and np.all(result.data <= 1):\n",
    "        print(\"‚úÖ Range test passed: all outputs in (0, 1)\")\n",
    "    else:\n",
    "        print(\"‚ùå Range test failed: outputs should be in (0, 1)\")\n",
    "    \n",
    "    # 2. Sigmoid(0) should be 0.5\n",
    "    zero_input = Tensor([[0]])\n",
    "    zero_result = sigmoid(zero_input)\n",
    "    if abs(zero_result.data.item() - 0.5) < 1e-6:\n",
    "        print(\"‚úÖ Sigmoid(0) = 0.5 test passed!\")\n",
    "    else:\n",
    "        print(f\"‚ùå Sigmoid(0) should be 0.5, got {zero_result.data.item()}\")\n",
    "    \n",
    "    # 3. Test symmetry: sigmoid(-x) = 1 - sigmoid(x)\n",
    "    x_val = 2.0\n",
    "    pos_result = sigmoid(Tensor([[x_val]])).data.item()\n",
    "    neg_result = sigmoid(Tensor([[-x_val]])).data.item()\n",
    "    \n",
    "    if abs(pos_result + neg_result - 1.0) < 1e-6:\n",
    "        print(\"‚úÖ Symmetry test passed!\")\n",
    "    else:\n",
    "        print(f\"‚ùå Symmetry test failed: sigmoid({x_val}) + sigmoid({-x_val}) should equal 1\")\n",
    "    \n",
    "    # 4. Test numerical stability with extreme values\n",
    "    extreme_input = Tensor([[-1000, 1000]])\n",
    "    extreme_result = sigmoid(extreme_input)\n",
    "    \n",
    "    # Should not produce NaN or inf\n",
    "    if not np.any(np.isnan(extreme_result.data)) and not np.any(np.isinf(extreme_result.data)):\n",
    "        print(\"‚úÖ Numerical stability test passed!\")\n",
    "    else:\n",
    "        print(\"‚ùå Numerical stability test failed: extreme values produced NaN/inf\")\n",
    "    \n",
    "    print(\"‚úÖ Sigmoid tests complete!\")\n",
    "    \n",
    "    # üé® Visualize Sigmoid behavior (development only)\n",
    "    if _should_show_plots():\n",
    "        print(\"\\nüé® Visualizing Sigmoid behavior...\")\n",
    "        visualize_activation_function(sigmoid, \"Sigmoid\", x_range=(-5, 5))\n",
    "        \n",
    "        # Show Sigmoid with real data\n",
    "        sample_data = Tensor([[-3.0, -1.0, 0.0, 1.0, 3.0]])\n",
    "        visualize_activation_on_data(sigmoid, \"Sigmoid\", sample_data)\n",
    "    \n",
    "except NotImplementedError:\n",
    "    print(\"‚ö†Ô∏è  Sigmoid not implemented yet - complete the forward method above!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in Sigmoid: {e}\")\n",
    "    print(\"   Check your implementation in the forward method\")\n",
    "\n",
    "print()  # Add spacing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a987dc2f",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 4: Tanh - The Centered Alternative\n",
    "\n",
    "### What is Tanh?\n",
    "**Tanh (Hyperbolic Tangent)** is similar to Sigmoid but centered around zero, with range (-1, 1).\n",
    "\n",
    "**Mathematical Definition:**\n",
    "```\n",
    "f(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "```\n",
    "\n",
    "**Alternative form:**\n",
    "```\n",
    "f(x) = 2 * sigmoid(2x) - 1\n",
    "```\n",
    "\n",
    "**Key Properties:**\n",
    "- **Range**: (-1, 1) - symmetric around zero\n",
    "- **Zero-centered**: Output has mean closer to zero\n",
    "- **Smooth**: Differentiable everywhere\n",
    "- **Stronger gradients**: Steeper than sigmoid\n",
    "\n",
    "### Why Tanh is Better Than Sigmoid\n",
    "1. **Zero-centered**: Helps with gradient flow in deep networks\n",
    "2. **Stronger gradients**: Faster convergence in some cases\n",
    "3. **Symmetric**: Better for certain applications\n",
    "\n",
    "### Real-World Analogy\n",
    "Tanh is like a **balanced scale** - it can tip strongly in either direction (-1 to +1) but defaults to neutral (0).\n",
    "\n",
    "### When to Use Tanh\n",
    "- **Hidden layers** (alternative to ReLU)\n",
    "- **Recurrent networks** (RNNs, LSTMs)\n",
    "- **When you need zero-centered outputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ecd200",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class Tanh:\n",
    "    \"\"\"\n",
    "    Tanh Activation Function: f(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "    \n",
    "    Zero-centered activation function with range (-1, 1).\n",
    "    Often preferred over Sigmoid for hidden layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply Tanh activation: f(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "        \n",
    "        TODO: Implement Tanh activation\n",
    "        \n",
    "        APPROACH:\n",
    "        1. Use numpy's built-in tanh function: np.tanh(x.data)\n",
    "        2. Return a new Tensor with the results\n",
    "        \n",
    "        ALTERNATIVE APPROACH:\n",
    "        1. Compute e^x and e^(-x)\n",
    "        2. Use formula: (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "        \n",
    "        EXAMPLE:\n",
    "        Input: Tensor([[-2, -1, 0, 1, 2]])\n",
    "        Expected: Tensor([[-0.964, -0.762, 0.0, 0.762, 0.964]]) (approximately)\n",
    "        \n",
    "        HINTS:\n",
    "        - np.tanh() is the simplest approach\n",
    "        - Output range is (-1, 1)\n",
    "        - tanh(0) = 0 (zero-centered)\n",
    "        - Remember to return a new Tensor object\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Student implementation required\")\n",
    "    \n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Allow calling the activation like a function: tanh(x)\"\"\"\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdb8bc3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "class Tanh:\n",
    "    \"\"\"Tanh Activation: f(x) = (e^x - e^(-x)) / (e^x + e^(-x))\"\"\"\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        result = np.tanh(x.data)\n",
    "        return Tensor(result)\n",
    "        \n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05e8d68",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### üß™ Test Your Tanh Implementation\n",
    "\n",
    "Let's test your Tanh implementation to ensure it's working correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eafad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Create Tanh activation\n",
    "    tanh = Tanh()\n",
    "    \n",
    "    print(\"üîß Testing Tanh Implementation\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Test 1: Basic functionality\n",
    "    test_input = Tensor([[-2, -1, 0, 1, 2]])\n",
    "    result = tanh(test_input)\n",
    "    \n",
    "    print(f\"Input:  {test_input.data.flatten()}\")\n",
    "    print(f\"Output: {result.data.flatten()}\")\n",
    "    \n",
    "    # Check properties\n",
    "    # 1. All outputs should be between -1 and 1\n",
    "    if np.all(result.data >= -1) and np.all(result.data <= 1):\n",
    "        print(\"‚úÖ Range test passed: all outputs in (-1, 1)\")\n",
    "    else:\n",
    "        print(\"‚ùå Range test failed: outputs should be in (-1, 1)\")\n",
    "    \n",
    "    # 2. Tanh(0) should be 0\n",
    "    zero_input = Tensor([[0]])\n",
    "    zero_result = tanh(zero_input)\n",
    "    if abs(zero_result.data.item()) < 1e-6:\n",
    "        print(\"‚úÖ Tanh(0) = 0 test passed!\")\n",
    "    else:\n",
    "        print(f\"‚ùå Tanh(0) should be 0, got {zero_result.data.item()}\")\n",
    "    \n",
    "    # 3. Test antisymmetry: tanh(-x) = -tanh(x)\n",
    "    x_val = 1.5\n",
    "    pos_result = tanh(Tensor([[x_val]])).data.item()\n",
    "    neg_result = tanh(Tensor([[-x_val]])).data.item()\n",
    "    \n",
    "    if abs(pos_result + neg_result) < 1e-6:\n",
    "        print(\"‚úÖ Antisymmetry test passed!\")\n",
    "    else:\n",
    "        print(f\"‚ùå Antisymmetry test failed: tanh({x_val}) + tanh({-x_val}) should equal 0\")\n",
    "    \n",
    "    # 4. Test that tanh is stronger than sigmoid\n",
    "    # For the same input, |tanh(x)| should be > |sigmoid(x) - 0.5|\n",
    "    test_val = 1.0\n",
    "    tanh_result = abs(tanh(Tensor([[test_val]])).data.item())\n",
    "    sigmoid_result = abs(sigmoid(Tensor([[test_val]])).data.item() - 0.5)\n",
    "    \n",
    "    if tanh_result > sigmoid_result:\n",
    "        print(\"‚úÖ Stronger gradient test passed!\")\n",
    "    else:\n",
    "        print(\"‚ùå Tanh should have stronger gradients than sigmoid\")\n",
    "    \n",
    "    print(\"‚úÖ Tanh tests complete!\")\n",
    "    \n",
    "    # üé® Visualize Tanh behavior (development only)\n",
    "    if _should_show_plots():\n",
    "        print(\"\\nüé® Visualizing Tanh behavior...\")\n",
    "        visualize_activation_function(tanh, \"Tanh\", x_range=(-3, 3))\n",
    "        \n",
    "        # Show Tanh with real data\n",
    "        sample_data = Tensor([[-2.0, -1.0, 0.0, 1.0, 2.0]])\n",
    "        visualize_activation_on_data(tanh, \"Tanh\", sample_data)\n",
    "    \n",
    "except NotImplementedError:\n",
    "    print(\"‚ö†Ô∏è  Tanh not implemented yet - complete the forward method above!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in Tanh: {e}\")\n",
    "    print(\"   Check your implementation in the forward method\")\n",
    "\n",
    "print()  # Add spacing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af77df8",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 5: Softmax - The Probability Maker\n",
    "\n",
    "### What is Softmax?\n",
    "**Softmax** converts a vector of real numbers into a probability distribution. It's essential for multi-class classification.\n",
    "\n",
    "**Mathematical Definition:**\n",
    "```\n",
    "f(x_i) = e^(x_i) / Œ£(e^(x_j)) for all j\n",
    "```\n",
    "\n",
    "**Key Properties:**\n",
    "- **Probability distribution**: All outputs sum to 1\n",
    "- **Non-negative**: All outputs ‚â• 0\n",
    "- **Differentiable**: Smooth for optimization\n",
    "- **Relative**: Emphasizes the largest input\n",
    "\n",
    "### Why Softmax is Special\n",
    "1. **Probability interpretation**: Perfect for classification\n",
    "2. **Competitive**: Emphasizes the winner (largest input)\n",
    "3. **Differentiable**: Works well with gradient descent\n",
    "\n",
    "### Real-World Analogy\n",
    "Softmax is like **voting with enthusiasm** - not only does the most popular choice win, but the \"votes\" are weighted by how much more popular it is.\n",
    "\n",
    "### When to Use Softmax\n",
    "- **Multi-class classification** (output layer)\n",
    "- **Attention mechanisms** (in Transformers)\n",
    "- **When you need probability distributions**\n",
    "\n",
    "### Numerical Stability Note\n",
    "For numerical stability, we subtract the maximum value before computing exponentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8601324",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class Softmax:\n",
    "    \"\"\"\n",
    "    Softmax Activation Function: f(x_i) = e^(x_i) / Œ£(e^(x_j))\n",
    "    \n",
    "    Converts a vector of real numbers into a probability distribution.\n",
    "    Essential for multi-class classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply Softmax activation: f(x_i) = e^(x_i) / Œ£(e^(x_j))\n",
    "        \n",
    "        TODO: Implement Softmax activation\n",
    "        \n",
    "        APPROACH:\n",
    "        1. For numerical stability, subtract the maximum value from each row\n",
    "        2. Compute exponentials of the shifted values\n",
    "        3. Divide each exponential by the sum of exponentials in its row\n",
    "        4. Return a new Tensor with the results\n",
    "        \n",
    "        EXAMPLE:\n",
    "        Input: Tensor([[1, 2, 3]])\n",
    "        Expected: Tensor([[0.090, 0.245, 0.665]]) (approximately)\n",
    "        Sum should be 1.0\n",
    "        \n",
    "        HINTS:\n",
    "        - Use np.max(x.data, axis=1, keepdims=True) to find row maximums\n",
    "        - Subtract max from x.data for numerical stability\n",
    "        - Use np.exp() for exponentials\n",
    "        - Use np.sum(exp_vals, axis=1, keepdims=True) for row sums\n",
    "        - Remember to return a new Tensor object\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Student implementation required\")\n",
    "    \n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Allow calling the activation like a function: softmax(x)\"\"\"\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59da816",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "class Softmax:\n",
    "    \"\"\"Softmax Activation: f(x_i) = e^(x_i) / Œ£(e^(x_j))\"\"\"\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # Subtract max for numerical stability\n",
    "        shifted = x.data - np.max(x.data, axis=1, keepdims=True)\n",
    "        exp_vals = np.exp(shifted)\n",
    "        result = exp_vals / np.sum(exp_vals, axis=1, keepdims=True)\n",
    "        return Tensor(result)\n",
    "        \n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc394348",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### üß™ Test Your Softmax Implementation\n",
    "\n",
    "Let's test your Softmax implementation to ensure it's working correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f960109",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Create Softmax activation\n",
    "    softmax = Softmax()\n",
    "    \n",
    "    print(\"üîß Testing Softmax Implementation\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Test 1: Basic functionality\n",
    "    test_input = Tensor([[1, 2, 3]])\n",
    "    result = softmax(test_input)\n",
    "    \n",
    "    print(f\"Input:  {test_input.data.flatten()}\")\n",
    "    print(f\"Output: {result.data.flatten()}\")\n",
    "    \n",
    "    # Check properties\n",
    "    # 1. All outputs should be non-negative\n",
    "    if np.all(result.data >= 0):\n",
    "        print(\"‚úÖ Non-negative test passed!\")\n",
    "    else:\n",
    "        print(\"‚ùå Non-negative test failed: all outputs should be ‚â• 0\")\n",
    "    \n",
    "    # 2. Sum should equal 1 (probability distribution)\n",
    "    row_sums = np.sum(result.data, axis=1)\n",
    "    if np.allclose(row_sums, 1.0):\n",
    "        print(\"‚úÖ Probability distribution test passed!\")\n",
    "    else:\n",
    "        print(f\"‚ùå Sum test failed: sum should be 1.0, got {row_sums}\")\n",
    "    \n",
    "    # 3. Test with multiple rows\n",
    "    multi_input = Tensor([[1, 2, 3], [0, 0, 0], [10, 20, 30]])\n",
    "    multi_result = softmax(multi_input)\n",
    "    multi_sums = np.sum(multi_result.data, axis=1)\n",
    "    \n",
    "    if np.allclose(multi_sums, 1.0):\n",
    "        print(\"‚úÖ Multi-row test passed!\")\n",
    "    else:\n",
    "        print(f\"‚ùå Multi-row test failed: all row sums should be 1.0, got {multi_sums}\")\n",
    "    \n",
    "    # 4. Test numerical stability\n",
    "    large_input = Tensor([[1000, 1001, 1002]])\n",
    "    large_result = softmax(large_input)\n",
    "    \n",
    "    # Should not produce NaN or inf\n",
    "    if not np.any(np.isnan(large_result.data)) and not np.any(np.isinf(large_result.data)):\n",
    "        print(\"‚úÖ Numerical stability test passed!\")\n",
    "    else:\n",
    "        print(\"‚ùå Numerical stability test failed: large values produced NaN/inf\")\n",
    "    \n",
    "    # 5. Test that largest input gets highest probability\n",
    "    test_logits = Tensor([[1, 5, 2]])\n",
    "    test_probs = softmax(test_logits)\n",
    "    max_idx = np.argmax(test_probs.data)\n",
    "    \n",
    "    if max_idx == 1:  # Second element (index 1) should be largest\n",
    "        print(\"‚úÖ Max probability test passed!\")\n",
    "    else:\n",
    "        print(\"‚ùå Max probability test failed: largest input should get highest probability\")\n",
    "    \n",
    "    print(\"‚úÖ Softmax tests complete!\")\n",
    "    \n",
    "    # üé® Visualize Softmax behavior (development only)\n",
    "    if _should_show_plots():\n",
    "        print(\"\\nüé® Visualizing Softmax behavior...\")\n",
    "        # Note: Softmax is different - it's a vector function, so we show it differently\n",
    "        sample_logits = Tensor([[1.0, 2.0, 3.0]])  # Simple 3-class example\n",
    "        softmax_output = softmax(sample_logits)\n",
    "        \n",
    "        print(f\"   Example: logits {sample_logits.data.flatten()} ‚Üí probabilities {softmax_output.data.flatten()}\")\n",
    "        print(f\"   Sum of probabilities: {softmax_output.data.sum():.6f} (should be 1.0)\")\n",
    "        \n",
    "        # Show how different input scales affect output\n",
    "        scale_examples = [\n",
    "            Tensor([[1.0, 2.0, 3.0]]),    # Original\n",
    "            Tensor([[2.0, 4.0, 6.0]]),    # Scaled up\n",
    "            Tensor([[0.1, 0.2, 0.3]]),    # Scaled down\n",
    "        ]\n",
    "        \n",
    "        print(\"\\n   üìä Scale sensitivity:\")\n",
    "        for i, example in enumerate(scale_examples):\n",
    "            output = softmax(example)\n",
    "            print(f\"   Scale {i+1}: {example.data.flatten()} ‚Üí {output.data.flatten()}\")\n",
    "    \n",
    "except NotImplementedError:\n",
    "    print(\"‚ö†Ô∏è  Softmax not implemented yet - complete the forward method above!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in Softmax: {e}\")\n",
    "    print(\"   Check your implementation in the forward method\")\n",
    "\n",
    "print()  # Add spacing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dd27a4",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## üé® Comprehensive Activation Function Comparison\n",
    "\n",
    "Now that we've implemented all four activation functions, let's compare them side by side to understand their differences and use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0ed7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive comparison of all activation functions\n",
    "print(\"üé® Comprehensive Activation Function Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Create all activation functions\n",
    "    activations = {\n",
    "        'ReLU': ReLU(),\n",
    "        'Sigmoid': Sigmoid(),\n",
    "        'Tanh': Tanh(),\n",
    "        'Softmax': Softmax()\n",
    "    }\n",
    "    \n",
    "    # Test with sample data\n",
    "    test_data = Tensor([[-2, -1, 0, 1, 2]])\n",
    "    \n",
    "    print(\"üìä Activation Function Outputs:\")\n",
    "    print(f\"Input: {test_data.data.flatten()}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for name, activation in activations.items():\n",
    "        try:\n",
    "            result = activation(test_data)\n",
    "            print(f\"{name:8}: {result.data.flatten()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"{name:8}: Error - {e}\")\n",
    "    \n",
    "    print(\"\\nüìà Key Properties Summary:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"ReLU     : Range [0, ‚àû), sparse, fast\")\n",
    "    print(\"Sigmoid  : Range (0, 1), smooth, probability-like\")\n",
    "    print(\"Tanh     : Range (-1, 1), zero-centered, symmetric\")\n",
    "    print(\"Softmax  : Probability distribution, sums to 1\")\n",
    "    \n",
    "    print(\"\\nüéØ When to Use Each:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"ReLU     : Hidden layers, CNNs, most deep networks\")\n",
    "    print(\"Sigmoid  : Binary classification, gates, probabilities\")\n",
    "    print(\"Tanh     : RNNs, when you need zero-centered output\")\n",
    "    print(\"Softmax  : Multi-class classification, attention\")\n",
    "    \n",
    "    # Show comprehensive visualization if available\n",
    "    if _should_show_plots():\n",
    "        print(\"\\nüé® Generating comprehensive comparison plot...\")\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            \n",
    "            fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "            fig.suptitle('Activation Function Comparison', fontsize=16)\n",
    "            \n",
    "            x_vals = np.linspace(-5, 5, 100)\n",
    "            \n",
    "            # Plot each activation function\n",
    "            for i, (name, activation) in enumerate(list(activations.items())[:3]):  # Skip Softmax for now\n",
    "                row, col = i // 2, i % 2\n",
    "                ax = axes[row, col]\n",
    "                \n",
    "                y_vals = []\n",
    "                for x in x_vals:\n",
    "                    try:\n",
    "                        input_tensor = Tensor([[x]])\n",
    "                        output = activation(input_tensor)\n",
    "                        y_vals.append(output.data.item())\n",
    "                    except:\n",
    "                        y_vals.append(0)\n",
    "                \n",
    "                ax.plot(x_vals, y_vals, 'b-', linewidth=2)\n",
    "                ax.set_title(f'{name} Activation')\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                ax.set_xlabel('Input (x)')\n",
    "                ax.set_ylabel(f'{name}(x)')\n",
    "            \n",
    "            # Special handling for Softmax\n",
    "            ax = axes[1, 1]\n",
    "            sample_inputs = np.array([[1, 2, 3], [0, 0, 0], [-1, 0, 1]])\n",
    "            softmax_results = []\n",
    "            \n",
    "            for inp in sample_inputs:\n",
    "                result = softmax(Tensor([inp]))\n",
    "                softmax_results.append(result.data.flatten())\n",
    "            \n",
    "            x_pos = np.arange(len(sample_inputs))\n",
    "            width = 0.25\n",
    "            \n",
    "            for i in range(3):  # 3 classes\n",
    "                values = [result[i] for result in softmax_results]\n",
    "                ax.bar(x_pos + i * width, values, width, label=f'Class {i+1}')\n",
    "            \n",
    "            ax.set_title('Softmax Activation')\n",
    "            ax.set_xlabel('Input Examples')\n",
    "            ax.set_ylabel('Probability')\n",
    "            ax.set_xticks(x_pos + width)\n",
    "            ax.set_xticklabels(['[1,2,3]', '[0,0,0]', '[-1,0,1]'])\n",
    "            ax.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"   üìä Matplotlib not available - skipping comprehensive plot\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Comprehensive plot error: {e}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in comprehensive comparison: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ Congratulations! You've implemented all four activation functions!\")\n",
    "print(\"You now understand the building blocks that make neural networks intelligent.\")\n",
    "print(\"=\" * 60) "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
