{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2843fa68",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Module 2: Layers - Neural Network Building Blocks\n",
    "\n",
    "Welcome to the Layers module! This is where neural networks begin. You'll implement the fundamental building blocks that transform tensors.\n",
    "\n",
    "## Learning Goals\n",
    "- Understand layers as functions that transform tensors: `y = f(x)`\n",
    "- Implement Dense layers with linear transformations: `y = Wx + b`\n",
    "- Add activation functions for nonlinearity (ReLU, Sigmoid, Tanh)\n",
    "- See how neural networks are just function composition\n",
    "- Build intuition before diving into training\n",
    "\n",
    "## Build ‚Üí Use ‚Üí Understand\n",
    "1. **Build**: Dense layers and activation functions\n",
    "2. **Use**: Transform tensors and see immediate results\n",
    "3. **Understand**: How neural networks transform information\n",
    "\n",
    "## Module ‚Üí Package Structure\n",
    "**üéì Teaching vs. üîß Building**: \n",
    "- **Learning side**: Work in `modules/layers/layers_dev.py`  \n",
    "- **Building side**: Exports to `tinytorch/core/layers.py`\n",
    "\n",
    "This module builds the fundamental transformations that compose into neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d285d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core.layers\n",
    "\n",
    "# Setup and imports\n",
    "import numpy as np\n",
    "import sys\n",
    "from typing import Union, Optional, Callable\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12b7f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "from typing import Union, Optional, Callable\n",
    "from tinytorch.core.tensor import Tensor\n",
    "\n",
    "# Import our Tensor class\n",
    "# sys.path.append('../../')\n",
    "# from modules.tensor.tensor_dev import Tensor\n",
    "\n",
    "# print(\"üî• TinyTorch Layers Module\")\n",
    "# print(f\"NumPy version: {np.__version__}\")\n",
    "# print(f\"Python version: {sys.version_info.major}.{sys.version_info.minor}\")\n",
    "# print(\"Ready to build neural network layers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8b760c",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 1: What is a Layer?\n",
    "\n",
    "A **layer** is a function that transforms tensors. Think of it as:\n",
    "- **Input**: Tensor with some shape\n",
    "- **Transformation**: Mathematical operation (linear, nonlinear, etc.)\n",
    "- **Output**: Tensor with possibly different shape\n",
    "\n",
    "**The fundamental insight**: Neural networks are just function composition!\n",
    "```\n",
    "x ‚Üí Layer1 ‚Üí Layer2 ‚Üí Layer3 ‚Üí y\n",
    "```\n",
    "\n",
    "**Why layers matter**:\n",
    "- They're the building blocks of all neural networks\n",
    "- Each layer learns a different transformation\n",
    "- Composing layers creates complex functions\n",
    "- Understanding layers = understanding neural networks\n",
    "\n",
    "Let's start with the most important layer: **Dense** (also called Linear or Fully Connected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabf403c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class Dense:\n",
    "    \"\"\"\n",
    "    Dense (Linear) Layer: y = Wx + b\n",
    "    \n",
    "    The fundamental building block of neural networks.\n",
    "    Performs linear transformation: matrix multiplication + bias addition.\n",
    "    \n",
    "    Args:\n",
    "        input_size: Number of input features\n",
    "        output_size: Number of output features\n",
    "        use_bias: Whether to include bias term (default: True)\n",
    "        \n",
    "    TODO: Implement the Dense layer with weight initialization and forward pass.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, output_size: int, use_bias: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize Dense layer with random weights.\n",
    "        \n",
    "        TODO: \n",
    "        1. Store layer parameters (input_size, output_size, use_bias)\n",
    "        2. Initialize weights with small random values\n",
    "        3. Initialize bias to zeros (if use_bias=True)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Student implementation required\")\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass: y = Wx + b\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, input_size)\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, output_size)\n",
    "            \n",
    "        TODO: Implement matrix multiplication and bias addition\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Student implementation required\")\n",
    "    \n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Make layer callable: layer(x) same as layer.forward(x)\"\"\"\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718aafe5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "class Dense:\n",
    "    \"\"\"\n",
    "    Dense (Linear) Layer: y = Wx + b\n",
    "    \n",
    "    The fundamental building block of neural networks.\n",
    "    Performs linear transformation: matrix multiplication + bias addition.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, output_size: int, use_bias: bool = True):\n",
    "        \"\"\"Initialize Dense layer with random weights.\"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.use_bias = use_bias\n",
    "        \n",
    "        # Initialize weights with Xavier/Glorot initialization\n",
    "        # This helps with gradient flow during training\n",
    "        limit = math.sqrt(6.0 / (input_size + output_size))\n",
    "        self.weights = Tensor(\n",
    "            np.random.uniform(-limit, limit, (input_size, output_size)).astype(np.float32)\n",
    "        )\n",
    "        \n",
    "        # Initialize bias to zeros\n",
    "        if use_bias:\n",
    "            self.bias = Tensor(np.zeros(output_size, dtype=np.float32))\n",
    "        else:\n",
    "            self.bias = None\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Forward pass: y = Wx + b\"\"\"\n",
    "        # Matrix multiplication: x @ weights\n",
    "        # x shape: (batch_size, input_size)\n",
    "        # weights shape: (input_size, output_size)\n",
    "        # result shape: (batch_size, output_size)\n",
    "        output = Tensor(x.data @ self.weights.data)\n",
    "        \n",
    "        # Add bias if present\n",
    "        if self.bias is not None:\n",
    "            output = Tensor(output.data + self.bias.data)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Make layer callable: layer(x) same as layer.forward(x)\"\"\"\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54390574",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### üß™ Test Your Dense Layer\n",
    "\n",
    "Once you implement the Dense layer above, run this cell to test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24b9bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Dense layer\n",
    "try:\n",
    "    print(\"=== Testing Dense Layer ===\")\n",
    "    \n",
    "    # Create a simple Dense layer: 3 inputs ‚Üí 2 outputs\n",
    "    layer = Dense(input_size=3, output_size=2)\n",
    "    print(f\"Created Dense layer: {layer.input_size} ‚Üí {layer.output_size}\")\n",
    "    print(f\"Weights shape: {layer.weights.shape}\")\n",
    "    print(f\"Bias shape: {layer.bias.shape if layer.bias else 'No bias'}\")\n",
    "    \n",
    "    # Test with a single example\n",
    "    x = Tensor([[1.0, 2.0, 3.0]])  # Shape: (1, 3)\n",
    "    y = layer(x)\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {y.shape}\")\n",
    "    print(f\"Input: {x.data}\")\n",
    "    print(f\"Output: {y.data}\")\n",
    "    \n",
    "    # Test with batch of examples\n",
    "    x_batch = Tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])  # Shape: (2, 3)\n",
    "    y_batch = layer(x_batch)\n",
    "    print(f\"\\nBatch input shape: {x_batch.shape}\")\n",
    "    print(f\"Batch output shape: {y_batch.shape}\")\n",
    "    print(f\"Batch output: {y_batch.data}\")\n",
    "    \n",
    "    print(\"‚úÖ Dense layer working!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"Make sure to implement the Dense layer above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ccc78d",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Step 2: Activation Functions\n",
    "\n",
    "Dense layers alone can only learn **linear** transformations. But most real-world problems need **nonlinear** transformations.\n",
    "\n",
    "**Activation functions** add nonlinearity:\n",
    "- **ReLU**: `max(0, x)` - Most common, simple and effective\n",
    "- **Sigmoid**: `1 / (1 + e^(-x))` - Squashes to (0, 1)\n",
    "- **Tanh**: `tanh(x)` - Squashes to (-1, 1)\n",
    "\n",
    "**Why nonlinearity matters**: Without it, stacking layers is pointless!\n",
    "```\n",
    "Linear ‚Üí Linear ‚Üí Linear = Just one big Linear transformation\n",
    "Linear ‚Üí NonLinear ‚Üí Linear = Can learn complex patterns\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85818dc3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class ReLU:\n",
    "    \"\"\"\n",
    "    ReLU Activation: f(x) = max(0, x)\n",
    "    \n",
    "    The most popular activation function in deep learning.\n",
    "    Simple, effective, and computationally efficient.\n",
    "    \n",
    "    TODO: Implement ReLU activation function.\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply ReLU: f(x) = max(0, x)\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor with ReLU applied element-wise\n",
    "            \n",
    "        TODO: Implement element-wise max(0, x) operation\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Student implementation required\")\n",
    "    \n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Make activation callable: relu(x) same as relu.forward(x)\"\"\"\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e807f1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "class ReLU:\n",
    "    \"\"\"ReLU Activation: f(x) = max(0, x)\"\"\"\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Apply ReLU: f(x) = max(0, x)\"\"\"\n",
    "        return Tensor(np.maximum(0, x.data))\n",
    "    \n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0bb26a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class Sigmoid:\n",
    "    \"\"\"\n",
    "    Sigmoid Activation: f(x) = 1 / (1 + e^(-x))\n",
    "    \n",
    "    Squashes input to range (0, 1). Often used for binary classification.\n",
    "    \n",
    "    TODO: Implement Sigmoid activation function.\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply Sigmoid: f(x) = 1 / (1 + e^(-x))\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor with Sigmoid applied element-wise\n",
    "            \n",
    "        TODO: Implement sigmoid function (be careful with numerical stability!)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Student implementation required\")\n",
    "    \n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972e9668",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "class Sigmoid:\n",
    "    \"\"\"Sigmoid Activation: f(x) = 1 / (1 + e^(-x))\"\"\"\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Apply Sigmoid with numerical stability\"\"\"\n",
    "        # Use the numerically stable version to avoid overflow\n",
    "        # For x >= 0: sigmoid(x) = 1 / (1 + exp(-x))\n",
    "        # For x < 0: sigmoid(x) = exp(x) / (1 + exp(x))\n",
    "        x_data = x.data\n",
    "        result = np.zeros_like(x_data)\n",
    "        \n",
    "        # Stable computation\n",
    "        positive_mask = x_data >= 0\n",
    "        result[positive_mask] = 1.0 / (1.0 + np.exp(-x_data[positive_mask]))\n",
    "        result[~positive_mask] = np.exp(x_data[~positive_mask]) / (1.0 + np.exp(x_data[~positive_mask]))\n",
    "        \n",
    "        return Tensor(result)\n",
    "    \n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2babe8a8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class Tanh:\n",
    "    \"\"\"\n",
    "    Tanh Activation: f(x) = tanh(x)\n",
    "    \n",
    "    Squashes input to range (-1, 1). Zero-centered output.\n",
    "    \n",
    "    TODO: Implement Tanh activation function.\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply Tanh: f(x) = tanh(x)\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor with Tanh applied element-wise\n",
    "            \n",
    "        TODO: Implement tanh function\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Student implementation required\")\n",
    "    \n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eff4e44",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "class Tanh:\n",
    "    \"\"\"Tanh Activation: f(x) = tanh(x)\"\"\"\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Apply Tanh\"\"\"\n",
    "        return Tensor(np.tanh(x.data))\n",
    "    \n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39e4420",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "### üß™ Test Your Activation Functions\n",
    "\n",
    "Once you implement the activation functions above, run this cell to test them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73687cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test activation functions\n",
    "try:\n",
    "    print(\"=== Testing Activation Functions ===\")\n",
    "    \n",
    "    # Test data: mix of positive, negative, and zero\n",
    "    x = Tensor([[-2.0, -1.0, 0.0, 1.0, 2.0]])\n",
    "    print(f\"Input: {x.data}\")\n",
    "    \n",
    "    # Test ReLU\n",
    "    relu = ReLU()\n",
    "    y_relu = relu(x)\n",
    "    print(f\"ReLU output: {y_relu.data}\")\n",
    "    \n",
    "    # Test Sigmoid\n",
    "    sigmoid = Sigmoid()\n",
    "    y_sigmoid = sigmoid(x)\n",
    "    print(f\"Sigmoid output: {y_sigmoid.data}\")\n",
    "    \n",
    "    # Test Tanh\n",
    "    tanh = Tanh()\n",
    "    y_tanh = tanh(x)\n",
    "    print(f\"Tanh output: {y_tanh.data}\")\n",
    "    \n",
    "    print(\"‚úÖ Activation functions working!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"Make sure to implement the activation functions above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec82e933",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 3: Layer Composition - Building Neural Networks\n",
    "\n",
    "Now comes the magic! We can **compose** layers to build neural networks:\n",
    "\n",
    "```\n",
    "Input ‚Üí Dense ‚Üí ReLU ‚Üí Dense ‚Üí Sigmoid ‚Üí Output\n",
    "```\n",
    "\n",
    "This is a 2-layer neural network that can learn complex nonlinear patterns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c5692f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple 2-layer neural network\n",
    "try:\n",
    "    print(\"=== Building a 2-Layer Neural Network ===\")\n",
    "    \n",
    "    # Network architecture: 3 ‚Üí 4 ‚Üí 2\n",
    "    # Input: 3 features\n",
    "    # Hidden: 4 neurons with ReLU\n",
    "    # Output: 2 neurons with Sigmoid\n",
    "    \n",
    "    layer1 = Dense(input_size=3, output_size=4)\n",
    "    activation1 = ReLU()\n",
    "    layer2 = Dense(input_size=4, output_size=2)\n",
    "    activation2 = Sigmoid()\n",
    "    \n",
    "    print(\"Network architecture:\")\n",
    "    print(f\"  Input: 3 features\")\n",
    "    print(f\"  Hidden: {layer1.input_size} ‚Üí {layer1.output_size} (Dense + ReLU)\")\n",
    "    print(f\"  Output: {layer2.input_size} ‚Üí {layer2.output_size} (Dense + Sigmoid)\")\n",
    "    \n",
    "    # Test with sample data\n",
    "    x = Tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])  # 2 examples, 3 features each\n",
    "    print(f\"\\nInput shape: {x.shape}\")\n",
    "    print(f\"Input data: {x.data}\")\n",
    "    \n",
    "    # Forward pass through the network\n",
    "    h1 = layer1(x)           # Dense layer 1\n",
    "    h1_activated = activation1(h1)  # ReLU activation\n",
    "    h2 = layer2(h1_activated)       # Dense layer 2  \n",
    "    output = activation2(h2)        # Sigmoid activation\n",
    "    \n",
    "    print(f\"\\nAfter layer 1: {h1.shape}\")\n",
    "    print(f\"After ReLU: {h1_activated.shape}\")\n",
    "    print(f\"After layer 2: {h2.shape}\")\n",
    "    print(f\"Final output: {output.shape}\")\n",
    "    print(f\"Output values: {output.data}\")\n",
    "    \n",
    "    print(\"\\nüéâ Neural network working! You just built your first neural network!\")\n",
    "    print(\"Notice how the network transforms 3D input into 2D output through learned transformations.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"Make sure to implement the layers and activations above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dc6d9a",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 4: Understanding What We Built\n",
    "\n",
    "Congratulations! You just implemented the fundamental building blocks of neural networks:\n",
    "\n",
    "### üß± **What You Built**\n",
    "1. **Dense Layer**: Linear transformation `y = Wx + b`\n",
    "2. **Activation Functions**: Nonlinear transformations (ReLU, Sigmoid, Tanh)\n",
    "3. **Layer Composition**: Chaining layers to build networks\n",
    "\n",
    "### üéØ **Key Insights**\n",
    "- **Layers are functions**: They transform tensors from one space to another\n",
    "- **Composition creates complexity**: Simple layers ‚Üí complex networks\n",
    "- **Nonlinearity is crucial**: Without it, deep networks are just linear transformations\n",
    "- **Neural networks are function approximators**: They learn to map inputs to outputs\n",
    "\n",
    "### üöÄ **What's Next**\n",
    "In the next modules, you'll learn:\n",
    "- **Training**: How networks learn from data (backpropagation, optimizers)\n",
    "- **Architectures**: Specialized layers for different problems (CNNs, RNNs)\n",
    "- **Applications**: Using networks for real problems\n",
    "\n",
    "### üîß **Export to Package**\n",
    "Run this to export your layers to the TinyTorch package:\n",
    "```bash\n",
    "python bin/tito.py sync\n",
    "```\n",
    "\n",
    "Then test your implementation:\n",
    "```bash\n",
    "python bin/tito.py test --module layers\n",
    "```\n",
    "\n",
    "**Great job! You've built the foundation of neural networks!** üéâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54d8ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final demonstration: A more complex example\n",
    "try:\n",
    "    print(\"=== Final Demo: Image Classification Network ===\")\n",
    "    \n",
    "    # Simulate a small image: 28x28 pixels flattened to 784 features\n",
    "    # This is like a tiny MNIST digit\n",
    "    image_size = 28 * 28  # 784 pixels\n",
    "    num_classes = 10      # 10 digits (0-9)\n",
    "    \n",
    "    # Build a 3-layer network for digit classification\n",
    "    # 784 ‚Üí 128 ‚Üí 64 ‚Üí 10\n",
    "    layer1 = Dense(input_size=image_size, output_size=128)\n",
    "    relu1 = ReLU()\n",
    "    layer2 = Dense(input_size=128, output_size=64)\n",
    "    relu2 = ReLU()\n",
    "    layer3 = Dense(input_size=64, output_size=num_classes)\n",
    "    softmax = Sigmoid()  # Using Sigmoid as a simple \"probability-like\" output\n",
    "    \n",
    "    print(f\"Image classification network:\")\n",
    "    print(f\"  Input: {image_size} pixels (28x28 image)\")\n",
    "    print(f\"  Hidden 1: {layer1.input_size} ‚Üí {layer1.output_size} (Dense + ReLU)\")\n",
    "    print(f\"  Hidden 2: {layer2.input_size} ‚Üí {layer2.output_size} (Dense + ReLU)\")\n",
    "    print(f\"  Output: {layer3.input_size} ‚Üí {layer3.output_size} (Dense + Sigmoid)\")\n",
    "    \n",
    "    # Simulate a batch of 5 images\n",
    "    batch_size = 5\n",
    "    fake_images = Tensor(np.random.randn(batch_size, image_size).astype(np.float32))\n",
    "    \n",
    "    # Forward pass\n",
    "    h1 = relu1(layer1(fake_images))\n",
    "    h2 = relu2(layer2(h1))\n",
    "    predictions = softmax(layer3(h2))\n",
    "    \n",
    "    print(f\"\\nBatch processing:\")\n",
    "    print(f\"  Input batch shape: {fake_images.shape}\")\n",
    "    print(f\"  Predictions shape: {predictions.shape}\")\n",
    "    print(f\"  Sample predictions: {predictions.data[0]}\")  # First image predictions\n",
    "    \n",
    "    print(\"\\nüéâ You built a neural network that could classify images!\")\n",
    "    print(\"With training, this network could learn to recognize handwritten digits!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"Check your layer implementations!\") "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
