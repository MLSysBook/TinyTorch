{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "jupyter:\n",
        "  jupytext:\n",
        "    text_representation:\n",
        "      extension: .py\n",
        "      format_name: percent\n",
        "      format_version: '1.3'\n",
        "      jupytext_version: 1.17.1\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Module 3: Networks - Neural Network Architectures\n",
        "\n",
        "Welcome to the Networks module! This is where we compose layers into complete neural network architectures.\n",
        "\n",
        "## Learning Goals\n",
        "- Understand networks as function composition: `f(x) = layer_n(...layer_2(layer_1(x)))`\n",
        "- Build common architectures (MLP, CNN) from layers\n",
        "- Visualize network structure and data flow\n",
        "- See how architecture affects capability\n",
        "- Master forward pass inference (no training yet!)\n",
        "\n",
        "## Build \u2192 Use \u2192 Understand\n",
        "1. **Build**: Compose layers into complete networks\n",
        "2. **Use**: Create different architectures and run inference\n",
        "3. **Understand**: How architecture design affects network behavior\n",
        "\n",
        "## Module Dependencies\n",
        "This module builds on previous modules:\n",
        "- **tensor** \u2192 **activations** \u2192 **layers** \u2192 **networks**\n",
        "- Clean composition: math functions \u2192 building blocks \u2192 complete systems\n",
        "\n",
        "## Module \u2192 Package Structure\n",
        "**\ud83c\udf93 Teaching vs. \ud83d\udd27 Building**: \n",
        "- **Learning side**: Work in `modules/networks/networks_dev.py`  \n",
        "- **Building side**: Exports to `tinytorch/core/networks.py`\n",
        "\n",
        "This module teaches how to compose layers into complete neural network architectures.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| default_exp core.networks\n",
        "\n",
        "# Setup and imports\n",
        "import numpy as np\n",
        "import sys\n",
        "from typing import List, Union, Optional, Callable\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from matplotlib.patches import FancyBboxPatch, ConnectionPatch\n",
        "import seaborn as sns\n",
        "\n",
        "# Import our building blocks\n",
        "from tinytorch.core.tensor import Tensor\n",
        "from tinytorch.core.layers import Dense\n",
        "from tinytorch.core.activations import ReLU, Sigmoid, Tanh\n",
        "\n",
        "print(\"\ud83d\udd25 TinyTorch Networks Module\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"Python version: {sys.version_info.major}.{sys.version_info.minor}\")\n",
        "print(\"Ready to build neural network architectures!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| export\n",
        "import numpy as np\n",
        "import sys\n",
        "from typing import List, Union, Optional, Callable\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from matplotlib.patches import FancyBboxPatch, ConnectionPatch\n",
        "import seaborn as sns\n",
        "\n",
        "# Import our building blocks\n",
        "from tinytorch.core.tensor import Tensor\n",
        "from tinytorch.core.layers import Dense\n",
        "from tinytorch.core.activations import ReLU, Sigmoid, Tanh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| hide\n",
        "#| export\n",
        "def _should_show_plots():\n",
        "    \"\"\"Check if we should show plots (disable during testing)\"\"\"\n",
        "    return 'pytest' not in sys.modules and 'test' not in sys.argv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "## Step 1: What is a Network?\n",
        "\n",
        "A **network** is a composition of layers that transforms input data into output predictions. Think of it as:\n",
        "\n",
        "```\n",
        "Input \u2192 Layer1 \u2192 Layer2 \u2192 Layer3 \u2192 Output\n",
        "```\n",
        "\n",
        "**The fundamental insight**: Neural networks are just function composition!\n",
        "- Each layer is a function: `f_i(x)`\n",
        "- The network is: `f(x) = f_n(...f_2(f_1(x)))`\n",
        "- Complex behavior emerges from simple building blocks\n",
        "\n",
        "**Why networks matter**:\n",
        "- They solve real problems (classification, regression, etc.)\n",
        "- Architecture determines what problems you can solve\n",
        "- Understanding networks = understanding deep learning\n",
        "- They're the foundation for all modern AI\n",
        "\n",
        "Let's start by building the most fundamental network: **Sequential**.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| export\n",
        "class Sequential:\n",
        "    \"\"\"\n",
        "    Sequential Network: Composes layers in sequence\n",
        "    \n",
        "    The most fundamental network architecture.\n",
        "    Applies layers in order: f(x) = layer_n(...layer_2(layer_1(x)))\n",
        "    \n",
        "    Args:\n",
        "        layers: List of layers to compose\n",
        "        \n",
        "    TODO: Implement the Sequential network with forward pass.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, layers: List):\n",
        "        \"\"\"\n",
        "        Initialize Sequential network with layers.\n",
        "        \n",
        "        Args:\n",
        "            layers: List of layers to compose in order\n",
        "            \n",
        "        TODO: Store the layers and implement forward pass\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Student implementation required\")\n",
        "    \n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass through all layers in sequence.\n",
        "        \n",
        "        Args:\n",
        "            x: Input tensor\n",
        "            \n",
        "        Returns:\n",
        "            Output tensor after passing through all layers\n",
        "            \n",
        "        TODO: Implement sequential forward pass through all layers\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"Student implementation required\")\n",
        "    \n",
        "    def __call__(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"Make network callable: network(x) same as network.forward(x)\"\"\"\n",
        "        return self.forward(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| hide\n",
        "#| export\n",
        "class Sequential:\n",
        "    \"\"\"\n",
        "    Sequential Network: Composes layers in sequence\n",
        "    \n",
        "    The most fundamental network architecture.\n",
        "    Applies layers in order: f(x) = layer_n(...layer_2(layer_1(x)))\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, layers: List):\n",
        "        \"\"\"Initialize Sequential network with layers.\"\"\"\n",
        "        self.layers = layers\n",
        "    \n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"Forward pass through all layers in sequence.\"\"\"\n",
        "        # Apply each layer in order\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "    \n",
        "    def __call__(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"Make network callable: network(x) same as network.forward(x)\"\"\"\n",
        "        return self.forward(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "### \ud83e\uddea Test Your Sequential Network\n",
        "\n",
        "Once you implement the Sequential network above, run this cell to test it:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the Sequential network\n",
        "try:\n",
        "    print(\"=== Testing Sequential Network ===\")\n",
        "    \n",
        "    # Create a simple 2-layer network: 3 \u2192 4 \u2192 2\n",
        "    network = Sequential([\n",
        "        Dense(3, 4),\n",
        "        ReLU(),\n",
        "        Dense(4, 2),\n",
        "        Sigmoid()\n",
        "    ])\n",
        "    \n",
        "    # Test with sample data\n",
        "    x = Tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "    print(f\"Input shape: {x.shape}\")\n",
        "    print(f\"Input data: {x.data}\")\n",
        "    \n",
        "    # Forward pass\n",
        "    output = network(x)\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "    print(f\"Output data: {output.data}\")\n",
        "    \n",
        "    print(\"\u2705 Sequential network working!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\u274c Error: {e}\")\n",
        "    print(\"Make sure to implement the Sequential network!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "## Step 2: Network Visualization\n",
        "\n",
        "Now let's create powerful visualizations to understand what our networks look like and how they work!\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| export\n",
        "def visualize_network_architecture(network: Sequential, title: str = \"Network Architecture\"):\n",
        "    \"\"\"\n",
        "    Create a visual representation of network architecture.\n",
        "    \n",
        "    Args:\n",
        "        network: Sequential network to visualize\n",
        "        title: Title for the plot\n",
        "    \"\"\"\n",
        "    if not _should_show_plots():\n",
        "        print(\"\ud83d\udcca Plots disabled during testing - this is normal!\")\n",
        "        return\n",
        "    \n",
        "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
        "    \n",
        "    # Network parameters\n",
        "    layer_count = len(network.layers)\n",
        "    layer_height = 0.8\n",
        "    layer_spacing = 1.2\n",
        "    \n",
        "    # Colors for different layer types\n",
        "    colors = {\n",
        "        'Dense': '#4CAF50',      # Green\n",
        "        'ReLU': '#2196F3',       # Blue\n",
        "        'Sigmoid': '#FF9800',    # Orange\n",
        "        'Tanh': '#9C27B0',       # Purple\n",
        "        'default': '#757575'      # Gray\n",
        "    }\n",
        "    \n",
        "    # Draw layers\n",
        "    for i, layer in enumerate(network.layers):\n",
        "        # Determine layer type and color\n",
        "        layer_type = type(layer).__name__\n",
        "        color = colors.get(layer_type, colors['default'])\n",
        "        \n",
        "        # Layer position\n",
        "        x = i * layer_spacing\n",
        "        y = 0\n",
        "        \n",
        "        # Create layer box\n",
        "        layer_box = FancyBboxPatch(\n",
        "            (x - 0.3, y - layer_height/2),\n",
        "            0.6, layer_height,\n",
        "            boxstyle=\"round,pad=0.1\",\n",
        "            facecolor=color,\n",
        "            edgecolor='black',\n",
        "            linewidth=2,\n",
        "            alpha=0.8\n",
        "        )\n",
        "        ax.add_patch(layer_box)\n",
        "        \n",
        "        # Add layer label\n",
        "        ax.text(x, y, layer_type, ha='center', va='center', \n",
        "                fontsize=10, fontweight='bold', color='white')\n",
        "        \n",
        "        # Add layer details\n",
        "        if hasattr(layer, 'input_size') and hasattr(layer, 'output_size'):\n",
        "            details = f\"{layer.input_size}\u2192{layer.output_size}\"\n",
        "            ax.text(x, y - 0.3, details, ha='center', va='center',\n",
        "                   fontsize=8, color='white')\n",
        "        \n",
        "        # Draw connections to next layer\n",
        "        if i < layer_count - 1:\n",
        "            next_x = (i + 1) * layer_spacing\n",
        "            connection = ConnectionPatch(\n",
        "                (x + 0.3, y), (next_x - 0.3, y),\n",
        "                \"data\", \"data\",\n",
        "                arrowstyle=\"->\", shrinkA=5, shrinkB=5,\n",
        "                mutation_scale=20, fc=\"black\", lw=2\n",
        "            )\n",
        "            ax.add_patch(connection)\n",
        "    \n",
        "    # Formatting\n",
        "    ax.set_xlim(-0.5, (layer_count - 1) * layer_spacing + 0.5)\n",
        "    ax.set_ylim(-1, 1)\n",
        "    ax.set_aspect('equal')\n",
        "    ax.axis('off')\n",
        "    \n",
        "    # Add title\n",
        "    plt.title(title, fontsize=16, fontweight='bold', pad=20)\n",
        "    \n",
        "    # Add legend\n",
        "    legend_elements = []\n",
        "    for layer_type, color in colors.items():\n",
        "        if layer_type != 'default':\n",
        "            legend_elements.append(patches.Patch(color=color, label=layer_type))\n",
        "    \n",
        "    ax.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(1, 1))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| export\n",
        "def visualize_data_flow(network: Sequential, input_data: Tensor, title: str = \"Data Flow Through Network\"):\n",
        "    \"\"\"\n",
        "    Visualize how data flows through the network.\n",
        "    \n",
        "    Args:\n",
        "        network: Sequential network\n",
        "        input_data: Input tensor\n",
        "        title: Title for the plot\n",
        "    \"\"\"\n",
        "    if not _should_show_plots():\n",
        "        print(\"\ud83d\udcca Plots disabled during testing - this is normal!\")\n",
        "        return\n",
        "    \n",
        "    # Get intermediate outputs\n",
        "    intermediate_outputs = []\n",
        "    x = input_data\n",
        "    \n",
        "    for i, layer in enumerate(network.layers):\n",
        "        x = layer(x)\n",
        "        intermediate_outputs.append({\n",
        "            'layer': network.layers[i],\n",
        "            'output': x,\n",
        "            'layer_index': i\n",
        "        })\n",
        "    \n",
        "    # Create visualization\n",
        "    fig, axes = plt.subplots(2, len(network.layers), figsize=(4*len(network.layers), 8))\n",
        "    if len(network.layers) == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "    \n",
        "    for i, (layer, output) in enumerate(zip(network.layers, intermediate_outputs)):\n",
        "        # Top row: Layer information\n",
        "        ax_top = axes[0, i] if len(network.layers) > 1 else axes[0]\n",
        "        \n",
        "        # Layer type and details\n",
        "        layer_type = type(layer).__name__\n",
        "        ax_top.text(0.5, 0.8, layer_type, ha='center', va='center',\n",
        "                   fontsize=12, fontweight='bold')\n",
        "        \n",
        "        if hasattr(layer, 'input_size') and hasattr(layer, 'output_size'):\n",
        "            ax_top.text(0.5, 0.6, f\"{layer.input_size} \u2192 {layer.output_size}\", \n",
        "                       ha='center', va='center', fontsize=10)\n",
        "        \n",
        "        # Output shape\n",
        "        ax_top.text(0.5, 0.4, f\"Shape: {output['output'].shape}\", \n",
        "                   ha='center', va='center', fontsize=9)\n",
        "        \n",
        "        # Output statistics\n",
        "        output_data = output['output'].data\n",
        "        ax_top.text(0.5, 0.2, f\"Mean: {np.mean(output_data):.3f}\", \n",
        "                   ha='center', va='center', fontsize=9)\n",
        "        ax_top.text(0.5, 0.1, f\"Std: {np.std(output_data):.3f}\", \n",
        "                   ha='center', va='center', fontsize=9)\n",
        "        \n",
        "        ax_top.set_xlim(0, 1)\n",
        "        ax_top.set_ylim(0, 1)\n",
        "        ax_top.axis('off')\n",
        "        \n",
        "        # Bottom row: Output visualization\n",
        "        ax_bottom = axes[1, i] if len(network.layers) > 1 else axes[1]\n",
        "        \n",
        "        # Show output as heatmap or histogram\n",
        "        output_data = output['output'].data.flatten()\n",
        "        \n",
        "        if len(output_data) <= 20:  # Small output - show as bars\n",
        "            ax_bottom.bar(range(len(output_data)), output_data, alpha=0.7)\n",
        "            ax_bottom.set_title(f\"Layer {i+1} Output\")\n",
        "            ax_bottom.set_xlabel(\"Output Index\")\n",
        "            ax_bottom.set_ylabel(\"Value\")\n",
        "        else:  # Large output - show histogram\n",
        "            ax_bottom.hist(output_data, bins=20, alpha=0.7, edgecolor='black')\n",
        "            ax_bottom.set_title(f\"Layer {i+1} Output Distribution\")\n",
        "            ax_bottom.set_xlabel(\"Value\")\n",
        "            ax_bottom.set_ylabel(\"Frequency\")\n",
        "        \n",
        "        ax_bottom.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.suptitle(title, fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| export\n",
        "def compare_networks(networks: List[Sequential], network_names: List[str], \n",
        "                    input_data: Tensor, title: str = \"Network Comparison\"):\n",
        "    \"\"\"\n",
        "    Compare different network architectures side-by-side.\n",
        "    \n",
        "    Args:\n",
        "        networks: List of networks to compare\n",
        "        network_names: Names for each network\n",
        "        input_data: Input tensor to test with\n",
        "        title: Title for the plot\n",
        "    \"\"\"\n",
        "    if not _should_show_plots():\n",
        "        print(\"\ud83d\udcca Plots disabled during testing - this is normal!\")\n",
        "        return\n",
        "    \n",
        "    fig, axes = plt.subplots(2, len(networks), figsize=(6*len(networks), 10))\n",
        "    if len(networks) == 1:\n",
        "        axes = axes.reshape(2, -1)\n",
        "    \n",
        "    for i, (network, name) in enumerate(zip(networks, network_names)):\n",
        "        # Get network output\n",
        "        output = network(input_data)\n",
        "        \n",
        "        # Top row: Architecture visualization\n",
        "        ax_top = axes[0, i] if len(networks) > 1 else axes[0]\n",
        "        \n",
        "        # Count layer types\n",
        "        layer_types = {}\n",
        "        for layer in network.layers:\n",
        "            layer_type = type(layer).__name__\n",
        "            layer_types[layer_type] = layer_types.get(layer_type, 0) + 1\n",
        "        \n",
        "        # Create pie chart of layer types\n",
        "        if layer_types:\n",
        "            labels = list(layer_types.keys())\n",
        "            sizes = list(layer_types.values())\n",
        "            colors = plt.cm.Set3(np.linspace(0, 1, len(labels)))\n",
        "            \n",
        "            ax_top.pie(sizes, labels=labels, autopct='%1.1f%%', colors=colors)\n",
        "            ax_top.set_title(f\"{name}\\nLayer Distribution\")\n",
        "        \n",
        "        # Bottom row: Output comparison\n",
        "        ax_bottom = axes[1, i] if len(networks) > 1 else axes[1]\n",
        "        \n",
        "        output_data = output.data.flatten()\n",
        "        \n",
        "        # Show output statistics\n",
        "        ax_bottom.hist(output_data, bins=20, alpha=0.7, edgecolor='black')\n",
        "        ax_bottom.axvline(np.mean(output_data), color='red', linestyle='--', \n",
        "                         label=f'Mean: {np.mean(output_data):.3f}')\n",
        "        ax_bottom.axvline(np.median(output_data), color='green', linestyle='--',\n",
        "                         label=f'Median: {np.median(output_data):.3f}')\n",
        "        \n",
        "        ax_bottom.set_title(f\"{name} Output Distribution\")\n",
        "        ax_bottom.set_xlabel(\"Output Value\")\n",
        "        ax_bottom.set_ylabel(\"Frequency\")\n",
        "        ax_bottom.legend()\n",
        "        ax_bottom.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.suptitle(title, fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "## Step 3: Building Common Architectures\n",
        "\n",
        "Now let's build some common neural network architectures and visualize them!\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| export\n",
        "def create_mlp(input_size: int, hidden_sizes: List[int], output_size: int, \n",
        "               activation=ReLU, output_activation=Sigmoid) -> Sequential:\n",
        "    \"\"\"\n",
        "    Create a Multi-Layer Perceptron (MLP) network.\n",
        "    \n",
        "    Args:\n",
        "        input_size: Number of input features\n",
        "        hidden_sizes: List of hidden layer sizes\n",
        "        output_size: Number of output features\n",
        "        activation: Activation function for hidden layers\n",
        "        output_activation: Activation function for output layer\n",
        "        \n",
        "    Returns:\n",
        "        Sequential network\n",
        "    \"\"\"\n",
        "    layers = []\n",
        "    \n",
        "    # Input layer\n",
        "    if hidden_sizes:\n",
        "        layers.append(Dense(input_size, hidden_sizes[0]))\n",
        "        layers.append(activation())\n",
        "        \n",
        "        # Hidden layers\n",
        "        for i in range(len(hidden_sizes) - 1):\n",
        "            layers.append(Dense(hidden_sizes[i], hidden_sizes[i + 1]))\n",
        "            layers.append(activation())\n",
        "        \n",
        "        # Output layer\n",
        "        layers.append(Dense(hidden_sizes[-1], output_size))\n",
        "    else:\n",
        "        # Direct input to output\n",
        "        layers.append(Dense(input_size, output_size))\n",
        "    \n",
        "    layers.append(output_activation())\n",
        "    \n",
        "    return Sequential(layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test MLP creation and visualization\n",
        "try:\n",
        "    print(\"=== Testing MLP Creation and Visualization ===\")\n",
        "    \n",
        "    # Create different MLP architectures\n",
        "    mlp_small = create_mlp(input_size=3, hidden_sizes=[4], output_size=2)\n",
        "    mlp_medium = create_mlp(input_size=10, hidden_sizes=[16, 8], output_size=3)\n",
        "    mlp_large = create_mlp(input_size=784, hidden_sizes=[128, 64, 32], output_size=10)\n",
        "    \n",
        "    print(\"Created MLP architectures:\")\n",
        "    print(f\"  Small: 3 \u2192 4 \u2192 2\")\n",
        "    print(f\"  Medium: 10 \u2192 16 \u2192 8 \u2192 3\")\n",
        "    print(f\"  Large: 784 \u2192 128 \u2192 64 \u2192 32 \u2192 10\")\n",
        "    \n",
        "    # Test with sample data\n",
        "    x = Tensor(np.random.randn(5, 3).astype(np.float32))\n",
        "    \n",
        "    # Visualize architectures\n",
        "    visualize_network_architecture(mlp_small, \"Small MLP Architecture\")\n",
        "    visualize_network_architecture(mlp_medium, \"Medium MLP Architecture\")\n",
        "    visualize_network_architecture(mlp_large, \"Large MLP Architecture\")\n",
        "    \n",
        "    # Visualize data flow\n",
        "    visualize_data_flow(mlp_small, x, \"Data Flow Through Small MLP\")\n",
        "    \n",
        "    # Compare networks\n",
        "    networks = [mlp_small, mlp_medium]\n",
        "    names = [\"Small MLP\", \"Medium MLP\"]\n",
        "    compare_networks(networks, names, x, \"MLP Architecture Comparison\")\n",
        "    \n",
        "    print(\"\u2705 MLP creation and visualization working!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\u274c Error: {e}\")\n",
        "    print(\"Make sure to implement the visualization functions!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "## Step 4: Understanding Network Behavior\n",
        "\n",
        "Let's analyze how different network architectures behave with different types of input data.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| export\n",
        "def analyze_network_behavior(network: Sequential, input_data: Tensor, \n",
        "                           title: str = \"Network Behavior Analysis\"):\n",
        "    \"\"\"\n",
        "    Analyze how a network behaves with different types of input.\n",
        "    \n",
        "    Args:\n",
        "        network: Network to analyze\n",
        "        input_data: Input tensor\n",
        "        title: Title for the plot\n",
        "    \"\"\"\n",
        "    if not _should_show_plots():\n",
        "        print(\"\ud83d\udcca Plots disabled during testing - this is normal!\")\n",
        "        return\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "    \n",
        "    # 1. Input vs Output relationship\n",
        "    ax1 = axes[0, 0]\n",
        "    input_flat = input_data.data.flatten()\n",
        "    output = network(input_data)\n",
        "    output_flat = output.data.flatten()\n",
        "    \n",
        "    ax1.scatter(input_flat, output_flat, alpha=0.6)\n",
        "    ax1.plot([input_flat.min(), input_flat.max()], \n",
        "             [input_flat.min(), input_flat.max()], 'r--', alpha=0.5, label='y=x')\n",
        "    ax1.set_xlabel('Input Values')\n",
        "    ax1.set_ylabel('Output Values')\n",
        "    ax1.set_title('Input vs Output')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. Output distribution\n",
        "    ax2 = axes[0, 1]\n",
        "    ax2.hist(output_flat, bins=20, alpha=0.7, edgecolor='black')\n",
        "    ax2.axvline(np.mean(output_flat), color='red', linestyle='--', \n",
        "                label=f'Mean: {np.mean(output_flat):.3f}')\n",
        "    ax2.set_xlabel('Output Values')\n",
        "    ax2.set_ylabel('Frequency')\n",
        "    ax2.set_title('Output Distribution')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Layer-by-layer activation patterns\n",
        "    ax3 = axes[0, 2]\n",
        "    activations = []\n",
        "    x = input_data\n",
        "    \n",
        "    for layer in network.layers:\n",
        "        x = layer(x)\n",
        "        if hasattr(layer, 'input_size'):  # Dense layer\n",
        "            activations.append(np.mean(x.data))\n",
        "        else:  # Activation layer\n",
        "            activations.append(np.mean(x.data))\n",
        "    \n",
        "    ax3.plot(range(len(activations)), activations, 'bo-', linewidth=2, markersize=8)\n",
        "    ax3.set_xlabel('Layer Index')\n",
        "    ax3.set_ylabel('Mean Activation')\n",
        "    ax3.set_title('Layer-by-Layer Activations')\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. Network depth analysis\n",
        "    ax4 = axes[1, 0]\n",
        "    layer_types = [type(layer).__name__ for layer in network.layers]\n",
        "    layer_counts = {}\n",
        "    for layer_type in layer_types:\n",
        "        layer_counts[layer_type] = layer_counts.get(layer_type, 0) + 1\n",
        "    \n",
        "    if layer_counts:\n",
        "        ax4.bar(layer_counts.keys(), layer_counts.values(), alpha=0.7)\n",
        "        ax4.set_xlabel('Layer Type')\n",
        "        ax4.set_ylabel('Count')\n",
        "        ax4.set_title('Layer Type Distribution')\n",
        "        ax4.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 5. Shape transformation\n",
        "    ax5 = axes[1, 1]\n",
        "    shapes = [input_data.shape]\n",
        "    x = input_data\n",
        "    \n",
        "    for layer in network.layers:\n",
        "        x = layer(x)\n",
        "        shapes.append(x.shape)\n",
        "    \n",
        "    layer_indices = range(len(shapes))\n",
        "    shape_sizes = [np.prod(shape) for shape in shapes]\n",
        "    \n",
        "    ax5.plot(layer_indices, shape_sizes, 'go-', linewidth=2, markersize=8)\n",
        "    ax5.set_xlabel('Layer Index')\n",
        "    ax5.set_ylabel('Tensor Size')\n",
        "    ax5.set_title('Shape Transformation')\n",
        "    ax5.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 6. Network summary\n",
        "    ax6 = axes[1, 2]\n",
        "    ax6.axis('off')\n",
        "    \n",
        "    summary_text = f\"\"\"\n",
        "Network Summary:\n",
        "\u2022 Total Layers: {len(network.layers)}\n",
        "\u2022 Input Shape: {input_data.shape}\n",
        "\u2022 Output Shape: {output.shape}\n",
        "\u2022 Parameters: {sum(np.prod(layer.weights.data.shape) if hasattr(layer, 'weights') else 0 for layer in network.layers)}\n",
        "\u2022 Architecture: {' \u2192 '.join([type(layer).__name__ for layer in network.layers])}\n",
        "    \"\"\"\n",
        "    \n",
        "    ax6.text(0.05, 0.95, summary_text, transform=ax6.transAxes, \n",
        "             fontsize=10, verticalalignment='top', fontfamily='monospace')\n",
        "    \n",
        "    plt.suptitle(title, fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test network behavior analysis\n",
        "try:\n",
        "    print(\"=== Testing Network Behavior Analysis ===\")\n",
        "    \n",
        "    # Create a network for analysis\n",
        "    network = create_mlp(input_size=5, hidden_sizes=[8, 4], output_size=2)\n",
        "    \n",
        "    # Test with different types of input\n",
        "    x_normal = Tensor(np.random.randn(10, 5).astype(np.float32))\n",
        "    x_uniform = Tensor(np.random.uniform(-1, 1, (10, 5)).astype(np.float32))\n",
        "    x_zeros = Tensor(np.zeros((10, 5)).astype(np.float32))\n",
        "    \n",
        "    print(\"Analyzing network behavior with different inputs...\")\n",
        "    \n",
        "    # Analyze behavior\n",
        "    analyze_network_behavior(network, x_normal, \"Network Behavior: Normal Input\")\n",
        "    analyze_network_behavior(network, x_uniform, \"Network Behavior: Uniform Input\")\n",
        "    analyze_network_behavior(network, x_zeros, \"Network Behavior: Zero Input\")\n",
        "    \n",
        "    print(\"\u2705 Network behavior analysis working!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\u274c Error: {e}\")\n",
        "    print(\"Make sure to implement the behavior analysis function!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "## Step 5: Practical Applications\n",
        "\n",
        "Let's see how our networks can be applied to real-world problems!\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| export\n",
        "def create_classification_network(input_size: int, num_classes: int, \n",
        "                                hidden_sizes: List[int] = None) -> Sequential:\n",
        "    \"\"\"\n",
        "    Create a network for classification problems.\n",
        "    \n",
        "    Args:\n",
        "        input_size: Number of input features\n",
        "        num_classes: Number of output classes\n",
        "        hidden_sizes: List of hidden layer sizes (default: [input_size//2])\n",
        "        \n",
        "    Returns:\n",
        "        Sequential network for classification\n",
        "    \"\"\"\n",
        "    if hidden_sizes is None:\n",
        "        hidden_sizes = [input_size // 2]\n",
        "    \n",
        "    return create_mlp(\n",
        "        input_size=input_size,\n",
        "        hidden_sizes=hidden_sizes,\n",
        "        output_size=num_classes,\n",
        "        activation=ReLU,\n",
        "        output_activation=Sigmoid\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| export\n",
        "def create_regression_network(input_size: int, output_size: int = 1,\n",
        "                             hidden_sizes: List[int] = None) -> Sequential:\n",
        "    \"\"\"\n",
        "    Create a network for regression problems.\n",
        "    \n",
        "    Args:\n",
        "        input_size: Number of input features\n",
        "        output_size: Number of output values (default: 1)\n",
        "        hidden_sizes: List of hidden layer sizes (default: [input_size//2])\n",
        "        \n",
        "    Returns:\n",
        "        Sequential network for regression\n",
        "    \"\"\"\n",
        "    if hidden_sizes is None:\n",
        "        hidden_sizes = [input_size // 2]\n",
        "    \n",
        "    return create_mlp(\n",
        "        input_size=input_size,\n",
        "        hidden_sizes=hidden_sizes,\n",
        "        output_size=output_size,\n",
        "        activation=ReLU,\n",
        "        output_activation=Tanh  # No activation for regression\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test practical applications\n",
        "try:\n",
        "    print(\"=== Testing Practical Applications ===\")\n",
        "    \n",
        "    # Create networks for different tasks\n",
        "    digit_classifier = create_classification_network(\n",
        "        input_size=784,  # 28x28 image\n",
        "        num_classes=10,  # 10 digits\n",
        "        hidden_sizes=[128, 64]\n",
        "    )\n",
        "    \n",
        "    sentiment_analyzer = create_classification_network(\n",
        "        input_size=100,  # 100-dimensional word embeddings\n",
        "        num_classes=2,   # Positive/Negative\n",
        "        hidden_sizes=[32, 16]\n",
        "    )\n",
        "    \n",
        "    house_price_predictor = create_regression_network(\n",
        "        input_size=13,   # 13 house features\n",
        "        output_size=1,   # 1 price prediction\n",
        "        hidden_sizes=[8, 4]\n",
        "    )\n",
        "    \n",
        "    print(\"Created networks for different applications:\")\n",
        "    print(f\"  Digit Classifier: 784 \u2192 128 \u2192 64 \u2192 10\")\n",
        "    print(f\"  Sentiment Analyzer: 100 \u2192 32 \u2192 16 \u2192 2\")\n",
        "    print(f\"  House Price Predictor: 13 \u2192 8 \u2192 4 \u2192 1\")\n",
        "    \n",
        "    # Test with sample data\n",
        "    digit_input = Tensor(np.random.randn(1, 784).astype(np.float32))\n",
        "    sentiment_input = Tensor(np.random.randn(1, 100).astype(np.float32))\n",
        "    house_input = Tensor(np.random.randn(1, 13).astype(np.float32))\n",
        "    \n",
        "    # Get predictions\n",
        "    digit_pred = digit_classifier(digit_input)\n",
        "    sentiment_pred = sentiment_analyzer(sentiment_input)\n",
        "    house_pred = house_price_predictor(house_input)\n",
        "    \n",
        "    print(f\"\\nSample predictions:\")\n",
        "    print(f\"  Digit classifier output: {digit_pred.data[0]}\")\n",
        "    print(f\"  Sentiment analyzer output: {sentiment_pred.data[0]}\")\n",
        "    print(f\"  House price predictor output: {house_pred.data[0]}\")\n",
        "    \n",
        "    # Visualize architectures\n",
        "    visualize_network_architecture(digit_classifier, \"Digit Classification Network\")\n",
        "    visualize_network_architecture(sentiment_analyzer, \"Sentiment Analysis Network\")\n",
        "    visualize_network_architecture(house_price_predictor, \"House Price Prediction Network\")\n",
        "    \n",
        "    print(\"\u2705 Practical applications working!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\u274c Error: {e}\")\n",
        "    print(\"Make sure to implement the application functions!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "## \ud83c\udf93 Module Summary\n",
        "\n",
        "### What You Learned\n",
        "1. **Network Composition**: Building complete networks from layers\n",
        "2. **Architecture Design**: How to choose network structures\n",
        "3. **Visualization**: Understanding networks through visual analysis\n",
        "4. **Practical Applications**: Real-world network use cases\n",
        "\n",
        "### Key Architectural Insights\n",
        "- **Function Composition**: Networks as `f(x) = layer_n(...layer_1(x))`\n",
        "- **Modular Design**: Clean separation between layers and networks\n",
        "- **Visual Understanding**: How to analyze network behavior\n",
        "- **Application Patterns**: Classification vs regression architectures\n",
        "\n",
        "### Network Design Principles\n",
        "- **Depth vs Width**: Trade-offs in network architecture\n",
        "- **Activation Functions**: How they affect network behavior\n",
        "- **Shape Management**: Understanding tensor transformations\n",
        "- **Practical Considerations**: Choosing architectures for specific tasks\n",
        "\n",
        "### Next Steps\n",
        "- **Training**: Learn how networks learn from data (autograd, optimization)\n",
        "- **Advanced Architectures**: CNNs, RNNs, Transformers\n",
        "- **Real Data**: Working with actual datasets\n",
        "- **Production**: Deploying networks in real applications\n",
        "\n",
        "**Congratulations on mastering neural network architectures!** \ud83d\ude80\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}