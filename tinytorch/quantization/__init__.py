"""
TinyTorch Quantization - Model Compression for Deployment

Matches torch.quantization functionality:
- INT8 quantization for 4x memory reduction
- Quantization-aware training utilities  
- Model conversion tools

This is Module 17 of TinyTorch.
"""

# Exports will be populated by nbdev
__all__ = []