# AUTOGENERATED! DO NOT EDIT! File to edit: ../../modules/source/14_kvcaching/kvcaching_dev.ipynb.

# %% auto 0
__all__ = ['Tensor']

# %% ../../modules/source/14_kvcaching/kvcaching_dev.ipynb 1
import numpy as np
import time
from typing import Tuple, Optional, Dict, List
from dataclasses import dataclass

# Import our TinyTorch components (Modules 01-13)
### BEGIN SOLUTION
# Note: In real implementation, these would import from previous modules
# For now, we'll implement minimal versions to focus on caching concepts

class Tensor:
    """Minimal Tensor for KV Caching focus (from Module 01)"""
    def __init__(self, data, requires_grad=False):
        self.data = np.array(data)
        self.shape = self.data.shape
        self.requires_grad = requires_grad
        self.grad = None

    def __getitem__(self, key):
        return Tensor(self.data[key])

    def __setitem__(self, key, value):
        if isinstance(value, Tensor):
            self.data[key] = value.data
        else:
            self.data[key] = value

    def size(self, dim=None):
        if dim is None:
            return self.shape
        return self.shape[dim]

    def view(self, *shape):
        return Tensor(self.data.reshape(shape))

    def transpose(self, dim0, dim1):
        axes = list(range(len(self.shape)))
        axes[dim0], axes[dim1] = axes[dim1], axes[dim0]
        return Tensor(np.transpose(self.data, axes))

    @staticmethod
    def cat(tensors, dim=0):
        """Concatenate tensors along dimension"""
        arrays = [t.data for t in tensors]
        return Tensor(np.concatenate(arrays, axis=dim))

    @staticmethod
    def zeros(*shape):
        """Create zero tensor"""
        return Tensor(np.zeros(shape))
### END SOLUTION
