# AUTOGENERATED! DO NOT EDIT! File to edit: ../../modules/02_activations/activations_dev.ipynb.

# %% auto 0
__all__ = ['visualize_activation_function', 'visualize_activation_on_data', 'ReLU', 'Sigmoid', 'Tanh', 'Softmax']

# %% ../../modules/02_activations/activations_dev.ipynb 2
import math
import numpy as np
import matplotlib.pyplot as plt
import os
import sys
from typing import Union, List

# Import our Tensor class from the main package (rock solid foundation)
from .tensor import Tensor

# %% ../../modules/02_activations/activations_dev.ipynb 3
def _should_show_plots():
    """Check if we should show plots (disable during testing)"""
    # Check multiple conditions that indicate we're in test mode
    is_pytest = (
        'pytest' in sys.modules or
        'test' in sys.argv or
        os.environ.get('PYTEST_CURRENT_TEST') is not None or
        any('test' in arg for arg in sys.argv) or
        any('pytest' in arg for arg in sys.argv)
    )
    
    # Show plots in development mode (when not in test mode)
    return not is_pytest

# %% ../../modules/02_activations/activations_dev.ipynb 4
def visualize_activation_function(activation_fn, name: str, x_range: tuple = (-5, 5), num_points: int = 100):
    """Visualize an activation function's behavior"""
    if not _should_show_plots():
        return
        
    try:
        
        # Generate input values
        x_vals = np.linspace(x_range[0], x_range[1], num_points)
        
        # Apply activation function
        y_vals = []
        for x in x_vals:
            input_tensor = Tensor([[x]])
            output = activation_fn(input_tensor)
            y_vals.append(output.data.item())
        
        # Create plot
        plt.figure(figsize=(10, 6))
        plt.plot(x_vals, y_vals, 'b-', linewidth=2, label=f'{name} Activation')
        plt.grid(True, alpha=0.3)
        plt.xlabel('Input (x)')
        plt.ylabel(f'{name}(x)')
        plt.title(f'{name} Activation Function')
        plt.legend()
        plt.show()
        
    except ImportError:
        print("   📊 Matplotlib not available - skipping visualization")
    except Exception as e:
        print(f"   ⚠️  Visualization error: {e}")

def visualize_activation_on_data(activation_fn, name: str, data: Tensor):
    """Show activation function applied to sample data"""
    if not _should_show_plots():
        return
        
    try:
        output = activation_fn(data)
        print(f"   📊 {name} Example:")
        print(f"      Input:  {data.data.flatten()}")
        print(f"      Output: {output.data.flatten()}")
        print(f"      Range:  [{output.data.min():.3f}, {output.data.max():.3f}]")
        
    except Exception as e:
        print(f"   ⚠️  Data visualization error: {e}")

# %% ../../modules/02_activations/activations_dev.ipynb 7
class ReLU:
    """
    ReLU Activation Function: f(x) = max(0, x)
    
    The most popular activation function in deep learning.
    Simple, fast, and effective for most applications.
    """
    
    def forward(self, x: Tensor) -> Tensor:
        """
        Apply ReLU activation: f(x) = max(0, x)
        
        TODO: Implement ReLU activation
        
        APPROACH:
        1. For each element in the input tensor, apply max(0, element)
        2. Return a new Tensor with the results
        
        EXAMPLE:
        Input: Tensor([[-1, 0, 1, 2, -3]])
        Expected: Tensor([[0, 0, 1, 2, 0]])
        
        HINTS:
        - Use np.maximum(0, x.data) for element-wise max
        - Remember to return a new Tensor object
        - The shape should remain the same as input
        """
        raise NotImplementedError("Student implementation required")
    
    def __call__(self, x: Tensor) -> Tensor:
        """Allow calling the activation like a function: relu(x)"""
        return self.forward(x)

# %% ../../modules/02_activations/activations_dev.ipynb 8
class ReLU:
    """ReLU Activation: f(x) = max(0, x)"""
    
    def forward(self, x: Tensor) -> Tensor:
        result = np.maximum(0, x.data)
        return Tensor(result)
        
    def __call__(self, x: Tensor) -> Tensor:
        return self.forward(x)

# %% ../../modules/02_activations/activations_dev.ipynb 13
class Sigmoid:
    """
    Sigmoid Activation Function: f(x) = 1 / (1 + e^(-x))
    
    Squashes inputs to the range (0, 1), useful for binary classification
    and probability interpretation.
    """
    
    def forward(self, x: Tensor) -> Tensor:
        """
        Apply Sigmoid activation: f(x) = 1 / (1 + e^(-x))
        
        TODO: Implement Sigmoid activation
        
        APPROACH:
        1. For numerical stability, clip x to reasonable range (e.g., -500 to 500)
        2. Compute 1 / (1 + exp(-x)) for each element
        3. Return a new Tensor with the results
        
        EXAMPLE:
        Input: Tensor([[-2, -1, 0, 1, 2]])
        Expected: Tensor([[0.119, 0.269, 0.5, 0.731, 0.881]]) (approximately)
        
        HINTS:
        - Use np.clip(x.data, -500, 500) for numerical stability
        - Use np.exp(-clipped_x) for the exponential
        - Formula: 1 / (1 + np.exp(-clipped_x))
        - Remember to return a new Tensor object
        """
        raise NotImplementedError("Student implementation required")
    
    def __call__(self, x: Tensor) -> Tensor:
        """Allow calling the activation like a function: sigmoid(x)"""
        return self.forward(x)

# %% ../../modules/02_activations/activations_dev.ipynb 14
class Sigmoid:
    """Sigmoid Activation: f(x) = 1 / (1 + e^(-x))"""
    
    def forward(self, x: Tensor) -> Tensor:
        # Clip for numerical stability
        clipped = np.clip(x.data, -500, 500)
        result = 1 / (1 + np.exp(-clipped))
        return Tensor(result)
        
    def __call__(self, x: Tensor) -> Tensor:
        return self.forward(x)

# %% ../../modules/02_activations/activations_dev.ipynb 18
class Tanh:
    """
    Tanh Activation Function: f(x) = (e^x - e^(-x)) / (e^x + e^(-x))
    
    Zero-centered activation function with range (-1, 1).
    Often preferred over Sigmoid for hidden layers.
    """
    
    def forward(self, x: Tensor) -> Tensor:
        """
        Apply Tanh activation: f(x) = (e^x - e^(-x)) / (e^x + e^(-x))
        
        TODO: Implement Tanh activation
        
        APPROACH:
        1. Use numpy's built-in tanh function: np.tanh(x.data)
        2. Return a new Tensor with the results
        
        ALTERNATIVE APPROACH:
        1. Compute e^x and e^(-x)
        2. Use formula: (e^x - e^(-x)) / (e^x + e^(-x))
        
        EXAMPLE:
        Input: Tensor([[-2, -1, 0, 1, 2]])
        Expected: Tensor([[-0.964, -0.762, 0.0, 0.762, 0.964]]) (approximately)
        
        HINTS:
        - np.tanh() is the simplest approach
        - Output range is (-1, 1)
        - tanh(0) = 0 (zero-centered)
        - Remember to return a new Tensor object
        """
        raise NotImplementedError("Student implementation required")
    
    def __call__(self, x: Tensor) -> Tensor:
        """Allow calling the activation like a function: tanh(x)"""
        return self.forward(x)

# %% ../../modules/02_activations/activations_dev.ipynb 19
class Tanh:
    """Tanh Activation: f(x) = (e^x - e^(-x)) / (e^x + e^(-x))"""
    
    def forward(self, x: Tensor) -> Tensor:
        result = np.tanh(x.data)
        return Tensor(result)
        
    def __call__(self, x: Tensor) -> Tensor:
        return self.forward(x)

# %% ../../modules/02_activations/activations_dev.ipynb 23
class Softmax:
    """
    Softmax Activation Function: f(x_i) = e^(x_i) / Σ(e^(x_j))
    
    Converts a vector of real numbers into a probability distribution.
    Essential for multi-class classification.
    """
    
    def forward(self, x: Tensor) -> Tensor:
        """
        Apply Softmax activation: f(x_i) = e^(x_i) / Σ(e^(x_j))
        
        TODO: Implement Softmax activation
        
        APPROACH:
        1. For numerical stability, subtract the maximum value from each row
        2. Compute exponentials of the shifted values
        3. Divide each exponential by the sum of exponentials in its row
        4. Return a new Tensor with the results
        
        EXAMPLE:
        Input: Tensor([[1, 2, 3]])
        Expected: Tensor([[0.090, 0.245, 0.665]]) (approximately)
        Sum should be 1.0
        
        HINTS:
        - Use np.max(x.data, axis=1, keepdims=True) to find row maximums
        - Subtract max from x.data for numerical stability
        - Use np.exp() for exponentials
        - Use np.sum(exp_vals, axis=1, keepdims=True) for row sums
        - Remember to return a new Tensor object
        """
        raise NotImplementedError("Student implementation required")
    
    def __call__(self, x: Tensor) -> Tensor:
        """Allow calling the activation like a function: softmax(x)"""
        return self.forward(x)

# %% ../../modules/02_activations/activations_dev.ipynb 24
class Softmax:
    """Softmax Activation: f(x_i) = e^(x_i) / Σ(e^(x_j))"""
    
    def forward(self, x: Tensor) -> Tensor:
        # Subtract max for numerical stability
        shifted = x.data - np.max(x.data, axis=1, keepdims=True)
        exp_vals = np.exp(shifted)
        result = exp_vals / np.sum(exp_vals, axis=1, keepdims=True)
        return Tensor(result)
        
    def __call__(self, x: Tensor) -> Tensor:
        return self.forward(x)
