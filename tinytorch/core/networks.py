# AUTOGENERATED! DO NOT EDIT! File to edit: ../../modules/04_networks/networks_dev.ipynb.

# %% auto 0
__all__ = ['Sequential', 'create_mlp', 'visualize_network_architecture', 'visualize_data_flow', 'compare_networks',
           'create_classification_network', 'create_regression_network', 'analyze_network_behavior']

# %% ../../modules/04_networks/networks_dev.ipynb 3
import numpy as np
import sys
from typing import List, Union, Optional, Callable
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.patches import FancyBboxPatch, ConnectionPatch
import seaborn as sns

# Import our building blocks
from .tensor import Tensor
from .layers import Dense
from .activations import ReLU, Sigmoid, Tanh

# %% ../../modules/04_networks/networks_dev.ipynb 4
def _should_show_plots():
    """Check if we should show plots (disable during testing)"""
    return 'pytest' not in sys.modules and 'test' not in sys.argv

# %% ../../modules/04_networks/networks_dev.ipynb 6
class Sequential:
    """
    Sequential Network: Composes layers in sequence
    
    The most fundamental network architecture.
    Applies layers in order: f(x) = layer_n(...layer_2(layer_1(x)))
    
    Args:
        layers: List of layers to compose
        
    TODO: Implement the Sequential network with forward pass.
    
    APPROACH:
    1. Store the list of layers as an instance variable
    2. Implement forward pass that applies each layer in sequence
    3. Make the network callable for easy use
    
    EXAMPLE:
    network = Sequential([
        Dense(3, 4),
        ReLU(),
        Dense(4, 2),
        Sigmoid()
    ])
    x = Tensor([[1, 2, 3]])
    y = network(x)  # Forward pass through all layers
    
    HINTS:
    - Store layers in self.layers
    - Use a for loop to apply each layer in order
    - Each layer's output becomes the next layer's input
    - Return the final output
    """
    
    def __init__(self, layers: List):
        """
        Initialize Sequential network with layers.
        
        Args:
            layers: List of layers to compose in order
            
        TODO: Store the layers and implement forward pass
        
        STEP-BY-STEP:
        1. Store the layers list as self.layers
        2. This creates the network architecture
        
        EXAMPLE:
        Sequential([Dense(3,4), ReLU(), Dense(4,2)])
        creates a 3-layer network: Dense â†’ ReLU â†’ Dense
        """
        raise NotImplementedError("Student implementation required")
    
    def forward(self, x: Tensor) -> Tensor:
        """
        Forward pass through all layers in sequence.
        
        Args:
            x: Input tensor
            
        Returns:
            Output tensor after passing through all layers
            
        TODO: Implement sequential forward pass through all layers
        
        STEP-BY-STEP:
        1. Start with the input tensor: current = x
        2. Loop through each layer in self.layers
        3. Apply each layer: current = layer(current)
        4. Return the final output
        
        EXAMPLE:
        Input: Tensor([[1, 2, 3]])
        Layer1 (Dense): Tensor([[1.4, 2.8]])
        Layer2 (ReLU): Tensor([[1.4, 2.8]])
        Layer3 (Dense): Tensor([[0.7]])
        Output: Tensor([[0.7]])
        
        HINTS:
        - Use a for loop: for layer in self.layers:
        - Apply each layer: current = layer(current)
        - The output of one layer becomes input to the next
        - Return the final result
        """
        raise NotImplementedError("Student implementation required")
    
    def __call__(self, x: Tensor) -> Tensor:
        """Make network callable: network(x) same as network.forward(x)"""
        return self.forward(x)

# %% ../../modules/04_networks/networks_dev.ipynb 7
class Sequential:
    """
    Sequential Network: Composes layers in sequence
    
    The most fundamental network architecture.
    Applies layers in order: f(x) = layer_n(...layer_2(layer_1(x)))
    """
    
    def __init__(self, layers: List):
        """Initialize Sequential network with layers."""
        self.layers = layers
    
    def forward(self, x: Tensor) -> Tensor:
        """Forward pass through all layers in sequence."""
        # Apply each layer in order
        for layer in self.layers:
            x = layer(x)
        return x
    
    def __call__(self, x: Tensor) -> Tensor:
        """Make network callable: network(x) same as network.forward(x)"""
        return self.forward(x)

# %% ../../modules/04_networks/networks_dev.ipynb 11
def create_mlp(input_size: int, hidden_sizes: List[int], output_size: int, 
               activation=ReLU, output_activation=Sigmoid) -> Sequential:
    """
    Create a Multi-Layer Perceptron (MLP) network.
    
    Args:
        input_size: Number of input features
        hidden_sizes: List of hidden layer sizes
        output_size: Number of output features
        activation: Activation function for hidden layers (default: ReLU)
        output_activation: Activation function for output layer (default: Sigmoid)
        
    Returns:
        Sequential network with MLP architecture
        
    TODO: Implement MLP creation with alternating Dense and activation layers.
    
    APPROACH:
    1. Start with an empty list of layers
    2. Add the first Dense layer: input_size â†’ first hidden size
    3. For each hidden layer:
       - Add activation function
       - Add Dense layer connecting to next hidden size
    4. Add final activation function
    5. Add final Dense layer: last hidden size â†’ output_size
    6. Add output activation function
    7. Return Sequential(layers)
    
    EXAMPLE:
    create_mlp(3, [4, 2], 1) creates:
    Dense(3â†’4) â†’ ReLU â†’ Dense(4â†’2) â†’ ReLU â†’ Dense(2â†’1) â†’ Sigmoid
    
    HINTS:
    - Start with layers = []
    - Add Dense layers with appropriate input/output sizes
    - Add activation functions between Dense layers
    - Don't forget the final output activation
    """
    raise NotImplementedError("Student implementation required")

# %% ../../modules/04_networks/networks_dev.ipynb 12
def create_mlp(input_size: int, hidden_sizes: List[int], output_size: int, 
               activation=ReLU, output_activation=Sigmoid) -> Sequential:
    """Create a Multi-Layer Perceptron (MLP) network."""
    layers = []
    
    # Add first layer
    current_size = input_size
    for hidden_size in hidden_sizes:
        layers.append(Dense(input_size=current_size, output_size=hidden_size))
        layers.append(activation())
        current_size = hidden_size
    
    # Add output layer
    layers.append(Dense(input_size=current_size, output_size=output_size))
    layers.append(output_activation())
    
    return Sequential(layers)

# %% ../../modules/04_networks/networks_dev.ipynb 16
def visualize_network_architecture(network: Sequential, title: str = "Network Architecture"):
    """
    Visualize the architecture of a Sequential network.
    
    Args:
        network: Sequential network to visualize
        title: Title for the plot
        
    TODO: Create a visualization showing the network structure.
    
    APPROACH:
    1. Create a matplotlib figure
    2. For each layer, draw a box showing its type and size
    3. Connect the boxes with arrows showing data flow
    4. Add labels and formatting
    
    EXAMPLE:
    Input â†’ Dense(3â†’4) â†’ ReLU â†’ Dense(4â†’2) â†’ Sigmoid â†’ Output
    
    HINTS:
    - Use plt.subplots() to create the figure
    - Use plt.text() to add layer labels
    - Use plt.arrow() to show connections
    - Add proper spacing and formatting
    """
    raise NotImplementedError("Student implementation required")

# %% ../../modules/04_networks/networks_dev.ipynb 17
def visualize_network_architecture(network: Sequential, title: str = "Network Architecture"):
    """Visualize the architecture of a Sequential network."""
    if not _should_show_plots():
        print("ðŸ“Š Visualization disabled during testing")
        return
    
    fig, ax = plt.subplots(1, 1, figsize=(12, 6))
    
    # Calculate positions
    num_layers = len(network.layers)
    x_positions = np.linspace(0, 10, num_layers + 2)
    
    # Draw input
    ax.text(x_positions[0], 0, 'Input', ha='center', va='center', 
            bbox=dict(boxstyle='round,pad=0.3', facecolor='lightblue'))
    
    # Draw layers
    for i, layer in enumerate(network.layers):
        layer_name = type(layer).__name__
        ax.text(x_positions[i+1], 0, layer_name, ha='center', va='center',
                bbox=dict(boxstyle='round,pad=0.3', facecolor='lightgreen'))
        
        # Draw arrow
        ax.arrow(x_positions[i], 0, 0.8, 0, head_width=0.1, head_length=0.1, 
                fc='black', ec='black')
    
    # Draw output
    ax.text(x_positions[-1], 0, 'Output', ha='center', va='center',
            bbox=dict(boxstyle='round,pad=0.3', facecolor='lightcoral'))
    
    ax.set_xlim(-0.5, 10.5)
    ax.set_ylim(-0.5, 0.5)
    ax.set_title(title)
    ax.axis('off')
    plt.show()

# %% ../../modules/04_networks/networks_dev.ipynb 21
def visualize_data_flow(network: Sequential, input_data: Tensor, title: str = "Data Flow Through Network"):
    """
    Visualize how data flows through the network.
    
    Args:
        network: Sequential network to analyze
        input_data: Input tensor to trace through the network
        title: Title for the plot
        
    TODO: Create a visualization showing how data transforms through each layer.
    
    APPROACH:
    1. Trace the input through each layer
    2. Record the output of each layer
    3. Create a visualization showing the transformations
    4. Add statistics (mean, std, range) for each layer
    
    EXAMPLE:
    Input: [1, 2, 3] â†’ Layer1: [1.4, 2.8] â†’ Layer2: [1.4, 2.8] â†’ Output: [0.7]
    
    HINTS:
    - Use a for loop to apply each layer
    - Store intermediate outputs
    - Use plt.subplot() to create multiple subplots
    - Show statistics for each layer output
    """
    raise NotImplementedError("Student implementation required")

# %% ../../modules/04_networks/networks_dev.ipynb 22
def visualize_data_flow(network: Sequential, input_data: Tensor, title: str = "Data Flow Through Network"):
    """Visualize how data flows through the network."""
    if not _should_show_plots():
        print("ðŸ“Š Visualization disabled during testing")
        return
    
    # Trace data through network
    current_data = input_data
    layer_outputs = [current_data.data.flatten()]
    layer_names = ['Input']
    
    for layer in network.layers:
        current_data = layer(current_data)
        layer_outputs.append(current_data.data.flatten())
        layer_names.append(type(layer).__name__)
    
    # Create visualization
    fig, axes = plt.subplots(2, len(layer_outputs), figsize=(15, 8))
    
    for i, (output, name) in enumerate(zip(layer_outputs, layer_names)):
        # Histogram
        axes[0, i].hist(output, bins=20, alpha=0.7)
        axes[0, i].set_title(f'{name}\nShape: {output.shape}')
        axes[0, i].set_xlabel('Value')
        axes[0, i].set_ylabel('Frequency')
        
        # Statistics
        stats_text = f'Mean: {np.mean(output):.3f}\nStd: {np.std(output):.3f}\nRange: [{np.min(output):.3f}, {np.max(output):.3f}]'
        axes[1, i].text(0.1, 0.5, stats_text, transform=axes[1, i].transAxes, 
                        verticalalignment='center', fontsize=10)
        axes[1, i].set_title(f'{name} Statistics')
        axes[1, i].axis('off')
    
    plt.suptitle(title)
    plt.tight_layout()
    plt.show()

# %% ../../modules/04_networks/networks_dev.ipynb 26
def compare_networks(networks: List[Sequential], network_names: List[str], 
                    input_data: Tensor, title: str = "Network Comparison"):
    """
    Compare multiple networks on the same input.
    
    Args:
        networks: List of Sequential networks to compare
        network_names: Names for each network
        input_data: Input tensor to test all networks
        title: Title for the plot
        
    TODO: Create a comparison visualization showing how different networks process the same input.
    
    APPROACH:
    1. Run the same input through each network
    2. Collect the outputs and intermediate results
    3. Create a visualization comparing the results
    4. Show statistics and differences
    
    EXAMPLE:
    Compare MLP vs Deep Network vs Wide Network on same input
    
    HINTS:
    - Use a for loop to test each network
    - Store outputs and any relevant statistics
    - Use plt.subplot() to create comparison plots
    - Show both outputs and intermediate layer results
    """
    raise NotImplementedError("Student implementation required")

# %% ../../modules/04_networks/networks_dev.ipynb 27
def compare_networks(networks: List[Sequential], network_names: List[str], 
                    input_data: Tensor, title: str = "Network Comparison"):
    """Compare multiple networks on the same input."""
    if not _should_show_plots():
        print("ðŸ“Š Visualization disabled during testing")
        return
    
    # Test all networks
    outputs = []
    for network in networks:
        output = network(input_data)
        outputs.append(output.data.flatten())
    
    # Create comparison plot
    fig, axes = plt.subplots(2, len(networks), figsize=(15, 8))
    
    for i, (output, name) in enumerate(zip(outputs, network_names)):
        # Output distribution
        axes[0, i].hist(output, bins=20, alpha=0.7)
        axes[0, i].set_title(f'{name}\nOutput Distribution')
        axes[0, i].set_xlabel('Value')
        axes[0, i].set_ylabel('Frequency')
        
        # Statistics
        stats_text = f'Mean: {np.mean(output):.3f}\nStd: {np.std(output):.3f}\nRange: [{np.min(output):.3f}, {np.max(output):.3f}]\nSize: {len(output)}'
        axes[1, i].text(0.1, 0.5, stats_text, transform=axes[1, i].transAxes, 
                        verticalalignment='center', fontsize=10)
        axes[1, i].set_title(f'{name} Statistics')
        axes[1, i].axis('off')
    
    plt.suptitle(title)
    plt.tight_layout()
    plt.show()

# %% ../../modules/04_networks/networks_dev.ipynb 31
def create_classification_network(input_size: int, num_classes: int, 
                                hidden_sizes: List[int] = None) -> Sequential:
    """
    Create a network for classification tasks.
    
    Args:
        input_size: Number of input features
        num_classes: Number of output classes
        hidden_sizes: List of hidden layer sizes (default: [input_size * 2])
        
    Returns:
        Sequential network for classification
        
    TODO: Implement classification network creation.
    
    APPROACH:
    1. Use default hidden sizes if none provided
    2. Create MLP with appropriate architecture
    3. Use Sigmoid for binary classification (num_classes=1)
    4. Use appropriate activation for multi-class
    
    EXAMPLE:
    create_classification_network(10, 3) creates:
    Dense(10â†’20) â†’ ReLU â†’ Dense(20â†’3) â†’ Sigmoid
    
    HINTS:
    - Use create_mlp() function
    - Choose appropriate output activation based on num_classes
    - For binary classification (num_classes=1), use Sigmoid
    - For multi-class, you could use Sigmoid or no activation
    """
    raise NotImplementedError("Student implementation required")

# %% ../../modules/04_networks/networks_dev.ipynb 32
def create_classification_network(input_size: int, num_classes: int, 
                                hidden_sizes: List[int] = None) -> Sequential:
    """Create a network for classification tasks."""
    if hidden_sizes is None:
        hidden_sizes = [input_size // 2]  # Use input_size // 2 as default
    
    # Choose appropriate output activation
    output_activation = Sigmoid if num_classes == 1 else Softmax
    
    return create_mlp(input_size, hidden_sizes, num_classes, 
                     activation=ReLU, output_activation=output_activation)

# %% ../../modules/04_networks/networks_dev.ipynb 33
def create_regression_network(input_size: int, output_size: int = 1,
                             hidden_sizes: List[int] = None) -> Sequential:
    """
    Create a network for regression tasks.
    
    Args:
        input_size: Number of input features
        output_size: Number of output values (default: 1)
        hidden_sizes: List of hidden layer sizes (default: [input_size * 2])
        
    Returns:
        Sequential network for regression
        
    TODO: Implement regression network creation.
    
    APPROACH:
    1. Use default hidden sizes if none provided
    2. Create MLP with appropriate architecture
    3. Use no activation on output layer (linear output)
    
    EXAMPLE:
    create_regression_network(5, 1) creates:
    Dense(5â†’10) â†’ ReLU â†’ Dense(10â†’1) (no activation)
    
    HINTS:
    - Use create_mlp() but with no output activation
    - For regression, we want linear outputs (no activation)
    - You can pass None or identity function as output_activation
    """
    raise NotImplementedError("Student implementation required")

# %% ../../modules/04_networks/networks_dev.ipynb 34
def create_regression_network(input_size: int, output_size: int = 1,
                             hidden_sizes: List[int] = None) -> Sequential:
    """Create a network for regression tasks."""
    if hidden_sizes is None:
        hidden_sizes = [input_size // 2]  # Use input_size // 2 as default
    
    # Create MLP with Tanh output activation for regression
    return create_mlp(input_size, hidden_sizes, output_size, 
                     activation=ReLU, output_activation=Tanh)

# %% ../../modules/04_networks/networks_dev.ipynb 38
def analyze_network_behavior(network: Sequential, input_data: Tensor, 
                           title: str = "Network Behavior Analysis"):
    """
    Analyze how a network behaves with different inputs.
    
    Args:
        network: Sequential network to analyze
        input_data: Input tensor to test
        title: Title for the plot
        
    TODO: Create an analysis showing network behavior and capabilities.
    
    APPROACH:
    1. Test the network with the given input
    2. Analyze the output characteristics
    3. Test with variations of the input
    4. Create visualizations showing behavior patterns
    
    EXAMPLE:
    Test network with original input and noisy versions
    Show how output changes with input variations
    
    HINTS:
    - Test the original input
    - Create variations (noise, scaling, etc.)
    - Compare outputs across variations
    - Show statistics and patterns
    """
    raise NotImplementedError("Student implementation required")

# %% ../../modules/04_networks/networks_dev.ipynb 39
def analyze_network_behavior(network: Sequential, input_data: Tensor, 
                           title: str = "Network Behavior Analysis"):
    """Analyze how a network behaves with different inputs."""
    if not _should_show_plots():
        print("ðŸ“Š Visualization disabled during testing")
        return
    
    # Test original input
    original_output = network(input_data)
    
    # Create variations
    noise_levels = [0.0, 0.1, 0.2, 0.5]
    outputs = []
    
    for noise in noise_levels:
        noisy_input = Tensor(input_data.data + noise * np.random.randn(*input_data.data.shape))
        output = network(noisy_input)
        outputs.append(output.data.flatten())
    
    # Create analysis plot
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    # Original output
    axes[0, 0].hist(outputs[0], bins=20, alpha=0.7)
    axes[0, 0].set_title('Original Input Output')
    axes[0, 0].set_xlabel('Value')
    axes[0, 0].set_ylabel('Frequency')
    
    # Output stability
    output_means = [np.mean(out) for out in outputs]
    output_stds = [np.std(out) for out in outputs]
    axes[0, 1].plot(noise_levels, output_means, 'bo-', label='Mean')
    axes[0, 1].fill_between(noise_levels, 
                           [m-s for m, s in zip(output_means, output_stds)],
                           [m+s for m, s in zip(output_means, output_stds)], 
                           alpha=0.3, label='Â±1 Std')
    axes[0, 1].set_xlabel('Noise Level')
    axes[0, 1].set_ylabel('Output Value')
    axes[0, 1].set_title('Output Stability')
    axes[0, 1].legend()
    
    # Output distribution comparison
    for i, (output, noise) in enumerate(zip(outputs, noise_levels)):
        axes[1, 0].hist(output, bins=20, alpha=0.5, label=f'Noise={noise}')
    axes[1, 0].set_xlabel('Output Value')
    axes[1, 0].set_ylabel('Frequency')
    axes[1, 0].set_title('Output Distribution Comparison')
    axes[1, 0].legend()
    
    # Statistics
    stats_text = f'Original Mean: {np.mean(outputs[0]):.3f}\nOriginal Std: {np.std(outputs[0]):.3f}\nOutput Range: [{np.min(outputs[0]):.3f}, {np.max(outputs[0]):.3f}]'
    axes[1, 1].text(0.1, 0.5, stats_text, transform=axes[1, 1].transAxes, 
                    verticalalignment='center', fontsize=10)
    axes[1, 1].set_title('Network Statistics')
    axes[1, 1].axis('off')
    
    plt.suptitle(title)
    plt.tight_layout()
    plt.show()
