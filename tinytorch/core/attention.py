# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                        ğŸš¨ CRITICAL WARNING ğŸš¨                                â•‘
# â•‘                     AUTOGENERATED! DO NOT EDIT!                              â•‘
# â•‘                                                                               â•‘
# â•‘  This file is AUTOMATICALLY GENERATED from source modules.                   â•‘
# â•‘  ANY CHANGES MADE HERE WILL BE LOST when modules are re-exported!            â•‘
# â•‘                                                                               â•‘
# â•‘  âœ… TO EDIT: modules/source/07_attention/attention_dev.py           â•‘
# â•‘  âœ… TO EXPORT: Run 'tito module complete <module_name>'                      â•‘
# â•‘                                                                               â•‘
# â•‘  ğŸ›¡ï¸ STUDENT PROTECTION: This file contains optimized implementations.        â•‘
# â•‘     Editing it directly may break module functionality and training.         â•‘
# â•‘                                                                               â•‘
# â•‘  ğŸ“ LEARNING TIP: Work in modules/source/ - that's where real development    â•‘
# â•‘     happens! The tinytorch/ directory is just the compiled output.           â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# %% auto 0
__all__ = ['scaled_dot_product_attention', 'MultiHeadAttention']

# %% ../../modules/source/12_attention/attention_dev.ipynb 0
#| default_exp core.attention
#| export

# %% ../../modules/source/12_attention/attention_dev.ipynb 2
import numpy as np
import math
import time
from typing import Optional, Tuple, List

# Import dependencies from previous modules - following TinyTorch dependency chain
from .tensor import Tensor
from .layers import Linear

# %% ../../modules/source/12_attention/attention_dev.ipynb 6
def scaled_dot_product_attention(Q: Tensor, K: Tensor, V: Tensor, mask: Optional[Tensor] = None) -> Tuple[Tensor, Tensor]:
    """
    Compute scaled dot-product attention.

    This is the fundamental attention operation that powers all transformer models.
    We'll implement it with explicit loops first to show the O(nÂ²) complexity.

    TODO: Implement scaled dot-product attention step by step

    APPROACH:
    1. Extract dimensions and validate inputs
    2. Compute attention scores with explicit nested loops (show O(nÂ²) complexity)
    3. Scale by 1/âˆšd_k for numerical stability
    4. Apply causal mask if provided (set masked positions to -inf)
    5. Apply softmax to get attention weights
    6. Apply values with attention weights (another O(nÂ²) operation)
    7. Return output and attention weights

    Args:
        Q: Query tensor of shape (batch_size, seq_len, d_model)
        K: Key tensor of shape (batch_size, seq_len, d_model)
        V: Value tensor of shape (batch_size, seq_len, d_model)
        mask: Optional causal mask, True=allow, False=mask (batch_size, seq_len, seq_len)

    Returns:
        output: Attended values (batch_size, seq_len, d_model)
        attention_weights: Attention matrix (batch_size, seq_len, seq_len)

    EXAMPLE:
    >>> Q = Tensor(np.random.randn(2, 4, 64))  # batch=2, seq=4, dim=64
    >>> K = Tensor(np.random.randn(2, 4, 64))
    >>> V = Tensor(np.random.randn(2, 4, 64))
    >>> output, weights = scaled_dot_product_attention(Q, K, V)
    >>> print(output.shape)  # (2, 4, 64)
    >>> print(weights.shape)  # (2, 4, 4)
    >>> print(weights.data[0].sum(axis=1))  # Each row sums to ~1.0

    HINTS:
    - Use explicit nested loops to compute Q[i] @ K[j] for educational purposes
    - Scale factor is 1/âˆšd_k where d_k is the last dimension of Q
    - Masked positions should be set to -1e9 before softmax
    - Remember that softmax normalizes along the last dimension
    """
    ### BEGIN SOLUTION
    # Step 1: Extract dimensions and validate
    batch_size, seq_len, d_model = Q.shape
    assert K.shape == (batch_size, seq_len, d_model), f"K shape {K.shape} doesn't match Q shape {Q.shape}"
    assert V.shape == (batch_size, seq_len, d_model), f"V shape {V.shape} doesn't match Q shape {Q.shape}"

    # Step 2: Compute attention scores Q @ K^T using batched Tensor operations (NO loops!)
    # Q: (batch, seq, d_model)
    # K: (batch, seq, d_model)
    # K.transpose() swaps last two dims: (batch, d_model, seq)
    # Q @ K.T: (batch, seq, d_model) @ (batch, d_model, seq) â†’ (batch, seq, seq)
    K_T = K.transpose()  # (batch, d_model, seq) - Preserves requires_grad!
    scores = Q.matmul(K_T)  # (batch, seq, seq) - Module 05's tracked_matmul sets _grad_fn!

    # Step 3: Scale by 1/âˆšd_k for numerical stability (Tensor operation!)
    scale_factor = 1.0 / math.sqrt(d_model)
    scores = scores * scale_factor  # Tensor multiplication - Module 05's tracked_mul!

    # Step 4: Apply causal mask if provided (Tensor operation!)
    if mask is not None:
        # mask: True where attention is allowed, False where masked
        # Convert to additive mask: 0 where allowed, -1e9 where masked
        # This way we can use Tensor addition which preserves gradients!
        if mask.data.ndim == 2:
            # Broadcast (seq, seq) mask to (batch, seq, seq)
            mask_additive = Tensor(np.where(mask.data, 0.0, -1e9))
        else:
            # Already (batch, seq, seq)
            mask_additive = Tensor(np.where(mask.data, 0.0, -1e9))
        scores = scores + mask_additive  # Tensor addition - Module 05's tracked_add!

    # Step 5: Apply softmax (NO loops - softmax handles batched input!)
    from tinytorch.core.activations import Softmax
    softmax = Softmax()
    
    # Apply softmax along last dimension (over keys for each query)
    # scores: (batch, seq, seq) â†’ attention_weights: (batch, seq, seq)
    attention_weights = softmax.forward(scores, dim=-1)  # Tensor operation!

    # Step 6: Apply attention weights to values (NO loops - batched matmul!)
    # attention_weights: (batch, seq, seq)
    # V: (batch, seq, d_model)
    # weights @ V: (batch, seq, seq) @ (batch, seq, d_model) â†’ (batch, seq, d_model)
    output = attention_weights.matmul(V)  # Tensor operation - Module 05's tracked_matmul!

    return output, attention_weights
    ### END SOLUTION

# %% ../../modules/source/12_attention/attention_dev.ipynb 10
class MultiHeadAttention:
    """
    Multi-head attention mechanism.

    Runs multiple attention heads in parallel, each learning different relationships.
    This is the core component of transformer architectures.
    """

    def __init__(self, embed_dim: int, num_heads: int):
        """
        Initialize multi-head attention.

        TODO: Set up linear projections and validate configuration

        APPROACH:
        1. Validate that embed_dim is divisible by num_heads
        2. Calculate head_dim (embed_dim // num_heads)
        3. Create linear layers for Q, K, V projections
        4. Create output projection layer
        5. Store configuration parameters

        Args:
            embed_dim: Embedding dimension (d_model)
            num_heads: Number of parallel attention heads

        EXAMPLE:
        >>> mha = MultiHeadAttention(embed_dim=512, num_heads=8)
        >>> mha.head_dim  # 64 (512 / 8)
        >>> len(mha.parameters())  # 4 linear layers * 2 params each = 8 tensors

        HINTS:
        - head_dim = embed_dim // num_heads must be integer
        - Need 4 Linear layers: q_proj, k_proj, v_proj, out_proj
        - Each projection maps embed_dim â†’ embed_dim
        """
        ### BEGIN SOLUTION
        assert embed_dim % num_heads == 0, f"embed_dim ({embed_dim}) must be divisible by num_heads ({num_heads})"

        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads

        # Linear projections for queries, keys, values
        self.q_proj = Linear(embed_dim, embed_dim)
        self.k_proj = Linear(embed_dim, embed_dim)
        self.v_proj = Linear(embed_dim, embed_dim)

        # Output projection to mix information across heads
        self.out_proj = Linear(embed_dim, embed_dim)
        ### END SOLUTION

    def forward(self, x: Tensor, mask: Optional[Tensor] = None) -> Tensor:
        """
        Forward pass through multi-head attention.

        TODO: Implement the complete multi-head attention forward pass

        APPROACH:
        1. Extract input dimensions (batch_size, seq_len, embed_dim)
        2. Project input to Q, K, V using linear layers
        3. Reshape projections to separate heads: (batch, seq, heads, head_dim)
        4. Transpose to (batch, heads, seq, head_dim) for parallel processing
        5. Apply scaled dot-product attention to each head
        6. Transpose back and reshape to merge heads
        7. Apply output projection

        Args:
            x: Input tensor (batch_size, seq_len, embed_dim)
            mask: Optional attention mask (batch_size, seq_len, seq_len)

        Returns:
            output: Attended representation (batch_size, seq_len, embed_dim)

        EXAMPLE:
        >>> mha = MultiHeadAttention(embed_dim=64, num_heads=8)
        >>> x = Tensor(np.random.randn(2, 10, 64))  # batch=2, seq=10, dim=64
        >>> output = mha.forward(x)
        >>> print(output.shape)  # (2, 10, 64) - same as input

        HINTS:
        - Reshape: (batch, seq, embed_dim) â†’ (batch, seq, heads, head_dim)
        - Transpose: (batch, seq, heads, head_dim) â†’ (batch, heads, seq, head_dim)
        - After attention: reverse the process to merge heads
        - Use scaled_dot_product_attention for each head
        """
        ### BEGIN SOLUTION
        # Step 1: Extract dimensions
        batch_size, seq_len, embed_dim = x.shape
        assert embed_dim == self.embed_dim, f"Input dim {embed_dim} doesn't match expected {self.embed_dim}"

        # Step 2: Project to Q, K, V (Tensor operations!)
        Q = self.q_proj.forward(x)  # (batch, seq, embed_dim)
        K = self.k_proj.forward(x)
        V = self.v_proj.forward(x)

        # Step 3: Reshape to separate heads (batch, seq, embed) â†’ (batch, seq, heads, head_dim)
        Q_heads = Q.reshape(batch_size, seq_len, self.num_heads, self.head_dim)
        K_heads = K.reshape(batch_size, seq_len, self.num_heads, self.head_dim)
        V_heads = V.reshape(batch_size, seq_len, self.num_heads, self.head_dim)

        # Step 4: Rearrange dims to (batch, heads, seq, head_dim) for parallel processing
        # We need to permute axes (0, 2, 1, 3) to move heads before sequence
        # This must preserve the computation graph for autograd!
        from tinytorch.core.autograd import PermuteBackward
        
        def permute_axes(tensor, axes):
            """Helper to permute axes while preserving gradient tracking."""
            result = Tensor(np.transpose(tensor.data, axes), requires_grad=tensor.requires_grad)
            if tensor.requires_grad:
                result._grad_fn = PermuteBackward(tensor, axes)
            return result
        
        Q_heads = permute_axes(Q_heads, (0, 2, 1, 3))
        K_heads = permute_axes(K_heads, (0, 2, 1, 3))
        V_heads = permute_axes(V_heads, (0, 2, 1, 3))
        
        # Step 5: Process ALL heads in parallel (NO loops!)
        # Reshape to combine batch and head dims: (batch, heads, seq, head_dim) â†’ (batch*heads, seq, head_dim)
        batch_heads = batch_size * self.num_heads
        Q_flat = Q_heads.reshape(batch_heads, seq_len, self.head_dim)
        K_flat = K_heads.reshape(batch_heads, seq_len, self.head_dim)
        V_flat = V_heads.reshape(batch_heads, seq_len, self.head_dim)
        
        # Handle mask: Repeat for each head
        # mask: (batch, seq, seq) needs to become (batch*heads, seq, seq)
        if mask is not None:
            if mask.data.ndim == 2:
                # (seq, seq) â†’ repeat for each batch and head
                mask_data = np.tile(mask.data[np.newaxis, :, :], (batch_heads, 1, 1))
            else:
                # (batch, seq, seq) â†’ repeat for each head
                # For each batch element, repeat the mask num_heads times
                mask_data = np.repeat(mask.data, self.num_heads, axis=0)
            mask_flat = Tensor(mask_data)
        else:
            mask_flat = None
        
        # Apply attention to all heads at once! (Tensor operation)
        # This batches all heads together - efficient and preserves gradients!
        attn_output, _ = scaled_dot_product_attention(Q_flat, K_flat, V_flat, mask_flat)
        
        # Step 6: Reshape back to separate batch and heads: (batch*heads, seq, head_dim) â†’ (batch, heads, seq, head_dim)
        attn_output = attn_output.reshape(batch_size, self.num_heads, seq_len, self.head_dim)
        
        # Step 7: Transpose back: (batch, heads, seq, head_dim) â†’ (batch, seq, heads, head_dim)
        attn_output = permute_axes(attn_output, (0, 2, 1, 3))
        
        # Step 8: Merge heads: (batch, seq, heads, head_dim) â†’ (batch, seq, embed_dim)
        output = attn_output.reshape(batch_size, seq_len, self.embed_dim)

        # Step 9: Apply output projection (Tensor operation!)
        output = self.out_proj.forward(output)

        return output
        ### END SOLUTION

    def __call__(self, x: Tensor, mask: Optional[Tensor] = None) -> Tensor:
        """Allows the attention layer to be called like a function."""
        return self.forward(x, mask)

    def parameters(self) -> List[Tensor]:
        """
        Return all trainable parameters.

        TODO: Collect parameters from all linear layers

        APPROACH:
        1. Get parameters from q_proj, k_proj, v_proj, out_proj
        2. Combine into single list

        Returns:
            List of all parameter tensors
        """
        ### BEGIN SOLUTION
        params = []
        params.extend(self.q_proj.parameters())
        params.extend(self.k_proj.parameters())
        params.extend(self.v_proj.parameters())
        params.extend(self.out_proj.parameters())
        return params
        ### END SOLUTION
