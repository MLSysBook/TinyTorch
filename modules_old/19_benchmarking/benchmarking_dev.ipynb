{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ead5731b",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Module 20: TinyMLPerf - The Ultimate ML Systems Competition\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this module, you will be able to:\n",
    "\n",
    "1. **Build Competition Benchmarking Infrastructure**: Create standardized TinyMLPerf benchmark suite for fair competition\n",
    "2. **Use Profiling Tools for Systematic Measurement**: Apply Module 15's profiler to measure real performance gains\n",
    "3. **Compete Across Multiple Categories**: Optimize for speed, memory, model size, and innovation simultaneously\n",
    "4. **Calculate Relative Performance Improvements**: Show speedup ratios independent of hardware differences\n",
    "5. **Drive Innovation Through Competition**: Use competitive pressure to discover new optimization techniques\n",
    "\n",
    "## The TinyMLPerf Vision\n",
    "\n",
    "**Key Message**: Competition proves optimization mastery by measuring concrete performance improvements across all your TinyTorch implementations!\n",
    "\n",
    "**The TinyMLPerf Journey:**\n",
    "1. **Benchmark Suite**: Load standard models (MLP, CNN, Transformer) as competition workloads\n",
    "2. **Profiling Integration**: Use your Module 15 profiler for rigorous performance measurement\n",
    "3. **Competition Categories**: Three exciting events - MLP Sprint, CNN Marathon, Transformer Decathlon\n",
    "4. **Relative Scoring**: Hardware-independent speedup measurements (3x faster = 3.0 score)\n",
    "5. **Leaderboard Glory**: Track innovations and celebrate optimization achievements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36cf4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils.benchmark\n",
    "\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "import tracemalloc\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional, Tuple, Union, Callable\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Import TinyTorch profiler from Module 15\n",
    "try:\n",
    "    from tinytorch.utils.profiler import SimpleProfiler, profile_function\n",
    "    HAS_PROFILER = True\n",
    "except ImportError:\n",
    "    print(\"Warning: TinyTorch profiler not available. Using basic timing.\")\n",
    "    HAS_PROFILER = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242db3f2",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Part 1: TinyMLPerf Benchmark Suite - Standard Competition Models\n",
    "\n",
    "Let's build the TinyMLPerf benchmark suite with three exciting competition events using standard models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454686b7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TinyMLPerf:\n",
    "    \"\"\"\n",
    "    TinyMLPerf benchmark suite - The Olympics of ML Systems Optimization!\n",
    "    \n",
    "    Provides three standard competition events:\n",
    "    - MLP Sprint: Fastest feedforward inference\n",
    "    - CNN Marathon: Efficient convolution operations  \n",
    "    - Transformer Decathlon: Complete attention-based model performance\n",
    "    \n",
    "    Each event uses standardized models and datasets for fair competition.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, profiler_warmup_runs: int = 3, profiler_timing_runs: int = 10):\n",
    "        \"\"\"\n",
    "        Initialize TinyMLPerf benchmark suite.\n",
    "        \n",
    "        Args:\n",
    "            profiler_warmup_runs: Number of warmup runs for stable measurements\n",
    "            profiler_timing_runs: Number of timing runs for statistical reliability\n",
    "        \"\"\"\n",
    "        self.warmup_runs = profiler_warmup_runs\n",
    "        self.timing_runs = profiler_timing_runs\n",
    "        self.benchmark_models = {}\n",
    "        self.benchmark_datasets = {}\n",
    "        \n",
    "        print(\"üèÜ TinyMLPerf Competition Suite Initialized!\")\n",
    "        print(\"üéØ Three Events: MLP Sprint, CNN Marathon, Transformer Decathlon\")\n",
    "        \n",
    "        # Load standard benchmark models\n",
    "        self._load_benchmark_models()\n",
    "        self._load_benchmark_datasets()\n",
    "    \n",
    "    def _load_benchmark_models(self):\n",
    "        \"\"\"Load standard benchmark models for each competition event\"\"\"\n",
    "        print(\"üì• Loading TinyMLPerf Benchmark Models...\")\n",
    "        \n",
    "        # MLP Sprint - Simple feedforward model\n",
    "        class MLPBenchmark:\n",
    "            def __init__(self):\n",
    "                self.weights1 = np.random.randn(784, 128).astype(np.float32) * 0.1\n",
    "                self.bias1 = np.random.randn(128).astype(np.float32) * 0.1\n",
    "                self.weights2 = np.random.randn(128, 64).astype(np.float32) * 0.1\n",
    "                self.bias2 = np.random.randn(64).astype(np.float32) * 0.1  \n",
    "                self.weights3 = np.random.randn(64, 10).astype(np.float32) * 0.1\n",
    "                self.bias3 = np.random.randn(10).astype(np.float32) * 0.1\n",
    "            \n",
    "            def forward(self, x):\n",
    "                # 3-layer MLP with ReLU activations\n",
    "                h1 = np.maximum(0, x @ self.weights1 + self.bias1)  # ReLU\n",
    "                h2 = np.maximum(0, h1 @ self.weights2 + self.bias2)  # ReLU  \n",
    "                return h2 @ self.weights3 + self.bias3  # Output layer\n",
    "            \n",
    "            def predict(self, x):\n",
    "                return self.forward(x)\n",
    "        \n",
    "        # CNN Marathon - Convolutional model\n",
    "        class CNNBenchmark:\n",
    "            def __init__(self):\n",
    "                # Simplified CNN weights (real CNN would need proper conv operations)\n",
    "                self.conv1_weights = np.random.randn(3, 3, 1, 32).astype(np.float32) * 0.1\n",
    "                self.conv2_weights = np.random.randn(3, 3, 32, 64).astype(np.float32) * 0.1\n",
    "                self.fc_weights = np.random.randn(1600, 10).astype(np.float32) * 0.1  # Flattened size\n",
    "                self.fc_bias = np.random.randn(10).astype(np.float32) * 0.1\n",
    "            \n",
    "            def forward(self, x):\n",
    "                # Simplified CNN (students will optimize real convolutions)\n",
    "                batch_size = x.shape[0] \n",
    "                # Simulate conv + pooling by flattening and projecting\n",
    "                x_flat = x.reshape(batch_size, -1)  # Flatten input\n",
    "                if x_flat.shape[1] != 1600:\n",
    "                    # Adjust to expected size\n",
    "                    x_flat = x_flat[:, :1600] if x_flat.shape[1] > 1600 else np.pad(x_flat, ((0, 0), (0, 1600 - x_flat.shape[1])), 'constant')\n",
    "                return x_flat @ self.fc_weights + self.fc_bias\n",
    "            \n",
    "            def predict(self, x):\n",
    "                return self.forward(x)\n",
    "        \n",
    "        # Transformer Decathlon - Attention-based model  \n",
    "        class TransformerBenchmark:\n",
    "            def __init__(self, d_model=128, n_heads=8, seq_len=64):\n",
    "                self.d_model = d_model\n",
    "                self.n_heads = n_heads\n",
    "                self.seq_len = seq_len\n",
    "                self.head_dim = d_model // n_heads\n",
    "                \n",
    "                # Multi-head attention weights\n",
    "                self.wq = np.random.randn(d_model, d_model).astype(np.float32) * 0.1\n",
    "                self.wk = np.random.randn(d_model, d_model).astype(np.float32) * 0.1  \n",
    "                self.wv = np.random.randn(d_model, d_model).astype(np.float32) * 0.1\n",
    "                self.wo = np.random.randn(d_model, d_model).astype(np.float32) * 0.1\n",
    "                \n",
    "                # Feed forward weights\n",
    "                self.ff1 = np.random.randn(d_model, d_model * 4).astype(np.float32) * 0.1\n",
    "                self.ff2 = np.random.randn(d_model * 4, d_model).astype(np.float32) * 0.1\n",
    "            \n",
    "            def forward(self, x):\n",
    "                # Simplified transformer block (students will optimize real attention)\n",
    "                batch_size, seq_len, d_model = x.shape\n",
    "                \n",
    "                # Self-attention (simplified)\n",
    "                q = x @ self.wq  # [batch, seq, d_model]\n",
    "                k = x @ self.wk\n",
    "                v = x @ self.wv\n",
    "                \n",
    "                # Simplified attention computation (real would be multi-head)\n",
    "                scores = q @ k.transpose(0, 2, 1) / np.sqrt(d_model)  # [batch, seq, seq]\n",
    "                attn = np.exp(scores) / (np.sum(np.exp(scores), axis=-1, keepdims=True) + 1e-8)\n",
    "                out = attn @ v  # [batch, seq, d_model]\n",
    "                \n",
    "                # Skip connection + layer norm (simplified)\n",
    "                out = out + x  # Residual connection\n",
    "                \n",
    "                # Feed forward network\n",
    "                ff_out = np.maximum(0, out @ self.ff1)  # ReLU\n",
    "                ff_out = ff_out @ self.ff2\n",
    "                \n",
    "                # Another skip connection\n",
    "                out = ff_out + out\n",
    "                \n",
    "                # Global average pooling for classification\n",
    "                return np.mean(out, axis=1)  # [batch, d_model]\n",
    "            \n",
    "            def predict(self, x):\n",
    "                return self.forward(x)\n",
    "        \n",
    "        # Store benchmark models\n",
    "        self.benchmark_models = {\n",
    "            'mlp_sprint': MLPBenchmark(),\n",
    "            'cnn_marathon': CNNBenchmark(), \n",
    "            'transformer_decathlon': TransformerBenchmark()\n",
    "        }\n",
    "        \n",
    "        print(\"‚úÖ Benchmark models loaded successfully!\")\n",
    "        for event, model in self.benchmark_models.items():\n",
    "            print(f\"   üìã {event.title()}: {type(model).__name__}\")\n",
    "    \n",
    "    def _load_benchmark_datasets(self):\n",
    "        \"\"\"Load standard benchmark datasets for each competition event\"\"\"\n",
    "        print(\"üìä Loading TinyMLPerf Benchmark Datasets...\")\n",
    "        \n",
    "        # MLP Sprint dataset - MNIST-like flattened images\n",
    "        mlp_data = {\n",
    "            'inputs': np.random.randn(100, 784).astype(np.float32),  # Batch of 100 samples\n",
    "            'targets': np.eye(10)[np.random.randint(0, 10, 100)],    # One-hot labels\n",
    "            'event': 'MLP Sprint',\n",
    "            'description': 'Feedforward inference on flattened 28x28 images'\n",
    "        }\n",
    "        \n",
    "        # CNN Marathon dataset - Image-like data\n",
    "        cnn_data = {\n",
    "            'inputs': np.random.randn(50, 28, 28, 1).astype(np.float32),  # Batch of 50 images\n",
    "            'targets': np.eye(10)[np.random.randint(0, 10, 50)],\n",
    "            'event': 'CNN Marathon',  \n",
    "            'description': 'Convolutional inference on 28x28x1 images'\n",
    "        }\n",
    "        \n",
    "        # Transformer Decathlon dataset - Sequence data\n",
    "        transformer_data = {\n",
    "            'inputs': np.random.randn(32, 64, 128).astype(np.float32),  # Batch of 32 sequences\n",
    "            'targets': np.eye(10)[np.random.randint(0, 10, 32)],\n",
    "            'event': 'Transformer Decathlon',\n",
    "            'description': 'Self-attention inference on 64-token sequences'\n",
    "        }\n",
    "        \n",
    "        self.benchmark_datasets = {\n",
    "            'mlp_sprint': mlp_data,\n",
    "            'cnn_marathon': cnn_data,\n",
    "            'transformer_decathlon': transformer_data\n",
    "        }\n",
    "        \n",
    "        print(\"‚úÖ Benchmark datasets loaded successfully!\")\n",
    "        for event, data in self.benchmark_datasets.items():\n",
    "            print(f\"   üéØ {data['event']}: {data['inputs'].shape} -> {data['targets'].shape}\")\n",
    "    \n",
    "    def load_benchmark(self, event_name: str) -> Tuple[Any, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Load a specific benchmark model and dataset.\n",
    "        \n",
    "        Args:\n",
    "            event_name: Name of competition event ('mlp_sprint', 'cnn_marathon', 'transformer_decathlon')\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (model, dataset) for the specified event\n",
    "        \"\"\"\n",
    "        if event_name not in self.benchmark_models:\n",
    "            available = list(self.benchmark_models.keys())\n",
    "            raise ValueError(f\"Event '{event_name}' not found. Available: {available}\")\n",
    "        \n",
    "        model = self.benchmark_models[event_name]\n",
    "        dataset = self.benchmark_datasets[event_name]\n",
    "        \n",
    "        print(f\"üìã Loaded benchmark: {dataset['event']}\")\n",
    "        print(f\"   Model: {type(model).__name__}\")\n",
    "        print(f\"   Data: {dataset['description']}\")\n",
    "        \n",
    "        return model, dataset\n",
    "    \n",
    "    def get_available_events(self) -> Dict[str, str]:\n",
    "        \"\"\"Get list of available competition events with descriptions\"\"\"\n",
    "        return {\n",
    "            'mlp_sprint': 'Fastest feedforward neural network inference',\n",
    "            'cnn_marathon': 'Efficient convolutional neural network processing',\n",
    "            'transformer_decathlon': 'Complete attention mechanism optimization'\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3676ceeb",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### Test TinyMLPerf Benchmark Suite\n",
    "\n",
    "Let's test the benchmark suite to ensure all models and datasets load correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919f5680",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_tinymlperf_benchmark_suite():\n",
    "    \"\"\"Test the TinyMLPerf benchmark suite\"\"\"\n",
    "    print(\"Testing TinyMLPerf Benchmark Suite...\")\n",
    "    \n",
    "    # Initialize benchmark suite\n",
    "    tinyperf = TinyMLPerf(profiler_warmup_runs=2, profiler_timing_runs=3)\n",
    "    \n",
    "    # Test each event\n",
    "    events = tinyperf.get_available_events()\n",
    "    print(f\"\\nüèÜ Available Events: {len(events)}\")\n",
    "    \n",
    "    for event_name, description in events.items():\n",
    "        print(f\"\\nüìã Testing {event_name}...\")\n",
    "        model, dataset = tinyperf.load_benchmark(event_name)\n",
    "        \n",
    "        # Test model inference\n",
    "        inputs = dataset['inputs']\n",
    "        outputs = model.predict(inputs)\n",
    "        \n",
    "        print(f\"   ‚úÖ Inference successful: {inputs.shape} -> {outputs.shape}\")\n",
    "        \n",
    "        # Verify output shape makes sense\n",
    "        batch_size = inputs.shape[0]\n",
    "        assert outputs.shape[0] == batch_size, f\"Batch size mismatch: {outputs.shape[0]} != {batch_size}\"\n",
    "        print(f\"   ‚úÖ Output shape verified\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ TinyMLPerf benchmark suite test complete!\")\n",
    "    return tinyperf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b18f42",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Part 2: Performance Benchmarking Using Module 15's Profiler\n",
    "\n",
    "Now let's build the core benchmarking infrastructure that uses the profiler from Module 15 to measure performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89d870e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class CompetitionProfiler:\n",
    "    \"\"\"\n",
    "    Competition profiling infrastructure using TinyTorch's Module 15 profiler.\n",
    "    \n",
    "    Provides rigorous performance measurement for fair competition by:\n",
    "    - Using standardized profiling from Module 15\n",
    "    - Multiple timing runs with statistical analysis\n",
    "    - Memory usage tracking and analysis\n",
    "    - Hardware-independent relative scoring\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, warmup_runs: int = 3, timing_runs: int = 10):\n",
    "        \"\"\"\n",
    "        Initialize competition profiler.\n",
    "        \n",
    "        Args:\n",
    "            warmup_runs: Number of warmup runs to stabilize performance\n",
    "            timing_runs: Number of timing runs for statistical reliability  \n",
    "        \"\"\"\n",
    "        self.warmup_runs = warmup_runs\n",
    "        self.timing_runs = timing_runs\n",
    "        self.has_profiler = HAS_PROFILER\n",
    "        \n",
    "        if not self.has_profiler:\n",
    "            print(\"‚ö†Ô∏è  Warning: Advanced profiling unavailable, using basic timing\")\n",
    "        else:\n",
    "            print(\"‚úÖ Using TinyTorch Module 15 profiler for advanced metrics\")\n",
    "    \n",
    "    def benchmark_model(self, model, dataset: Dict[str, Any], \n",
    "                       baseline_model=None, baseline_time: Optional[float] = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Benchmark a model using rigorous profiling methodology.\n",
    "        \n",
    "        Args:\n",
    "            model: Model to benchmark (must have predict() or forward() method)\n",
    "            dataset: Dataset dictionary with 'inputs' key\n",
    "            baseline_model: Optional baseline model for speedup calculation\n",
    "            baseline_time: Optional baseline time for speedup calculation\n",
    "            \n",
    "        Returns:\n",
    "            Comprehensive benchmarking results with performance metrics\n",
    "        \"\"\"\n",
    "        print(f\"üèÅ Benchmarking {dataset.get('event', 'Model')}...\")\n",
    "        \n",
    "        inputs = dataset['inputs']\n",
    "        results = {\n",
    "            'event': dataset.get('event', 'Unknown'),\n",
    "            'model_type': type(model).__name__,\n",
    "            'input_shape': inputs.shape,\n",
    "            'benchmark_timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        if self.has_profiler:\n",
    "            # Use advanced profiling from Module 15\n",
    "            results.update(self._profile_with_tinytorch_profiler(model, inputs))\n",
    "        else:\n",
    "            # Fallback to basic timing\n",
    "            results.update(self._profile_basic_timing(model, inputs))\n",
    "        \n",
    "        # Calculate speedup if baseline provided\n",
    "        if baseline_model is not None:\n",
    "            baseline_results = self.benchmark_model(baseline_model, dataset)\n",
    "            speedup = baseline_results['mean_inference_time'] / results['mean_inference_time']\n",
    "            results['speedup_vs_baseline'] = speedup\n",
    "        elif baseline_time is not None:\n",
    "            speedup = baseline_time / results['mean_inference_time'] \n",
    "            results['speedup_vs_baseline'] = speedup\n",
    "        \n",
    "        self._print_benchmark_results(results)\n",
    "        return results\n",
    "    \n",
    "    def _profile_with_tinytorch_profiler(self, model, inputs: np.ndarray) -> Dict[str, Any]:\n",
    "        \"\"\"Profile using Module 15's advanced profiler\"\"\"\n",
    "        profiler = SimpleProfiler(track_memory=True, track_cpu=True)\n",
    "        \n",
    "        # Run multiple profiling sessions for statistical reliability\n",
    "        profile_results = []\n",
    "        \n",
    "        for run in range(self.timing_runs):\n",
    "            # Each profiling session includes warmup\n",
    "            result = profiler.profile(\n",
    "                model.predict, inputs, \n",
    "                name=f\"inference_run_{run}\",\n",
    "                warmup=True  # Profiler handles warmup\n",
    "            )\n",
    "            profile_results.append(result)\n",
    "        \n",
    "        # Aggregate statistics across runs\n",
    "        wall_times = [r['wall_time'] for r in profile_results]\n",
    "        cpu_times = [r['cpu_time'] for r in profile_results]\n",
    "        \n",
    "        aggregated = {\n",
    "            'mean_inference_time': np.mean(wall_times),\n",
    "            'std_inference_time': np.std(wall_times),\n",
    "            'min_inference_time': np.min(wall_times), \n",
    "            'max_inference_time': np.max(wall_times),\n",
    "            'p95_inference_time': np.percentile(wall_times, 95),\n",
    "            'mean_cpu_time': np.mean(cpu_times),\n",
    "            'cpu_efficiency': np.mean([r['cpu_efficiency'] for r in profile_results]),\n",
    "            'profiling_method': 'TinyTorch Module 15 Profiler'\n",
    "        }\n",
    "        \n",
    "        # Add memory metrics from last run (most representative)\n",
    "        last_result = profile_results[-1]\n",
    "        if 'memory_delta_mb' in last_result:\n",
    "            aggregated.update({\n",
    "                'memory_delta_mb': last_result['memory_delta_mb'],\n",
    "                'peak_memory_mb': last_result['peak_memory_mb'],\n",
    "                'result_size_mb': last_result.get('result_size_mb', 0)\n",
    "            })\n",
    "        \n",
    "        return aggregated\n",
    "    \n",
    "    def _profile_basic_timing(self, model, inputs: np.ndarray) -> Dict[str, Any]:\n",
    "        \"\"\"Fallback basic timing without advanced profiling\"\"\"\n",
    "        \n",
    "        # Warmup runs\n",
    "        for _ in range(self.warmup_runs):\n",
    "            _ = model.predict(inputs)\n",
    "        \n",
    "        # Timing runs  \n",
    "        times = []\n",
    "        for _ in range(self.timing_runs):\n",
    "            start = time.perf_counter()\n",
    "            _ = model.predict(inputs)\n",
    "            end = time.perf_counter()\n",
    "            times.append(end - start)\n",
    "        \n",
    "        return {\n",
    "            'mean_inference_time': np.mean(times),\n",
    "            'std_inference_time': np.std(times),\n",
    "            'min_inference_time': np.min(times),\n",
    "            'max_inference_time': np.max(times),\n",
    "            'p95_inference_time': np.percentile(times, 95),\n",
    "            'profiling_method': 'Basic Timing'\n",
    "        }\n",
    "    \n",
    "    def _print_benchmark_results(self, results: Dict[str, Any]):\n",
    "        \"\"\"Print formatted benchmark results\"\"\"\n",
    "        print(f\"\\nüìä {results['event']} Benchmark Results:\")\n",
    "        print(f\"   Model: {results['model_type']}\")\n",
    "        print(f\"   Input: {results['input_shape']}\")\n",
    "        print(f\"   Mean Time: {results['mean_inference_time']*1000:.2f} ¬± {results['std_inference_time']*1000:.2f} ms\")\n",
    "        print(f\"   Best Time: {results['min_inference_time']*1000:.2f} ms\")\n",
    "        print(f\"   P95 Time: {results['p95_inference_time']*1000:.2f} ms\")\n",
    "        \n",
    "        if 'speedup_vs_baseline' in results:\n",
    "            print(f\"   üöÄ Speedup: {results['speedup_vs_baseline']:.2f}x faster\")\n",
    "        \n",
    "        if 'memory_delta_mb' in results:\n",
    "            print(f\"   üíæ Memory: {results['memory_delta_mb']:.2f} MB delta, {results['peak_memory_mb']:.2f} MB peak\")\n",
    "        \n",
    "        print(f\"   üìè Method: {results['profiling_method']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea6de0e",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### Test Competition Profiler\n",
    "\n",
    "Let's test the competition profiler with TinyMLPerf benchmark models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4291ee9d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_competition_profiler():\n",
    "    \"\"\"Test the competition profiler with benchmark models\"\"\"\n",
    "    print(\"Testing Competition Profiler...\")\n",
    "    \n",
    "    # Initialize TinyMLPerf and profiler\n",
    "    tinyperf = TinyMLPerf(profiler_warmup_runs=2, profiler_timing_runs=3)\n",
    "    profiler = CompetitionProfiler(warmup_runs=2, timing_runs=3)\n",
    "    \n",
    "    # Test MLP Sprint profiling\n",
    "    mlp_model, mlp_dataset = tinyperf.load_benchmark('mlp_sprint')\n",
    "    mlp_results = profiler.benchmark_model(mlp_model, mlp_dataset)\n",
    "    \n",
    "    # Test CNN Marathon profiling\n",
    "    cnn_model, cnn_dataset = tinyperf.load_benchmark('cnn_marathon')  \n",
    "    cnn_results = profiler.benchmark_model(cnn_model, cnn_dataset)\n",
    "    \n",
    "    # Test speedup calculation with baseline\n",
    "    print(f\"\\nüèÉ Testing Speedup Calculation...\")\n",
    "    cnn_speedup_results = profiler.benchmark_model(\n",
    "        cnn_model, cnn_dataset, \n",
    "        baseline_time=mlp_results['mean_inference_time']  # Use MLP as baseline\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Competition profiler test complete!\")\n",
    "    return profiler, mlp_results, cnn_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982f40f9",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Part 3: Competition Framework - Leaderboards and Scoring\n",
    "\n",
    "Now let's build the exciting competition framework with leaderboards, relative scoring, and multiple categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016b4cc6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class TinyMLPerfCompetition:\n",
    "    \"\"\"\n",
    "    TinyMLPerf Competition Framework - The Olympics of ML Optimization!\n",
    "    \n",
    "    Manages three exciting competition events:\n",
    "    - MLP Sprint: Fastest feedforward network\n",
    "    - CNN Marathon: Most efficient convolutions  \n",
    "    - Transformer Decathlon: Ultimate attention optimization\n",
    "    \n",
    "    Features hardware-independent relative scoring and transparent leaderboards.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, results_dir: str = \"tinymlperf_results\"):\n",
    "        \"\"\"\n",
    "        Initialize TinyMLPerf competition.\n",
    "        \n",
    "        Args:\n",
    "            results_dir: Directory to store competition results and leaderboards\n",
    "        \"\"\"\n",
    "        self.results_dir = Path(results_dir)\n",
    "        self.results_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        self.tinyperf = TinyMLPerf()\n",
    "        self.profiler = CompetitionProfiler(warmup_runs=3, timing_runs=5)\n",
    "        \n",
    "        # Load baseline models for relative scoring\n",
    "        self.baselines = self._establish_baselines()\n",
    "        \n",
    "        print(\"üèÜ TinyMLPerf Competition Initialized!\")\n",
    "        print(\"üéØ Three Events Ready for Competition!\")\n",
    "    \n",
    "    def _establish_baselines(self) -> Dict[str, float]:\n",
    "        \"\"\"Establish baseline performance for relative scoring\"\"\"\n",
    "        print(\"üìè Establishing baseline performance for relative scoring...\")\n",
    "        \n",
    "        baselines = {}\n",
    "        events = ['mlp_sprint', 'cnn_marathon', 'transformer_decathlon']\n",
    "        \n",
    "        for event in events:\n",
    "            model, dataset = self.tinyperf.load_benchmark(event)\n",
    "            results = self.profiler.benchmark_model(model, dataset)\n",
    "            baselines[event] = results['mean_inference_time']\n",
    "            print(f\"   {event}: {baselines[event]*1000:.2f} ms baseline\")\n",
    "        \n",
    "        return baselines\n",
    "    \n",
    "    def submit_entry(self, team_name: str, event_name: str, optimized_model, \n",
    "                     optimization_description: str = \"\", github_url: str = \"\") -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Submit an optimized model to TinyMLPerf competition.\n",
    "        \n",
    "        Args:\n",
    "            team_name: Name of the competing team\n",
    "            event_name: Competition event ('mlp_sprint', 'cnn_marathon', 'transformer_decathlon')\n",
    "            optimized_model: The optimized model to submit\n",
    "            optimization_description: Description of optimization techniques used\n",
    "            github_url: Link to code repository (for transparency)\n",
    "            \n",
    "        Returns:\n",
    "            Submission results with performance metrics and scoring\n",
    "        \"\"\"\n",
    "        if event_name not in self.baselines:\n",
    "            available = list(self.baselines.keys())\n",
    "            raise ValueError(f\"Event '{event_name}' not available. Choose from: {available}\")\n",
    "        \n",
    "        print(f\"üöÄ TINYMLPERF SUBMISSION\")\n",
    "        print(f\"üèÜ Event: {event_name.replace('_', ' ').title()}\")\n",
    "        print(f\"üë• Team: {team_name}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Load benchmark dataset for this event\n",
    "        _, dataset = self.tinyperf.load_benchmark(event_name)\n",
    "        \n",
    "        # Benchmark the submitted model\n",
    "        results = self.profiler.benchmark_model(\n",
    "            optimized_model, dataset,\n",
    "            baseline_time=self.baselines[event_name]\n",
    "        )\n",
    "        \n",
    "        # Calculate competition score (relative speedup)\n",
    "        baseline_time = self.baselines[event_name]\n",
    "        submission_time = results['mean_inference_time']\n",
    "        speedup_score = baseline_time / submission_time\n",
    "        \n",
    "        # Create submission record\n",
    "        submission = {\n",
    "            'submission_id': self._generate_submission_id(team_name, event_name),\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'team_name': team_name,\n",
    "            'event_name': event_name,\n",
    "            'optimization_description': optimization_description,\n",
    "            'github_url': github_url,\n",
    "            'performance_metrics': results,\n",
    "            'speedup_score': speedup_score,\n",
    "            'baseline_time_ms': baseline_time * 1000,\n",
    "            'submission_time_ms': submission_time * 1000\n",
    "        }\n",
    "        \n",
    "        # Save submission\n",
    "        self._save_submission(submission)\n",
    "        \n",
    "        # Display results\n",
    "        self._display_submission_results(submission)\n",
    "        \n",
    "        return submission\n",
    "    \n",
    "    def _generate_submission_id(self, team_name: str, event_name: str) -> str:\n",
    "        \"\"\"Generate unique submission ID\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        team_hash = hashlib.md5(team_name.encode()).hexdigest()[:6]\n",
    "        return f\"{event_name}_{team_hash}_{timestamp}\"\n",
    "    \n",
    "    def _save_submission(self, submission: Dict[str, Any]):\n",
    "        \"\"\"Save submission to results directory\"\"\"\n",
    "        filename = f\"{submission['submission_id']}.json\"\n",
    "        filepath = self.results_dir / filename\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(submission, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"üíæ Submission saved: {filepath}\")\n",
    "    \n",
    "    def _display_submission_results(self, submission: Dict[str, Any]):\n",
    "        \"\"\"Display formatted submission results\"\"\"\n",
    "        metrics = submission['performance_metrics']\n",
    "        speedup = submission['speedup_score']\n",
    "        \n",
    "        print(f\"\\nüèÜ SUBMISSION RESULTS\")\n",
    "        print(f\"=\" * 50)\n",
    "        print(f\"Team: {submission['team_name']}\")\n",
    "        print(f\"Event: {submission['event_name'].replace('_', ' ').title()}\")\n",
    "        \n",
    "        print(f\"\\n‚è±Ô∏è  Performance:\")\n",
    "        print(f\"   Your Time:    {submission['submission_time_ms']:.2f} ms\")\n",
    "        print(f\"   Baseline:     {submission['baseline_time_ms']:.2f} ms\")\n",
    "        print(f\"   üöÄ Speedup:   {speedup:.2f}x {'FASTER' if speedup > 1.0 else 'slower'}\")\n",
    "        \n",
    "        if 'memory_delta_mb' in metrics:\n",
    "            print(f\"   üíæ Memory:    {metrics['memory_delta_mb']:.2f} MB\")\n",
    "        \n",
    "        # Award celebration for good performance\n",
    "        if speedup >= 3.0:\n",
    "            print(f\"\\nüéâ AMAZING! 3x+ speedup achieved!\")\n",
    "        elif speedup >= 2.0:\n",
    "            print(f\"\\nüèÜ EXCELLENT! 2x+ speedup!\")\n",
    "        elif speedup >= 1.5:\n",
    "            print(f\"\\n‚≠ê GREAT! 50%+ speedup!\")\n",
    "        elif speedup >= 1.1:\n",
    "            print(f\"\\n‚úÖ Good optimization!\")\n",
    "        else:\n",
    "            print(f\"\\nü§î Keep optimizing - you can do better!\")\n",
    "        \n",
    "        if submission['optimization_description']:\n",
    "            print(f\"\\nüí° Techniques Used:\")\n",
    "            print(f\"   {submission['optimization_description']}\")\n",
    "    \n",
    "    def display_leaderboard(self, event_name: str, top_n: int = 10) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Display leaderboard for a specific event.\n",
    "        \n",
    "        Args:\n",
    "            event_name: Event to show leaderboard for\n",
    "            top_n: Number of top entries to display\n",
    "            \n",
    "        Returns:\n",
    "            List of top submissions\n",
    "        \"\"\"\n",
    "        submissions = self._load_event_submissions(event_name)\n",
    "        \n",
    "        if not submissions:\n",
    "            print(f\"üèÜ {event_name.replace('_', ' ').title()} Leaderboard\")\n",
    "            print(\"No submissions yet! Be the first to compete!\")\n",
    "            return []\n",
    "        \n",
    "        # Sort by speedup score (highest first)\n",
    "        submissions.sort(key=lambda s: s['speedup_score'], reverse=True)\n",
    "        top_submissions = submissions[:top_n]\n",
    "        \n",
    "        print(f\"\\nüèÜ TINYMLPERF LEADERBOARD - {event_name.replace('_', ' ').title()}\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"{'Rank':<6} {'Team':<20} {'Speedup':<10} {'Time (ms)':<12} {'Techniques':<25}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for i, submission in enumerate(top_submissions):\n",
    "            rank = i + 1\n",
    "            team = submission['team_name'][:19]\n",
    "            speedup = f\"{submission['speedup_score']:.2f}x\"\n",
    "            time_ms = f\"{submission['submission_time_ms']:.2f}\"\n",
    "            techniques = submission['optimization_description'][:24] + \"...\" if len(submission['optimization_description']) > 24 else submission['optimization_description']\n",
    "            \n",
    "            print(f\"{rank:<6} {team:<20} {speedup:<10} {time_ms:<12} {techniques:<25}\")\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "        print(f\"Showing top {len(top_submissions)} of {len(submissions)} submissions\")\n",
    "        \n",
    "        return top_submissions\n",
    "    \n",
    "    def display_all_leaderboards(self):\n",
    "        \"\"\"Display leaderboards for all events\"\"\"\n",
    "        events = ['mlp_sprint', 'cnn_marathon', 'transformer_decathlon']\n",
    "        \n",
    "        for event in events:\n",
    "            self.display_leaderboard(event, top_n=5)\n",
    "            print()\n",
    "    \n",
    "    def _load_event_submissions(self, event_name: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Load all submissions for a specific event\"\"\"\n",
    "        submissions = []\n",
    "        \n",
    "        for filepath in self.results_dir.glob(f\"{event_name}_*.json\"):\n",
    "            try:\n",
    "                with open(filepath, 'r') as f:\n",
    "                    submission = json.load(f)\n",
    "                    submissions.append(submission)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load {filepath}: {e}\")\n",
    "        \n",
    "        return submissions\n",
    "    \n",
    "    def get_team_progress(self, team_name: str) -> Dict[str, List[Dict[str, Any]]]:\n",
    "        \"\"\"Get all submissions from a specific team across all events\"\"\"\n",
    "        all_files = list(self.results_dir.glob(\"*.json\"))\n",
    "        team_submissions = {'mlp_sprint': [], 'cnn_marathon': [], 'transformer_decathlon': []}\n",
    "        \n",
    "        for filepath in all_files:\n",
    "            try:\n",
    "                with open(filepath, 'r') as f:\n",
    "                    submission = json.load(f)\n",
    "                    if submission['team_name'] == team_name:\n",
    "                        event = submission['event_name']\n",
    "                        if event in team_submissions:\n",
    "                            team_submissions[event].append(submission)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        # Sort by timestamp\n",
    "        for event in team_submissions:\n",
    "            team_submissions[event].sort(key=lambda s: s['timestamp'])\n",
    "        \n",
    "        return team_submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c164bce1",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### Test TinyMLPerf Competition Framework\n",
    "\n",
    "Let's test the competition framework with multiple team submissions and leaderboards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64308dff",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_tinymlperf_competition():\n",
    "    \"\"\"Test the TinyMLPerf competition framework\"\"\"\n",
    "    print(\"Testing TinyMLPerf Competition Framework...\")\n",
    "    \n",
    "    # Initialize competition\n",
    "    competition = TinyMLPerfCompetition()\n",
    "    \n",
    "    # Create some test optimized models\n",
    "    class FastMLPModel:\n",
    "        \"\"\"Simulated optimized MLP - smaller and faster\"\"\"\n",
    "        def __init__(self):\n",
    "            # Smaller model for speed\n",
    "            self.weights1 = np.random.randn(784, 64).astype(np.float32) * 0.1\n",
    "            self.bias1 = np.random.randn(64).astype(np.float32) * 0.1\n",
    "            self.weights2 = np.random.randn(64, 10).astype(np.float32) * 0.1  \n",
    "            self.bias2 = np.random.randn(10).astype(np.float32) * 0.1\n",
    "        \n",
    "        def predict(self, x):\n",
    "            h1 = np.maximum(0, x @ self.weights1 + self.bias1)\n",
    "            return h1 @ self.weights2 + self.bias2\n",
    "    \n",
    "    class EfficientCNNModel:\n",
    "        \"\"\"Simulated optimized CNN\"\"\"\n",
    "        def __init__(self):\n",
    "            # Optimized weights\n",
    "            self.fc_weights = np.random.randn(1600, 10).astype(np.float32) * 0.05\n",
    "            self.fc_bias = np.random.randn(10).astype(np.float32) * 0.05\n",
    "        \n",
    "        def predict(self, x):\n",
    "            batch_size = x.shape[0]\n",
    "            x_flat = x.reshape(batch_size, -1)\n",
    "            if x_flat.shape[1] != 1600:\n",
    "                x_flat = x_flat[:, :1600] if x_flat.shape[1] > 1600 else np.pad(x_flat, ((0, 0), (0, 1600 - x_flat.shape[1])), 'constant')\n",
    "            return x_flat @ self.fc_weights + self.fc_bias\n",
    "    \n",
    "    # Submit optimized models to competition\n",
    "    print(\"\\nüöÄ Submitting Competition Entries...\")\n",
    "    \n",
    "    # MLP Sprint submissions\n",
    "    mlp_submission1 = competition.submit_entry(\n",
    "        team_name=\"Speed Demons\",\n",
    "        event_name=\"mlp_sprint\",\n",
    "        optimized_model=FastMLPModel(),\n",
    "        optimization_description=\"Reduced hidden layer size for 2x speedup\",\n",
    "        github_url=\"https://github.com/speed-demons/fast-mlp\"\n",
    "    )\n",
    "    \n",
    "    mlp_submission2 = competition.submit_entry(\n",
    "        team_name=\"Lightning Fast\",  \n",
    "        event_name=\"mlp_sprint\",\n",
    "        optimized_model=FastMLPModel(),\n",
    "        optimization_description=\"Quantization + kernel optimization\",\n",
    "        github_url=\"https://github.com/lightning-fast/mlp-opt\"\n",
    "    )\n",
    "    \n",
    "    # CNN Marathon submission\n",
    "    cnn_submission = competition.submit_entry(\n",
    "        team_name=\"CNN Champions\",\n",
    "        event_name=\"cnn_marathon\", \n",
    "        optimized_model=EfficientCNNModel(),\n",
    "        optimization_description=\"Custom convolution kernels + memory optimization\",\n",
    "        github_url=\"https://github.com/cnn-champions/efficient-cnn\"\n",
    "    )\n",
    "    \n",
    "    # Display leaderboards\n",
    "    print(\"\\nüìä Competition Leaderboards:\")\n",
    "    competition.display_all_leaderboards()\n",
    "    \n",
    "    print(\"\\n‚úÖ TinyMLPerf competition framework test complete!\")\n",
    "    return competition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89abe4e",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Part 4: Innovation Tracking and Advanced Scoring\n",
    "\n",
    "Let's add innovation detection and advanced scoring to reward creative optimization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a4324b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class InnovationDetector:\n",
    "    \"\"\"\n",
    "    Detect and score innovative optimization techniques in submitted models.\n",
    "    \n",
    "    Rewards creativity by analyzing models for advanced optimization patterns:\n",
    "    - Quantization techniques\n",
    "    - Pruning strategies  \n",
    "    - Knowledge distillation\n",
    "    - Custom kernel implementations\n",
    "    - Novel architectural innovations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize innovation detector\"\"\"\n",
    "        self.innovation_patterns = {\n",
    "            'quantization': ['quantized', 'int8', 'int16', 'low_precision', 'quantize'],\n",
    "            'pruning': ['pruned', 'sparse', 'sparsity', 'prune', 'structured_pruning'],\n",
    "            'distillation': ['distilled', 'teacher', 'student', 'knowledge_distillation', 'kd'],\n",
    "            'custom_kernels': ['custom_kernel', 'optimized_kernel', 'cuda', 'vectorized', 'simd'],\n",
    "            'memory_optimization': ['memory_pool', 'in_place', 'gradient_checkpointing', 'memory_efficient'],\n",
    "            'compression': ['compressed', 'huffman', 'lz4', 'weight_sharing', 'parameter_sharing']\n",
    "        }\n",
    "    \n",
    "    def analyze_innovation(self, model, optimization_description: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Analyze a model for innovative optimization techniques.\n",
    "        \n",
    "        Args:\n",
    "            model: The optimized model to analyze\n",
    "            optimization_description: Text description of optimizations\n",
    "            \n",
    "        Returns:\n",
    "            Innovation analysis with detected techniques and scores\n",
    "        \"\"\"\n",
    "        innovation_score = 0.0\n",
    "        detected_techniques = []\n",
    "        \n",
    "        # Analyze optimization description\n",
    "        desc_lower = optimization_description.lower()\n",
    "        \n",
    "        for technique, patterns in self.innovation_patterns.items():\n",
    "            for pattern in patterns:\n",
    "                if pattern in desc_lower:\n",
    "                    detected_techniques.append(technique)\n",
    "                    innovation_score += 0.2\n",
    "                    break  # Only count each technique once\n",
    "        \n",
    "        # Analyze model attributes for innovation markers\n",
    "        model_innovation = self._analyze_model_attributes(model)\n",
    "        detected_techniques.extend(model_innovation['techniques'])\n",
    "        innovation_score += model_innovation['score']\n",
    "        \n",
    "        # Bonus for multiple techniques (creativity reward)\n",
    "        if len(detected_techniques) >= 3:\n",
    "            innovation_score += 0.3  # Combination bonus\n",
    "        \n",
    "        # Cap innovation score\n",
    "        innovation_score = min(innovation_score, 1.0)\n",
    "        \n",
    "        return {\n",
    "            'innovation_score': innovation_score,\n",
    "            'detected_techniques': list(set(detected_techniques)),  # Remove duplicates\n",
    "            'num_techniques': len(set(detected_techniques)),\n",
    "            'creativity_bonus': len(detected_techniques) >= 3\n",
    "        }\n",
    "    \n",
    "    def _analyze_model_attributes(self, model) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze model object for innovation attributes\"\"\"\n",
    "        techniques = []\n",
    "        score = 0.0\n",
    "        \n",
    "        # Check for common optimization attributes\n",
    "        optimization_attributes = [\n",
    "            ('quantized', 'quantization'),\n",
    "            ('pruned', 'pruning'),\n",
    "            ('distilled', 'distillation'),\n",
    "            ('compressed', 'compression'),\n",
    "            ('memory_optimized', 'memory_optimization'),\n",
    "            ('custom_kernels', 'custom_kernels')\n",
    "        ]\n",
    "        \n",
    "        for attr, technique in optimization_attributes:\n",
    "            if hasattr(model, attr) and getattr(model, attr):\n",
    "                techniques.append(technique)\n",
    "                score += 0.15\n",
    "        \n",
    "        # Check for unusual model architectures (creativity indicator)\n",
    "        if hasattr(model, 'innovative_architecture') and getattr(model, 'innovative_architecture'):\n",
    "            techniques.append('novel_architecture')\n",
    "            score += 0.25\n",
    "        \n",
    "        return {'techniques': techniques, 'score': score}\n",
    "    \n",
    "    def generate_innovation_report(self, analysis: Dict[str, Any]) -> str:\n",
    "        \"\"\"Generate human-readable innovation report\"\"\"\n",
    "        score = analysis['innovation_score']\n",
    "        techniques = analysis['detected_techniques']\n",
    "        \n",
    "        if score == 0:\n",
    "            return \"No innovative techniques detected. Consider exploring quantization, pruning, or custom optimizations!\"\n",
    "        \n",
    "        report = f\"Innovation Score: {score:.2f}/1.00\\n\"\n",
    "        report += f\"Detected Techniques ({len(techniques)}):\\n\"\n",
    "        \n",
    "        for technique in techniques:\n",
    "            report += f\"  ‚Ä¢ {technique.replace('_', ' ').title()}\\n\"\n",
    "        \n",
    "        if analysis['creativity_bonus']:\n",
    "            report += \"üåü Creativity Bonus: Multiple optimization techniques combined!\\n\"\n",
    "        \n",
    "        # Award levels\n",
    "        if score >= 0.8:\n",
    "            report += \"üèÜ INNOVATION MASTER - Outstanding creativity!\"\n",
    "        elif score >= 0.6:\n",
    "            report += \"üöÄ INNOVATION EXPERT - Excellent techniques!\"\n",
    "        elif score >= 0.4:\n",
    "            report += \"‚≠ê INNOVATION PRACTITIONER - Good optimization work!\"\n",
    "        else:\n",
    "            report += \"üîç INNOVATION EXPLORER - Keep experimenting!\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Enhanced competition class with innovation scoring\n",
    "class TinyMLPerfCompetitionPlus(TinyMLPerfCompetition):\n",
    "    \"\"\"\n",
    "    Enhanced TinyMLPerf Competition with innovation detection and advanced scoring.\n",
    "    \n",
    "    Extends the base competition with:\n",
    "    - Innovation technique detection\n",
    "    - Advanced composite scoring\n",
    "    - Creativity rewards\n",
    "    - Multi-dimensional leaderboards\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, results_dir: str = \"tinymlperf_results\"):\n",
    "        \"\"\"Initialize enhanced competition with innovation detection\"\"\"\n",
    "        super().__init__(results_dir)\n",
    "        self.innovation_detector = InnovationDetector()\n",
    "        print(\"üî¨ Innovation detection enabled!\")\n",
    "    \n",
    "    def submit_entry(self, team_name: str, event_name: str, optimized_model,\n",
    "                     optimization_description: str = \"\", github_url: str = \"\") -> Dict[str, Any]:\n",
    "        \"\"\"Submit entry with innovation analysis\"\"\"\n",
    "        \n",
    "        # Get base submission\n",
    "        submission = super().submit_entry(team_name, event_name, optimized_model, \n",
    "                                        optimization_description, github_url)\n",
    "        \n",
    "        # Add innovation analysis\n",
    "        innovation_analysis = self.innovation_detector.analyze_innovation(\n",
    "            optimized_model, optimization_description\n",
    "        )\n",
    "        \n",
    "        submission['innovation_analysis'] = innovation_analysis\n",
    "        \n",
    "        # Calculate composite score (speed + innovation)\n",
    "        speed_score = submission['speedup_score']  # Relative speedup\n",
    "        innovation_score = innovation_analysis['innovation_score']\n",
    "        \n",
    "        # Weighted composite: 70% speed, 30% innovation\n",
    "        composite_score = 0.7 * speed_score + 0.3 * innovation_score\n",
    "        submission['composite_score'] = composite_score\n",
    "        \n",
    "        # Display innovation results\n",
    "        print(f\"\\nüî¨ Innovation Analysis:\")\n",
    "        innovation_report = self.innovation_detector.generate_innovation_report(innovation_analysis)\n",
    "        print(innovation_report)\n",
    "        print(f\"\\nüèÜ Composite Score: {composite_score:.3f} (Speed: {speed_score:.2f}, Innovation: {innovation_score:.2f})\")\n",
    "        \n",
    "        # Re-save with innovation data\n",
    "        self._save_submission(submission)\n",
    "        \n",
    "        return submission\n",
    "    \n",
    "    def display_innovation_leaderboard(self, event_name: str, top_n: int = 10):\n",
    "        \"\"\"Display leaderboard ranked by innovation score\"\"\"\n",
    "        submissions = self._load_event_submissions(event_name)\n",
    "        \n",
    "        # Filter submissions with innovation data\n",
    "        innovation_submissions = [s for s in submissions if 'innovation_analysis' in s]\n",
    "        \n",
    "        if not innovation_submissions:\n",
    "            print(f\"üî¨ Innovation Leaderboard - {event_name.replace('_', ' ').title()}\")\n",
    "            print(\"No innovation submissions yet!\")\n",
    "            return\n",
    "        \n",
    "        # Sort by innovation score\n",
    "        innovation_submissions.sort(key=lambda s: s['innovation_analysis']['innovation_score'], reverse=True)\n",
    "        top_submissions = innovation_submissions[:top_n]\n",
    "        \n",
    "        print(f\"\\nüî¨ INNOVATION LEADERBOARD - {event_name.replace('_', ' ').title()}\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"{'Rank':<6} {'Team':<20} {'Innovation':<12} {'Techniques':<8} {'Description':<25}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for i, submission in enumerate(top_submissions):\n",
    "            rank = i + 1\n",
    "            team = submission['team_name'][:19]\n",
    "            innovation = f\"{submission['innovation_analysis']['innovation_score']:.3f}\"\n",
    "            num_tech = submission['innovation_analysis']['num_techniques']\n",
    "            description = submission['optimization_description'][:24]\n",
    "            \n",
    "            print(f\"{rank:<6} {team:<20} {innovation:<12} {num_tech:<8} {description:<25}\")\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "        print(f\"Top {len(top_submissions)} most innovative submissions\")\n",
    "    \n",
    "    def display_composite_leaderboard(self, event_name: str, top_n: int = 10):\n",
    "        \"\"\"Display leaderboard ranked by composite score (speed + innovation)\"\"\"\n",
    "        submissions = self._load_event_submissions(event_name)\n",
    "        \n",
    "        # Filter submissions with composite scores\n",
    "        composite_submissions = [s for s in submissions if 'composite_score' in s]\n",
    "        \n",
    "        if not composite_submissions:\n",
    "            print(f\"üèÜ Composite Leaderboard - {event_name.replace('_', ' ').title()}\")\n",
    "            print(\"No composite submissions yet!\")\n",
    "            return\n",
    "        \n",
    "        # Sort by composite score\n",
    "        composite_submissions.sort(key=lambda s: s['composite_score'], reverse=True)\n",
    "        top_submissions = composite_submissions[:top_n]\n",
    "        \n",
    "        print(f\"\\nüèÜ COMPOSITE LEADERBOARD - {event_name.replace('_', ' ').title()}\")\n",
    "        print(\"=\" * 90)  \n",
    "        print(f\"{'Rank':<6} {'Team':<18} {'Composite':<11} {'Speed':<9} {'Innovation':<11} {'Techniques'}\")\n",
    "        print(\"-\" * 90)\n",
    "        \n",
    "        for i, submission in enumerate(top_submissions):\n",
    "            rank = i + 1\n",
    "            team = submission['team_name'][:17]\n",
    "            composite = f\"{submission['composite_score']:.3f}\"\n",
    "            speed = f\"{submission['speedup_score']:.2f}x\"\n",
    "            innovation = f\"{submission['innovation_analysis']['innovation_score']:.3f}\"\n",
    "            techniques = \", \".join(submission['innovation_analysis']['detected_techniques'][:3])[:20]\n",
    "            \n",
    "            print(f\"{rank:<6} {team:<18} {composite:<11} {speed:<9} {innovation:<11} {techniques}\")\n",
    "        \n",
    "        print(\"-\" * 90)\n",
    "        print(f\"Top {len(top_submissions)} best overall submissions (70% speed + 30% innovation)\")\n",
    "    \n",
    "    def display_all_enhanced_leaderboards(self):\n",
    "        \"\"\"Display all leaderboard types for all events\"\"\"\n",
    "        events = ['mlp_sprint', 'cnn_marathon', 'transformer_decathlon']\n",
    "        \n",
    "        for event in events:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"üèÜ {event.replace('_', ' ').title()} - All Leaderboards\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            # Speed leaderboard  \n",
    "            self.display_leaderboard(event, top_n=5)\n",
    "            print()\n",
    "            \n",
    "            # Innovation leaderboard\n",
    "            self.display_innovation_leaderboard(event, top_n=5)\n",
    "            print()\n",
    "            \n",
    "            # Composite leaderboard\n",
    "            self.display_composite_leaderboard(event, top_n=5)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34233c4",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "### Test Enhanced Competition with Innovation Detection\n",
    "\n",
    "Let's test the enhanced competition framework with innovation detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d82963",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_enhanced_competition():\n",
    "    \"\"\"Test enhanced competition with innovation detection\"\"\"\n",
    "    print(\"Testing Enhanced TinyMLPerf Competition...\")\n",
    "    \n",
    "    # Initialize enhanced competition\n",
    "    competition = TinyMLPerfCompetitionPlus()\n",
    "    \n",
    "    # Create innovative models with optimization attributes\n",
    "    class QuantizedFastMLP:\n",
    "        \"\"\"Simulated quantized MLP\"\"\"\n",
    "        def __init__(self):\n",
    "            self.weights1 = np.random.randn(784, 64).astype(np.int8)  # Quantized weights\n",
    "            self.bias1 = np.random.randn(64).astype(np.float32) * 0.1\n",
    "            self.weights2 = np.random.randn(64, 10).astype(np.int8)\n",
    "            self.bias2 = np.random.randn(10).astype(np.float32) * 0.1\n",
    "            self.quantized = True  # Innovation marker\n",
    "        \n",
    "        def predict(self, x):\n",
    "            # Simulate quantized computation\n",
    "            h1 = np.maximum(0, x @ self.weights1.astype(np.float32) * 0.1 + self.bias1)\n",
    "            return h1 @ self.weights2.astype(np.float32) * 0.1 + self.bias2\n",
    "    \n",
    "    class PrunedCNN:\n",
    "        \"\"\"Simulated pruned CNN\"\"\"\n",
    "        def __init__(self):\n",
    "            self.fc_weights = np.random.randn(1600, 10).astype(np.float32) * 0.05\n",
    "            self.fc_bias = np.random.randn(10).astype(np.float32) * 0.05\n",
    "            self.pruned = True  # Innovation marker\n",
    "            self.sparsity = 0.7  # 70% of weights pruned\n",
    "        \n",
    "        def predict(self, x):\n",
    "            batch_size = x.shape[0]\n",
    "            x_flat = x.reshape(batch_size, -1)\n",
    "            if x_flat.shape[1] != 1600:\n",
    "                x_flat = x_flat[:, :1600] if x_flat.shape[1] > 1600 else np.pad(x_flat, ((0, 0), (0, 1600 - x_flat.shape[1])), 'constant')\n",
    "            return x_flat @ self.fc_weights + self.fc_bias\n",
    "    \n",
    "    # Submit innovative entries\n",
    "    print(\"\\nüöÄ Submitting Innovative Entries...\")\n",
    "    \n",
    "    # Quantized MLP submission\n",
    "    quantized_submission = competition.submit_entry(\n",
    "        team_name=\"Quantum Quantizers\",\n",
    "        event_name=\"mlp_sprint\",\n",
    "        optimized_model=QuantizedFastMLP(),\n",
    "        optimization_description=\"INT8 quantization with custom SIMD kernels for 3x speedup\",\n",
    "        github_url=\"https://github.com/quantum-quantizers/quantized-mlp\"\n",
    "    )\n",
    "    \n",
    "    # Pruned CNN submission\n",
    "    pruned_submission = competition.submit_entry(\n",
    "        team_name=\"Pruning Pioneers\", \n",
    "        event_name=\"cnn_marathon\",\n",
    "        optimized_model=PrunedCNN(),\n",
    "        optimization_description=\"Structured pruning + knowledge distillation + memory optimization\",\n",
    "        github_url=\"https://github.com/pruning-pioneers/pruned-cnn\"\n",
    "    )\n",
    "    \n",
    "    # Display enhanced leaderboards\n",
    "    print(\"\\nüìä Enhanced Competition Leaderboards:\")\n",
    "    competition.display_all_enhanced_leaderboards()\n",
    "    \n",
    "    print(\"\\n‚úÖ Enhanced competition test complete!\")\n",
    "    return competition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065ec776",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1
   },
   "source": [
    "## Comprehensive Testing\n",
    "\n",
    "Let's run a complete TinyMLPerf competition demonstration with all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ec3a07",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def run_complete_tinymlperf_demo():\n",
    "    \"\"\"Run comprehensive TinyMLPerf competition demonstration\"\"\"\n",
    "    print(\"üèÜ TINYMLPERF - THE ULTIMATE ML SYSTEMS COMPETITION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\n1. üèóÔ∏è  Setting up TinyMLPerf Benchmark Suite...\")\n",
    "    # Test benchmark suite\n",
    "    tinyperf = test_tinymlperf_benchmark_suite()\n",
    "    \n",
    "    print(\"\\n2. ‚ö° Testing Competition Profiling...\")  \n",
    "    # Test profiling infrastructure\n",
    "    profiler, mlp_results, cnn_results = test_competition_profiler()\n",
    "    \n",
    "    print(\"\\n3. üöÄ Running Basic Competition...\")\n",
    "    # Test basic competition\n",
    "    basic_competition = test_tinymlperf_competition()\n",
    "    \n",
    "    print(\"\\n4. üî¨ Testing Enhanced Competition with Innovation...\")\n",
    "    # Test enhanced competition\n",
    "    enhanced_competition = test_enhanced_competition()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üéâ TINYMLPERF DEMO COMPLETE!\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\nüèÜ TinyMLPerf Competition Ready:\")\n",
    "    print(\"‚úÖ Three exciting events: MLP Sprint, CNN Marathon, Transformer Decathlon\") \n",
    "    print(\"‚úÖ TinyTorch Module 15 profiler integration for rigorous benchmarking\")\n",
    "    print(\"‚úÖ Hardware-independent relative scoring (speedup ratios)\")\n",
    "    print(\"‚úÖ Transparent leaderboards with evidence requirements\")\n",
    "    print(\"‚úÖ Innovation detection and creativity rewards\")\n",
    "    print(\"‚úÖ Composite scoring balancing speed and innovation\")\n",
    "    \n",
    "    print(\"\\nüöÄ Competition Features:\")\n",
    "    print(\"‚Ä¢ Standardized benchmark models and datasets\")\n",
    "    print(\"‚Ä¢ Statistical reliability with multiple timing runs\")\n",
    "    print(\"‚Ä¢ Multiple leaderboard categories (speed, innovation, composite)\")\n",
    "    print(\"‚Ä¢ GitHub integration for transparency and reproducibility\")\n",
    "    print(\"‚Ä¢ Automatic technique detection and innovation scoring\")\n",
    "    \n",
    "    print(\"\\nüéØ Ready to Compete:\")\n",
    "    print(\"1. Optimize your models using techniques from Modules 16-19\")\n",
    "    print(\"2. Submit to TinyMLPerf events using competition.submit_entry()\")\n",
    "    print(\"3. See your results on leaderboards instantly\") \n",
    "    print(\"4. Iterate and improve based on performance feedback\")\n",
    "    print(\"5. Prove your ML systems optimization mastery!\")\n",
    "    \n",
    "    return {\n",
    "        'benchmark_suite': tinyperf,\n",
    "        'profiler': profiler,\n",
    "        'basic_competition': basic_competition, \n",
    "        'enhanced_competition': enhanced_competition\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1145585e",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Systems Analysis Summary\n",
    "\n",
    "This TinyMLPerf competition module demonstrates advanced ML systems engineering through competitive benchmarking:\n",
    "\n",
    "### üèóÔ∏è **Competition Infrastructure Excellence**\n",
    "- **Standardized Benchmarking**: Fair competition through consistent profiling protocols using Module 15's profiler\n",
    "- **Statistical Rigor**: Multiple timing runs with warmup periods ensure reliable performance measurements\n",
    "- **Hardware Independence**: Relative speedup scoring allows fair competition across different hardware platforms\n",
    "- **Transparency Requirements**: GitHub integration and evidence tracking prevent gaming and ensure reproducibility\n",
    "\n",
    "### ‚ö° **Multi-Dimensional Performance Optimization**\n",
    "- **Speed Optimization**: Direct latency measurement rewarding inference performance improvements\n",
    "- **Innovation Detection**: Automated recognition of advanced techniques like quantization, pruning, distillation\n",
    "- **Composite Scoring**: Balanced evaluation combining speed improvements with optimization creativity\n",
    "- **Multiple Event Categories**: MLP Sprint, CNN Marathon, Transformer Decathlon test different optimization domains\n",
    "\n",
    "### üìä **Systematic Competition Analysis**\n",
    "- **TinyTorch Profiler Integration**: Leverages Module 15's profiling infrastructure for consistent measurement\n",
    "- **Memory Tracking**: Comprehensive resource usage analysis beyond just timing measurements\n",
    "- **Progress Tracking**: Team improvement analysis across multiple submissions and iterations\n",
    "- **Leaderboard Visualization**: Multiple ranking systems (speed, innovation, composite) prevent tunnel vision\n",
    "\n",
    "### üí° **Production ML Systems Insights**\n",
    "- **Benchmarking Best Practices**: Industry-standard profiling methodology with warmup and statistical analysis\n",
    "- **Optimization Technique Recognition**: Systematic detection of real-world optimization approaches\n",
    "- **Performance Claims Validation**: Evidence-based performance reporting with reproducible results\n",
    "- **Resource Constraint Awareness**: Multi-metric evaluation reflecting production deployment considerations\n",
    "\n",
    "### üéØ **Key Educational Insights**\n",
    "- Competition accelerates optimization learning by making improvements concrete and measurable\n",
    "- Hardware-independent scoring ensures fair comparison while teaching relative performance analysis\n",
    "- Innovation detection rewards creativity and exposure to diverse optimization techniques\n",
    "- Multiple leaderboards prevent single-metric optimization and encourage balanced system thinking\n",
    "- Evidence requirements teach reproducibility and honest performance reporting practices\n",
    "\n",
    "### üèÜ **The Ultimate Learning Achievement**\n",
    "This competition framework proves students can systematically optimize ML systems for real production constraints. By combining techniques from Modules 16-19 (quantization, pruning, acceleration, memory optimization), students demonstrate mastery of the complete ML systems optimization stack through measurable competitive performance.\n",
    "\n",
    "The TinyMLPerf competition transforms optimization from abstract concepts into concrete, competitive achievements that mirror real-world ML systems engineering challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e34927e",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Main Execution Block\n",
    "\n",
    "Run the complete TinyMLPerf competition system when this module is executed directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dfaddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Module 20: TinyMLPerf - The Ultimate ML Systems Competition\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Run complete TinyMLPerf demonstration\n",
    "    results = run_complete_tinymlperf_demo()\n",
    "    \n",
    "    print(f\"\\nüéâ Module 20 complete!\")\n",
    "    print(f\"üèÜ TinyMLPerf competition infrastructure ready!\")\n",
    "    print(f\"üöÄ Time to optimize your models and climb the leaderboards!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f95ba18",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## ü§î ML Systems Thinking: Interactive Questions\n",
    "\n",
    "1. **Why use hardware-independent relative scoring in ML competitions?** Your TinyMLPerf uses speedup ratios rather than absolute timing. Explain why this enables fair competition across different hardware platforms and how this mirrors real production environments where optimization techniques must be portable across diverse deployment targets.\n",
    "\n",
    "2. **How does competitive benchmarking accelerate optimization learning compared to individual assignments?** You've built leaderboards, innovation detection, and multi-dimensional scoring. Analyze why competition pressure drives deeper exploration of optimization techniques and how this mirrors real industry environments where performance benchmarks determine system adoption.\n",
    "\n",
    "3. **What makes innovation detection crucial for preventing optimization tunnel vision?** Your system detects quantization, pruning, distillation, and custom kernels automatically. Explain why rewarding diverse techniques prevents students from over-optimizing single metrics and how this teaches balanced systems thinking rather than algorithmic tunnel vision.\n",
    "\n",
    "4. **How does evidence-based competition ensure educational integrity and real-world relevance?** Your framework requires GitHub links, generates checksums, and validates reproducibility. Analyze why these requirements prevent academic dishonesty while teaching students the performance reporting standards expected in production ML systems development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708f21f3",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## üéØ MODULE SUMMARY: TinyMLPerf - The Ultimate ML Systems Competition\n",
    "\n",
    "This capstone module creates the ultimate ML systems competition, proving optimization mastery through measurable performance improvements in three exciting events.\n",
    "\n",
    "### üõ§Ô∏è **The TinyMLPerf Journey**\n",
    "- **Modules 1-19**: You built comprehensive optimization techniques across the entire ML systems stack\n",
    "- **Module 20**: You compete to prove mastery through concrete, measurable performance improvements\n",
    "- **Ultimate Goal**: Demonstrate professional-level ML systems optimization through competitive achievement\n",
    "\n",
    "### üõ†Ô∏è **What We Built**\n",
    "- **TinyMLPerf Benchmark Suite**: Three standardized competition events - MLP Sprint, CNN Marathon, Transformer Decathlon\n",
    "- **Competition Profiler**: Integration with Module 15's profiler for rigorous, statistical performance measurement\n",
    "- **Multi-Dimensional Leaderboards**: Speed, innovation, and composite scoring systems preventing tunnel vision\n",
    "- **Innovation Detection**: Automatic recognition and scoring of advanced optimization techniques\n",
    "\n",
    "### üß† **Key Learning Outcomes**\n",
    "- **Competitive Optimization**: Apply learned techniques competitively with measurable, hardware-independent results\n",
    "- **Systematic Benchmarking**: Use statistical profiling methodology for reliable performance measurement\n",
    "- **Innovation Recognition**: Understand and apply diverse optimization approaches beyond simple speed improvements\n",
    "- **Evidence-Based Performance**: Support optimization claims with reproducible benchmarking and transparent evidence\n",
    "\n",
    "### ‚ö° **Competition Events Mastered**\n",
    "- **MLP Sprint**: Fastest feedforward neural network inference optimization\n",
    "- **CNN Marathon**: Most efficient convolutional neural network processing\n",
    "- **Transformer Decathlon**: Ultimate attention mechanism and sequence processing optimization\n",
    "\n",
    "### üèÜ **Technical Skills Developed**\n",
    "- Design and implement standardized benchmarking infrastructure for fair ML competition\n",
    "- Integrate profiling tools for statistical performance measurement and analysis\n",
    "- Build multi-dimensional leaderboard systems balancing multiple optimization objectives\n",
    "- Detect and score innovation techniques automatically to reward optimization creativity\n",
    "\n",
    "### üìä **Systems Engineering Insights Gained**\n",
    "- **Competition accelerates learning**: Measurable challenges drive deeper optimization exploration than individual assignments\n",
    "- **Hardware-independent scoring**: Relative performance metrics enable fair comparison across diverse deployment environments  \n",
    "- **Innovation detection prevents tunnel vision**: Multi-dimensional scoring teaches balanced systems optimization\n",
    "- **Evidence requirements ensure integrity**: Reproducible results and transparency are essential for professional optimization claims\n",
    "\n",
    "### üí° **The Capstone Achievement**\n",
    "You've completed the ultimate ML systems optimization journey! Through competitive pressure in TinyMLPerf, you've applied quantization, pruning, distillation, acceleration, memory optimization, and innovation techniques to achieve measurable performance improvements. This competition framework proves you can optimize ML systems like a professional engineer, balancing speed, memory, innovation, and deployment constraints to build production-ready systems.\n",
    "\n",
    "### üéâ **Competition Glory Awaits**\n",
    "Ready to prove your optimization mastery? Load your optimized models into TinyMLPerf, submit to the three events, and climb the leaderboards! Your journey from basic tensors to competition-winning ML systems optimization is complete - now show the world what you can build!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
