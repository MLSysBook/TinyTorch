{
  "submission_id": "mlp_sprint_bae657_20250929_095832",
  "timestamp": "2025-09-29T09:58:32.608106",
  "team_name": "Quantized Team",
  "event_name": "mlp_sprint",
  "optimization_description": "INT8 quantization with custom kernels",
  "github_url": "https://github.com/quantized-team/mlp-opt",
  "performance_metrics": {
    "event": "MLP Sprint",
    "model_type": "FastMLPModel",
    "input_shape": [
      100,
      784
    ],
    "benchmark_timestamp": "2025-09-29T09:58:32.548478",
    "mean_inference_time": 0.0002787633200023265,
    "std_inference_time": 6.730044234907107e-06,
    "min_inference_time": 0.00026638760000423644,
    "max_inference_time": 0.000285820700014483,
    "p95_inference_time": 0.0002851124000198979,
    "mean_cpu_time": 0.0002787633200023265,
    "cpu_efficiency": 0.85,
    "profiling_method": "TinyTorch Module 15 Profiler",
    "memory_delta_mb": 0.004241943359375,
    "peak_memory_mb": 0.074676513671875,
    "result_size_mb": 0.1,
    "speedup_vs_baseline": 1.3380529402967942
  },
  "speedup_score": 1.3380529402967942,
  "baseline_time_ms": 0.37300007997600915,
  "submission_time_ms": 0.2787633200023265
}