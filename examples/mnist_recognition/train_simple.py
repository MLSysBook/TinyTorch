#!/usr/bin/env python3
"""
Simplified MNIST-style Training with TinyTorch

This demonstrates the complete MNIST milestone pattern using synthetic data
that mimics MNIST's structure (28x28 images, 10 classes).

This proves:
- Multi-layer neural network works
- Training loop with optimization works  
- Classification accuracy can be measured
- All components integrate properly

Once MNISTDataset is implemented, this same code will work with real MNIST data.
"""

import numpy as np
from tinytorch.core.tensor import Tensor
from tinytorch.core.layers import Dense
from tinytorch.core.activations import ReLU, Softmax
from tinytorch.core.dataloader import DataLoader, SimpleDataset
from tinytorch.core.optimizers import Adam
from tinytorch.core.training import MeanSquaredError as MSELoss
from tinytorch.core.autograd import Variable


class MNISTStyleDataset(SimpleDataset):
    """MNIST-style synthetic dataset for testing the complete pipeline."""
    
    def __init__(self, size=1000, train=True):
        # MNIST dimensions: 28x28 = 784 features, 10 classes
        super().__init__(size=size, num_features=784, num_classes=10)
        self.train = train
        
        # Make data more MNIST-like: normalize to [0,1] range
        self.data = np.abs(self.data)  # Make positive
        self.data = self.data / np.max(self.data)  # Normalize to [0,1]
        
        print(f"âœ… Created {'training' if train else 'test'} dataset: {size:,} samples")


class MNISTClassifier:
    """Multi-layer perceptron for MNIST-style classification."""
    
    def __init__(self, input_size=784, hidden_size=128, num_classes=10):
        # Three-layer architecture: 784 -> 128 -> 64 -> 10
        self.fc1 = Dense(input_size, hidden_size)
        self.fc2 = Dense(hidden_size, 64)
        self.fc3 = Dense(64, num_classes)
        
        # Activations
        self.relu = ReLU()
        self.softmax = Softmax()
        
        # Convert to Variables for training
        self._make_trainable()
    
    def _make_trainable(self):
        """Convert parameters to Variables for autograd."""
        self.fc1.weights = Variable(self.fc1.weights, requires_grad=True)
        self.fc1.bias = Variable(self.fc1.bias, requires_grad=True)
        self.fc2.weights = Variable(self.fc2.weights, requires_grad=True)
        self.fc2.bias = Variable(self.fc2.bias, requires_grad=True)
        self.fc3.weights = Variable(self.fc3.weights, requires_grad=True)
        self.fc3.bias = Variable(self.fc3.bias, requires_grad=True)
    
    def forward(self, x):
        """Forward pass through the network."""
        # Convert input to Variable if needed
        if not hasattr(x, 'requires_grad'):
            x = Variable(x, requires_grad=True)
        
        # Layer 1: 784 -> 128
        x = self.fc1(x)
        x = self.relu(x)
        
        # Layer 2: 128 -> 64
        x = self.fc2(x)
        x = self.relu(x)
        
        # Output layer: 64 -> 10
        x = self.fc3(x)
        x = self.softmax(x)
        
        return x
    
    def parameters(self):
        """Get all trainable parameters."""
        return [
            self.fc1.weights, self.fc1.bias,
            self.fc2.weights, self.fc2.bias,
            self.fc3.weights, self.fc3.bias
        ]


def train_epoch(model, dataloader, optimizer, loss_fn, epoch):
    """Train for one epoch."""
    total_loss = 0
    correct = 0
    total = 0
    
    print(f"\n--- Epoch {epoch + 1} Training ---")
    
    for batch_idx, (images, labels) in enumerate(dataloader):
        # Forward pass
        predictions = model.forward(images)
        
        # Convert labels to one-hot for CrossEntropyLoss
        batch_size = labels.shape[0]
        num_classes = 10
        labels_onehot = np.zeros((batch_size, num_classes))
        for i in range(batch_size):
            label_idx = int(labels.data[i])
            labels_onehot[i, label_idx] = 1
        labels_var = Variable(Tensor(labels_onehot), requires_grad=False)
        
        # Compute loss
        loss = loss_fn(predictions, labels_var)
        total_loss += float(loss.data.data if hasattr(loss.data, 'data') else loss.data)
        
        # Compute accuracy
        pred_data = predictions.data.data if hasattr(predictions.data, 'data') else predictions.data
        pred_classes = np.argmax(pred_data, axis=1)
        true_classes = labels.data
        correct += np.sum(pred_classes == true_classes)
        total += labels.shape[0]  # Use shape[0] instead of len() for Tensor
        
        # Backward pass
        if hasattr(loss, 'backward'):
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        
        # Log progress every few batches
        if batch_idx % 5 == 0:
            curr_acc = 100 * correct / total if total > 0 else 0
            print(f"  Batch {batch_idx:2d}/{len(dataloader)} | "
                  f"Loss: {loss.data.data if hasattr(loss.data, 'data') else loss.data:.4f} | "
                  f"Acc: {curr_acc:.1f}%")
    
    epoch_loss = total_loss / len(dataloader)
    epoch_acc = correct / total
    return epoch_loss, epoch_acc


def evaluate(model, dataloader):
    """Evaluate model on test set."""
    correct = 0
    total = 0
    
    print("\n--- Evaluation ---")
    
    for batch_idx, (images, labels) in enumerate(dataloader):
        predictions = model.forward(images)
        
        pred_data = predictions.data.data if hasattr(predictions.data, 'data') else predictions.data
        pred_classes = np.argmax(pred_data, axis=1)
        true_classes = labels.data
        
        correct += np.sum(pred_classes == true_classes)
        total += labels.shape[0]  # Use shape[0] instead of len() for Tensor
        
        if batch_idx % 3 == 0:
            print(f"  Batch {batch_idx}: {100*correct/total:.1f}% accuracy")
    
    return correct / total


def main():
    print("=" * 60)
    print("ğŸ§  MNIST-Style Classification with TinyTorch")
    print("=" * 60)
    print()
    
    # Create datasets
    print("ğŸ“š Creating synthetic MNIST-style datasets...")
    train_dataset = MNISTStyleDataset(size=2000, train=True)
    test_dataset = MNISTStyleDataset(size=400, train=False)
    
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)  # Smaller batches
    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)
    
    print(f"  Training batches: {len(train_loader)}")
    print(f"  Test batches: {len(test_loader)}")
    print()
    
    # Build model
    print("ğŸ—ï¸ Building neural network...")
    model = MNISTClassifier()
    print("  Architecture: 784 â†’ 128 â†’ 64 â†’ 10")
    print("  Total parameters:", sum(p.data.size for p in model.parameters()))
    print()
    
    # Setup training
    optimizer = Adam(model.parameters(), learning_rate=0.001)
    loss_fn = MSELoss()  # Using MSE for now since CrossEntropy has integration issues
    
    # Training loop
    print("ğŸ¯ Training...")
    print("-" * 60)
    
    num_epochs = 3  # Short training for demo
    best_accuracy = 0
    
    for epoch in range(num_epochs):
        # Train
        train_loss, train_acc = train_epoch(model, train_loader, optimizer, loss_fn, epoch)
        
        # Evaluate
        test_acc = evaluate(model, test_loader)
        
        print(f"\nEpoch {epoch+1} Summary:")
        print(f"  Train Loss: {train_loss:.4f}")
        print(f"  Train Accuracy: {train_acc:.1%}")
        print(f"  Test Accuracy: {test_acc:.1%}")
        
        if test_acc > best_accuracy:
            best_accuracy = test_acc
            print(f"  ğŸ¯ New best accuracy!")
    
    # Final evaluation
    print("\n" + "=" * 60)
    print("ğŸ“Š Final Results:")
    print("-" * 60)
    
    final_accuracy = evaluate(model, test_loader)
    print(f"\nFinal Test Accuracy: {final_accuracy:.1%}")
    print(f"Best Accuracy Achieved: {best_accuracy:.1%}")
    
    # Milestone check (using 70% for synthetic data, 85% for real MNIST)
    target_accuracy = 0.70
    if final_accuracy >= target_accuracy:
        print(f"\nğŸ‰ MILESTONE ACHIEVED!")
        print(f"Your TinyTorch achieves {final_accuracy:.1%} accuracy!")
        print("Ready for real MNIST dataset integration!")
    else:
        print(f"\nğŸ“ˆ Progress: {final_accuracy:.1%} (Target: {target_accuracy:.1%})")
        print("When using real MNIST data, target will be 85%+")
    
    # Show some predictions
    print("\nğŸ” Sample Predictions:")
    print("-" * 40)
    
    for images, labels in test_loader:
        predictions = model.forward(images)
        pred_data = predictions.data.data if hasattr(predictions.data, 'data') else predictions.data
        pred_classes = np.argmax(pred_data, axis=1)
        true_classes = labels.data
        
        # Show first 5
        for i in range(min(5, images.shape[0])):
            status = "âœ…" if pred_classes[i] == true_classes[i] else "âŒ"
            print(f"  True: {true_classes[i]}, Predicted: {pred_classes[i]} {status}")
        break
    
    print("\nğŸ“¦ Components Used:")
    print("  âœ… Dense layers with autograd")
    print("  âœ… ReLU and Softmax activations") 
    print("  âœ… Adam optimizer")
    print("  âœ… MSE loss (CrossEntropy coming soon)")
    print("  âœ… DataLoader with batching")
    print("  âœ… Complete training pipeline")
    
    return final_accuracy


if __name__ == "__main__":
    accuracy = main()