# Text Generation with TinyGPT

Generate text using a transformer model built with YOUR TinyTorch!

## What This Demonstrates

- **Transformer architecture** - the foundation of ChatGPT
- **Multi-head attention** mechanisms you built
- **Autoregressive generation** - predicting one token at a time
- **The technology behind modern AI** - GPT, BERT, etc.

## How It Works

```
Input Tokens â†’ Embeddings â†’ Transformer Blocks â†’ Output Logits â†’ Next Token
                              â†‘__________________|
                              (Autoregressive Loop)
```

## Running the Example

```bash
python generate.py
```

Expected output:
```
ðŸ¤– Text Generation with TinyGPT
======================================================================

ðŸŽ¯ Generating Python-like code:
--------------------------------------------------

Prompt: 'def'
Generated: 'def function_name ( self ) : return None'

Prompt: 'class'  
Generated: 'class MyClass : def __init__ ( self ) :'

Prompt: 'for i in'
Generated: 'for i in range ( 10 ) : print ( i )'

ðŸ’¡ What This Demonstrates:
âœ… Transformer architecture with self-attention
âœ… Multi-head attention you built from scratch
âœ… Autoregressive text generation
âœ… The foundation of ChatGPT and GitHub Copilot!

ðŸŽ‰ You've built the technology behind modern AI!
```

## Architecture

```
TinyGPT Model:
â”œâ”€â”€ Token Embeddings (vocab_size â†’ embed_dim)
â”œâ”€â”€ Position Embeddings (max_length â†’ embed_dim)
â”œâ”€â”€ Transformer Blocks (Ã—4)
â”‚   â”œâ”€â”€ Multi-Head Attention
â”‚   â”œâ”€â”€ Layer Normalization
â”‚   â””â”€â”€ Feed-Forward Network (MLP)
â””â”€â”€ Output Projection (embed_dim â†’ vocab_size)
```

## Key Components

- **Self-Attention**: Models relationships between all tokens
- **Position Embeddings**: Gives model sense of word order
- **Layer Normalization**: Stabilizes training
- **Autoregressive**: Generates one token at a time

## What You've Built

This is the same architecture as:
- GPT (Generative Pre-trained Transformer)
- ChatGPT (with more layers and parameters)
- GitHub Copilot (for code generation)
- BERT (with bidirectional attention)

## Requirements

- Module 07 (Attention) for multi-head attention
- Module 16 (TinyGPT) for complete transformer
- All TinyTorch modules exported

## Next Steps

The full Module 16 implementation will:
- Generate complete Python functions
- Work with natural language prompts
- Show beam search and sampling strategies
- Demonstrate real code generation!