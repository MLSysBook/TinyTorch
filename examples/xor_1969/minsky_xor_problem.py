#!/usr/bin/env python3
"""
The XOR Problem (1969) - Minsky & Papert
========================================

ğŸ“š HISTORICAL CONTEXT:
In 1969, Marvin Minsky and Seymour Papert published "Perceptrons," proving that 
single-layer perceptrons CANNOT solve the XOR problem. This killed neural network 
research for a decade (the "AI Winter") until multi-layer networks solved it!

ğŸ¯ WHAT YOU'RE BUILDING:
Using YOUR TinyTorch implementations, you'll solve the "impossible" XOR problem
that stumped AI for years - proving that YOUR hidden layers enable non-linear learning!

âœ… REQUIRED MODULES (Run after Module 6):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  Module 02 (Tensor)        : YOUR data structure with autodiff
  Module 03 (Activations)   : YOUR ReLU for non-linearity (the key!)
  Module 04 (Layers)        : YOUR Linear layers for transformations
  Module 06 (Autograd)      : YOUR gradient computation for learning
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ—ï¸ ARCHITECTURE (Multi-Layer Solution):
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Input   â”‚    â”‚ Linear  â”‚    â”‚  ReLU   â”‚    â”‚ Linear  â”‚    â”‚ Binary  â”‚
    â”‚ (x1,x2) â”‚â”€â”€â”€â–¶â”‚  2â†’4    â”‚â”€â”€â”€â–¶â”‚ Hidden  â”‚â”€â”€â”€â–¶â”‚  4â†’1    â”‚â”€â”€â”€â–¶â”‚ Output  â”‚
    â”‚ 2 dims  â”‚    â”‚ YOUR M4 â”‚    â”‚ YOUR M3 â”‚    â”‚ YOUR M4 â”‚    â”‚ 0 or 1  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   Hidden Layer    Non-linearity  Output Layer

ğŸ” WHY XOR IS SPECIAL - THE NON-LINEAR SEPARABILITY PROBLEM:

The XOR (exclusive OR) problem outputs 1 when inputs differ, 0 when they match:

    Input Space:                    XOR Truth Table:
    
    1 â”‚ (0,1)â†’1     (1,1)â†’0        â”‚ x1 â”‚ x2 â”‚ XOR â”‚
      â”‚    RED        BLUE          â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
      â”‚                             â”‚ 0  â”‚ 0  â”‚  0  â”‚ (same â†’ 0)
    0 â”‚ (0,0)â†’0     (1,0)â†’1        â”‚ 0  â”‚ 1  â”‚  1  â”‚ (diff â†’ 1)
      â”‚   BLUE        RED           â”‚ 1  â”‚ 0  â”‚  1  â”‚ (diff â†’ 1)
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”‚ 1  â”‚ 1  â”‚  0  â”‚ (same â†’ 0)
        0            1              â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜

    ğŸš« IMPOSSIBLE with single line:     âœ… POSSIBLE with hidden layer:
    
    No single line can separate         Hidden units learn features:
    RED from BLUE points!                - Unit 1: (x1 AND NOT x2)
                                        - Unit 2: (x2 AND NOT x1)
    1 â”‚ R â•± â•± â•± B                      Then combine: Unit1 OR Unit2
      â”‚ â•± â•± â•± â•± â•±
    0 â”‚ B â•± â•± â•± R                      The hidden layer creates a new
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                     feature space where XOR becomes
        0        1                      linearly separable!

This is why neural networks need DEPTH - hidden layers create new representations!

ğŸ“Š EXPECTED PERFORMANCE:
- Dataset: 1,000 XOR samples with slight noise
- Training time: 1 minute  
- Expected accuracy: 95%+ (non-linear problem solved!)
- Key insight: Hidden layer enables non-linear decision boundary
"""

import sys
import os
import numpy as np
import argparse

# Add project root to path for TinyTorch imports
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

# Import TinyTorch components YOU BUILT!
from tinytorch.core.tensor import Tensor      # Module 02: YOU built this!
from tinytorch.core.layers import Linear      # Module 04: YOU built this!
from tinytorch.core.activations import ReLU, Sigmoid  # Module 03: YOU built this!

# Import dataset manager for XOR data
try:
    from examples.data_manager import DatasetManager
except ImportError:
    # Fallback if running from different location
    sys.path.append(os.path.join(project_root, 'examples'))
    from data_manager import DatasetManager

class XORNetwork:
    """
    Multi-layer network that solves XOR using YOUR TinyTorch implementations!
    
    The hidden layer is the KEY - it learns features that make XOR separable.
    """
    
    def __init__(self, input_size=2, hidden_size=4, output_size=1):
        print("ğŸ§  Building XOR Network with YOUR TinyTorch modules...")
        
        # Hidden layer - this is what Minsky said was needed!
        self.hidden = Linear(input_size, hidden_size)  # Module 04: YOUR Linear layer!
        self.activation = ReLU()                       # Module 03: YOUR ReLU (key to non-linearity!)
        self.output = Linear(hidden_size, output_size) # Module 04: YOUR output layer!
        self.sigmoid = Sigmoid()                       # Module 03: YOUR final activation!
        
        print(f"   Input â†’ Hidden: {input_size} â†’ {hidden_size} (YOUR Linear layer)")
        print(f"   Hidden activation: ReLU (YOUR non-linearity - this solves XOR!)")
        print(f"   Hidden â†’ Output: {hidden_size} â†’ {output_size} (YOUR Linear layer)")
        print(f"   Output activation: Sigmoid (YOUR Module 03)")
        
    def forward(self, x):
        """Forward pass through YOUR multi-layer network."""
        # Hidden layer with non-linearity (the SECRET to solving XOR!)
        x = self.hidden(x)        # Module 04: YOUR Linear transformation!
        x = self.activation(x)    # Module 03: YOUR ReLU - creates non-linear features!
        
        # Output layer
        x = self.output(x)        # Module 04: YOUR final transformation!
        x = self.sigmoid(x)       # Module 03: YOUR sigmoid for probability!
        
        return x
    
    def parameters(self):
        """Get all trainable parameters from YOUR layers."""
        return [
            self.hidden.weight, self.hidden.bias,    # Module 04: YOUR hidden parameters!
            self.output.weight, self.output.bias     # Module 04: YOUR output parameters!
        ]

def visualize_xor_problem():
    """Show why XOR is non-linearly separable using ASCII art."""
    print("\n" + "="*70)
    print("ğŸ¨ VISUALIZING THE XOR PROBLEM - Why Single Layers Fail:")
    print("="*70)
    
    print("""
    XOR DATA POINTS:                  SINGLE LAYER ATTEMPT:
    
    1.0 â”‚ â—‹(0,1)=1    â—(1,1)=0       1.0 â”‚ â—‹         â—    
        â”‚   RED        BLUE               â”‚    â•²           
        â”‚                                 â”‚     â•²  â† No single line
    0.5 â”‚                             0.5 â”‚      â•²    can separate!
        â”‚                                 â”‚       â•²        
        â”‚                                 â”‚        â•²       
    0.0 â”‚ â—(0,0)=0    â—‹(1,0)=1       0.0 â”‚ â—        â•² â—‹   
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          0.0   0.5   1.0                  0.0   0.5   1.0
    
    Legend: â—‹ = Output 1 (RED)       Problem: RED and BLUE points
            â— = Output 0 (BLUE)               are diagonally mixed!
    """)
    
    print("ğŸ”„ THE MULTI-LAYER SOLUTION:")
    print("""
    Hidden Layer Features:            New Feature Space:
    
    Hidden Unit 1: x1 AND NOT x2      In hidden space, XOR becomes
    Hidden Unit 2: x2 AND NOT x1      linearly separable!
    
    Original â†’ Hidden Transform:       Now a single line works:
    (0,0) â†’ [0,0] â†’ 0 âœ“               
    (0,1) â†’ [0,1] â†’ 1 âœ“               H2 â”‚     â—‹(0,1)
    (1,0) â†’ [1,0] â†’ 1 âœ“                  â”‚    â•± 
    (1,1) â†’ [0,0] â†’ 0 âœ“                  â”‚   â•±  â—‹(1,0)
                                          â”‚  â•±
    YOUR hidden layer learned         0  â”‚ â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    to transform the problem!            0        H1
    """)
    print("="*70)

def train_xor_network(model, X, y, learning_rate=0.1, epochs=1000):
    """
    Train XOR network using YOUR autograd system!
    
    This uses gradient descent with YOUR automatic differentiation.
    """
    print("\nğŸš€ Training XOR Network with YOUR TinyTorch autograd!")
    print(f"   Learning rate: {learning_rate}")
    print(f"   Epochs: {epochs}")
    print(f"   YOUR Module 06 autograd computes all gradients!")
    
    # Convert to YOUR Tensor format
    X_tensor = Tensor(X)  # Module 02: YOUR Tensor!
    y_tensor = Tensor(y.reshape(-1, 1))  # Module 02: YOUR data structure!
    
    for epoch in range(epochs):
        # Forward pass using YOUR network
        predictions = model.forward(X_tensor)  # YOUR multi-layer forward!
        
        # Binary cross-entropy loss
        loss_value = np.mean(-y_tensor.data * np.log(predictions.data + 1e-8) - 
                            (1 - y_tensor.data) * np.log(1 - predictions.data + 1e-8))
        loss = Tensor([loss_value])
        
        # Backward pass using YOUR autograd
        loss.backward()  # Module 06: YOUR automatic differentiation!
        
        # Update parameters using gradient descent
        for param in model.parameters():
            if param.grad is not None:
                param.data -= learning_rate * param.grad
                param.grad = None
        
        # Progress updates
        if epoch % 100 == 0 or epoch == epochs - 1:
            accuracy = np.mean((predictions.data > 0.5) == y_tensor.data) * 100
            print(f"   Epoch {epoch:4d}: Loss = {loss_value:.4f}, "
                  f"Accuracy = {accuracy:.1f}% (YOUR training!)")
    
    return model

def test_xor_solution(model, show_examples=True):
    """Test YOUR XOR solution on the classic 4 points."""
    print("\nğŸ§ª Testing YOUR XOR Network on Classic Examples:")
    print("   " + "â”€"*45)
    
    # The classic XOR test cases
    test_cases = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)
    expected = np.array([0, 1, 1, 0])
    
    # Test with YOUR network
    X_test = Tensor(test_cases)  # Module 02: YOUR Tensor!
    predictions = model.forward(X_test)  # YOUR forward pass!
    predicted_classes = (predictions.data > 0.5).astype(int).flatten()
    
    # Display results
    print("   â”‚ x1 â”‚ x2 â”‚ Expected â”‚ YOUR Output â”‚ âœ“/âœ— â”‚")
    print("   â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤")
    
    all_correct = True
    for i in range(4):
        x1, x2 = test_cases[i]
        exp = expected[i]
        pred = predicted_classes[i]
        prob = predictions.data[i, 0]
        status = "âœ“" if pred == exp else "âœ—"
        if pred != exp:
            all_correct = False
        
        print(f"   â”‚ {x1:.0f}  â”‚ {x2:.0f}  â”‚    {exp}     â”‚  {pred} ({prob:.3f})  â”‚  {status}  â”‚")
    
    print("   " + "â”€"*45)
    
    if all_correct:
        print("   ğŸ‰ SUCCESS! YOUR network solved XOR perfectly!")
        print("   Hidden layers enabled non-linear learning!")
    else:
        print("   ğŸ”„ Network still training... (try more epochs)")
    
    return all_correct

def analyze_xor_systems(model):
    """Analyze YOUR XOR solution from an ML systems perspective."""
    print("\nğŸ”¬ SYSTEMS ANALYSIS of YOUR XOR Network:")
    
    # Parameter count
    total_params = sum(p.data.size for p in model.parameters())
    
    print(f"   Parameters: {total_params} weights (YOUR Linear layers)")
    print(f"   Architecture: 2 â†’ 4 â†’ 1 (minimal for XOR)")
    print(f"   Key innovation: Hidden layer creates non-linear features")
    print(f"   Memory: {total_params * 4} bytes (float32)")
    
    print("\n   ğŸ›ï¸ Historical Impact:")
    print("   â€¢ 1969: Minsky showed single layers CAN'T solve XOR")
    print("   â€¢ 1970s: 'AI Winter' - neural networks abandoned")  
    print("   â€¢ 1980s: Backprop + hidden layers solved it (YOUR approach!)")
    print("   â€¢ Today: Deep networks with many hidden layers power AI")
    
    print("\n   ğŸ’¡ Why This Matters:")
    print("   â€¢ YOUR hidden layer transforms the feature space")
    print("   â€¢ Non-linear activation (ReLU) is ESSENTIAL")
    print("   â€¢ This principle scales to ImageNet, GPT, etc.")
    print("   â€¢ Modern AI = deeper versions of YOUR XOR network!")

def main():
    """Demonstrate the XOR solution using YOUR TinyTorch system!"""
    
    parser = argparse.ArgumentParser(description='XOR Problem 1969')
    parser.add_argument('--test-only', action='store_true',
                       help='Test architecture without training')
    parser.add_argument('--epochs', type=int, default=1000,
                       help='Number of training epochs')
    parser.add_argument('--visualize', action='store_true', default=True,
                       help='Show XOR visualization')
    args = parser.parse_args()
    
    print("ğŸ¯ XOR PROBLEM 1969 - Breaking the Linear Barrier!")
    print("   Historical significance: Proved need for hidden layers")
    print("   YOUR achievement: Solving 'impossible' problem with YOUR network")
    print("   Components used: YOUR Tensor + Linear + ReLU + Autograd")
    
    # Show why XOR is special
    if args.visualize:
        visualize_xor_problem()
    
    # Step 1: Get XOR data
    print("\nğŸ“Š Generating XOR dataset...")
    data_manager = DatasetManager()
    X, y = data_manager.get_xor_data(num_samples=1000)
    print(f"   Generated {len(X)} XOR samples with noise")
    
    # Step 2: Create network with YOUR components
    model = XORNetwork(input_size=2, hidden_size=4, output_size=1)
    
    if args.test_only:
        print("\nğŸ§ª ARCHITECTURE TEST MODE")
        test_input = Tensor(X[:4])  # Module 02: YOUR Tensor!
        test_output = model.forward(test_input)  # YOUR architecture!
        print(f"âœ… Forward pass successful! Output shape: {test_output.data.shape}")
        print("âœ… YOUR multi-layer network works!")
        return
    
    # Step 3: Train using YOUR autograd
    model = train_xor_network(model, X, y, epochs=args.epochs)
    
    # Step 4: Test on classic XOR cases
    solved = test_xor_solution(model)
    
    # Step 5: Systems analysis
    analyze_xor_systems(model)
    
    print("\nâœ… SUCCESS! XOR Milestone Complete!")
    print("\nğŸ“ What YOU Accomplished:")
    print("   â€¢ YOU solved the 'impossible' XOR problem")
    print("   â€¢ YOUR hidden layer creates non-linear decision boundaries")
    print("   â€¢ YOUR ReLU activation enables feature learning")
    print("   â€¢ YOUR autograd trains multi-layer networks")
    
    print("\nğŸš€ Next Steps:")
    print("   â€¢ Continue to MNIST MLP after Module 08 (Training)")
    print("   â€¢ YOUR XOR solution scales to real vision problems!")
    print("   â€¢ Hidden layers principle powers all modern deep learning!")

if __name__ == "__main__":
    main()