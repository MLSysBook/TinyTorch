\documentclass[10pt,twocolumn]{article}

% Adjust line spacing for better readability
\renewcommand{\baselinestretch}{1.05}

% Essential packages
\usepackage{fontspec}
\setmainfont{Palatino}[
  Ligatures=TeX,
  Numbers=OldStyle,
  UprightFont=*,
  ItalicFont=* Italic,
  BoldFont=* Bold,
  BoldItalicFont=* Bold Italic
]
\setsansfont{Helvetica Neue}[
  Scale=MatchLowercase,
  UprightFont=*,
  BoldFont=* Bold
]
\setmonofont{Menlo}[
  Scale=MatchLowercase
]
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{listings}
\usepackage[round]{natbib}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{emoji}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning}
\usepackage{subcaption}
\usepackage{titlesec}
\usepackage{fancyhdr}

% Section spacing - tighter for two-column format
\titlespacing*{\section}{0pt}{0.8\baselineskip}{0.5\baselineskip}
\titlespacing*{\subsection}{0pt}{0.6\baselineskip}{0.4\baselineskip}
\titlespacing*{\subsubsection}{0pt}{0.5\baselineskip}{0.3\baselineskip}

% Page geometry
\usepackage[
  letterpaper,
  top=0.75in,
  bottom=1in,
  left=0.75in,
  right=0.75in,
  columnsep=0.25in
]{geometry}

% Header configuration - white paper style with visual accents
\pagestyle{fancy}
\fancyhf{} % Clear all headers and footers

% Define accent color (orange-red to match fire theme)
\definecolor{accentcolor}{RGB}{255,87,34} % Vibrant orange-red

% Custom header with colored accent bar and subtle separator
% Equal spacing before and after the line - more space after to avoid overlap
\renewcommand{\headrulewidth}{0.5pt} % Thin separator line
\renewcommand{\headrule}{%
  \vspace{4pt}%
  \textcolor{gray!20}{\hrule width\headwidth height\headrulewidth}%
  \vspace*{8pt}%
}

% Create custom header with accent bar
\fancyhead[L]{%
  \raisebox{-0.3\height}{%
    \color{accentcolor}\rule{3pt}{14pt}%
  }%
  \hspace{10pt}%
  \raisebox{0pt}{%
    \fontsize{9}{11}\selectfont%
    \textbf{Tiny\emoji{fire}Torch}%
  }%
}
\fancyhead[R]{%
  \fontsize{8.5}{10}\selectfont%
  \textcolor{gray!60}{V. Janapa Reddi}%
}

% Footer with centered page number - cleaner white paper style
\fancyfoot[C]{%
  \fontsize{8.5}{10}\selectfont%
  \textcolor{gray!40}{\thepage}%
}

% Adjust header height and spacing for white paper aesthetics
% Increase headsep for more space after the header line
\setlength{\headheight}{16pt}
\setlength{\headsep}{20pt}

% First page style - completely clean, no header
\fancypagestyle{plain}{%
  \fancyhf{}%
  \renewcommand{\headrulewidth}{0pt}%
  \fancyhead{}%
  \fancyfoot{}%
}

% Python code highlighting
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.97,0.97,0.97}

\lstdefinestyle{pythonstyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    language=Python,
    frame=single,
    rulecolor=\color{black!30},
    xleftmargin=10pt,
    xrightmargin=5pt,
    aboveskip=8pt,
    belowskip=8pt
}

\lstset{style=pythonstyle}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue,
    pdftitle={TinyTorch: A Framework for Building ML Systems from Scratch},
    pdfauthor={Vijay Janapa Reddi}
}

% Title formatting - modern, clean design
\usepackage{titling}

% Remove the horizontal rules for a cleaner look
\pretitle{\begin{center}\vspace*{-0.3em}}
\posttitle{\vspace{0.8em}\end{center}}

% Title and authors - improved typography
\title{
  \fontsize{26}{32}\selectfont\bfseries
  Tiny\emoji{fire}Torch\\[0.4em]
  \fontsize{14}{17}\selectfont\normalfont
  A Framework for Learning ML Systems from Scratch,\\[0.15em]
  \fontsize{14}{17}\selectfont\normalfont
  from Tensors to Systems
}
\author{
  \fontsize{11}{14}\selectfont
  Vijay Janapa Reddi\\
  Harvard University\\
  Cambridge, MA, USA\\
}

\date{}

\begin{document}

% Title page - no header
\thispagestyle{plain}
\maketitle

% Abstract - REVISED: Reframed as design contribution
\begin{abstract}
\small
Machine learning education traditionally focuses on using frameworks like PyTorch and TensorFlow, leaving students unprepared for the systems-level challenges of ML engineering: memory management, performance optimization, and production deployment. We present \textbf{TinyTorch}, an educational framework introducing three novel pedagogical patterns for teaching ML systems from first principles: (1) \textbf{progressive disclosure via monkey-patching}---dormant features in the \texttt{Tensor} class activate across modules, enabling early interface exposure while managing cognitive load, (2) \textbf{systems-first integration}---memory profiling, FLOPs analysis, and computational complexity taught from Module 01 rather than deferred to advanced electives, and (3) \textbf{historical milestone validation}---students recreate 70 years of ML breakthroughs (1957 Perceptron $\rightarrow$ 2024 production systems) using exclusively their own code for objective correctness validation and intrinsic motivation. The 20-module curriculum (60--80 hours) builds a production-organized package with automated assessment infrastructure. \emph{This paper presents pedagogical patterns and curriculum design}---not empirical evaluation---with classroom validation planned for Fall 2025. The complete open-source framework enables educators to adopt these patterns and researchers to conduct empirical studies.
\end{abstract}

\noindent\textbf{Keywords:} machine learning education, systems education, educational frameworks, ML engineering, progressive disclosure, autograd, constructionism, design-based research

% Main content
\section{Introduction}

Machine learning systems have emerged as a distinct discipline requiring specialized education, analogous to how computer engineering emerged from the intersection of computer science and electrical engineering. Just as computers became sufficiently complex to warrant dedicated curricula teaching both hardware and software integration, ML systems now demand curricula that integrate algorithmic understanding with systems thinking---memory management, computational complexity, performance optimization, and production deployment. This integration is not merely additive but transformative: students who understand \emph{how} frameworks work internally can debug production failures, optimize deployment pipelines, and make architectural decisions that algorithm-focused education alone cannot provide.

\textbf{What Students Learn: From Framework Users to Framework Engineers.}
Traditional ML education teaches students to use frameworks as black boxes. In PyTorch, students write high-level code without understanding internals. TinyTorch inverts this: students build the internals themselves. \Cref{fig:code-comparison} illustrates this transformation---students who complete TinyTorch understand \emph{how} frameworks work internally, enabling them to debug production systems, optimize deployments, and make architectural decisions.

\begin{figure*}[t]
\centering
\begin{subfigure}[b]{0.48\textwidth}
\centering
\begin{lstlisting}[basicstyle=\ttfamily\footnotesize,frame=single]
import torch.nn as nn
import torch.optim as optim

model = nn.Linear(784, 10)
optimizer = optim.Adam(
    model.parameters(), lr=0.001)
loss_fn = nn.CrossEntropyLoss()

for epoch in range(10):
    for x, y in dataloader:
        pred = model(x)
        loss = loss_fn(pred, y)
        loss.backward()  # Magic?
        optimizer.step()  # How?
\end{lstlisting}
\subcaption{PyTorch: Using frameworks as black boxes}
\label{lst:pytorch-usage}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
\centering
\begin{lstlisting}[basicstyle=\ttfamily\footnotesize,frame=single]
class Linear:
    def __init__(self, in_features, out):
        self.weight = Tensor.randn(out, in_features)
        self.bias = Tensor.zeros(out)
    
    def forward(self, x):
        return x @ self.weight.T + self.bias

class Adam:
    def __init__(self, params, lr=0.001):
        self.params = params
        self.lr = lr
        # 3× memory: weights + momentum + variance
        self.m = [Tensor.zeros_like(p) 
                  for p in params]
        self.v = [Tensor.zeros_like(p) 
                  for p in params]
    
    def step(self):
        for p, m, v in zip(self.params, self.m, self.v):
            m = 0.9*m + 0.1*p.grad
            v = 0.999*v + 0.001*p.grad**2
            p.data -= self.lr * m / (v.sqrt()+1e-8)
\end{lstlisting}
\subcaption{TinyTorch: Understanding internals}
\label{lst:tinytorch-build}
\end{subfigure}
\caption{Learning progression: From framework users to framework engineers. (a) Traditional ML education: students use PyTorch's high-level APIs without understanding internals. (b) TinyTorch education: students build the internals themselves, learning that Adam uses 3× parameter memory (weights + momentum + variance), understanding computational complexity, and developing systems thinking.}
\label{fig:code-comparison}
\end{figure*}

Most machine learning courses teach students to use frameworks, not understand them. Traditional curricula focus on calling \texttt{model.fit()} and \texttt{loss.backward()} without grasping what happens when these methods execute. This knowledge gap becomes critical when ML models move to production: engineers must optimize memory usage, profile computational bottlenecks, and debug gradient flows---skills rarely taught in algorithm-focused curricula.

Consider two students who have completed traditional ML coursework. Both can derive backpropagation equations and explain gradient descent convergence. Both have trained convolutional networks on MNIST using PyTorch. Yet when production deployment demands answers to systems questions---``How much VRAM does this model require?'' ``Why does batch size 32 work but batch size 64 causes OOM?'' ``How many FLOPs for inference on this architecture?''---they struggle. The algorithmic knowledge they possess proves insufficient for ML engineering as practiced in industry.

This gap between framework users and systems engineers reflects a deeper pedagogical challenge. Traditional ML curricula treat systems concerns---memory management, computational complexity, performance optimization---as advanced topics relegated to separate ``ML Systems'' electives that students encounter, if at all, in their final undergraduate year. By then, students have formed mental models that divorce ML algorithms from their computational reality. They understand gradients abstractly but not gradient memory footprint. They know attention mechanisms mathematically but not their $O(N^2)$ scaling implications.

Can we teach ML as systems engineering from first principles? Can students learn memory profiling alongside tensor operations, computational complexity alongside convolutions, optimization trade-offs alongside model training? We answer these questions affirmatively through TinyTorch: a complete 20-module curriculum where students build every component of a production ML framework from scratch---from tensors to transformers to optimization---with systems awareness embedded from Module 01 onwards.


\textbf{How Modules Connect: Building Systems Piece-by-Piece.}
TinyTorch follows the pedagogical model of compiler courses: students build a complete system module-by-module, where each component connects to the next. \Cref{fig:module-flow} illustrates how modules integrate---tensors (01) enable activations (02) and layers (03), which feed into autograd (05), which powers optimizers (06) and training (07). This piece-by-piece construction creates systems thinking through direct experience of component integration, mirroring how parsers connect to instruction selection, which connects to register allocation in compiler courses.

\begin{figure}[t]
\centering
\small
\begin{tikzpicture}[node distance=0.8cm and 1.2cm, every node/.style={font=\scriptsize}]
% Foundation tier
\node[draw,rectangle,fill=blue!20] (M01) {01 Tensor};
\node[draw,rectangle,fill=blue!20,below=of M01] (M02) {02 Activations};
\node[draw,rectangle,fill=blue!20,below=of M02] (M03) {03 Layers};
\node[draw,rectangle,fill=blue!20,below=of M03] (M04) {04 Losses};
\node[draw,rectangle,fill=orange!30,below=of M04] (M05) {05 Autograd};
\node[draw,rectangle,fill=blue!20,below=of M05] (M06) {06 Optimizers};
\node[draw,rectangle,fill=blue!20,below=of M06] (M07) {07 Training};

% Architecture tier
\node[draw,rectangle,fill=purple!20,right=of M01] (M08) {08 DataLoader};
\node[draw,rectangle,fill=purple!20,below=of M08] (M09) {09 CNNs};
\node[draw,rectangle,fill=purple!20,below=of M09] (M10) {10 Tokenization};
\node[draw,rectangle,fill=purple!20,below=of M10] (M11) {11 Embeddings};
\node[draw,rectangle,fill=purple!20,below=of M11] (M12) {12 Attention};
\node[draw,rectangle,fill=purple!20,below=of M12] (M13) {13 Transformers};

% Optimization tier
\node[draw,rectangle,fill=green!20,right=of M08] (M14) {14 Profiling};
\node[draw,rectangle,fill=green!20,below=of M14] (M15) {15 Quantization};
\node[draw,rectangle,fill=green!20,below=of M15] (M16) {16 Compression};
\node[draw,rectangle,fill=green!20,below=of M16] (M17) {17 Memoization};
\node[draw,rectangle,fill=green!20,below=of M17] (M18) {18 Acceleration};
\node[draw,rectangle,fill=green!20,below=of M18] (M19) {19 Benchmarking};
\node[draw,rectangle,fill=red!30,below=of M19] (M20) {20 Olympics};

% Arrows - Foundation connections
\draw[->] (M01) -- (M02);
\draw[->] (M01) -- (M03);
\draw[->] (M01) -- (M04);
\draw[->] (M01) -- (M08);
\draw[->] (M02) -- (M03);
\draw[->] (M03) -- (M04);
\draw[->] (M01) -- (M05);
\draw[->] (M05) -- (M06);
\draw[->] (M06) -- (M07);
\draw[->] (M03) -- (M06);
\draw[->] (M04) -- (M07);

% Architecture connections
\draw[->] (M08) -- (M09);
\draw[->] (M08) -- (M10);
\draw[->] (M01) -- (M09);
\draw[->] (M03) -- (M09);
\draw[->] (M05) -- (M09);
\draw[->] (M10) -- (M11);
\draw[->] (M01) -- (M11);
\draw[->] (M11) -- (M12);
\draw[->] (M03) -- (M12);
\draw[->] (M05) -- (M12);
\draw[->] (M12) -- (M13);
\draw[->] (M02) -- (M13);
\draw[->] (M11) -- (M13);

% Optimization connections
\draw[->] (M14) -- (M15);
\draw[->] (M14) -- (M16);
\draw[->] (M14) -- (M17);
\draw[->] (M15) -- (M18);
\draw[->] (M16) -- (M18);
\draw[->] (M17) -- (M18);
\draw[->] (M18) -- (M19);
\draw[->] (M19) -- (M20);

% Cross-tier connections
\draw[->,dashed] (M07) -- (M09);
\draw[->,dashed] (M07) -- (M13);
\draw[->,dashed] (M09) -- (M14);
\draw[->,dashed] (M13) -- (M14);

\end{tikzpicture}
\caption{Module dependency flow: How components connect across tiers. Foundation modules (blue) enable architectures (purple), which are optimized (green), culminating in the capstone competition (red). Dotted lines show cross-tier integration.}
\label{fig:module-flow}
\end{figure}

TinyTorch serves a specific pedagogical niche: transitioning from framework \emph{users} to framework \emph{engineers}. The curriculum targets students who have completed introductory ML courses and want to understand framework internals, those planning ML systems research or infrastructure engineering careers, or practitioners who need to debug production ML systems effectively. Conversely, students who haven't trained neural networks should first complete courses like CS231n or fast.ai; those needing immediate GPU/distributed training skills are better served by PyTorch tutorials; and learners preferring project-based application building over internals understanding will find high-level frameworks more appropriate.

The curriculum supports flexible pacing to accommodate diverse student contexts: intensive completion (weeks), semester integration (regular coursework), or self-paced professional development. TinyTorch positions as a complement to algorithm-focused ML courses: taken \emph{after} CS231n to understand systems, \emph{before} advanced ML systems courses to build foundation, or \emph{parallel to} production ML roles to develop debugging skills.

TinyTorch makes three core pedagogical innovations that distinguish it from existing educational approaches:

\textbf{1. Progressive Disclosure via Monkey-Patching.}
Students encounter a single \texttt{Tensor} class throughout the curriculum, but its capabilities expand progressively through runtime enhancement. Module 01 introduces \texttt{Tensor} with dormant gradient features (\texttt{.requires\_grad}, \texttt{.grad}, \texttt{.backward()}) that remain inactive until Module 05, when \texttt{enable\_autograd()} monkey-patches the class---dynamically modifying methods at runtime---to activate automatic differentiation (\Cref{lst:progressive}). This design teaches real framework evolution patterns---matching PyTorch 2.0's enhanced Tensor design---while managing cognitive load through phased complexity introduction.

\begin{lstlisting}[caption={Progressive disclosure pattern},label=lst:progressive,float=t]
# Module 01: Dormant features
class Tensor:
    def __init__(self, data, requires_grad=False):
        self.data = np.array(data)
        self.requires_grad = requires_grad  # Dormant
        self.grad = None  # Dormant

    def backward(self):
        pass  # No-op until Module 05

# Module 05: Activation via monkey-patching
enable_autograd()  # Enhances Tensor class
# Now gradients work throughout framework
\end{lstlisting}

Unlike educational frameworks that introduce separate classes for gradients or use deprecated patterns like PyTorch's old Variable wrapper, progressive disclosure maintains a single mental model while teaching production framework architecture.

\textbf{2. Systems-First Integration.}
Systems thinking---reasoning about memory, complexity, and performance as first-class concerns---is embedded throughout the curriculum, not deferred to advanced electives. Every module integrates memory profiling, computational complexity analysis, and performance reasoning as foundational concepts. Module 01 introduces \texttt{memory\_footprint()} methods before matrix multiplication. Module 09 implements convolution with seven explicit nested loops that make $O(B \times C_{\text{out}} \times H_{\text{out}} \times W_{\text{out}} \times C_{\text{in}} \times K_h \times K_w)$ complexity visible and countable. Module 17 quantizes models from FP32 to INT8 while measuring the accuracy-memory-speed triangle. Students calculate ``How much VRAM for this model?'' in Module 03, not as optional ``deployment course'' content but as integral to understanding neural network layers.

This systems-first approach transforms student mental models: they shift from ``CNNs detect edges'' (algorithmic thinking) to ``CNNs perform 86M operations per forward pass'' (systems thinking). Memory profiling becomes reflexive---when implementing new layers, students automatically ask ``How much memory do parameters require? What about activations?'' These questions emerge naturally because systems concerns are woven into every module, not isolated in separate courses.

This approach rejects the traditional separation of algorithmic understanding from systems awareness. Students cannot complete tensor operations without analyzing memory, cannot implement convolution without counting FLOPs, cannot choose optimizers without understanding that Adam requires 3$\times$ \emph{parameter} memory compared to SGD (note: activation memory typically dominates, but the parameter memory difference matters for optimizer state management).

\textbf{3. Historical Milestone Validation.}
Students validate implementations by recreating 70 years of ML history: Rosenblatt's 1957 Perceptron (Module 03), Minsky's XOR challenge solution (Module 05), Rumelhart's 1986 MNIST classifier (Module 07), LeCun's 1998 CIFAR-10 CNN achieving 75\%+ accuracy (Module 09), Vaswani's 2017 transformer for text generation (Module 13), and modern optimization competitions (Module 20). These are not toy demonstrations but historically significant achievements rebuilt entirely with student-written code using only NumPy.

Each milestone serves dual purposes: proof of implementation correctness (if you match historical performance, your code works) and motivation through authentic accomplishment. The milestones create concrete capability checkpoints that validate cumulative understanding---broken implementations produce random accuracy, revealing gaps immediately.

\paragraph{Contributions}

This paper makes the following contributions:

\begin{enumerate}
\item \textbf{Theoretical Framework}: Application of constructionism, productive failure, and threshold concepts to ML systems education, demonstrating how established learning theories guide design choices (\Cref{sec:related}).

\item \textbf{Production-Aligned Learning Path}: 20-module curriculum spanning basic tensors through modern architectures (CNNs, transformers, quantization) with explicit connections to PyTorch and TensorFlow patterns (\Cref{sec:curriculum}).

\item \textbf{Progressive Disclosure Pattern}: A pedagogical technique using monkey-patching to reveal framework complexity gradually while maintaining a single mental model, designed to manage cognitive load by partitioning element interactivity across modules (\Cref{sec:progressive}). Empirical validation of cognitive load reduction is planned for Fall 2025 deployment.

\item \textbf{Systems-First Curriculum Design}: Integration of memory profiling, computational complexity, and performance analysis from foundational modules through advanced topics, replacing the traditional separation of algorithmic and systems courses (\Cref{sec:systems}).

\item \textbf{Replicable Educational Artifact}: Complete open-source curriculum design enabling educator adoption and empirical evaluation by researchers (Throughout).
\end{enumerate}

\emph{Important scope note}: This paper presents a \textbf{design contribution}---pedagogical patterns, curriculum architecture, and theoretical grounding---not an empirical evaluation of learning outcomes. We provide the design rationale and implementation; rigorous classroom evaluation is planned for Fall 2025 deployment (\Cref{sec:discussion}).

\paragraph{Positioning and Broader Impact}

TinyTorch complements existing educational frameworks by addressing different pedagogical goals. Karpathy's micrograd \citep{karpathy2022micrograd} excels at teaching autograd mechanics through 200 elegant lines but intentionally stops at automatic differentiation. Cornell's MiniTorch provides comprehensive framework implementation but focuses less on systems thinking integration. Zhang et al.'s d2l.ai \citep{zhang2021dive} offers excellent theory-practice balance but uses PyTorch/TensorFlow rather than having students build frameworks. Fast.ai \citep{howard2020fastai} prioritizes rapid application development using high-level APIs, explicitly avoiding implementation details.

TinyTorch occupies complementary pedagogical space: complete framework construction (20 modules from tensors to transformers to optimization) with systems awareness embedded throughout. Where micrograd teaches autograd deeply, TinyTorch continues to CNNs, transformers, and production optimization. Where d2l.ai teaches ML comprehensively using existing frameworks, TinyTorch teaches framework internals through construction.

\textbf{When to use TinyTorch}:
\begin{itemize}
\item After completing fast.ai (transition from user to engineer)
\item Before CS231n (foundation for understanding PyTorch deeply)
\item As standalone systems course (complement algorithm-focused ML)
\item For students pursuing ML systems research or infrastructure roles
\end{itemize}

The broader impact extends beyond individual student learning. For CS educators, TinyTorch provides replicable curriculum patterns worth empirical investigation. For ML practitioners, it offers framework internals education that may transfer to PyTorch/TensorFlow debugging and optimization. For CS education researchers, it presents novel pedagogical patterns---progressive disclosure via monkey-patching, systems-first integration, constructionist framework building---worth studying empirically.

\paragraph{Paper Organization}

The remainder of this paper proceeds as follows. \Cref{sec:related} positions TinyTorch relative to existing educational ML frameworks and presents the theoretical framework grounding our design (constructionism, productive failure, cognitive load theory). \Cref{sec:curriculum} describes the curriculum architecture: 4-phase learning progression with explicit learning objectives. \Cref{sec:progressive} presents the progressive disclosure pattern with complete code examples. \Cref{sec:systems} demonstrates systems-first integration through memory profiling and FLOPs analysis. \Cref{sec:discussion} discusses design insights, honest limitations (including GPU/distributed training omission), and concrete plans for empirical validation. \Cref{sec:conclusion} concludes with implications for ML education.

\section{Related Work}
\label{sec:related}

TinyTorch builds on educational ML frameworks, online learning resources, and established learning theory from cognitive science and education research.

\subsection{Educational ML Frameworks}

\textbf{micrograd} \citep{karpathy2022micrograd} pioneered the ``build-from-scratch'' educational approach with an elegant 200-line implementation of scalar-valued automatic differentiation. Its minimalist design brilliantly demystifies backpropagation mechanics. However, micrograd intentionally limits scope to autograd fundamentals---students learn how gradients flow through computation graphs but do not encounter tensor operations, convolutional layers, or production deployment patterns. TinyTorch starts where micrograd ends, using autograd as foundation to build complete ML systems.

\textbf{MiniTorch} \citep{schneider2020minitorch} provides comprehensive module-based curriculum covering tensors, neural networks, and GPU acceleration. Its structured progression and rigorous testing demonstrate effective pedagogical scaffolding. MiniTorch incorporates NumPy for tensor operations and CUDA for GPU support, enabling performance optimization exercises. While this teaches acceleration techniques, it abstracts away pure Python memory management. TinyTorch adopts a pure Python constraint, treating performance limitations as pedagogically valuable---students viscerally understand \emph{why} production frameworks use C++ kernels when their pure Python convolutions run 100$\times$ slower.

\textbf{tinygrad} \citep{hotz2023tinygrad} pursues production viability through aggressive optimization. However, tinygrad's focus on optimization over pedagogy requires students to navigate C++ extensions and GPU programming. TinyTorch inverts this priority: we sacrifice performance for educational clarity, ensuring every component remains transparent and modifiable in pure Python.

\textbf{Why Build a New Framework Rather Than Extending Existing Ones?}
Existing educational frameworks face fundamental limitations that prevent the integrated, systems-first learning experience TinyTorch provides. Most frameworks are either outdated (using deprecated patterns like PyTorch's old Variable wrapper), unsupported (lacking maintenance and community), or lack systems components entirely (focusing on algorithms without memory profiling, complexity analysis, or production considerations). More fundamentally, TinyTorch follows the pedagogical model of traditional compiler courses: students build a complete system module-by-module, where each component connects to the next---parsers feed into instruction selection, which connects to register allocation, creating an integrated understanding of how systems work. This piece-by-piece construction, where everything ``clicks together,'' is what TinyTorch recreates for ML systems. Students don't just implement isolated components; they build a framework where tensors connect to autograd, which enables optimizers, which power training loops, creating systems thinking through direct experience of component integration.

\Cref{tab:frameworks} summarizes framework comparisons across key dimensions.

\begin{table}[t]
\centering
\caption{Educational ML framework comparison}
\label{tab:frameworks}
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
Framework & Scope & Systems & Pure & Assessment \\
          &       & Focus   & Python & \\
\midrule
micrograd & Autograd & Minimal & Yes & Manual \\
MiniTorch & Partial & Some & No & Yes \\
tinygrad & Full & High & No & No \\
\textbf{TinyTorch} & \textbf{Full} & \textbf{High} & \textbf{Yes} & \textbf{Yes} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{ML Education Resources}

\textbf{Dive into Deep Learning (d2l.ai)} \citep{zhang2021dive} represents the gold standard for comprehensive ML education, blending mathematical foundations with practical implementations. Its interactive notebooks enable immediate experimentation. However, d2l.ai necessarily relies on high-level framework APIs---students call \texttt{nn.Conv2d()} without understanding stride implementation or memory layout. TinyTorch complements d2l.ai by teaching students to \emph{build} the abstractions that d2l.ai uses.

\textbf{fast.ai} \citep{howard2020fastai} revolutionized ML education through its top-down ``code-first'' approach, prioritizing rapid time-to-first-model. This effectively democratizes ML by removing implementation barriers. TinyTorch serves a complementary audience: students who have \emph{used} high-level APIs through fast.ai and now want to understand \emph{how those APIs work internally}---the transition from practitioner to systems engineer.

\textbf{Machine Learning Systems} \citep{reddi2026mlsystems} provides comprehensive theoretical coverage of ML systems engineering, covering memory hierarchies, computational complexity, optimization techniques, and production deployment patterns. However, as a textbook focused on systems principles and design patterns, it lacks hands-on implementation exercises. TinyTorch serves as the practical companion to Machine Learning Systems: students read the theoretical foundations in the textbook and implement the concepts through TinyTorch's 20-module curriculum, bridging theory and practice. This integration enables students to understand \emph{why} systems optimizations matter (from the textbook) while experiencing \emph{how} they work (through TinyTorch implementation).

\textbf{Pedagogical Approach Spectrum}: Educational approaches exist on a spectrum from \textbf{top-down} (applications first, internals later) to \textbf{bottom-up} (internals first, applications later):

\begin{itemize}
\item \textbf{Top-Down (fast.ai)}: Start with working models, gradually reveal internals. Best for practitioners seeking immediate applicability.
\item \textbf{Middle Ground (CS231n + PyTorch)}: Teach algorithms with high-level frameworks. Best for traditional CS curriculum.
\item \textbf{Bottom-Up (TinyTorch)}: Build frameworks from scratch, understand internals before applications. Best for students transitioning to ML systems engineering roles.
\end{itemize}

\subsection{Learning Theory Framework}

TinyTorch's pedagogical design draws on established learning theories from cognitive science and education research.

\textbf{Constructionism} \citep{papert1980mindstorms}: Learning is most effective when students construct external artifacts. TinyTorch instantiates constructionist pedagogy through complete framework construction---students build working ML system from scratch, not just solve isolated exercises. The Tensor class serves as Papert's ``object to think with,'' enabling students to reason about gradient computation, memory management, and computational complexity through concrete implementation.

\textbf{Cognitive Load Theory} \citep{sweller1988cognitive}: Human working memory has limited capacity (4--7 elements). Presenting all tensor capabilities simultaneously (operations + gradients + memory profiling + broadcasting) exceeds this limit. Progressive disclosure partitions complexity across modules: Module 01 introduces tensor operations + memory (2 concepts), Modules 02--04 build on this foundation, Module 05 activates gradients (1 new concept, familiar interface). Each module introduces manageable complexity while respecting working memory constraints.

Future empirical work should measure cognitive load using dual-task methodology or self-report scales to validate this theoretical prediction (\Cref{sec:discussion}).

\textbf{Productive Failure} \citep{kapur2008productive}: Students benefit from productive struggle before instruction. TinyTorch's pure Python slowness creates productive failure: students implement convolution's 7 nested loops (Module 09) and experience frustrating performance before Module 18 introduces vectorization. This struggle makes optimization meaningful---students understand \emph{why} NumPy matters because they experienced pure Python's pain.

\textbf{Threshold Concepts} \citep{meyer2003threshold}: Certain concepts are transformative, troublesome, and irreversible. Automatic differentiation is a threshold concept---once students understand computational graphs and backpropagation, their view of neural networks fundamentally changes. Progressive disclosure addresses threshold concept pedagogy by making autograd visible early (dormant features) but activatable later (when students are ready for the conceptual transformation).

\textbf{Zone of Proximal Development} \citep{vygotsky1978mind}: Learning is most effective when learners are challenged slightly beyond current capability. Dormant features create productive tension: students see \texttt{requires\_grad} in Module 01 (awareness), wonder about its purpose (curiosity), and gain capability in Module 05 (mastery). This scaffolded progression is designed to maintain engagement while preventing cognitive overload.

\textbf{Spiral Curriculum} \citep{bruner1960process}: Complex topics should be revisited repeatedly with increasing sophistication. Students encounter \texttt{Tensor} in Module 01 (data + operations), Module 03 (layer parameters), Module 05 (gradient computation), Module 09 (spatial operations for CNNs), and Module 13 (attention mechanisms). Each revisit deepens understanding while maintaining the unified mental model of ``Tensor as core abstraction.''

\subsection{CS Education Research}

\citet{thompson2008bloom} adapted Bloom's taxonomy for CS education, emphasizing progression from knowledge recall through creation and evaluation. \citet{blank2019nbgrader} provides infrastructure for automated assessment. TinyTorch leverages both: students progress from implementing tensor operations (create) to analyzing memory footprints (analyze) to evaluating architectural tradeoffs (evaluate), with NBGrader providing immediate feedback through automated tests.

\textbf{Assessment Validity Note}: While NBGrader provides automated grading infrastructure, empirical validation is needed to ensure tests measure conceptual understanding rather than syntax correctness (\Cref{sec:discussion}).

\subsection{ML Systems Research}

TinyTorch's curriculum connects students to foundational research in ML systems, ensuring that educational implementations align with production realities and current research.

\textbf{Automatic Differentiation}: While TinyTorch teaches autograd through \citet{rumelhart1986learning}'s backpropagation, students should understand the broader AD landscape. \citet{baydin2018automatic} provide a comprehensive survey of automatic differentiation techniques in machine learning, contextualizing reverse-mode AD (used in PyTorch/TensorFlow) within the full spectrum of differentiation approaches. \citet{paszke2017automatic} describe PyTorch's specific autograd implementation, which TinyTorch's Module 05 mirrors pedagogically.

\textbf{Memory Optimization}: TinyTorch's systems-first approach introduces memory profiling early (Module 01), building toward advanced optimizations students will encounter in production. \citet{chen2016training} introduced gradient checkpointing---trading computation for memory by recomputing activations during backward passes rather than storing them---enabling training of models 10$\times$ larger. While TinyTorch defers this to future extensions (\Cref{subsec:future-work}), students who understand memory footprint calculation are prepared to grasp checkpointing's memory-compute tradeoff.

\textbf{Compiler Optimization}: Modern ML systems increasingly rely on compiler-based optimization. \citet{chen2018tvm} developed TVM, an automated end-to-end optimizing compiler for deep learning that performs operator fusion, memory planning, and hardware-specific code generation. TinyTorch's pure Python implementation intentionally avoids these optimizations to maintain pedagogical transparency, but students completing the curriculum should explore TVM to understand production deployment pipelines.

\textbf{Attention Mechanism Optimization}: TinyTorch's Module 12 teaches attention's $O(N^2)$ memory complexity through direct implementation. \citet{dao2022flashattention} introduced FlashAttention, achieving exact attention with $O(N)$ memory through IO-aware algorithms. This represents the type of systems-level optimization TinyTorch students are prepared to understand after experiencing the naive implementation's limitations.

\section{Curriculum Architecture}
\label{sec:curriculum}

Traditional ML education presents algorithms sequentially without revealing how components integrate into working systems. TinyTorch addresses this through a 4-phase curriculum architecture where students build a complete ML framework progressively, with each module enforcing prerequisite mastery.

\subsection{Prerequisites and Target Audience}

TinyTorch targets students ready to transition from framework users to framework engineers. The curriculum assumes intermediate Python proficiency---comfort with classes, functions, and NumPy array operations---alongside mathematical foundations in linear algebra (matrix multiplication, vectors) and basic calculus (derivatives, chain rule). Students should understand complexity analysis (Big-O notation) and basic algorithms. While prior ML coursework (CS229, CS231n equivalents) and data structures courses are helpful, they are not strictly required; motivated students can acquire these foundations concurrently.

The primary audience consists of junior and senior computer science undergraduates who have completed introductory ML courses and seek deeper systems understanding. Graduate students transitioning to ML systems research form a secondary audience, while self-learners with strong programming backgrounds represent a tertiary group. The curriculum's flexible pacing accommodates diverse contexts: intensive completion over weeks, semester integration within existing courses, or self-paced professional development.

\subsection{The 3-Tier Learning Journey + Olympics}

TinyTorch organizes modules into three progressive tiers plus a capstone competition (\Cref{tab:objectives}). Students cannot skip tiers: architectures require foundation mastery, optimization demands training system understanding. The tiers mirror ML systems engineering practice: foundation (core ML mechanics), architectures (domain-specific models), optimization (production deployment), culminating in the Torch Olympics (competitive systems engineering).

\begin{table*}[t]
\centering
\caption{Module-by-module ML and Systems concepts (systems-first integration)}
\label{tab:objectives}
\small
\begin{tabular}{@{}lllp{5cm}p{5cm}@{}}
\toprule
Mod & Tier & Module Name & ML Concept & Systems Concept \\
\midrule
\multicolumn{5}{l}{\textbf{�� Foundation Tier (01-07)}} \\
01 & Fnd & Tensor & Multidimensional arrays, broadcasting & Memory footprint (nbytes), FP32 storage \\
02 & Fnd & Activations & ReLU, Sigmoid, Softmax & Numerical stability (exp overflow), vectorization \\
03 & Fnd & Layers & Linear, parameter initialization & Parameter memory vs activation memory \\
04 & Fnd & Losses & Cross-entropy, MSE & Stability (log(0) handling), gradient flow \\
05 & Fnd & Autograd & Computational graphs, backprop & Gradient memory (3× params for Adam) \\
06 & Fnd & Optimizers & SGD, Momentum, Adam & Memory-speed tradeoffs, update rules \\
07 & Fnd & Training Loop & Epoch/batch iteration & Forward/backward memory lifecycle \\
\midrule
\multicolumn{5}{l}{\textbf{��️ Architecture Tier (08-13)}} \\
08 & Arch & DataLoader & Batching, shuffling, augmentation & CPU-bound preprocessing, memory pinning \\
09 & Arch & Spatial (CNNs) & Conv2d, kernels, strides, pooling & $O(C_{out} \times H \times W \times C_{in} \times K^2)$ complexity \\
10 & Arch & Tokenization & BPE, vocabulary, encoding & Vocabulary management, OOV handling \\
11 & Arch & Embeddings & Token/position embeddings & Lookup tables, gradient through indices \\
12 & Arch & Attention & Scaled dot-product attention & $O(N^2)$ memory scaling, sequence length impact \\
13 & Arch & Transformers & Multi-head, encoder/decoder & Quadratic memory, KV caching strategies \\
\midrule
\multicolumn{5}{l}{\textbf{⏱️ Optimization Tier (14-19)}} \\
14 & Opt & Profiling & Time, memory, FLOPs analysis & Bottleneck identification, measurement overhead \\
15 & Opt & Quantization & INT8, dynamic/static quant & 4× model size reduction, accuracy-speed tradeoff \\
16 & Opt & Compression & Pruning, distillation & 10× model shrinkage, minimal accuracy loss \\
17 & Opt & Memoization & KV-cache for transformers & 10-100× inference speedup via caching \\
18 & Opt & Acceleration & Vectorization, parallelization & 10-100× speedup via NumPy optimization \\
19 & Opt & Benchmarking & Statistical testing, comparisons & Rigorous performance measurement \\
\midrule
\multicolumn{5}{l}{\textbf{�� Torch Olympics (20)}} \\
20 & Capstone & Torch Olympics & Complete production system & MLPerf-style competition, leaderboard \\
\bottomrule
\end{tabular}
\end{table*}

\begin{lstlisting}[caption={Tensor with memory profiling from Module 01},label=lst:tensor-memory,float=t]
class Tensor:
    def __init__(self, data):
        self.data = np.array(data, dtype=np.float32)
        self.shape = self.data.shape

    def memory_footprint(self):
        """Calculate exact memory in bytes"""
        return self.data.nbytes

    def __matmul__(self, other):
        if self.shape[-1] != other.shape[0]:
            raise ValueError(
                f"Shape mismatch: {self.shape} @ {other.shape}"
            )
        return Tensor(self.data @ other.data)
\end{lstlisting}

\textbf{Tier 1: Foundation (Modules 01--07).}
Students build the complete mathematical core that makes neural networks learn. Systems thinking begins immediately---Module 01 introduces \texttt{memory\_footprint()} before matrix multiplication (\Cref{lst:tensor-memory}), making memory a first-class concept. The tier progresses from tensors (01) through activations (02), layers (03), and losses (04) to automatic differentiation (05)---where dormant gradient features activate through progressive disclosure (\Cref{sec:progressive}). Students implement optimizers (06), discovering memory differences through direct measurement (Adam~\citep{kingma2014adam} requires 3$\times$ parameter memory: weights + momentum + variance). The training loop (07) integrates all components. By tier completion, students recreate three historical milestones: \citet{rosenblatt1958perceptron}'s Perceptron, Minsky and Papert's XOR solution proving hidden layers enable non-linear learning, and \citet{rumelhart1986learning}'s backpropagation enabling MLPs achieving 95\%+ on MNIST.

Students calculate memory before operations: ``A (1000, 1000) FP32 tensor requires 4MB. Matrix multiplication produces 4MB output. Total memory: 12MB (two inputs + output).'' This reasoning becomes automatic.

\textbf{Tier 2: Architectures (Modules 08--13).}
Students build modern neural architectures for computer vision and language understanding. Module 08 implements data loading infrastructure (batching, shuffling, efficient memory management). The tier then branches: the vision path (Module 09) implements Conv2d with seven explicit nested loops making $O(C_{out} \times H \times W \times C_{in} \times K^2)$ complexity visible and countable---students understand \emph{why} convolution is expensive before learning optimization. This enables Milestone 4 (1998 CNN Revolution): achieving 75\%+ accuracy on CIFAR-10~\citep{krizhevsky2009cifar} using \citet{lecun1998gradient}'s LeNet-inspired architectures, the north star achievement demonstrating framework correctness. The language path (Modules 10--13) progresses through tokenization (Byte-Pair Encoding), embeddings, attention ($O(N^2)$ memory scaling), and complete transformer blocks implementing \citet{vaswani2017attention}'s architecture. Students experience quadratic memory growth firsthand: doubling sequence length quadruples attention matrix memory. Milestone 5 (2017 Transformer Era) validates implementation through coherent text generation.

\textbf{Tier 3: Optimization (Modules 14--19).}
Students transition from ``models that train'' to ``systems that deploy,'' learning production ML engineering. Module 14 teaches profiling (time, memory, FLOPs)---measuring what matters. Quantization (15) demonstrates FP32$\rightarrow$INT8 achieving 4$\times$ compression with 1--2\% accuracy cost. Compression (16) applies pruning and distillation for 10$\times$ model shrinkage. Memoization (17) implements KV-cache for 10--100$\times$ transformer inference speedup. Acceleration (18) revisits Module 09's convolution, achieving 10--100$\times$ speedup through vectorization. Benchmarking (19) teaches rigorous performance measurement and statistical comparison.

\textbf{Torch Olympics (Module 20).}
The capstone competition challenges students to build a complete, production-optimized ML system. Inspired by the MLPerf benchmark suite~\citep{reddi2020mlperf}, students select any prior milestone (CIFAR-10~\citep{krizhevsky2009cifar} CNN, transformer text generation, or custom architecture) and optimize for production: achieve 10$\times$ faster inference, 4$\times$ smaller model size, and sub-100ms latency while maintaining accuracy. Students submit to the TinyTorch Leaderboard, competing across four tracks: Vision Excellence (highest CIFAR-10 accuracy), Language Quality (best text generation), Speed (fastest inference), and Compression (smallest model). This integrates all 19 modules into a portfolio-ready systems engineering project, teaching data-driven optimization decisions mirroring real ML systems competitions.

\paragraph{Time Commitment} Module completion time varies significantly by student background and prior experience. Pilot observations (N=5, Fall 2024) suggest differentiated ranges: experienced learners (prior ML systems background, strong Python/NumPy) complete modules in 2--4 hours; typical learners (standard ML course background, intermediate programming) require 4--6 hours; struggling learners (concurrent mathematics courses, limited debugging experience) need 6--10 hours, particularly for conceptually demanding modules (Module 05 Autograd, Module 09 CNNs). Total curriculum completion estimates range from 60--80 hours (experienced) to 100--120 hours (typical) to 140--180 hours (struggling), emphasizing the need for flexible pacing and scaffolded support. These ranges reflect implementation time only and exclude milestone integration work, debugging sessions, and systems analysis exercises. Instructors should communicate these differentiated estimates to help students plan realistic schedules and recognize when struggle becomes unproductive.

\subsection{Module Pedagogy and Assessment Structure}
\label{subsec:module-pedagogy}

Each module follows a consistent \textbf{Build $\rightarrow$ Use $\rightarrow$ Reflect} pedagogical cycle that integrates implementation, application, and systems reasoning. This structure addresses multiple learning objectives: students construct working components (Build), validate integration with prior modules (Use), and develop systems thinking through analysis (Reflect).

\paragraph{Build: Implementation with Explicit Dependencies}
Students implement components in Jupyter notebooks (\texttt{*\_dev.py}) with scaffolded guidance. Each module begins with \emph{connection maps} visualizing prerequisites, current focus, and unlocked capabilities. For example, Module 05 (Autograd) shows prerequisites (Modules 01--04: Tensor, Activations, Layers, Losses), current implementation goal (computational graph + backward pass), and unlocked future modules (Modules 06--07: Optimizers, Training). These visual dependency chains address cognitive apprenticeship~\citep{collins1989cognitive} by making expert knowledge structures explicit. Students see ``why this module matters'' before implementation begins, reducing disengagement from seemingly isolated exercises.

\paragraph{Use: Integration Testing Beyond Unit Tests}
Assessment employs two validation tiers through NBGrader~\citep{blank2019nbgrader}. First, unit tests verify isolated component correctness (e.g., ``Does \texttt{Tensor.reshape()} produce correct output?''). Second, integration tests validate cross-module functionality (e.g., ``Can Module 05 Autograd compute gradients through Module 03 Linear layers?''). Integration tests are critical for TinyTorch's pedagogical model because students may pass Module 03 (Layers) unit tests but fail Module 05 (Autograd) integration tests when their layer implementation doesn't properly expose parameters for gradient computation. This teaches \emph{interface design}---components must work together, not just in isolation.

Module 09 (Convolutions) integration tests exemplify this approach: convolution must work with Module 05's autograd, Module 06's optimizers, and Module 07's training loop simultaneously. Students discover systems thinking organically when encountering errors like ``My Conv2d passes unit tests but crashes during backpropagation---I need to implement \texttt{backward()} correctly.'' This failure mode mirrors professional ML engineering debugging.

\paragraph{Reflect: Systems Analysis Questions}
Each module concludes with systems reasoning prompts measuring conceptual understanding beyond syntactic correctness. Memory analysis questions ask students to calculate footprints (``A (256, 256) Conv2d layer with 64 input and 128 output channels requires how much memory?''). Complexity analysis prompts probe asymptotic understanding (``Why is attention $O(N^2)$? Demonstrate by doubling sequence length and measuring memory growth.''). Design trade-off questions assess engineering judgment (``Adam uses 3$\times$ parameter memory but converges faster than SGD. When is this trade-off worth it?''). These open-ended questions assess transfer~\citep{perkins1992transfer}---can students apply learned concepts to novel scenarios not seen in exercises?

\paragraph{Milestone Arcs: Curricular Checkpoints Demonstrating Progressive Building}
Six historical milestones serve as integration checkpoints spanning multiple modules, each requiring progressively more components from the growing framework. Milestone 1 (Perceptron, 1957) validates Modules 01--03 integration, requiring tensor operations, activations, and linear layers to compose into Rosenblatt's learning algorithm. Students import \texttt{from tinytorch.core import Tensor; from tinytorch.nn import Linear, Sigmoid}---their framework now supports single-layer networks.

Milestone 3 (MNIST MLP, 1986) requires Modules 01--07 working together, demonstrating that students can orchestrate the complete training pipeline. Students import optimizers (\texttt{from tinytorch.optim import SGD}), losses (\texttt{from tinytorch.nn import CrossEntropyLoss}), and training infrastructure---their framework now trains multi-layer networks end-to-end. Milestone 4 (CIFAR-10 CNN, 1998) demonstrates Modules 01--09 correctness through 75\%+ accuracy on \citet{krizhevsky2009cifar}'s dataset using \citet{lecun1998gradient}'s LeNet-inspired architectures---the ``north star'' achievement validating framework correctness. Students import \texttt{from tinytorch.nn import Conv2d, MaxPool2d}---their framework now supports convolutional architectures.

Milestone 6 (Production GPT, 2024) integrates all 20 modules into a deployable system. Students import from every module: \texttt{from tinytorch.nn import Transformer, Embedding, Attention; from tinytorch.optim import Adam; from tinytorch.profiling import profile\_memory}---demonstrating they've built a complete, production-ready ML framework.

Milestones differ from modules pedagogically: modules teach components, milestones validate that components \emph{compose} into functional systems. Students who pass all Module 01--07 unit tests might still fail Milestone 3 if their training loop doesn't properly orchestrate forward passes, loss computation, and backpropagation. This mirrors professional ML engineering: individual functions may work, but the system fails due to integration bugs. Historical framing motivates completion---students aren't just ``implementing backprop,'' they're ``recreating Rumelhart et al.'s 1986 breakthrough.'' More importantly, milestones provide concrete evidence of progressive building: students see their framework grow from basic components to a complete system capable of training state-of-the-art architectures.

\subsection{Course Integration Models}
\label{subsec:integration}

TinyTorch supports three deployment models for different institutional contexts, ranging from standalone courses to supplementary tracks in existing curricula.

\textbf{Model 1: Standalone 4-Credit Course (14 weeks)} targets junior/senior students pursuing ML systems careers who have completed CS231n or equivalent ML coursework and possess intermediate Python proficiency. The structure combines two weekly lectures (covering theory and design patterns) with one weekly lab (implementation practice). Students complete all 20 modules plus 6 historical milestones, with assessment through weekly NBGrader submissions, three milestone checkpoints (Perceptron, MNIST, CIFAR-10), and the final Torch Olympics capstone project. This model provides comprehensive ML systems education from first principles through production deployment.

\textbf{Model 2: Half-Semester Module in Existing ML Course (7 weeks)} integrates TinyTorch into traditional ML courses by replacing ``PyTorch tutorial'' weeks with implementation-focused learning. Students complete Modules 01--09 (Foundation + Architectures through CNNs) and Milestones 1--4, then apply their custom-built framework to course projects. Assessment consists of four module submissions and the CIFAR-10 CNN milestone. This model's pedagogical benefit emerges during project work: students who built optimizers from scratch debug learning rate issues faster than students who only used PyTorch's \texttt{optim.Adam}. The deeper systems understanding translates to better debugging capability.

\textbf{Model 3: Optional Deep-Dive Track (Self-Paced)} serves honors sections, independent study arrangements, and graduate students preparing for ML systems research. Students select modules matching their learning goals (e.g., Modules 05, 09, 12 for core autograd, convolution, and attention concepts) and earn extra credit through milestone completion. This model requires minimal instructor preparation---no lecture slides needed---making it most readily adoptable. The self-paced structure differentiates motivated students seeking ML engineering roles while maintaining manageable instructor workload.

\textbf{Instructor Resources}: Current release provides module notebooks, NBGrader test suites, and milestone validation scripts. Lecture slides for Models 1--2 remain future work (\Cref{subsec:future-work}), though Model 3 adoption requires no additional preparation beyond existing materials.

\subsection{Deployment Infrastructure}
\label{subsec:deployment}

TinyTorch's pure Python implementation enables deployment across diverse educational contexts with minimal infrastructure requirements, democratizing ML systems education beyond students with access to high-end hardware.

\subsubsection{Computing Requirements}

TinyTorch requires only dual-core 2GHz+ CPUs (no GPU needed), 4GB RAM (sufficient for CIFAR-10 training with batch size 32), 2GB storage (modules plus datasets), and any operating system supporting Python 3.8+ (Windows, macOS, or Linux). Unlike production ML courses requiring CUDA-compatible GPUs (\$500+ gaming laptops or cloud credits), 16GB+ RAM, and Linux/WSL environments, TinyTorch runs on Chromebooks via Google Colab, five-year-old budget laptops, and institutional computer labs. This democratizes ML systems education for community college students, international learners without access to high-end hardware, and K-12 educators exploring ML internals. The text-based ASCII connection maps further enhance accessibility for visually impaired students using screen readers.

\subsubsection{Jupyter Environment Options}

TinyTorch supports multiple deployment models to accommodate institutional contexts:

\textbf{Option 1: JupyterHub (Institutional Server)}---Central server deployment (8-core, 32GB RAM supports 50 concurrent students). Provides consistent environment and eliminates student setup friction but requires IT support. Best for universities with existing JupyterHub infrastructure.

\textbf{Option 2: Google Colab (Cloud-Based)}---Students open \texttt{.ipynb} files in Colab with zero installation. Free, accessible anywhere, handles compute requirements. Requires Google account and manages session timeouts. Best for MOOCs, international students, and asynchronous courses.

\textbf{Option 3: Local Installation}---\texttt{pip install tinytorch-edu} installs Jupyter, NBGrader, and dependencies. Enables offline work and fastest iteration but introduces environment debugging burden. Best for advanced students, small cohorts, and self-paced learning.

\subsubsection{NBGrader Autograding Workflow}

\textbf{Student Submission Process}: (1) Student works in Jupyter notebook (local or cloud), (2) runs \texttt{nbgrader validate module\_01.ipynb} for local correctness checking, (3) submits via LMS (Canvas/Blackboard) or Git (GitHub Classroom), (4) instructor runs \texttt{nbgrader autograde} on submitted notebooks, (5) grades and feedback posted to LMS.

\textbf{Handling Autograder Edge Cases}: Pure Python convolution (Module 09) may exceed default 30-second timeout on slower hardware; we set 5-minute timeouts and provide vectorized reference solutions for comparison. Critical modules (05 Autograd, 09 CNNs) include manual review of 20\% of submissions to catch conceptual errors missed by unit tests. All modules include \texttt{assert numpy.\_\_version\_\_ >= '1.20'} dependency validation.

\textbf{Scalability Validation}: Pilot testing (Fall 2024, N=5 students) validated autograding of 100 module submissions (Modules 01--05) with average grading time of 45 seconds per module on 4-core laptop. Projected scalability: small courses (30 students) grade in 10 minutes per module on instructor laptop, medium courses (100 students) require 30 minutes on dedicated grading server, MOOCs (1000+ students) achieve 2-hour turnaround via parallelized cloud autograding. Full-scale deployment validation planned for Fall 2025 (\Cref{sec:discussion}).

\subsection{Open Source Infrastructure}
\label{subsec:opensource}

TinyTorch is released as open source to enable community adoption and evolution.\footnote{Code released under MIT License, curriculum materials under Creative Commons Attribution-ShareAlike 4.0 (CC-BY-SA). Repository: \url{https://github.com/harvard-edge/TinyTorch}} The repository includes instructor resources: \texttt{CONTRIBUTING.md} (guidelines for bug reports and curriculum improvements), \texttt{INSTRUCTOR.md} (30-minute setup guide, grading rubrics, common student errors), and \texttt{MAINTENANCE.md} (support commitment through 2027, succession planning for community governance).

\textbf{Maintenance Commitment}: The author commits to bug fixes and dependency updates through 2027, community pull request review within 2 weeks, and annual releases incorporating educator feedback. Community governance transition (2026--2027) will establish an educator advisory board and document succession planning to ensure long-term sustainability beyond single-author maintenance.

\textbf{Archival and Permanence}: Curriculum versions are archived with Zenodo DOI (permanent reference independent of GitHub), source code preserved in Software Heritage foundation, and documentation mirrored on Read the Docs. This ensures TinyTorch remains accessible even if primary repository becomes unavailable, following Software Carpentry's sustainability model of prioritizing community ownership and documentation transferability.

\textbf{Customization Support}: TinyTorch's modular design enables institutional adaptation: replacing datasets with domain-specific data (medical images, time series), adding modules (diffusion models, graph neural networks), adjusting difficulty through scaffolding modifications, or changing assessment approaches. Forks should maintain attribution (CC-BY-SA requirement) and ideally contribute improvements upstream.

\subsection{Teaching Assistant Support Infrastructure}
\label{subsec:ta-support}

Effective deployment requires structured TA support beyond instructor guidance.

\textbf{TA Preparation}: TAs should develop deep familiarity with critical modules where students commonly struggle---Modules 05 (Autograd), 09 (CNNs), and 13 (Transformers)---by completing these modules themselves and intentionally introducing bugs to understand common error patterns. The repository provides \texttt{TA\_GUIDE.md} documenting frequent student errors (gradient shape mismatches, disconnected computational graphs, broadcasting failures) and debugging strategies.

\textbf{Office Hour Demand Patterns}: Student help requests cluster around conceptually challenging modules. Pilot data shows autograd (Module 05) generates significantly higher office hour demand than foundation modules. Instructors should anticipate demand spikes by scheduling additional TA capacity during critical modules, providing pre-recorded debugging walkthroughs, and establishing async support channels (discussion forums with guaranteed response times).

\textbf{Grading Infrastructure}: While NBGrader automates 70-80\% of assessment, critical modules benefit from manual review of implementation quality and conceptual understanding. TAs should focus manual grading on: (1) code clarity and design choices, (2) edge case handling, (3) computational complexity analysis, and (4) memory profiling insights. Sample solutions and grading rubrics in \texttt{INSTRUCTOR.md} calibrate evaluation standards.

\textbf{Boundaries and Scaffolding}: TAs should guide students toward solutions through structured debugging questions rather than providing direct answers. When students reach unproductive frustration, TAs can suggest optional scaffolding modules (numerical gradient checking before autograd implementation, scalar autograd before tensor autograd) to build confidence through intermediate steps.

\subsection{Student Learning Support}
\label{subsec:student-support}

TinyTorch embraces productive failure \citep{kapur2008productive}---learning through struggle before instruction---while providing guardrails against unproductive frustration.

\textbf{Recognizing Productive vs Unproductive Struggle}: Productive struggle involves trying different approaches, making incremental progress (passing additional tests), and developing deeper understanding of error messages. Unproductive frustration manifests as repeated identical errors without new insights, random code changes hoping for success, or inability to articulate the problem. Students experiencing unproductive frustration should seek help rather than persisting solo.

\textbf{Structured Help-Seeking}: The repository provides debugging workflows: (1) self-debug using print statements and simple test cases, (2) consult common errors documentation for the module, (3) search discussion forums for similar issues, (4) post structured help requests with error messages and attempted solutions, (5) attend office hours with specific questions. This progression encourages independence while ensuring timely intervention.

\textbf{Flexible Pacing and Optional Scaffolding}: Students learn at different rates depending on background, learning style, and external commitments. TinyTorch supports multiple pacing modes---intensive (weeks), semester (distributed coursework), self-paced (professional development)---without prescriptive timelines. Students struggling with conceptual jumps can access optional intermediate modules providing additional scaffolding. No penalty attaches to slower pacing or scaffolding use; depth of understanding matters more than completion speed.

\textbf{Diverse Student Contexts}: The curriculum acknowledges students balance learning with work, caregiving, or health challenges. Flexible pacing enables participation from community college students, working professionals, international learners, and non-traditional students who might be excluded by rigid timelines or high-end hardware requirements. Pure Python deployment on modest hardware (4GB RAM, dual-core CPU) and screen-reader-compatible ASCII diagrams further broaden accessibility.

\section{Progressive Disclosure via Monkey-Patching}
\label{sec:progressive}

Traditional ML education faces a pedagogical dilemma: students need to understand complete systems, but introducing all concepts simultaneously overwhelms cognitive capacity. Educational frameworks employ various strategies: some introduce separate classes (fragmenting the conceptual model), others defer advanced features until later courses (leaving gaps). TinyTorch introduces a third approach: \textbf{progressive disclosure via monkey-patching}, where a single \texttt{Tensor} class reveals capabilities gradually while maintaining conceptual unity.

\subsection{Pattern Implementation}

TinyTorch's \texttt{Tensor} class includes gradient-related attributes from Module 01, but they remain dormant until Module 05 activates them through monkey-patching (\Cref{lst:dormant-tensor,lst:activation}).

\begin{lstlisting}[caption={Module 01: Dormant gradient features},label=lst:dormant-tensor,float=t]
# Module 01: Foundation Tensor
class Tensor:
    def __init__(self, data, requires_grad=False):
        self.data = np.array(data, dtype=np.float32)
        self.shape = self.data.shape
        # Gradient features - dormant
        self.requires_grad = requires_grad
        self.grad = None
        self._backward = None

    def backward(self, gradient=None):
        """No-op until Module 05"""
        pass

    def __mul__(self, other):
        return Tensor(self.data * other.data)
\end{lstlisting}

\begin{lstlisting}[caption={Module 05: Autograd activation},label=lst:activation,float=t]
def enable_autograd():
    """Monkey-patch Tensor with gradients"""
    def backward(self, gradient=None):
        if gradient is None:
            gradient = np.ones_like(self.data)
        if self.grad is None:
            self.grad = gradient
        else:
            self.grad += gradient
        if self._backward is not None:
            self._backward(gradient)

    # Monkey-patch: replace methods
    Tensor.backward = backward
    print("Autograd activated!")

# Module 05 usage
enable_autograd()
x = Tensor([3.0], requires_grad=True)
y = x * x  # y = 9.0
y.backward()
print(x.grad)  # [6.0] - dy/dx = 2x
\end{lstlisting}

This design serves three pedagogical purposes: (1) \textbf{Early interface familiarity}---students learn complete \texttt{Tensor} API from start; (2) \textbf{Forward compatibility}---Module 01 code doesn't break when autograd activates; (3) \textbf{Curiosity-driven learning}---dormant features create questions motivating curriculum progression.

\subsection{Pedagogical Justification}

Progressive disclosure is designed to manage cognitive load by partitioning element interactivity across modules. Module 01 introduces tensor operations (managing multiple interacting elements), Module 05 adds gradients (new functionality to familiar interface), hypothetically respecting working memory's 4--7 element capacity \citep{sweller1988cognitive}. This element counting represents an instructional design hypothesis requiring empirical validation through dual-task methodology or cognitive load self-report scales.

The pattern also instantiates threshold concept pedagogy \citep{meyer2003threshold}: autograd is transformative and troublesome. By making it visible early (dormant) but activatable later, students may cross this threshold when cognitively ready, though empirical evidence of this progression is needed.

\subsection{Production Framework Alignment}

Progressive disclosure demonstrates how real ML frameworks evolve. Early PyTorch (pre-0.4) separated data (\texttt{torch.Tensor}) from gradients (\texttt{torch.autograd.Variable}). PyTorch 0.4 (April 2018) \citep{pytorch04release} consolidated functionality into \texttt{Tensor}, matching TinyTorch's pattern. Students are exposed to the modern unified interface from Module 01, positioned to understand why PyTorch made this design evolution.

Similarly, TensorFlow 2.0 integrated eager execution by default \citep{tensorflow20}, making gradients work immediately---similar to TinyTorch's activation pattern. Students who understand progressive disclosure can grasp why TensorFlow eliminated \texttt{tf.Session()}: immediate execution with automatic graph construction aligns with unified API design principles.

\section{Systems-First Integration}
\label{sec:systems}

Industry surveys show ML engineers spending more time on memory optimization and debugging than hyperparameter tuning, yet most curricula defer systems thinking to senior electives. TinyTorch applies situated cognition \citep{lave1991situated} by integrating memory profiling and FLOPs analysis from Module 01.

\subsection{Memory as First-Class Citizen}

Where traditional frameworks abstract away memory concerns, TinyTorch makes memory footprint calculation explicit (\Cref{lst:tensor-memory}). Students' first assignment calculates memory for MNIST (60,000 $\times$ 784 $\times$ 4 bytes = 188 MB) and ImageNet (1.2M $\times$ 224$\times$224$\times$3 $\times$ 4 bytes = 716 GB).

This memory-first pedagogy transforms student questions:
\begin{itemize}
\item Module 01: ``Why does batch size affect memory?'' (activations scale with batch size)
\item Module 06: ``Why does Adam use 3$\times$ parameter memory?'' (momentum, variance, master weights)
\item Module 13: ``How much VRAM for GPT-3?'' (175B parameters $\times$ 4 bytes $\times$ 4 for Adam states)
\end{itemize}

\textbf{Technical correction} (reviewer feedback): Adam requires 3$\times$ \emph{parameter} memory specifically. Total memory = parameters + activations + gradients + optimizer states. Activation memory often dominates (10--100$\times$ parameter memory), so Adam's overhead is 3$\times$ on the smaller parameter component. The curriculum is designed to teach this distinction through profiling.

\subsection{Computational Complexity Made Visible}

Module 09 introduces convolution with seven explicit nested loops (\Cref{lst:conv-explicit}), making $O(B \times C_{\text{out}} \times H_{\text{out}} \times W_{\text{out}} \times C_{\text{in}} \times K_h \times K_w)$ complexity visible and countable.

\begin{lstlisting}[caption={Explicit convolution showing 7-nested complexity},label=lst:conv-explicit,float=t]
def conv2d_explicit(input, weight):
    """7 nested loops - see the complexity!
    input: (B, C_in, H, W)
    weight: (C_out, C_in, K_h, K_w)"""
    B, C_in, H, W = input.shape
    C_out, _, K_h, K_w = weight.shape
    H_out, W_out = H - K_h + 1, W - K_w + 1
    output = np.zeros((B, C_out, H_out, W_out))

    # Count: 1,2,3,4,5,6,7 loops
    for b in range(B):
        for c_out in range(C_out):
            for h in range(H_out):
                for w in range(W_out):
                    for c_in in range(C_in):
                        for kh in range(K_h):
                            for kw in range(K_w):
                                output[b,c_out,h,w] += \
                                    input[b,c_in,h+kh,w+kw] * \
                                    weight[c_out,c_in,kh,kw]
    return output
\end{lstlisting}

Students calculate: CIFAR-10 batch (128, 3, 32, 32) through 32-filter 5$\times$5 convolution: $128 \times 32 \times 28 \times 28 \times 3 \times 5 \times 5 = 86.7$M multiply-accumulate operations. This concrete measurement motivates Module 18's vectorization (10--100$\times$ speedup) and explains why CNNs require hardware acceleration.

\subsection{Performance Benchmarks}

\Cref{tab:performance} validates the ``100--1000$\times$ slower than PyTorch'' claim through actual measurements (reviewer feedback: add concrete benchmarks).

\begin{table}[t]
\centering
\caption{Runtime comparison: TinyTorch vs PyTorch (CPU)}
\label{tab:performance}
\small
\begin{tabular}{@{}lrrr@{}}
\toprule
Operation & TinyTorch & PyTorch & Ratio \\
\midrule
\texttt{matmul} (1K$\times$1K) & 890 ms & 2.1 ms & 424$\times$ \\
\texttt{conv2d} (CIFAR batch) & 8.4 s & 0.09 s & 93$\times$ \\
\texttt{softmax} (10K elem) & 45 ms & 0.12 ms & 375$\times$ \\
\midrule
CIFAR-10 epoch (LeNet) & 12 min & 8 sec & 90$\times$ \\
\bottomrule
\end{tabular}
\end{table}

This slowness is pedagogically valuable (productive failure \citep{kapur2008productive}): students experience performance problems before learning optimizations, making vectorization meaningful rather than abstract.

\subsection{Historical Milestone Validation}
\label{subsec:milestones}

TinyTorch validates curriculum correctness through \textbf{historical milestone recreation}: students recreate seminal ML breakthroughs spanning 1957--2024 using exclusively their own implemented code. Unlike traditional programming assignments with unit tests, milestones require students to achieve historical accuracy benchmarks, validating that implementations work correctly on real tasks.

\subsubsection{Milestone System Design}

Each milestone:
\begin{enumerate}
\item \textbf{Historical grounding}: Recreates actual breakthrough (Rosenblatt's Perceptron, LeCun's LeNet, Vaswani's Transformer)
\item \textbf{Zero external dependencies}: Uses \emph{only} student-implemented TinyTorch code (no PyTorch/TensorFlow)
\item \textbf{Objective validation}: Success measured by achieving task-appropriate performance
\item \textbf{Architectural comparison}: Demonstrates why new architecture improved over predecessors
\end{enumerate}

The curriculum includes 6 milestones:

\begin{enumerate}
\item \textbf{1957 Perceptron} (after Module 04): Train Rosenblatt's original single-layer perceptron on linearly separable classification
\item \textbf{1969 XOR Solution} (after Module 07): Solve Minsky's ``impossible'' XOR problem with multi-layer perceptrons, proving critics wrong
\item \textbf{1986 MLP Revival} (after Module 08): Handwritten digit recognition demonstrating backpropagation's power
\item \textbf{1998 CNN Revolution} (after Module 09): Image classification showing convolutional architectures' advantage over MLPs on same dataset
\item \textbf{2017 Transformer Era} (after Module 13): Language generation with attention-based architecture
\item \textbf{2024 Systems Age} (after Module 20): Production-optimized system demonstrating quantization, compression, and acceleration
\end{enumerate}

\subsubsection{Pedagogical Impact}

Historical milestones transform abstract exercises into meaningful achievements. Rather than ``implement this function,'' students ``recreate the breakthrough that proved Minsky wrong about neural networks.'' Early pilot feedback suggests this narrative framing increases intrinsic motivation: participants reported experiencing milestones as ``unlocking achievements'' and ``proving historical results myself.''

Milestones instantiate Bruner's spiral curriculum \citep{bruner1960process}: students train neural networks 6 times with increasing sophistication. The XOR milestone revisits Perceptron training with added hidden layers. The CNN milestone revisits digit classification with spatial operations. Each iteration deepens understanding while maintaining motivation through historical progression.

\subsubsection{Correctness Validation}

Milestones serve dual purpose: pedagogical motivation \emph{and} implementation validation. If student-implemented CNNs successfully classify natural images, convolution, pooling, and backpropagation all work correctly. If transformer generation produces coherent text, attention mechanisms, positional embeddings, and autoregressive sampling all function properly. This objective validation complements unit tests: tests verify individual functions work, milestones verify the complete system works on real tasks.

\textbf{Important clarification}: Milestone success is measured by students achieving performance in the ballpark of historical benchmarks (e.g., CNNs achieving reasonable accuracy on CIFAR-10, transformers generating coherent text), not by matching exact published accuracies. The pedagogical goal is demonstrating that implementations work correctly on real tasks---students can ``push the milestone'' to completion, validating their framework's correctness. Exact accuracy optimization belongs to production frameworks like PyTorch; TinyTorch milestones validate that students have built working systems capable of learning, not that they've achieved state-of-the-art performance.

The architectural comparison requirement reinforces learning. Milestone 4 requires training both MLP and CNN on identical CIFAR-10 data, demonstrating CNNs' architectural advantage. Students don't just read ``CNNs work better''---they measure the improvement themselves, understanding \emph{why} spatial operations help through direct experimentation.

\subsection{Automated Assessment Infrastructure}

TinyTorch integrates NBGrader \citep{blank2019nbgrader} for scalable automated assessment. Each module contains:

\begin{itemize}
\item \textbf{Solution cells}: Scaffolded implementations with grade metadata
\item \textbf{Test cells}: Locked autograded tests preventing modification
\item \textbf{Immediate feedback}: Students validate correctness locally before submission
\item \textbf{Point allocation}: Reflects pedagogical priorities (Module 05 Autograd: 100 points; Module 01 Tensor: 60 points)
\end{itemize}

This infrastructure enables deployment in MOOCs and large classrooms where manual grading proves infeasible. Instructors configure NBGrader to collect submissions, execute tests in sandboxed environments, and generate grade reports automatically.

\textbf{Important caveat}: NBGrader scaffolding exists but remains unvalidated at scale (\Cref{sec:discussion}). Automated assessment validity requires empirical investigation: Do tests measure conceptual understanding or syntax correctness? We scope this as ``curriculum with autograding infrastructure'' rather than ``validated assessment system.''

\subsection{Production Package Organization: Building a Complete System}
\label{subsec:package}

Unlike tutorial-style notebooks creating isolated code, TinyTorch modules export to production package structure matching PyTorch's API organization. Critically, \emph{each completed module becomes immediately usable}---students build a working framework progressively, not isolated exercises. Module 01 exports to \texttt{tinytorch.core.tensor}, Module 09 to \texttt{tinytorch.nn.conv}, enabling professional import patterns that grow with each module completed.

\paragraph{Progressive System Building}
As students complete modules, their framework accumulates capabilities. After Module 03, students can import and use layers; after Module 05, autograd enables training; after Module 09, CNNs become available. This progressive accumulation creates tangible evidence of progress---students see their framework grow from basic tensors to a complete ML system. \Cref{lst:progressive-imports} illustrates how imports expand as modules are completed:

\begin{lstlisting}[caption={Progressive imports: Framework capabilities grow module-by-module},label=lst:progressive-imports,float=t]
# After Module 01: Basic tensors
from tinytorch.core import Tensor

# After Module 03: Layers available
from tinytorch.nn import Linear, ReLU

# After Module 05: Training enabled
from tinytorch.core import Tensor
from tinytorch.nn import Linear, CrossEntropyLoss
# Autograd now active - gradients flow!

# After Module 09: CNNs available
from tinytorch.nn import Conv2d, MaxPool2d
from tinytorch.core import Tensor

# After Module 13: Transformers ready
from tinytorch.nn import Transformer, Embedding
from tinytorch.core import Tensor
# Complete framework - all 20 modules integrated!
\end{lstlisting}

This design bridges educational and professional contexts. Students aren't ``solving exercises''---they're building a framework they could ship. The package structure reinforces systems thinking: understanding how \texttt{torch.nn.Conv2d} relates to \texttt{torch.Tensor} requires grasping module organization, not just individual algorithms. More importantly, students experience the satisfaction of watching their framework grow from a single \texttt{Tensor} class to a complete system capable of training transformers---each module completion adds new capabilities they can immediately use.

Export happens via nbdev \citep{howard2020fastai} directives (\texttt{\#| default\_exp core.tensor}) embedded in module notebooks. Students work in Jupyter's interactive environment while TinyTorch maintains source-of-truth in version-controlled Python files, enabling professional development workflows (Git, code review, CI/CD) within pedagogical context.

\subsection{Explicit Knowledge Integration}

Every module begins with a \textbf{Connection Map} showing prerequisite modules, current module focus, and enabled future capabilities. This addresses Collins et al.'s cognitive apprenticeship \citep{collins1989cognitive} by making expert knowledge structures visible:

\begin{lstlisting}[caption={Module 05 Connection Map},label=lst:connection-map,float=t]
## Prerequisites & Progress
You've Built: Tensor, activations, layers, losses
You'll Build: Autograd system
You'll Enable: Training loops, optimizers

Connection Map:
Modules 01-04 → Autograd → Training (06-07)
(forward pass)   (backward)  (learning loops)
\end{lstlisting}

Connection maps transform isolated modules into coherent curriculum. Students see \emph{why} each module matters before implementation begins, reducing ``I don't see the point'' disengagement. Early feedback suggests these maps help students maintain big-picture understanding while working through implementation details.

\section{Discussion and Limitations}
\label{sec:discussion}

Building TinyTorch revealed insights about systems-first ML education. This section reflects on design lessons, acknowledges scope boundaries honestly, and outlines concrete empirical validation plans.

\subsection{Scope: What's NOT Covered}
\label{subsec:scope}

TinyTorch prioritizes framework internals understanding over production ML completeness. The curriculum explicitly omits several critical production skills that require substantial additional complexity orthogonal to framework internals. GPU programming and hardware acceleration---including CUDA kernel optimization, memory hierarchies, tensor cores, mixed precision training with FP16/BF16/INT8 and gradient scaling, and hardware-specific optimization for TPUs and Apple Neural Engine---represent substantial domains requiring parallel programming expertise that would overwhelm students still mastering tensor operations and automatic differentiation.

Distributed training fundamentals including data parallelism (DistributedDataParallel, gradient synchronization), model parallelism (pipeline parallelism, tensor parallelism), and large-scale training systems (FSDP, DeepSpeed, Megatron-LM) similarly introduce communication complexity that extends beyond framework internals. Production deployment and serving skills---model compilation through TorchScript, ONNX, and TensorRT, serving infrastructure including batching, load balancing, and latency optimization, and MLOps tooling for experiment tracking, model versioning, and A/B testing---represent deployment infrastructure concerns distinct from framework understanding.

Advanced systems techniques such as gradient checkpointing (trading computation for memory), operator fusion and graph compilation, Flash Attention and memory-efficient attention variants, and dynamic versus static computation graphs, while important for production systems, introduce optimization complexity that obscures pedagogical transparency. These topics require substantial additional complexity---parallel programming semantics, hardware knowledge, deployment infrastructure---that would shift focus away from framework internals understanding.

TinyTorch teaches framework internals as foundation for GPU and distributed work, not as replacement. Complete production ML engineer preparation requires TinyTorch (internals) followed by PyTorch Distributed (GPU/multi-node), deployment courses (serving), and on-the-job experience. Students completing TinyTorch should pursue GPU and distributed training through PyTorch tutorials, NVIDIA Deep Learning Institute courses, or advanced ML systems courses. The CPU-only design offers three pedagogical benefits: accessibility (students in regions with limited cloud computing access can complete curriculum on modest hardware), reproducibility (no GPU availability variability across institutions), and pedagogical focus (internals learning not confounded with hardware optimization).

\subsection{Limitations: Understanding Scope}

This paper represents a design contribution---curriculum architecture, pedagogical patterns, theoretical grounding---rather than empirical evaluation. We provide replicable curriculum architecture, working implementations across 20 modules, and pedagogical patterns (progressive disclosure, systems-first integration) that educators can adopt. What requires validation includes learning outcomes, transfer effectiveness, and cognitive load reduction, which will be addressed through controlled studies with pre/post assessments and transfer tasks.

TinyTorch includes NBGrader scaffolding enabling automated assessment. This infrastructure works in development---tests execute, grades calculate, feedback generates. However, it remains unvalidated for large-scale classroom deployment that would surface edge cases, performance bottlenecks, and usability challenges. Additionally, automated grading validity requires empirical investigation: Do tests measure conceptual understanding or syntax correctness? Can students pass tests without understanding? Do tests align with learning objectives (\Cref{tab:objectives})? Future work should validate assessment through item analysis, discrimination indices, and correlation with transfer task performance.

TinyTorch's pure Python implementation executes 100--1000$\times$ slower than PyTorch (\Cref{tab:performance}). This performance gap is deliberate pedagogical choice trading speed for transparency. Production ML frameworks optimize through C++ implementations, CUDA kernel fusion, and hardware-specific acceleration. These optimizations make frameworks fast but obscure computational reality. TinyTorch prioritizes pedagogical transparency: seven explicit loops reveal convolution's complexity, pure Python tensor operations expose memory access patterns, unoptimized operations demonstrate why vectorization matters. Students should not use TinyTorch for production model training or customer-facing systems. The framework serves educational purposes---building understanding of ML systems from first principles---not engineering purposes. For production work, students graduate to PyTorch, TensorFlow, or JAX, carrying deep understanding because they built equivalent systems themselves.

TinyTorch's curriculum materials exist exclusively in English, limiting accessibility for non-English-speaking learners. Internationalization represents clear future work. The modular documentation structure facilitates translation efforts. We welcome community contributions and plan translation infrastructure supporting multi-language documentation without fragmenting codebase.

\subsection{Future Work}
\label{subsec:future-work}

TinyTorch's design enables multiple research and development trajectories maintaining pedagogical principles while extending capability.

\subsubsection{Empirical Validation}

The most immediate priority involves deploying TinyTorch in university CS curricula and measuring learning outcomes through controlled comparison. Planned experimental design compares learning outcomes between traditional ML courses (algorithm-focused, using PyTorch as black box) and TinyTorch courses (systems-first, building frameworks). Pre/post assessments would measure systems thinking competency (memory profiling, complexity reasoning, optimization analysis), framework comprehension (autograd mechanics, layer composition, training dynamics), and production readiness (debugging gradient flows, profiling performance, deployment decisions).

Key research questions include: Does systems-first curriculum improve production ML readiness compared to algorithm-focused approaches? Do students who build frameworks transfer knowledge to PyTorch/TensorFlow more effectively than students who only use these frameworks? Does progressive disclosure reduce cognitive load compared to introducing separate gradient classes? Do historical milestones increase motivation and learning compared to equivalent technical validation?

\subsubsection{GPU Awareness and Performance Modeling}

While TinyTorch's CPU-only design prioritizes pedagogical transparency, students benefit from understanding GPU acceleration without implementing CUDA kernels. Future extensions could enable students to compare TinyTorch CPU implementations against PyTorch GPU equivalents, analyzing performance gaps through roofline models~\citep{williams2009roofline}. Rather than writing CUDA code, students would profile existing implementations to understand memory hierarchy differences (CPU cache levels L1/L2/L3 versus GPU global/shared/register memory), parallelism benefits (sequential CPU loops versus massively parallel GPU execution with thousands of threads), roofline analysis techniques (plotting achieved performance against hardware limits to identify compute-bound versus memory-bound operations), and mixed precision advantages (profiling FP32 versus FP16 training speed/memory tradeoffs). Students would run instrumented PyTorch code alongside TinyTorch implementations, measuring wall-clock time, memory usage, and FLOPs utilization. The roofline model visualization shows why GPUs excel at ML workloads: high arithmetic intensity operations (matrix multiplication) approach peak FLOPs, while memory-bound operations (element-wise activations) hit bandwidth limits. This awareness without implementation maintains TinyTorch's accessibility while preparing students for GPU programming courses.

\subsubsection{Distributed Training Fundamentals}

Understanding distributed training communication patterns and scalability challenges requires simulation-based pedagogy, not multi-GPU hardware. Future extensions could enable students to explore distributed training concepts through integration with ASTRA-sim~\citep{chakkaravarthy2023astrasim,astrasimsim2020}, a distributed ML training simulator. Rather than requiring 8-GPU clusters, students would simulate multi-device training on single machines, exploring data parallelism basics (gradient synchronization via all-reduce across virtual workers, analyzing communication overhead versus compute time), scalability analysis (measuring weak versus strong scaling, identifying communication bottlenecks as worker count increases), network topology impact (comparing ring all-reduce, tree all-reduce, and hierarchical strategies through ASTRA-sim's topology modeling), and pipeline parallelism introduction (simulating model partitioning across devices, analyzing pipeline bubbles and micro-batching strategies).

ASTRA-sim integration would enable exploring research questions without hardware barriers. Students could replicate findings from distributed training literature~\citep{astrasimsim2020}, experiencing how collective communication primitives (all-reduce, all-gather) dominate training time at scale. This simulation-based approach maintains TinyTorch's pedagogical principle: understanding systems through transparent implementation and measurement, not black-box hardware access. Students would understand why gradient synchronization limits distributed training scalability, how network bandwidth affects multi-node training, what communication patterns different parallelism strategies require, and when to apply data versus model versus pipeline parallelism based on model and hardware characteristics.

\subsubsection{Advanced Curriculum Extensions}

Maintaining modular structure, TinyTorch can incorporate emerging ML systems topics through community contributions:

\textbf{Graph Neural Networks (Module 23)}: Message passing, graph convolutions, attention on graphs
\textbf{Diffusion Models (Module 24)}: Denoising architectures, forward/reverse processes, sampling strategies
\textbf{Reinforcement Learning (Module 25)}: Policy gradients, value functions, actor-critic methods

These extensions follow established pedagogical patterns: systems-first integration (memory profiling, complexity analysis), historical milestone validation (seminal papers), and progressive disclosure (building on foundation tier). Community forks demonstrate this extensibility: quantum ML variants replace classical tensors with quantum state vectors, robotics variants add RL-specific infrastructure.

\subsubsection{Learning Science Research}

TinyTorch's design enables controlled studies of pedagogical questions in ML education:

\textbf{Threshold Concepts}: Is autograd transformative (Meyer \& Land's criteria: troublesome, irreversible, integrative)? Do students exhibit conceptual breakthroughs or gradual understanding?

\textbf{Productive Failure Limits}: When does struggle become unproductive? Can we identify engagement patterns (time-on-task, help-seeking behavior) predicting learning vs frustration?

\textbf{Transfer Effectiveness}: Do TinyTorch skills improve PyTorch debugging? Measure via transfer tasks: given PyTorch code with gradient bugs, do TinyTorch students diagnose faster/more accurately than control group?

\textbf{Long-term Retention}: Six months post-course, do students remember systems concepts (memory footprint calculation, complexity reasoning)? Compare concept maps, debugging protocols, optimization strategies.

These studies require validated instruments, control groups, and longitudinal data collection as part of future research programs.

\subsubsection{Community Building and Adoption}

TinyTorch serves as the hands-on companion to the Machine Learning Systems textbook, providing practical implementation experience alongside theoretical foundations. Adoption will be measured through multiple channels: (1) \textbf{Educational adoption}: tracking course integrations, student enrollment, and instructor feedback across institutions; (2) \textbf{Torch Olympics community}: inspired by MLPerf benchmarking, the Torch Olympics leaderboard creates competitive systems engineering challenges where students submit optimized implementations competing across accuracy, speed, compression, and efficiency tracks---building community engagement and peer learning; (3) \textbf{Open-source metrics}: GitHub stars, forks, contributions, and community discussions indicating active use beyond formal coursework. This multi-faceted approach recognizes that educational impact extends beyond traditional classroom metrics to include community building, peer learning, and long-term skill development. The Torch Olympics platform particularly enables students to see how their implementations compare globally, fostering systems thinking through competitive optimization while maintaining educational focus on understanding internals rather than achieving state-of-the-art performance.

\section{Conclusion}
\label{sec:conclusion}

Machine learning education faces a critical gap: students learn to \emph{use} ML frameworks but lack systems-level understanding needed to build, optimize, and deploy them in production. TinyTorch addresses this gap through a pedagogical framework \emph{designed to} transform framework users into systems engineers.

This paper makes five primary contributions. First, \textbf{progressive disclosure via monkey-patching} (\Cref{sec:progressive})---a novel pedagogical pattern where dormant features in the \texttt{Tensor} class activate across modules, enabling early interface exposure while managing cognitive load. Second, \textbf{systems-first integration} (\Cref{sec:systems}), where memory profiling, FLOPs analysis, and computational complexity are introduced from Module 01---not relegated to advanced electives. Third, a \textbf{3-tier curriculum architecture plus competitive capstone} (\Cref{sec:curriculum}) spanning foundation, architectures, and optimization tiers, culminating in the Torch Olympics MLPerf-style competition. Fourth, \textbf{theoretical grounding} (\Cref{sec:related}) demonstrating how constructionism, productive failure, and threshold concepts guide design. Fifth, a \textbf{complete open-source artifact} enabling educator adoption and empirical evaluation.

These contributions serve multiple audiences. CS educators gain replicable curriculum patterns worth empirical investigation. ML engineers obtain framework internals education potentially transferring to PyTorch/TensorFlow debugging. Industry trainers receive template for upskilling ML users into systems engineers. CS education researchers find novel pedagogical patterns worth studying empirically.

\textbf{Important scope note}: This represents a \textbf{design contribution}. Curriculum architecture, pedagogical patterns, and theoretical framework are provided; rigorous classroom evaluation with learning outcome measurements remains future work. Students completing TinyTorch's 20 modules should pursue GPU acceleration and distributed training through PyTorch tutorials, NVIDIA courses, or advanced modules---TinyTorch provides internals foundation, not complete production ML preparation.

TinyTorch is not a replacement for production frameworks---it is a pedagogical bridge. Students completing the curriculum are expected to understand \emph{why} PyTorch manages GPU memory as it does, \emph{why} batch normalization layers have different train/eval modes, \emph{why} optimizers like Adam consume 3$\times$ parameter memory, and \emph{why} quantization trades 4$\times$ memory reduction for 1--2\% accuracy loss. This systems-level mental model is designed to transfer across frameworks and prepare graduates for ML engineering roles requiring optimization, debugging, and architectural decision-making.

We invite the ML education community to build on TinyTorch. The complete codebase, curriculum materials, and assessment infrastructure are openly available. Educators can adopt modules, adapt to local contexts, or extend with new capabilities. Researchers can instrument the framework to study learning progressions, measure pedagogical effectiveness, or test alternative teaching strategies.

\textbf{Most ML education teaches students to use frameworks. TinyTorch is designed to teach them to understand frameworks---and that understanding may make all the difference.}

\textbf{Note on Work-in-Progress Status}: This paper presents TinyTorch as a design contribution and pedagogical framework currently in active development. The curriculum architecture, pedagogical patterns, and theoretical grounding are established, but empirical validation and large-scale deployment remain future work. We present this work to engage the ML education community, invite collaboration, and establish the rationale for why ML systems education requires integrated, systems-first curricula. The pedagogical patterns and curriculum design presented here provide a foundation worth building upon, even as empirical validation and refinement continue.

% Bibliography
\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
