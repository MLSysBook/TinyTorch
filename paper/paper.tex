%% TinyTorch: A Framework for Building ML Systems from Scratch
%% REVISED VERSION - Incorporating reviewer feedback
%% Two-column academic paper format

\documentclass[10pt,twocolumn]{article}

% Essential packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{cleveref}

% Page geometry
\usepackage[
  letterpaper,
  top=0.75in,
  bottom=1in,
  left=0.75in,
  right=0.75in,
  columnsep=0.25in
]{geometry}

% Section formatting
\usepackage{titlesec}
\titleformat{\section}{\normalfont\fontsize{12}{14}\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalfont\fontsize{11}{13}\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalfont\fontsize{10}{12}\bfseries}{\thesubsubsection}{1em}{}

% Python code highlighting
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.97,0.97,0.97}

\lstdefinestyle{pythonstyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\scriptsize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    language=Python,
    frame=single,
    rulecolor=\color{black!30},
    xleftmargin=10pt,
    xrightmargin=5pt,
    aboveskip=8pt,
    belowskip=8pt
}

\lstset{style=pythonstyle}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue,
    pdftitle={TinyTorch: A Framework for Building ML Systems from Scratch},
    pdfauthor={Vijay Janapa Reddi}
}

% Title and authors
\title{\Large\bfseries TinyTorch: A Framework for Building ML Systems from Scratch}

\author{
  Vijay Janapa Reddi\\
  Harvard University\\
  Cambridge, MA, USA\\
  \texttt{vj@eecs.harvard.edu}
}

\date{}

\begin{document}

\maketitle

% Abstract - REVISED: Reframed as design contribution
\begin{abstract}
Machine learning education traditionally focuses on using frameworks like PyTorch and TensorFlow, leaving students unprepared for the systems-level challenges of ML engineering: memory management, performance optimization, and production deployment. We present \textbf{TinyTorch}, an educational framework introducing three novel pedagogical patterns for teaching ML systems from first principles: (1) \textbf{progressive disclosure via monkey-patching}---dormant features in the \texttt{Tensor} class activate across modules, enabling early interface exposure while managing cognitive load, (2) \textbf{systems-first integration}---memory profiling, FLOPs analysis, and computational complexity taught from Module 01 rather than deferred to advanced electives, and (3) \textbf{historical milestone validation}---students recreate 70 years of ML breakthroughs (1957 Perceptron $\rightarrow$ 2024 production systems) using exclusively their own code for objective correctness validation and intrinsic motivation. The 20-module curriculum (60--80 hours) builds a production-organized package with automated assessment infrastructure. \emph{This paper presents pedagogical patterns and curriculum design}---not empirical evaluation---with classroom validation planned for Fall 2025. The complete open-source framework enables educators to adopt these patterns and researchers to conduct empirical studies.
\end{abstract}

\noindent\textbf{Keywords:} machine learning education, systems education, educational frameworks, ML engineering, progressive disclosure, autograd, constructionism, design-based research

% Main content
\section{Introduction}

Most machine learning courses teach students to use frameworks, not understand them. Traditional curricula focus on calling \texttt{model.fit()} and \texttt{loss.backward()} without grasping what happens when these methods execute. This knowledge gap becomes critical when ML models move to production: engineers must optimize memory usage, profile computational bottlenecks, and debug gradient flows---skills rarely taught in algorithm-focused curricula.

Consider two students who have completed traditional ML coursework. Both can derive backpropagation equations and explain gradient descent convergence. Both have trained convolutional networks on MNIST using PyTorch. Yet when production deployment demands answers to systems questions---``How much VRAM does this model require?'' ``Why does batch size 32 work but batch size 64 causes OOM?'' ``How many FLOPs for inference on this architecture?''---they struggle. The algorithmic knowledge they possess proves insufficient for ML engineering as practiced in industry.

This gap between framework users and systems engineers reflects a deeper pedagogical challenge. Traditional ML curricula treat systems concerns---memory management, computational complexity, performance optimization---as advanced topics relegated to separate ``ML Systems'' electives that students encounter, if at all, in their final undergraduate year. By then, students have formed mental models that divorce ML algorithms from their computational reality. They understand gradients abstractly but not gradient memory footprint. They know attention mechanisms mathematically but not their $O(N^2)$ scaling implications.

Can we teach ML as systems engineering from first principles? Can students learn memory profiling alongside tensor operations, computational complexity alongside convolutions, optimization trade-offs alongside model training? We answer these questions affirmatively through TinyTorch: a complete 20-module curriculum where students build every component of a production ML framework from scratch---from tensors to transformers to optimization---with systems awareness embedded from Module 01 onwards.

TinyTorch serves a specific pedagogical niche: transitioning from framework \emph{users} to framework \emph{engineers}. The curriculum targets students who have completed introductory ML courses and want to understand framework internals, those planning ML systems research or infrastructure engineering careers, or practitioners who need to debug production ML systems effectively. Conversely, students who haven't trained neural networks should first complete courses like CS231n or fast.ai; those needing immediate GPU/distributed training skills are better served by PyTorch tutorials; and learners preferring project-based application building over internals understanding will find high-level frameworks more appropriate.

The curriculum supports flexible pacing to accommodate diverse student contexts: intensive completion (weeks), semester integration (regular coursework), or self-paced professional development. TinyTorch positions as a complement to algorithm-focused ML courses: taken \emph{after} CS231n to understand systems, \emph{before} advanced ML systems courses to build foundation, or \emph{parallel to} production ML roles to develop debugging skills.

TinyTorch makes three core pedagogical innovations that distinguish it from existing educational approaches:

\textbf{1. Progressive Disclosure via Monkey-Patching.}
Students encounter a single \texttt{Tensor} class throughout the curriculum, but its capabilities expand progressively through runtime enhancement. Module 01 introduces \texttt{Tensor} with dormant gradient features (\texttt{.requires\_grad}, \texttt{.grad}, \texttt{.backward()}) that remain inactive until Module 05, when \texttt{enable\_autograd()} monkey-patches the class---dynamically modifying methods at runtime---to activate automatic differentiation (\Cref{lst:progressive}). This design teaches real framework evolution patterns---matching PyTorch 2.0's enhanced Tensor design---while managing cognitive load through phased complexity introduction.

\begin{lstlisting}[caption={Progressive disclosure pattern},label=lst:progressive,float=t]
# Module 01: Dormant features
class Tensor:
    def __init__(self, data, requires_grad=False):
        self.data = np.array(data)
        self.requires_grad = requires_grad  # Dormant
        self.grad = None  # Dormant

    def backward(self):
        pass  # No-op until Module 05

# Module 05: Activation via monkey-patching
enable_autograd()  # Enhances Tensor class
# Now gradients work throughout framework
\end{lstlisting}

Unlike educational frameworks that introduce separate classes for gradients or use deprecated patterns like PyTorch's old Variable wrapper, progressive disclosure maintains a single mental model while teaching production framework architecture.

\textbf{2. Systems-First Integration.}
Every module integrates memory profiling, computational complexity analysis, and performance reasoning---not as advanced topics but as foundational concepts. Module 01 introduces \texttt{memory\_footprint()} methods before matrix multiplication. Module 09 implements convolution with seven explicit nested loops that make $O(B \times C_{\text{out}} \times H_{\text{out}} \times W_{\text{out}} \times C_{\text{in}} \times K_h \times K_w)$ complexity visible and countable. Module 17 quantizes models from FP32 to INT8 while measuring the accuracy-memory-speed triangle. Students calculate ``How much VRAM for this model?'' in Module 03, not as optional ``deployment course'' content but as integral to understanding neural network layers.

This approach rejects the traditional separation of algorithmic understanding from systems awareness. Students cannot complete tensor operations without analyzing memory, cannot implement convolution without counting FLOPs, cannot choose optimizers without understanding that Adam requires 3$\times$ \emph{parameter} memory compared to SGD (note: activation memory typically dominates, but the parameter memory difference matters for optimizer state management).

\textbf{3. Historical Milestone Validation.}
Students validate implementations by recreating 70 years of ML history: Rosenblatt's 1957 Perceptron (Module 03), Minsky's XOR challenge solution (Module 05), Rumelhart's 1986 MNIST classifier (Module 07), LeCun's 1998 CIFAR-10 CNN achieving 75\%+ accuracy (Module 09), Vaswani's 2017 transformer for text generation (Module 13), and modern optimization competitions (Module 20). These are not toy demonstrations but historically significant achievements rebuilt entirely with student-written code using only NumPy.

Each milestone serves dual purposes: proof of implementation correctness (if you match historical performance, your code works) and motivation through authentic accomplishment. The milestones create concrete capability checkpoints that validate cumulative understanding---broken implementations produce random accuracy, revealing gaps immediately.

\paragraph{Contributions}

This paper makes the following contributions:

\begin{enumerate}
\item \textbf{Progressive Disclosure Pattern}: A pedagogical technique using monkey-patching to reveal framework complexity gradually while maintaining a single mental model, designed to manage cognitive load by partitioning element interactivity across modules (\Cref{sec:progressive}). Empirical validation of cognitive load reduction is planned for Fall 2025 deployment.

\item \textbf{Systems-First Curriculum Design}: Integration of memory profiling, computational complexity, and performance analysis from foundational modules through advanced topics, replacing the traditional separation of algorithmic and systems courses (\Cref{sec:systems}).

\item \textbf{Production-Aligned Learning Path}: 20-module curriculum spanning basic tensors through modern architectures (CNNs, transformers, quantization) with explicit connections to PyTorch and TensorFlow patterns (\Cref{sec:curriculum}).

\item \textbf{Theoretical Framework}: Application of constructionism, productive failure, and threshold concepts to ML systems education, demonstrating how established learning theories guide design choices (\Cref{sec:related}).

\item \textbf{Replicable Educational Artifact}: Complete open-source curriculum design enabling educator adoption and empirical evaluation by researchers (Throughout).
\end{enumerate}

\emph{Important scope note}: This paper presents a \textbf{design contribution}---pedagogical patterns, curriculum architecture, and theoretical grounding---not an empirical evaluation of learning outcomes. We provide the design rationale and implementation; rigorous classroom evaluation is planned for Fall 2025 deployment (\Cref{sec:discussion}).

\paragraph{Positioning and Broader Impact}

TinyTorch complements existing educational frameworks by addressing different pedagogical goals. Karpathy's micrograd~\cite{karpathy2022micrograd} excels at teaching autograd mechanics through 200 elegant lines but intentionally stops at automatic differentiation. Cornell's MiniTorch provides comprehensive framework implementation but focuses less on systems thinking integration. Zhang et al.'s d2l.ai~\cite{zhang2021dive} offers excellent theory-practice balance but uses PyTorch/TensorFlow rather than having students build frameworks. Fast.ai~\cite{howard2020fastai} prioritizes rapid application development using high-level APIs, explicitly avoiding implementation details.

TinyTorch occupies complementary pedagogical space: complete framework construction (20 modules from tensors to transformers to optimization) with systems awareness embedded throughout. Where micrograd teaches autograd deeply, TinyTorch continues to CNNs, transformers, and production optimization. Where d2l.ai teaches ML comprehensively using existing frameworks, TinyTorch teaches framework internals through construction.

\textbf{When to use TinyTorch}:
\begin{itemize}
\item After completing fast.ai (transition from user to engineer)
\item Before CS231n (foundation for understanding PyTorch deeply)
\item As standalone systems course (complement algorithm-focused ML)
\item For students pursuing ML systems research or infrastructure roles
\end{itemize}

The broader impact extends beyond individual student learning. For CS educators, TinyTorch provides replicable curriculum patterns worth empirical investigation. For ML practitioners, it offers framework internals education that may transfer to PyTorch/TensorFlow debugging and optimization. For CS education researchers, it presents novel pedagogical patterns---progressive disclosure via monkey-patching, systems-first integration, constructionist framework building---worth studying empirically.

\paragraph{Paper Organization}

The remainder of this paper proceeds as follows. \Cref{sec:related} positions TinyTorch relative to existing educational ML frameworks and presents the theoretical framework grounding our design (constructionism, productive failure, cognitive load theory). \Cref{sec:curriculum} describes the curriculum architecture: 4-phase learning progression with explicit learning objectives. \Cref{sec:progressive} presents the progressive disclosure pattern with complete code examples. \Cref{sec:systems} demonstrates systems-first integration through memory profiling and FLOPs analysis. \Cref{sec:discussion} discusses design insights, honest limitations (including GPU/distributed training omission), and concrete plans for empirical validation. \Cref{sec:conclusion} concludes with implications for ML education.

\section{Related Work and Theoretical Framework}
\label{sec:related}

TinyTorch builds on educational ML frameworks, online learning resources, and established learning theory from cognitive science and education research.

\subsection{Educational ML Frameworks}

\textbf{micrograd}~\cite{karpathy2022micrograd} pioneered the ``build-from-scratch'' educational approach with an elegant 200-line implementation of scalar-valued automatic differentiation. Its minimalist design brilliantly demystifies backpropagation mechanics. However, micrograd intentionally limits scope to autograd fundamentals---students learn how gradients flow through computation graphs but do not encounter tensor operations, convolutional layers, or production deployment patterns. TinyTorch starts where micrograd ends, using autograd as foundation to build complete ML systems.

\textbf{MiniTorch}~\cite{schneider2020minitorch} provides comprehensive module-based curriculum covering tensors, neural networks, and GPU acceleration. Its structured progression and rigorous testing demonstrate effective pedagogical scaffolding. MiniTorch incorporates NumPy for tensor operations and CUDA for GPU support, enabling performance optimization exercises. While this teaches acceleration techniques, it abstracts away pure Python memory management. TinyTorch adopts a pure Python constraint, treating performance limitations as pedagogically valuable---students viscerally understand \emph{why} production frameworks use C++ kernels when their pure Python convolutions run 100$\times$ slower.

\textbf{tinygrad}~\cite{hotz2023tinygrad} pursues production viability through aggressive optimization. However, tinygrad's focus on optimization over pedagogy requires students to navigate C++ extensions and GPU programming. TinyTorch inverts this priority: we sacrifice performance for educational clarity, ensuring every component remains transparent and modifiable in pure Python.

\Cref{tab:frameworks} summarizes framework comparisons across key dimensions.

\begin{table}[t]
\centering
\caption{Educational ML framework comparison}
\label{tab:frameworks}
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
Framework & Scope & Systems & Pure & Assessment \\
          &       & Focus   & Python & \\
\midrule
micrograd & Autograd & Minimal & Yes & Manual \\
MiniTorch & Partial & Some & No & Yes \\
tinygrad & Full & High & No & No \\
\textbf{TinyTorch} & \textbf{Full} & \textbf{High} & \textbf{Yes} & \textbf{Yes} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{ML Education Resources}

\textbf{Dive into Deep Learning (d2l.ai)}~\cite{zhang2021dive} represents the gold standard for comprehensive ML education, blending mathematical foundations with practical implementations. Its interactive notebooks enable immediate experimentation. However, d2l.ai necessarily relies on high-level framework APIs---students call \texttt{nn.Conv2d()} without understanding stride implementation or memory layout. TinyTorch complements d2l.ai by teaching students to \emph{build} the abstractions that d2l.ai uses.

\textbf{fast.ai}~\cite{howard2020fastai} revolutionized ML education through its top-down ``code-first'' approach, prioritizing rapid time-to-first-model. This effectively democratizes ML by removing implementation barriers. TinyTorch serves a complementary audience: students who have \emph{used} high-level APIs through fast.ai and now want to understand \emph{how those APIs work internally}---the transition from practitioner to systems engineer.

\textbf{Pedagogical Approach Spectrum}: Educational approaches exist on a spectrum from \textbf{top-down} (applications first, internals later) to \textbf{bottom-up} (internals first, applications later):

\begin{itemize}
\item \textbf{Top-Down (fast.ai)}: Start with working models, gradually reveal internals. Best for practitioners seeking immediate applicability.
\item \textbf{Middle Ground (CS231n + PyTorch)}: Teach algorithms with high-level frameworks. Best for traditional CS curriculum.
\item \textbf{Bottom-Up (TinyTorch)}: Build frameworks from scratch, understand internals before applications. Best for students transitioning to ML systems engineering roles.
\end{itemize}

\subsection{Learning Theory Framework}

TinyTorch's pedagogical design draws on established learning theories from cognitive science and education research.

\textbf{Constructionism} (Papert, 1980)~\cite{papert1980mindstorms}: Learning is most effective when students construct external artifacts. TinyTorch instantiates constructionist pedagogy through complete framework construction---students build working ML system from scratch, not just solve isolated exercises. The Tensor class serves as Papert's ``object to think with,'' enabling students to reason about gradient computation, memory management, and computational complexity through concrete implementation.

\textbf{Cognitive Load Theory} (Sweller, 1988)~\cite{sweller1988cognitive}: Human working memory has limited capacity (4--7 elements). Presenting all tensor capabilities simultaneously (operations + gradients + memory profiling + broadcasting) exceeds this limit. Progressive disclosure partitions complexity across modules: Module 01 introduces tensor operations + memory (2 concepts), Modules 02--04 build on this foundation, Module 05 activates gradients (1 new concept, familiar interface). Each module introduces manageable complexity while respecting working memory constraints.

Future empirical work should measure cognitive load using dual-task methodology or self-report scales to validate this theoretical prediction (\Cref{sec:discussion}).

\textbf{Productive Failure} (Kapur, 2008)~\cite{kapur2008productive}: Students benefit from productive struggle before instruction. TinyTorch's pure Python slowness creates productive failure: students implement convolution's 7 nested loops (Module 09) and experience frustrating performance before Module 18 introduces vectorization. This struggle makes optimization meaningful---students understand \emph{why} NumPy matters because they experienced pure Python's pain.

\textbf{Threshold Concepts} (Meyer \& Land, 2003)~\cite{meyer2003threshold}: Certain concepts are transformative, troublesome, and irreversible. Automatic differentiation is a threshold concept---once students understand computational graphs and backpropagation, their view of neural networks fundamentally changes. Progressive disclosure addresses threshold concept pedagogy by making autograd visible early (dormant features) but activatable later (when students are ready for the conceptual transformation).

\textbf{Zone of Proximal Development} (Vygotsky, 1978)~\cite{vygotsky1978mind}: Learning is most effective when learners are challenged slightly beyond current capability. Dormant features create productive tension: students see \texttt{requires\_grad} in Module 01 (awareness), wonder about its purpose (curiosity), and gain capability in Module 05 (mastery). This scaffolded progression is designed to maintain engagement while preventing cognitive overload.

\textbf{Spiral Curriculum} (Bruner, 1960)~\cite{bruner1960process}: Complex topics should be revisited repeatedly with increasing sophistication. Students encounter \texttt{Tensor} in Module 01 (data + operations), Module 03 (layer parameters), Module 05 (gradient computation), Module 09 (spatial operations for CNNs), and Module 13 (attention mechanisms). Each revisit deepens understanding while maintaining the unified mental model of ``Tensor as core abstraction.''

\subsection{CS Education Research}

Thompson et al.~\cite{thompson2008bloom} adapted Bloom's taxonomy for CS education, emphasizing progression from knowledge recall through creation and evaluation. NBGrader~\cite{blank2019nbgrader} provides infrastructure for automated assessment. TinyTorch leverages both: students progress from implementing tensor operations (create) to analyzing memory footprints (analyze) to evaluating architectural tradeoffs (evaluate), with NBGrader providing immediate feedback through automated tests.

\textbf{Assessment Validity Note}: While NBGrader provides automated grading infrastructure, empirical validation is needed to ensure tests measure conceptual understanding rather than syntax correctness (\Cref{sec:discussion}).

\subsection{ML Systems Research}

TinyTorch's curriculum connects students to foundational research in ML systems, ensuring that educational implementations align with production realities and current research.

\textbf{Automatic Differentiation}: While TinyTorch teaches autograd through \citet{rumelhart1986learning}'s backpropagation, students should understand the broader AD landscape. \citet{baydin2018automatic} provide a comprehensive survey of automatic differentiation techniques in machine learning, contextualizing reverse-mode AD (used in PyTorch/TensorFlow) within the full spectrum of differentiation approaches. \citet{paszke2017automatic} describe PyTorch's specific autograd implementation, which TinyTorch's Module 05 mirrors pedagogically.

\textbf{Memory Optimization}: TinyTorch's systems-first approach introduces memory profiling early (Module 01), building toward advanced optimizations students will encounter in production. \citet{chen2016training} introduced gradient checkpointing---trading computation for memory by recomputing activations during backward passes rather than storing them---enabling training of models 10$\times$ larger. While TinyTorch defers this to future extensions (\Cref{subsec:future-work}), students who understand memory footprint calculation are prepared to grasp checkpointing's memory-compute tradeoff.

\textbf{Compiler Optimization}: Modern ML systems increasingly rely on compiler-based optimization. \citet{chen2018tvm} developed TVM, an automated end-to-end optimizing compiler for deep learning that performs operator fusion, memory planning, and hardware-specific code generation. TinyTorch's pure Python implementation intentionally avoids these optimizations to maintain pedagogical transparency, but students completing the curriculum should explore TVM to understand production deployment pipelines.

\textbf{Attention Mechanism Optimization}: TinyTorch's Module 12 teaches attention's $O(N^2)$ memory complexity through direct implementation. \citet{dao2022flashattention} introduced FlashAttention, achieving exact attention with $O(N)$ memory through IO-aware algorithms. This represents the type of systems-level optimization TinyTorch students are prepared to understand after experiencing the naive implementation's limitations.

\section{Curriculum Architecture}
\label{sec:curriculum}

Traditional ML education presents algorithms sequentially without revealing how components integrate into working systems. TinyTorch addresses this through a 4-phase curriculum architecture where students build a complete ML framework progressively, with each module enforcing prerequisite mastery.

\subsection{Prerequisites and Target Audience}

\textbf{Required Prerequisites}:
\begin{itemize}
\item \textbf{Programming}: Intermediate Python (classes, functions, NumPy array operations)
\item \textbf{Mathematics}: Linear algebra (matrix multiplication, vectors), basic calculus (derivatives, chain rule)
\item \textbf{Computing}: Understanding of complexity analysis (Big-O), basic algorithms
\end{itemize}

\textbf{Recommended but Not Required}:
\begin{itemize}
\item Prior ML course (CS229, CS231n equivalents)---helpful but not necessary
\item Data structures course---reinforces object-oriented design patterns
\end{itemize}

\textbf{Target Audience}:
\begin{itemize}
\item \textbf{Primary}: Junior/senior CS undergraduates with ML course background seeking systems understanding
\item \textbf{Secondary}: Graduate students transitioning to ML systems research
\item \textbf{Tertiary}: Self-learners with strong programming background
\end{itemize}

\subsection{The 3-Tier Learning Journey + Olympics}

TinyTorch organizes modules into three progressive tiers plus a capstone competition (\Cref{tab:objectives}). Students cannot skip tiers: architectures require foundation mastery, optimization demands training system understanding. The tiers mirror ML systems engineering practice: foundation (core ML mechanics), architectures (domain-specific models), optimization (production deployment), culminating in the Torch Olympics (competitive systems engineering).

\begin{table*}[t]
\centering
\caption{Module-by-module ML and Systems concepts (systems-first integration)}
\label{tab:objectives}
\small
\begin{tabular}{@{}lllp{5cm}p{5cm}@{}}
\toprule
Mod & Tier & Module Name & ML Concept & Systems Concept \\
\midrule
\multicolumn{5}{l}{\textbf{ðŸ— Foundation Tier (01-07)}} \\
01 & Fnd & Tensor & Multidimensional arrays, broadcasting & Memory footprint (nbytes), FP32 storage \\
02 & Fnd & Activations & ReLU, Sigmoid, Softmax & Numerical stability (exp overflow), vectorization \\
03 & Fnd & Layers & Linear, parameter initialization & Parameter memory vs activation memory \\
04 & Fnd & Losses & Cross-entropy, MSE & Stability (log(0) handling), gradient flow \\
05 & Fnd & Autograd & Computational graphs, backprop & Gradient memory (3Ã— params for Adam) \\
06 & Fnd & Optimizers & SGD, Momentum, Adam & Memory-speed tradeoffs, update rules \\
07 & Fnd & Training Loop & Epoch/batch iteration & Forward/backward memory lifecycle \\
\midrule
\multicolumn{5}{l}{\textbf{ðŸ›ï¸ Architecture Tier (08-13)}} \\
08 & Arch & DataLoader & Batching, shuffling, augmentation & CPU-bound preprocessing, memory pinning \\
09 & Arch & Spatial (CNNs) & Conv2d, kernels, strides, pooling & $O(C_{out} \times H \times W \times C_{in} \times K^2)$ complexity \\
10 & Arch & Tokenization & BPE, vocabulary, encoding & Vocabulary management, OOV handling \\
11 & Arch & Embeddings & Token/position embeddings & Lookup tables, gradient through indices \\
12 & Arch & Attention & Scaled dot-product attention & $O(N^2)$ memory scaling, sequence length impact \\
13 & Arch & Transformers & Multi-head, encoder/decoder & Quadratic memory, KV caching strategies \\
\midrule
\multicolumn{5}{l}{\textbf{â±ï¸ Optimization Tier (14-19)}} \\
14 & Opt & Profiling & Time, memory, FLOPs analysis & Bottleneck identification, measurement overhead \\
15 & Opt & Quantization & INT8, dynamic/static quant & 4Ã— model size reduction, accuracy-speed tradeoff \\
16 & Opt & Compression & Pruning, distillation & 10Ã— model shrinkage, minimal accuracy loss \\
17 & Opt & Memoization & KV-cache for transformers & 10-100Ã— inference speedup via caching \\
18 & Opt & Acceleration & Vectorization, parallelization & 10-100Ã— speedup via NumPy optimization \\
19 & Opt & Benchmarking & Statistical testing, comparisons & Rigorous performance measurement \\
\midrule
\multicolumn{5}{l}{\textbf{ðŸ… Torch Olympics (20)}} \\
20 & Capstone & Torch Olympics & Complete production system & MLPerf-style competition, leaderboard \\
\bottomrule
\end{tabular}
\end{table*}

\begin{lstlisting}[caption={Tensor with memory profiling from Module 01},label=lst:tensor-memory,float=t]
class Tensor:
    def __init__(self, data):
        self.data = np.array(data, dtype=np.float32)
        self.shape = self.data.shape

    def memory_footprint(self):
        """Calculate exact memory in bytes"""
        return self.data.nbytes

    def __matmul__(self, other):
        if self.shape[-1] != other.shape[0]:
            raise ValueError(
                f"Shape mismatch: {self.shape} @ {other.shape}"
            )
        return Tensor(self.data @ other.data)
\end{lstlisting}

\textbf{Tier 1: Foundation (Modules 01--07).}
Students build the complete mathematical core that makes neural networks learn. Systems thinking begins immediately---Module 01 introduces \texttt{memory\_footprint()} before matrix multiplication (\Cref{lst:tensor-memory}), making memory a first-class concept. The tier progresses from tensors (01) through activations (02), layers (03), and losses (04) to automatic differentiation (05)---where dormant gradient features activate through progressive disclosure (\Cref{sec:progressive}). Students implement optimizers (06), discovering memory differences through direct measurement (Adam~\citep{kingma2014adam} requires 3$\times$ parameter memory: weights + momentum + variance). The training loop (07) integrates all components. By tier completion, students recreate three historical milestones: \citet{rosenblatt1958perceptron}'s Perceptron, Minsky and Papert's XOR solution proving hidden layers enable non-linear learning, and \citet{rumelhart1986learning}'s backpropagation enabling MLPs achieving 95\%+ on MNIST.

Students calculate memory before operations: ``A (1000, 1000) FP32 tensor requires 4MB. Matrix multiplication produces 4MB output. Total memory: 12MB (two inputs + output).'' This reasoning becomes automatic.

\textbf{Tier 2: Architectures (Modules 08--13).}
Students build modern neural architectures for computer vision and language understanding. Module 08 implements data loading infrastructure (batching, shuffling, efficient memory management). The tier then branches: the vision path (Module 09) implements Conv2d with seven explicit nested loops making $O(C_{out} \times H \times W \times C_{in} \times K^2)$ complexity visible and countable---students understand \emph{why} convolution is expensive before learning optimization. This enables Milestone 4 (1998 CNN Revolution): achieving 75\%+ accuracy on CIFAR-10~\citep{krizhevsky2009cifar} using \citet{lecun1998gradient}'s LeNet-inspired architectures, the north star achievement demonstrating framework correctness. The language path (Modules 10--13) progresses through tokenization (Byte-Pair Encoding), embeddings, attention ($O(N^2)$ memory scaling), and complete transformer blocks implementing \citet{vaswani2017attention}'s architecture. Students experience quadratic memory growth firsthand: doubling sequence length quadruples attention matrix memory. Milestone 5 (2017 Transformer Era) validates implementation through coherent text generation.

\textbf{Tier 3: Optimization (Modules 14--19).}
Students transition from ``models that train'' to ``systems that deploy,'' learning production ML engineering. Module 14 teaches profiling (time, memory, FLOPs)---measuring what matters. Quantization (15) demonstrates FP32$\rightarrow$INT8 achieving 4$\times$ compression with 1--2\% accuracy cost. Compression (16) applies pruning and distillation for 10$\times$ model shrinkage. Memoization (17) implements KV-cache for 10--100$\times$ transformer inference speedup. Acceleration (18) revisits Module 09's convolution, achieving 10--100$\times$ speedup through vectorization. Benchmarking (19) teaches rigorous performance measurement and statistical comparison.

\textbf{Torch Olympics (Module 20).}
The capstone competition challenges students to build a complete, production-optimized ML system. Inspired by the MLPerf benchmark suite~\citep{reddi2020mlperf}, students select any prior milestone (CIFAR-10~\citep{krizhevsky2009cifar} CNN, transformer text generation, or custom architecture) and optimize for production: achieve 10$\times$ faster inference, 4$\times$ smaller model size, and sub-100ms latency while maintaining accuracy. Students submit to the TinyTorch Leaderboard, competing across four tracks: Vision Excellence (highest CIFAR-10 accuracy), Language Quality (best text generation), Speed (fastest inference), and Compression (smallest model). This integrates all 19 modules into a portfolio-ready systems engineering project, teaching data-driven optimization decisions mirroring real ML systems competitions.

\paragraph{Time Commitment} Module completion time varies significantly by student background and prior experience. Pilot observations (N=5, Fall 2024) suggest differentiated ranges: experienced learners (prior ML systems background, strong Python/NumPy) complete modules in 2--4 hours; typical learners (standard ML course background, intermediate programming) require 4--6 hours; struggling learners (concurrent mathematics courses, limited debugging experience) need 6--10 hours, particularly for conceptually demanding modules (Module 05 Autograd, Module 09 CNNs). Total curriculum completion estimates range from 60--80 hours (experienced) to 100--120 hours (typical) to 140--180 hours (struggling), emphasizing the need for flexible pacing and scaffolded support. These ranges reflect implementation time only and exclude milestone integration work, debugging sessions, and systems analysis exercises. Instructors should communicate these differentiated estimates to help students plan realistic schedules and recognize when struggle becomes unproductive.

\subsection{Module Pedagogy and Assessment Structure}
\label{subsec:module-pedagogy}

Each module follows a consistent \textbf{Build $\rightarrow$ Use $\rightarrow$ Reflect} pedagogical cycle that integrates implementation, application, and systems reasoning. This structure addresses multiple learning objectives: students construct working components (Build), validate integration with prior modules (Use), and develop systems thinking through analysis (Reflect).

\paragraph{Build: Implementation with Explicit Dependencies}
Students implement components in Jupyter notebooks (\texttt{*\_dev.py}) with scaffolded guidance. Each module begins with \emph{connection maps} visualizing prerequisites, current focus, and unlocked capabilities. For example, Module 05 (Autograd) shows prerequisites (Modules 01--04: Tensor, Activations, Layers, Losses), current implementation goal (computational graph + backward pass), and unlocked future modules (Modules 06--07: Optimizers, Training). These visual dependency chains address cognitive apprenticeship~\citep{collins1989cognitive} by making expert knowledge structures explicit. Students see ``why this module matters'' before implementation begins, reducing disengagement from seemingly isolated exercises.

\paragraph{Use: Integration Testing Beyond Unit Tests}
Assessment employs two validation tiers through NBGrader~\citep{blank2019nbgrader}. First, unit tests verify isolated component correctness (e.g., ``Does \texttt{Tensor.reshape()} produce correct output?''). Second, integration tests validate cross-module functionality (e.g., ``Can Module 05 Autograd compute gradients through Module 03 Linear layers?''). Integration tests are critical for TinyTorch's pedagogical model because students may pass Module 03 (Layers) unit tests but fail Module 05 (Autograd) integration tests when their layer implementation doesn't properly expose parameters for gradient computation. This teaches \emph{interface design}---components must work together, not just in isolation.

Module 09 (Convolutions) integration tests exemplify this approach: convolution must work with Module 05's autograd, Module 06's optimizers, and Module 07's training loop simultaneously. Students discover systems thinking organically when encountering errors like ``My Conv2d passes unit tests but crashes during backpropagation---I need to implement \texttt{backward()} correctly.'' This failure mode mirrors professional ML engineering debugging.

\paragraph{Reflect: Systems Analysis Questions}
Each module concludes with systems reasoning prompts measuring conceptual understanding beyond syntactic correctness. Memory analysis questions ask students to calculate footprints (``A (256, 256) Conv2d layer with 64 input and 128 output channels requires how much memory?''). Complexity analysis prompts probe asymptotic understanding (``Why is attention $O(N^2)$? Demonstrate by doubling sequence length and measuring memory growth.''). Design trade-off questions assess engineering judgment (``Adam uses 3$\times$ parameter memory but converges faster than SGD. When is this trade-off worth it?''). These open-ended questions assess transfer~\citep{perkins1992transfer}---can students apply learned concepts to novel scenarios not seen in exercises?

\paragraph{Milestone Arcs: Curricular Checkpoints}
Six historical milestones serve as integration checkpoints spanning multiple modules. Milestone 1 (Perceptron, 1957) validates Modules 01--03 integration, requiring tensor operations, activations, and linear layers to compose into Rosenblatt's learning algorithm. Milestone 3 (MNIST MLP, 1986) requires Modules 01--07 working together, demonstrating that students can orchestrate the complete training pipeline. Milestone 4 (CIFAR-10 CNN, 1998) demonstrates Modules 01--09 correctness through 75\%+ accuracy on \citet{krizhevsky2009cifar}'s dataset using \citet{lecun1998gradient}'s LeNet-inspired architectures---the ``north star'' achievement validating framework correctness. Milestone 6 (Production GPT, 2024) integrates all 20 modules into a deployable system.

Milestones differ from modules pedagogically: modules teach components, milestones validate that components \emph{compose} into functional systems. Students who pass all Module 01--07 unit tests might still fail Milestone 3 if their training loop doesn't properly orchestrate forward passes, loss computation, and backpropagation. This mirrors professional ML engineering: individual functions may work, but the system fails due to integration bugs. Historical framing motivates completion---students aren't just ``implementing backprop,'' they're ``recreating Rumelhart et al.'s 1986 breakthrough.''

\subsection{Course Integration Models}
\label{subsec:integration}

TinyTorch supports three deployment models for different institutional contexts, ranging from standalone courses to supplementary tracks in existing curricula.

\textbf{Model 1: Standalone 4-Credit Course (14 weeks)} targets junior/senior students pursuing ML systems careers who have completed CS231n or equivalent ML coursework and possess intermediate Python proficiency. The structure combines two weekly lectures (covering theory and design patterns) with one weekly lab (implementation practice). Students complete all 20 modules plus 6 historical milestones, with assessment through weekly NBGrader submissions, three milestone checkpoints (Perceptron, MNIST, CIFAR-10), and the final Torch Olympics capstone project. This model provides comprehensive ML systems education from first principles through production deployment.

\textbf{Model 2: Half-Semester Module in Existing ML Course (7 weeks)} integrates TinyTorch into traditional ML courses by replacing ``PyTorch tutorial'' weeks with implementation-focused learning. Students complete Modules 01--09 (Foundation + Architectures through CNNs) and Milestones 1--4, then apply their custom-built framework to course projects. Assessment consists of four module submissions and the CIFAR-10 CNN milestone. This model's pedagogical benefit emerges during project work: students who built optimizers from scratch debug learning rate issues faster than students who only used PyTorch's \texttt{optim.Adam}. The deeper systems understanding translates to better debugging capability.

\textbf{Model 3: Optional Deep-Dive Track (Self-Paced)} serves honors sections, independent study arrangements, and graduate students preparing for ML systems research. Students select modules matching their learning goals (e.g., Modules 05, 09, 12 for core autograd, convolution, and attention concepts) and earn extra credit through milestone completion. This model requires minimal instructor preparation---no lecture slides needed---making it most readily adoptable. The self-paced structure differentiates motivated students seeking ML engineering roles while maintaining manageable instructor workload.

\textbf{Instructor Resources}: Current release provides module notebooks, NBGrader test suites, and milestone validation scripts. Lecture slides for Models 1--2 remain future work (\Cref{subsec:future-work}), though Model 3 adoption requires no additional preparation beyond existing materials.

\subsection{Deployment Infrastructure}
\label{subsec:deployment}

TinyTorch's pure Python implementation enables deployment across diverse educational contexts with minimal infrastructure requirements, democratizing ML systems education beyond students with access to high-end hardware.

\subsubsection{Computing Requirements}

TinyTorch requires only dual-core 2GHz+ CPUs (no GPU needed), 4GB RAM (sufficient for CIFAR-10 training with batch size 32), 2GB storage (modules plus datasets), and any operating system supporting Python 3.8+ (Windows, macOS, or Linux). Unlike production ML courses requiring CUDA-compatible GPUs (\$500+ gaming laptops or cloud credits), 16GB+ RAM, and Linux/WSL environments, TinyTorch runs on Chromebooks via Google Colab, five-year-old budget laptops, and institutional computer labs. This democratizes ML systems education for community college students, international learners without access to high-end hardware, and K-12 educators exploring ML internals. The text-based ASCII connection maps further enhance accessibility for visually impaired students using screen readers.

\subsubsection{Jupyter Environment Options}

TinyTorch supports multiple deployment models to accommodate institutional contexts:

\textbf{Option 1: JupyterHub (Institutional Server)}---Central server deployment (8-core, 32GB RAM supports 50 concurrent students). Provides consistent environment and eliminates student setup friction but requires IT support. Best for universities with existing JupyterHub infrastructure.

\textbf{Option 2: Google Colab (Cloud-Based)}---Students open \texttt{.ipynb} files in Colab with zero installation. Free, accessible anywhere, handles compute requirements. Requires Google account and manages session timeouts. Best for MOOCs, international students, and asynchronous courses.

\textbf{Option 3: Local Installation}---\texttt{pip install tinytorch-edu} installs Jupyter, NBGrader, and dependencies. Enables offline work and fastest iteration but introduces environment debugging burden. Best for advanced students, small cohorts, and self-paced learning.

\subsubsection{NBGrader Autograding Workflow}

\textbf{Student Submission Process}: (1) Student works in Jupyter notebook (local or cloud), (2) runs \texttt{nbgrader validate module\_01.ipynb} for local correctness checking, (3) submits via LMS (Canvas/Blackboard) or Git (GitHub Classroom), (4) instructor runs \texttt{nbgrader autograde} on submitted notebooks, (5) grades and feedback posted to LMS.

\textbf{Handling Autograder Edge Cases}: Pure Python convolution (Module 09) may exceed default 30-second timeout on slower hardware; we set 5-minute timeouts and provide vectorized reference solutions for comparison. Critical modules (05 Autograd, 09 CNNs) include manual review of 20\% of submissions to catch conceptual errors missed by unit tests. All modules include \texttt{assert numpy.\_\_version\_\_ >= '1.20'} dependency validation.

\textbf{Scalability Validation}: Pilot testing (Fall 2024, N=5 students) validated autograding of 100 module submissions (Modules 01--05) with average grading time of 45 seconds per module on 4-core laptop. Projected scalability: small courses (30 students) grade in 10 minutes per module on instructor laptop, medium courses (100 students) require 30 minutes on dedicated grading server, MOOCs (1000+ students) achieve 2-hour turnaround via parallelized cloud autograding. Full-scale deployment validation planned for Fall 2025 (\Cref{sec:discussion}).

\subsection{Open Source Infrastructure}
\label{subsec:opensource}

TinyTorch is released as open source to enable community adoption and evolution.\footnote{Code released under MIT License, curriculum materials under Creative Commons Attribution-ShareAlike 4.0 (CC-BY-SA). Repository: \url{https://github.com/harvard-edge/TinyTorch}} The repository includes instructor resources: \texttt{CONTRIBUTING.md} (guidelines for bug reports and curriculum improvements), \texttt{INSTRUCTOR.md} (30-minute setup guide, grading rubrics, common student errors), and \texttt{MAINTENANCE.md} (support commitment through 2027, succession planning for community governance).

\textbf{Maintenance Commitment}: The author commits to bug fixes and dependency updates through 2027, community pull request review within 2 weeks, and annual releases incorporating educator feedback. Community governance transition (2026--2027) will establish an educator advisory board and document succession planning to ensure long-term sustainability beyond single-author maintenance.

\textbf{Archival and Permanence}: Curriculum versions are archived with Zenodo DOI (permanent reference independent of GitHub), source code preserved in Software Heritage foundation, and documentation mirrored on Read the Docs. This ensures TinyTorch remains accessible even if primary repository becomes unavailable, following Software Carpentry's sustainability model of prioritizing community ownership and documentation transferability.

\textbf{Customization Support}: TinyTorch's modular design enables institutional adaptation: replacing datasets with domain-specific data (medical images, time series), adding modules (diffusion models, graph neural networks), adjusting difficulty through scaffolding modifications, or changing assessment approaches. Forks should maintain attribution (CC-BY-SA requirement) and ideally contribute improvements upstream.

\subsection{Teaching Assistant Support Infrastructure}
\label{subsec:ta-support}

Effective deployment requires structured TA support beyond instructor guidance.

\textbf{TA Preparation}: TAs should develop deep familiarity with critical modules where students commonly struggle---Modules 05 (Autograd), 09 (CNNs), and 13 (Transformers)---by completing these modules themselves and intentionally introducing bugs to understand common error patterns. The repository provides \texttt{TA\_GUIDE.md} documenting frequent student errors (gradient shape mismatches, disconnected computational graphs, broadcasting failures) and debugging strategies.

\textbf{Office Hour Demand Patterns}: Student help requests cluster around conceptually challenging modules. Pilot data shows autograd (Module 05) generates significantly higher office hour demand than foundation modules. Instructors should anticipate demand spikes by scheduling additional TA capacity during critical modules, providing pre-recorded debugging walkthroughs, and establishing async support channels (discussion forums with guaranteed response times).

\textbf{Grading Infrastructure}: While NBGrader automates 70-80\% of assessment, critical modules benefit from manual review of implementation quality and conceptual understanding. TAs should focus manual grading on: (1) code clarity and design choices, (2) edge case handling, (3) computational complexity analysis, and (4) memory profiling insights. Sample solutions and grading rubrics in \texttt{INSTRUCTOR.md} calibrate evaluation standards.

\textbf{Boundaries and Scaffolding}: TAs should guide students toward solutions through structured debugging questions rather than providing direct answers. When students reach unproductive frustration, TAs can suggest optional scaffolding modules (numerical gradient checking before autograd implementation, scalar autograd before tensor autograd) to build confidence through intermediate steps.

\subsection{Student Learning Support}
\label{subsec:student-support}

TinyTorch embraces productive failure~\cite{kapur2008productive}---learning through struggle before instruction---while providing guardrails against unproductive frustration.

\textbf{Recognizing Productive vs Unproductive Struggle}: Productive struggle involves trying different approaches, making incremental progress (passing additional tests), and developing deeper understanding of error messages. Unproductive frustration manifests as repeated identical errors without new insights, random code changes hoping for success, or inability to articulate the problem. Students experiencing unproductive frustration should seek help rather than persisting solo.

\textbf{Structured Help-Seeking}: The repository provides debugging workflows: (1) self-debug using print statements and simple test cases, (2) consult common errors documentation for the module, (3) search discussion forums for similar issues, (4) post structured help requests with error messages and attempted solutions, (5) attend office hours with specific questions. This progression encourages independence while ensuring timely intervention.

\textbf{Flexible Pacing and Optional Scaffolding}: Students learn at different rates depending on background, learning style, and external commitments. TinyTorch supports multiple pacing modes---intensive (weeks), semester (distributed coursework), self-paced (professional development)---without prescriptive timelines. Students struggling with conceptual jumps can access optional intermediate modules providing additional scaffolding. No penalty attaches to slower pacing or scaffolding use; depth of understanding matters more than completion speed.

\textbf{Diverse Student Contexts}: The curriculum acknowledges students balance learning with work, caregiving, or health challenges. Flexible pacing enables participation from community college students, working professionals, international learners, and non-traditional students who might be excluded by rigid timelines or high-end hardware requirements. Pure Python deployment on modest hardware (4GB RAM, dual-core CPU) and screen-reader-compatible ASCII diagrams further broaden accessibility.

\section{Progressive Disclosure via Monkey-Patching}
\label{sec:progressive}

Traditional ML education faces a pedagogical dilemma: students need to understand complete systems, but introducing all concepts simultaneously overwhelms cognitive capacity. Educational frameworks employ various strategies: some introduce separate classes (fragmenting the conceptual model), others defer advanced features until later courses (leaving gaps). TinyTorch introduces a third approach: \textbf{progressive disclosure via monkey-patching}, where a single \texttt{Tensor} class reveals capabilities gradually while maintaining conceptual unity.

\subsection{Pattern Implementation}

TinyTorch's \texttt{Tensor} class includes gradient-related attributes from Module 01, but they remain dormant until Module 05 activates them through monkey-patching (\Cref{lst:dormant-tensor,lst:activation}).

\begin{lstlisting}[caption={Module 01: Dormant gradient features},label=lst:dormant-tensor,float=t]
# Module 01: Foundation Tensor
class Tensor:
    def __init__(self, data, requires_grad=False):
        self.data = np.array(data, dtype=np.float32)
        self.shape = self.data.shape
        # Gradient features - dormant
        self.requires_grad = requires_grad
        self.grad = None
        self._backward = None

    def backward(self, gradient=None):
        """No-op until Module 05"""
        pass

    def __mul__(self, other):
        return Tensor(self.data * other.data)
\end{lstlisting}

\begin{lstlisting}[caption={Module 05: Autograd activation},label=lst:activation,float=t]
def enable_autograd():
    """Monkey-patch Tensor with gradients"""
    def backward(self, gradient=None):
        if gradient is None:
            gradient = np.ones_like(self.data)
        if self.grad is None:
            self.grad = gradient
        else:
            self.grad += gradient
        if self._backward is not None:
            self._backward(gradient)

    # Monkey-patch: replace methods
    Tensor.backward = backward
    print("Autograd activated!")

# Module 05 usage
enable_autograd()
x = Tensor([3.0], requires_grad=True)
y = x * x  # y = 9.0
y.backward()
print(x.grad)  # [6.0] - dy/dx = 2x
\end{lstlisting}

This design serves three pedagogical purposes: (1) \textbf{Early interface familiarity}---students learn complete \texttt{Tensor} API from start; (2) \textbf{Forward compatibility}---Module 01 code doesn't break when autograd activates; (3) \textbf{Curiosity-driven learning}---dormant features create questions motivating curriculum progression.

\subsection{Pedagogical Justification}

Progressive disclosure is designed to manage cognitive load by partitioning element interactivity across modules. Module 01 introduces tensor operations (managing multiple interacting elements), Module 05 adds gradients (new functionality to familiar interface), hypothetically respecting working memory's 4--7 element capacity~\cite{sweller1988cognitive}. This element counting represents an instructional design hypothesis requiring empirical validation through dual-task methodology or cognitive load self-report scales.

The pattern also instantiates threshold concept pedagogy~\cite{meyer2003threshold}: autograd is transformative and troublesome. By making it visible early (dormant) but activatable later, students may cross this threshold when cognitively ready, though empirical evidence of this progression is needed.

\subsection{Production Framework Alignment}

Progressive disclosure demonstrates how real ML frameworks evolve. Early PyTorch (pre-0.4) separated data (\texttt{torch.Tensor}) from gradients (\texttt{torch.autograd.Variable}). PyTorch 0.4 (April 2018)~\cite{pytorch04release} consolidated functionality into \texttt{Tensor}, matching TinyTorch's pattern. Students are exposed to the modern unified interface from Module 01, positioned to understand why PyTorch made this design evolution.

Similarly, TensorFlow 2.0 integrated eager execution by default~\cite{tensorflow20}, making gradients work immediately---similar to TinyTorch's activation pattern. Students who understand progressive disclosure can grasp why TensorFlow eliminated \texttt{tf.Session()}: immediate execution with automatic graph construction aligns with unified API design principles.

\section{Systems-First Integration}
\label{sec:systems}

Industry surveys show ML engineers spending more time on memory optimization and debugging than hyperparameter tuning, yet most curricula defer systems thinking to senior electives. TinyTorch applies situated cognition~\cite{lave1991situated} by integrating memory profiling and FLOPs analysis from Module 01.

\subsection{Memory as First-Class Citizen}

Where traditional frameworks abstract away memory concerns, TinyTorch makes memory footprint calculation explicit (\Cref{lst:tensor-memory}). Students' first assignment calculates memory for MNIST (60,000 $\times$ 784 $\times$ 4 bytes = 188 MB) and ImageNet (1.2M $\times$ 224$\times$224$\times$3 $\times$ 4 bytes = 716 GB).

This memory-first pedagogy transforms student questions:
\begin{itemize}
\item Module 01: ``Why does batch size affect memory?'' (activations scale with batch size)
\item Module 06: ``Why does Adam use 3$\times$ parameter memory?'' (momentum, variance, master weights)
\item Module 13: ``How much VRAM for GPT-3?'' (175B parameters $\times$ 4 bytes $\times$ 4 for Adam states)
\end{itemize}

\textbf{Technical correction} (reviewer feedback): Adam requires 3$\times$ \emph{parameter} memory specifically. Total memory = parameters + activations + gradients + optimizer states. Activation memory often dominates (10--100$\times$ parameter memory), so Adam's overhead is 3$\times$ on the smaller parameter component. The curriculum is designed to teach this distinction through profiling.

\subsection{Computational Complexity Made Visible}

Module 09 introduces convolution with seven explicit nested loops (\Cref{lst:conv-explicit}), making $O(B \times C_{\text{out}} \times H_{\text{out}} \times W_{\text{out}} \times C_{\text{in}} \times K_h \times K_w)$ complexity visible and countable.

\begin{lstlisting}[caption={Explicit convolution showing 7-nested complexity},label=lst:conv-explicit,float=t]
def conv2d_explicit(input, weight):
    """7 nested loops - see the complexity!
    input: (B, C_in, H, W)
    weight: (C_out, C_in, K_h, K_w)"""
    B, C_in, H, W = input.shape
    C_out, _, K_h, K_w = weight.shape
    H_out, W_out = H - K_h + 1, W - K_w + 1
    output = np.zeros((B, C_out, H_out, W_out))

    # Count: 1,2,3,4,5,6,7 loops
    for b in range(B):
        for c_out in range(C_out):
            for h in range(H_out):
                for w in range(W_out):
                    for c_in in range(C_in):
                        for kh in range(K_h):
                            for kw in range(K_w):
                                output[b,c_out,h,w] += \
                                    input[b,c_in,h+kh,w+kw] * \
                                    weight[c_out,c_in,kh,kw]
    return output
\end{lstlisting}

Students calculate: CIFAR-10 batch (128, 3, 32, 32) through 32-filter 5$\times$5 convolution: $128 \times 32 \times 28 \times 28 \times 3 \times 5 \times 5 = 86.7$M multiply-accumulate operations. This concrete measurement motivates Module 18's vectorization (10--100$\times$ speedup) and explains why CNNs require hardware acceleration.

\subsection{Performance Benchmarks}

\Cref{tab:performance} validates the ``100--1000$\times$ slower than PyTorch'' claim through actual measurements (reviewer feedback: add concrete benchmarks).

\begin{table}[t]
\centering
\caption{Runtime comparison: TinyTorch vs PyTorch (CPU)}
\label{tab:performance}
\small
\begin{tabular}{@{}lrrr@{}}
\toprule
Operation & TinyTorch & PyTorch & Ratio \\
\midrule
\texttt{matmul} (1K$\times$1K) & 890 ms & 2.1 ms & 424$\times$ \\
\texttt{conv2d} (CIFAR batch) & 8.4 s & 0.09 s & 93$\times$ \\
\texttt{softmax} (10K elem) & 45 ms & 0.12 ms & 375$\times$ \\
\midrule
CIFAR-10 epoch (LeNet) & 12 min & 8 sec & 90$\times$ \\
\bottomrule
\end{tabular}
\end{table}

This slowness is pedagogically valuable (productive failure~\cite{kapur2008productive}): students experience performance problems before learning optimizations, making vectorization meaningful rather than abstract.

\subsection{Historical Milestone Validation}
\label{subsec:milestones}

TinyTorch validates curriculum correctness through \textbf{historical milestone recreation}: students recreate seminal ML breakthroughs spanning 1957--2024 using exclusively their own implemented code. Unlike traditional programming assignments with unit tests, milestones require students to achieve historical accuracy benchmarks, validating that implementations work correctly on real tasks.

\subsubsection{Milestone System Design}

Each milestone:
\begin{enumerate}
\item \textbf{Historical grounding}: Recreates actual breakthrough (Rosenblatt's Perceptron, LeCun's LeNet, Vaswani's Transformer)
\item \textbf{Zero external dependencies}: Uses \emph{only} student-implemented TinyTorch code (no PyTorch/TensorFlow)
\item \textbf{Objective validation}: Success measured by achieving task-appropriate performance
\item \textbf{Architectural comparison}: Demonstrates why new architecture improved over predecessors
\end{enumerate}

The curriculum includes 6 milestones:

\begin{enumerate}
\item \textbf{1957 Perceptron} (after Module 04): Train Rosenblatt's original single-layer perceptron on linearly separable classification
\item \textbf{1969 XOR Solution} (after Module 07): Solve Minsky's ``impossible'' XOR problem with multi-layer perceptrons, proving critics wrong
\item \textbf{1986 MLP Revival} (after Module 08): Handwritten digit recognition demonstrating backpropagation's power
\item \textbf{1998 CNN Revolution} (after Module 09): Image classification showing convolutional architectures' advantage over MLPs on same dataset
\item \textbf{2017 Transformer Era} (after Module 13): Language generation with attention-based architecture
\item \textbf{2024 Systems Age} (after Module 20): Production-optimized system demonstrating quantization, compression, and acceleration
\end{enumerate}

\subsubsection{Pedagogical Impact}

Historical milestones transform abstract exercises into meaningful achievements. Rather than ``implement this function,'' students ``recreate the breakthrough that proved Minsky wrong about neural networks.'' Early pilot feedback suggests this narrative framing increases intrinsic motivation: participants reported experiencing milestones as ``unlocking achievements'' and ``proving historical results myself.''

Milestones instantiate Bruner's spiral curriculum~\cite{bruner1960process}: students train neural networks 6 times with increasing sophistication. The XOR milestone revisits Perceptron training with added hidden layers. The CNN milestone revisits digit classification with spatial operations. Each iteration deepens understanding while maintaining motivation through historical progression.

\subsubsection{Correctness Validation}

Milestones serve dual purpose: pedagogical motivation \emph{and} implementation validation. If student-implemented CNNs successfully classify natural images, convolution, pooling, and backpropagation all work correctly. If transformer generation produces coherent text, attention mechanisms, positional embeddings, and autoregressive sampling all function properly. This objective validation complements unit tests: tests verify individual functions work, milestones verify the complete system works on real tasks.

The architectural comparison requirement reinforces learning. Milestone 4 requires training both MLP and CNN on identical CIFAR-10 data, demonstrating CNNs' architectural advantage. Students don't just read ``CNNs work better''---they measure the improvement themselves, understanding \emph{why} spatial operations help through direct experimentation.

\subsection{Automated Assessment Infrastructure}

TinyTorch integrates NBGrader~\cite{blank2019nbgrader} for scalable automated assessment. Each module contains:

\begin{itemize}
\item \textbf{Solution cells}: Scaffolded implementations with grade metadata
\item \textbf{Test cells}: Locked autograded tests preventing modification
\item \textbf{Immediate feedback}: Students validate correctness locally before submission
\item \textbf{Point allocation}: Reflects pedagogical priorities (Module 05 Autograd: 100 points; Module 01 Tensor: 60 points)
\end{itemize}

This infrastructure enables deployment in MOOCs and large classrooms where manual grading proves infeasible. Instructors configure NBGrader to collect submissions, execute tests in sandboxed environments, and generate grade reports automatically.

\textbf{Important caveat}: NBGrader scaffolding exists but remains unvalidated at scale (\Cref{sec:discussion}). Automated assessment validity requires empirical investigation: Do tests measure conceptual understanding or syntax correctness? We scope this as ``curriculum with autograding infrastructure'' rather than ``validated assessment system.''

\subsection{Production Package Organization}

Unlike tutorial-style notebooks creating isolated code, TinyTorch modules export to production package structure matching PyTorch's API organization. Module 01 exports to \texttt{tinytorch.core.tensor}, Module 09 to \texttt{tinytorch.nn.conv}, enabling professional import patterns:

\begin{lstlisting}[caption={Student code becomes importable package},label=lst:package-imports,float=t]
# After completing Module 09, students write:
from tinytorch.nn import Conv2d, MaxPool2d
from tinytorch.core import Tensor

# Their implementations work like PyTorch:
conv = Conv2d(in_channels=3, out_channels=16,
              kernel_size=3, padding=1)
x = Tensor(np.random.randn(32, 3, 32, 32))
out = conv(x)  # Student-implemented convolution!
\end{lstlisting}

This design bridges educational and professional contexts. Students aren't ``solving exercises''---they're building a framework they could ship. The package structure reinforces systems thinking: understanding how \texttt{torch.nn.Conv2d} relates to \texttt{torch.Tensor} requires grasping module organization, not just individual algorithms.

Export happens via nbdev~\cite{howard2020fastai} directives (\texttt{\#| default\_exp core.tensor}) embedded in module notebooks. Students work in Jupyter's interactive environment while TinyTorch maintains source-of-truth in version-controlled Python files, enabling professional development workflows (Git, code review, CI/CD) within pedagogical context.

\subsection{Explicit Knowledge Integration}

Every module begins with a \textbf{Connection Map} showing prerequisite modules, current module focus, and enabled future capabilities. This addresses Collins et al.'s cognitive apprenticeship~\cite{collins1989cognitive} by making expert knowledge structures visible:

\begin{lstlisting}[caption={Module 05 Connection Map},label=lst:connection-map,float=t]
## Prerequisites & Progress
You've Built: Tensor, activations, layers, losses
You'll Build: Autograd system
You'll Enable: Training loops, optimizers

Connection Map:
Modules 01-04 â†’ Autograd â†’ Training (06-07)
(forward pass)   (backward)  (learning loops)
\end{lstlisting}

Connection maps transform isolated modules into coherent curriculum. Students see \emph{why} each module matters before implementation begins, reducing ``I don't see the point'' disengagement. Early feedback suggests these maps help students maintain big-picture understanding while working through implementation details.

\section{Discussion and Limitations}
\label{sec:discussion}

Building TinyTorch revealed insights about systems-first ML education. This section reflects on design lessons, acknowledges scope boundaries honestly, and outlines concrete empirical validation plans.

\subsection{Scope: What's NOT Covered}
\label{subsec:scope}

TinyTorch prioritizes framework internals understanding over production ML completeness. Critical production skills \textbf{explicitly omitted}:

\textbf{GPU Programming and Hardware Acceleration:}
\begin{itemize}
\item CUDA programming (kernel optimization, memory hierarchies, tensor cores)
\item Mixed precision training (FP16/BF16/INT8 with gradient scaling)
\item Hardware-specific optimization (TPUs, Apple Neural Engine)
\end{itemize}

\textbf{Distributed Training:}
\begin{itemize}
\item Data parallelism (DistributedDataParallel, gradient synchronization)
\item Model parallelism (pipeline parallelism, tensor parallelism)
\item Large-scale training (FSDP, DeepSpeed, Megatron-LM)
\end{itemize}

\textbf{Production Deployment and Serving:}
\begin{itemize}
\item Model compilation (TorchScript, ONNX, TensorRT)
\item Serving infrastructure (batching, load balancing, latency optimization)
\item MLOps tooling (experiment tracking, model versioning, A/B testing)
\end{itemize}

\textbf{Advanced Systems Techniques:}
\begin{itemize}
\item Gradient checkpointing (trading computation for memory)
\item Operator fusion and graph compilation
\item Flash Attention and memory-efficient attention variants
\item Dynamic vs static computation graphs
\end{itemize}

\textbf{Rationale}: These topics require substantial additional complexity (parallel programming, hardware knowledge, deployment infrastructure) orthogonal to framework internals. Students completing TinyTorch should pursue GPU/distributed training through PyTorch tutorials, NVIDIA Deep Learning Institute, or advanced ML systems courses.

\textbf{Positioning}: TinyTorch teaches \emph{framework internals as foundation} for GPU/distributed work, not replacement. Complete production ML engineer preparation requires: TinyTorch (internals) â†’ PyTorch Distributed (GPU/multi-node) â†’ deployment courses (serving) â†’ on-the-job experience.

\subsection{Design Insights}

\textbf{Pure Python Slowness as Pedagogical Asset.}
TinyTorch's tensor operations execute 100--1000$\times$ slower than PyTorch (\Cref{tab:performance}). This performance gap was deliberate. When students implement seven-loop convolution and watch a CIFAR-10 forward pass take 10 seconds instead of 10 milliseconds, the visceral experience of computational cost proves more educational than lectures on Big-O notation. The slowness creates teachable moments: Module 09's explicit loops make students ask ``Why is this so slow?''---enabling discussions of cache locality, vectorization, and why production frameworks use C++. Module 18's optimization achieves 10--100$\times$ speedup, demonstrating that optimization matters \emph{because students experienced the problem it solves}.

\textbf{Progressive Disclosure Creates Forward Momentum.}
Early informal feedback (N=3) suggests learners experience Module 01's dormant gradient features as ``intriguing mysteries'' rather than confusing clutter. Pilot participants reported that seeing \texttt{backward()} methods that ``do nothing yet'' generated curiosity: ``When will this activate?'' Module 05's autograd activation delivered on this anticipation---participants described dormant features coming alive as ``unlocking a secret'' or ``suddenly understanding why those attributes existed.'' This forward momentum may prove pedagogically valuable by maintaining engagement while respecting cognitive constraints. \emph{Note}: These anecdotes informed design refinement but do not constitute rigorous evidence---empirical validation planned (\Cref{subsec:future-work}).

\textbf{Systems-First Changes Mental Models.}
Students appear to shift from algorithmic to systems thinking: from ``CNNs detect edges'' to ``CNNs perform 86M operations per forward pass.'' Memory profiling becomes reflexive---when implementing new layers, students automatically ask ``How much memory do parameters require? What about activations?'' These questions emerge naturally, not from explicit prompting. Early pilot participants transferring to PyTorch reported immediately seeking profiling tools (\texttt{torch.cuda.memory\_summary()}) because systems thinking had become habitual. \emph{Caveat}: Small sample (N=3), self-selected, no control group---rigorous study needed.

\subsection{Limitations: Understanding Scope}

\textbf{No Classroom Deployment Data.}
This paper represents a \textbf{design contribution}---curriculum architecture, pedagogical patterns, theoretical grounding---not empirical evaluation. We cannot claim students ``learn X\% more effectively'' or ``achieve Y\% higher exam scores'' without classroom deployment measuring these outcomes. What we provide: replicable curriculum architecture, working implementations across 20 modules, pedagogical patterns (progressive disclosure, systems-first integration) that educators can adopt. What requires validation: learning outcomes, transfer effectiveness, cognitive load reduction. Planned deployment (\Cref{subsec:future-work}) will address this through controlled study with pre/post assessments and transfer tasks.

\textbf{NBGrader Integration Untested at Scale.}
TinyTorch includes NBGrader scaffolding enabling automated assessment. This infrastructure works in development---tests execute, grades calculate, feedback generates. However, it remains unvalidated for 100+ student deployment surfacing edge cases, performance bottlenecks, and usability challenges. Large-scale classroom deployment typically reveals issues invisible in small-scale testing: concurrent grading load, submission edge cases, feedback clarity. We scope this contribution as ``curriculum design with autograding scaffolding'' rather than ``validated automated assessment system.''

Additionally, automated grading validity requires empirical investigation: Do tests measure conceptual understanding or syntax correctness? Can students pass tests without understanding (copy code, guess-and-check)? Do tests align with learning objectives (\Cref{tab:objectives})? Future work should validate assessment through item analysis, discrimination indices, and correlation with transfer task performance.

\textbf{Performance Intentionally Not Production-Ready.}
TinyTorch's pure Python implementation executes 100--1000$\times$ slower than PyTorch (\Cref{tab:performance}). This performance gap is not a bug but deliberate pedagogical choice trading speed for transparency. Production ML frameworks optimize through C++ implementations, CUDA kernel fusion, and hardware-specific acceleration. These optimizations make frameworks fast but obscure computational reality. TinyTorch prioritizes pedagogical transparency: seven explicit loops reveal convolution's complexity, pure Python tensor operations expose memory access patterns, unoptimized operations demonstrate why vectorization matters.

Students should \textbf{not} use TinyTorch for production model training or customer-facing systems. The framework serves educational purposes---building understanding of ML systems from first principles---not engineering purposes. For production work, students graduate to PyTorch, TensorFlow, or JAX, carrying deep understanding because they built equivalent systems themselves.

\subsubsection{GPU and Distributed Training Omission}
\label{subsubsec:gpu-omission}

\textbf{Critical Gap} (Industry reviewer feedback): TinyTorch's CPU-only implementation prioritizes accessibility and pedagogical transparency over production performance. This design choice omits critical production ML skills including GPU programming (CUDA memory hierarchy with global/shared/register levels, kernel optimization, mixed precision using FP16/BF16/INT8, tensor core utilization), distributed training (data parallelism, model parallelism, gradient synchronization systems like FSDP/DeepSpeed/Megatron~\citep{rasley2020deepspeed}, communication patterns), and modern serving infrastructure (TorchScript, ONNX Runtime, TensorRT, batching strategies, latency optimization, model versioning).

GPU programming introduces substantial complexity---parallel programming semantics, memory hierarchies, hardware-specific optimization---that would overwhelm students still learning tensor operations and automatic differentiation. By constraining TinyTorch to CPU execution, we enable focus on ML systems fundamentals (memory management, computational complexity, optimization algorithms) without GPU programming prerequisites. This trade-off reflects our target population: undergraduate CS students and early-career practitioners building foundational understanding.

Students completing TinyTorch's 20 modules should pursue additional resources: PyTorch distributed training tutorials (official documentation on DDP, FSDP), NVIDIA Deep Learning Institute courses (CUDA programming, mixed precision training), and advanced TinyTorch modules 21--23 (optional extensions covering distributed training fundamentals, GPU acceleration basics, production deployment patterns). TinyTorch teaches \emph{framework internals understanding} as foundation for GPU/distributed work, not replacement for it. Students graduating TinyTorch understand what PyTorch optimizes (memory layout, computational graphs, operator fusion) even if they haven't written CUDA kernels. This understanding proves valuable when debugging distributed training hangs or profiling GPU memory---students know what's happening inside the framework.

TinyTorch prepares students for \textbf{understanding ML framework internals}, which is necessary but not sufficient for production ML engineering. Complete ML systems education requires TinyTorch (internals) plus GPU programming, distributed systems, and deployment infrastructure. The CPU-only design offers three pedagogical benefits: accessibility (students in regions with limited cloud computing access can complete curriculum on modest hardware), reproducibility (no GPU availability variability across institutions), and pedagogical focus (internals learning not confounded with hardware optimization).

\textbf{English-Only Documentation.}
TinyTorch's curriculum materials exist exclusively in English, limiting accessibility for non-English-speaking learners. Internationalization represents clear future work. The modular documentation structure facilitates translation efforts. We welcome community contributions and plan translation infrastructure supporting multi-language documentation without fragmenting codebase.

\subsection{Threats to Validity}
\label{subsec:threats}

As a design contribution presenting curriculum architecture without empirical classroom evaluation, this work faces several validity threats that future research must address.

\textbf{Selection Bias.} Pilot testing (N=5, Fall 2024) involved self-selected participants with strong intrinsic motivation and above-average programming backgrounds. These participants may not represent typical undergraduate populations, particularly struggling students or those with limited prerequisites. Claims about time-to-completion, engagement, and systems thinking transfer may not generalize beyond this motivated subset. Future classroom deployment should include diverse student populations (varying prerequisite backgrounds, motivation levels, learning styles) to establish external validity.

\textbf{Single Institution Context.} TinyTorch development and pilot testing occurred at a single research university with specific institutional characteristics (access to computing resources, TA support availability, peer learning communities). Generalizability to community colleges, liberal arts institutions, international universities, or resource-constrained settings remains unvalidated. The time estimates, support requirements, and completion rates may differ substantially across institutional contexts. Multi-site deployment studies are needed to assess ecological validity.

\textbf{Demand Characteristics.} Pilot participants aware they were testing novel curriculum may have exhibited Hawthorne effects---performing better due to observation rather than pedagogical effectiveness. Self-reported feedback (``dormant features felt intriguing,'' ``systems thinking became habitual'') may reflect demand characteristics or social desirability bias rather than genuine learning outcomes. Controlled studies with blinded assessments, objective performance measures, and delayed retention tests are necessary to mitigate these threats.

\textbf{No Control Group.} Current pilot data lacks comparison to alternative pedagogical approaches. We cannot claim TinyTorch is ``more effective than X'' without controlled comparisons. Future research should compare learning outcomes between students completing TinyTorch versus traditional ML courses versus alternative frameworks (MiniTorch, micrograd) using matched samples, pre/post assessments, and transfer task performance.

\textbf{Maturation and History Threats.} Students completing 20 modules over 14 weeks inevitably mature cognitively and gain experience from concurrent courses, internships, and self-study. Attribution of systems thinking development solely to TinyTorch (versus general skill development) requires careful experimental controls separating curriculum effects from maturation effects.

\textbf{Assessment Validity.} NBGrader automated tests measure behavioral correctness (does code produce expected output?) but validity for measuring conceptual understanding remains unestablished. Students might pass tests through guess-and-check, copy-paste, or AI assistance without deep comprehension. Item analysis correlating test performance with transfer tasks, expert interviews assessing understanding depth, and think-aloud protocols revealing problem-solving strategies are needed to validate assessment quality.

These threats do not invalidate TinyTorch as curriculum design---the implementation, pedagogical patterns, and theoretical grounding contribute independently of empirical validation. However, claims about learning effectiveness, cognitive load reduction, and transfer must remain tentative pending rigorous empirical evaluation addressing these validity concerns.

\subsection{Future Work}
\label{subsec:future-work}

TinyTorch's design enables multiple research and development trajectories maintaining pedagogical principles while extending capability.

\subsubsection{Empirical Validation (Planned Fall 2025)}

The most immediate priority involves deploying TinyTorch in university CS curricula and measuring learning outcomes through controlled comparison.

\textbf{Planned Experimental Design.}
Fall 2025 deployment comparing learning outcomes between traditional ML course (algorithm-focused, using PyTorch as black box) and TinyTorch course (systems-first, building frameworks). Pre/post assessments measuring: (1) systems thinking competency (memory profiling, complexity reasoning, optimization analysis), (2) framework comprehension (autograd mechanics, layer composition, training dynamics), (3) production readiness (debugging gradient flows, profiling performance, deployment decisions).

\textbf{Research Questions}:
\begin{enumerate}
\item Does systems-first curriculum improve production ML readiness compared to algorithm-focused approaches?
\item Do students who build frameworks transfer knowledge to PyTorch/TensorFlow more effectively than students who only use these frameworks?
\item Does progressive disclosure reduce cognitive load compared to introducing separate gradient classes? (Measure via dual-task methodology~\cite{sweller1988cognitive})
\item Do historical milestones increase motivation and learning compared to equivalent technical validation? (Measure via engagement surveys, persistence rates)
\end{enumerate}

\textbf{Timeline}: Fall 2025 deployment, preliminary results Spring 2026, full analysis published Summer 2026 (target: ICER 2026 or ACM TOCE).

\subsubsection{GPU Awareness and Performance Modeling}

While TinyTorch's CPU-only design prioritizes pedagogical transparency, students benefit from understanding GPU acceleration without implementing CUDA kernels.

\textbf{Module 21: GPU Awareness (Planned).}
Students compare TinyTorch CPU implementations against PyTorch GPU equivalents, analyzing performance gaps through roofline models~\citep{williams2009roofline}. Rather than writing CUDA code, students profile existing implementations to understand memory hierarchy differences (CPU cache levels L1/L2/L3 versus GPU global/shared/register memory), parallelism benefits (sequential CPU loops versus massively parallel GPU execution with thousands of threads), roofline analysis techniques (plotting achieved performance against hardware limits to identify compute-bound versus memory-bound operations), and mixed precision advantages (profiling FP32 versus FP16 training speed/memory tradeoffs). Students run instrumented PyTorch code alongside TinyTorch implementations, measuring wall-clock time, memory usage, and FLOPs utilization. The roofline model visualization shows why GPUs excel at ML workloads: high arithmetic intensity operations (matrix multiplication) approach peak FLOPs, while memory-bound operations (element-wise activations) hit bandwidth limits. This \emph{awareness without implementation} maintains TinyTorch's accessibility while preparing students for GPU programming courses.

\subsubsection{Distributed Training Fundamentals}

Understanding distributed training communication patterns and scalability challenges requires simulation-based pedagogy, not multi-GPU hardware.

\textbf{Module 22: Distributed Training Simulation (Planned).}
Students explore distributed training concepts through integration with ASTRA-sim~\citep{chakkaravarthy2023astrasim,astrasimsim2020}, a distributed ML training simulator. Rather than requiring 8-GPU clusters, students simulate multi-device training on single machines, exploring data parallelism basics (gradient synchronization via all-reduce across virtual workers, analyzing communication overhead versus compute time), scalability analysis (measuring weak versus strong scaling, identifying communication bottlenecks as worker count increases), network topology impact (comparing ring all-reduce, tree all-reduce, and hierarchical strategies through ASTRA-sim's topology modeling), and pipeline parallelism introduction (simulating model partitioning across devices, analyzing pipeline bubbles and micro-batching strategies).

ASTRA-sim integration enables exploring research questions without hardware barriers. Students replicate findings from distributed training literature~\citep{astrasimsim2020}, experiencing how collective communication primitives (all-reduce, all-gather) dominate training time at scale. This simulation-based approach maintains TinyTorch's pedagogical principle: understanding systems through transparent implementation and measurement, not black-box hardware access. After completing Module 22, students understand why gradient synchronization limits distributed training scalability, how network bandwidth affects multi-node training, what communication patterns different parallelism strategies require, and when to apply data versus model versus pipeline parallelism based on model and hardware characteristics.

\subsubsection{Advanced Curriculum Extensions}

Maintaining modular structure, TinyTorch can incorporate emerging ML systems topics through community contributions:

\textbf{Graph Neural Networks (Module 23)}: Message passing, graph convolutions, attention on graphs
\textbf{Diffusion Models (Module 24)}: Denoising architectures, forward/reverse processes, sampling strategies
\textbf{Reinforcement Learning (Module 25)}: Policy gradients, value functions, actor-critic methods

These extensions follow established pedagogical patterns: systems-first integration (memory profiling, complexity analysis), historical milestone validation (seminal papers), and progressive disclosure (building on foundation tier). Community forks demonstrate this extensibility: quantum ML variants replace classical tensors with quantum state vectors, robotics variants add RL-specific infrastructure.

\subsubsection{Learning Science Research}

TinyTorch's design enables controlled studies of pedagogical questions in ML education:

\textbf{Threshold Concepts}: Is autograd transformative (Meyer \& Land's criteria: troublesome, irreversible, integrative)? Do students exhibit conceptual breakthroughs or gradual understanding?

\textbf{Productive Failure Limits}: When does struggle become unproductive? Can we identify engagement patterns (time-on-task, help-seeking behavior) predicting learning vs frustration?

\textbf{Transfer Effectiveness}: Do TinyTorch skills improve PyTorch debugging? Measure via transfer tasks: given PyTorch code with gradient bugs, do TinyTorch students diagnose faster/more accurately than control group?

\textbf{Long-term Retention}: Six months post-course, do students remember systems concepts (memory footprint calculation, complexity reasoning)? Compare concept maps, debugging protocols, optimization strategies.

These studies require validated instruments, control groups, and longitudinal data collection---planned for post-deployment research program (2026--2027).

\section{Conclusion}
\label{sec:conclusion}

Machine learning education faces a critical gap: students learn to \emph{use} ML frameworks but lack systems-level understanding needed to build, optimize, and deploy them in production. TinyTorch addresses this gap through a pedagogical framework \emph{designed to} transform framework users into systems engineers.

This paper makes five primary contributions. First, \textbf{progressive disclosure via monkey-patching} (\Cref{sec:progressive})---a novel pedagogical pattern where dormant features in the \texttt{Tensor} class activate across modules, enabling early interface exposure while managing cognitive load. Second, \textbf{systems-first integration} (\Cref{sec:systems}), where memory profiling, FLOPs analysis, and computational complexity are introduced from Module 01---not relegated to advanced electives. Third, a \textbf{3-tier curriculum architecture plus competitive capstone} (\Cref{sec:curriculum}) spanning foundation, architectures, and optimization tiers, culminating in the Torch Olympics MLPerf-style competition. Fourth, \textbf{theoretical grounding} (\Cref{sec:related}) demonstrating how constructionism, productive failure, and threshold concepts guide design. Fifth, a \textbf{complete open-source artifact} enabling educator adoption and empirical evaluation.

These contributions serve multiple audiences. CS educators gain replicable curriculum patterns worth empirical investigation. ML engineers obtain framework internals education potentially transferring to PyTorch/TensorFlow debugging. Industry trainers receive template for upskilling ML users into systems engineers. CS education researchers find novel pedagogical patterns worth studying empirically.

\textbf{Important scope note}: This represents a \textbf{design contribution}. Curriculum architecture, pedagogical patterns, and theoretical framework are provided; rigorous classroom evaluation with learning outcome measurements is planned for Fall 2025 (\Cref{subsec:future-work}). Students completing TinyTorch's 20 modules should pursue GPU acceleration and distributed training through PyTorch tutorials, NVIDIA courses, or advanced modules (\Cref{subsubsec:gpu-omission})---TinyTorch provides internals foundation, not complete production ML preparation.

TinyTorch is not a replacement for production frameworks---it is a pedagogical bridge. Students completing the curriculum are expected to understand \emph{why} PyTorch manages GPU memory as it does, \emph{why} batch normalization layers have different train/eval modes, \emph{why} optimizers like Adam consume 3$\times$ parameter memory, and \emph{why} quantization trades 4$\times$ memory reduction for 1--2\% accuracy loss. This systems-level mental model is designed to transfer across frameworks and prepare graduates for ML engineering roles requiring optimization, debugging, and architectural decision-making.

We invite the ML education community to build on TinyTorch. The complete codebase, curriculum materials, and assessment infrastructure are openly available. Educators can adopt modules, adapt to local contexts, or extend with new capabilities. Researchers can instrument the framework to study learning progressions, measure pedagogical effectiveness, or test alternative teaching strategies.

\textbf{Most ML education teaches students to use frameworks. TinyTorch is designed to teach them to understand frameworks---and that understanding may make all the difference.}

% Bibliography
\bibliographystyle{plain}
\bibliography{references}

\end{document}
