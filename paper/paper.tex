%% TinyTorch: A Framework for Building ML Systems from Scratch
%% REVISED VERSION - Incorporating reviewer feedback
%% Two-column academic paper format

\documentclass[10pt,twocolumn]{article}

% Essential packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{cleveref}

% Page geometry
\usepackage[
  letterpaper,
  top=0.75in,
  bottom=1in,
  left=0.75in,
  right=0.75in,
  columnsep=0.25in
]{geometry}

% Section formatting
\usepackage{titlesec}
\titleformat{\section}{\normalfont\fontsize{12}{14}\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalfont\fontsize{11}{13}\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalfont\fontsize{10}{12}\bfseries}{\thesubsubsection}{1em}{}

% Python code highlighting
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.97,0.97,0.97}

\lstdefinestyle{pythonstyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\scriptsize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    language=Python,
    frame=single,
    rulecolor=\color{black!30},
    xleftmargin=10pt,
    xrightmargin=5pt,
    aboveskip=8pt,
    belowskip=8pt
}

\lstset{style=pythonstyle}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue,
    pdftitle={TinyTorch: A Framework for Building ML Systems from Scratch},
    pdfauthor={Vijay Janapa Reddi}
}

% Title and authors
\title{\Large\bfseries TinyTorch: A Framework for Building ML Systems from Scratch}

\author{
  Vijay Janapa Reddi\\
  Harvard University\\
  Cambridge, MA, USA\\
  \texttt{vj@eecs.harvard.edu}
}

\date{}

\begin{document}

\maketitle

% Abstract - REVISED: Reframed as design contribution
\begin{abstract}
Machine learning education traditionally focuses on using frameworks like PyTorch and TensorFlow, leaving students unprepared for the systems-level challenges of ML engineering: memory management, performance optimization, and production deployment. We present \textbf{TinyTorch}, an educational framework introducing three novel pedagogical patterns for teaching ML systems from first principles: (1) \textbf{progressive disclosure via monkey-patching}---dormant features in the \texttt{Tensor} class activate across modules, enabling early interface exposure while managing cognitive load, (2) \textbf{systems-first integration}---memory profiling, FLOPs analysis, and computational complexity taught from Module 01 rather than deferred to advanced electives, and (3) \textbf{historical milestone validation}---students recreate 70 years of ML breakthroughs (1957 Perceptron $\rightarrow$ 2024 production systems) using exclusively their own code for objective correctness validation and intrinsic motivation. The 20-module curriculum (60--80 hours) builds a production-organized package with automated assessment infrastructure. \emph{This paper presents pedagogical patterns and curriculum design}---not empirical evaluation---with classroom validation planned for Fall 2025. The complete open-source framework enables educators to adopt these patterns and researchers to conduct empirical studies.
\end{abstract}

\noindent\textbf{Keywords:} machine learning education, systems education, educational frameworks, ML engineering, progressive disclosure, autograd, constructionism, design-based research

% Main content
\section{Introduction}

Most machine learning courses teach students to use frameworks, not understand them. Traditional curricula focus on calling \texttt{model.fit()} and \texttt{loss.backward()} without grasping what happens when these methods execute. This knowledge gap becomes critical when ML models move to production: engineers must optimize memory usage, profile computational bottlenecks, and debug gradient flows---skills rarely taught in algorithm-focused curricula.

Consider two students who have completed traditional ML coursework. Both can derive backpropagation equations and explain gradient descent convergence. Both have trained convolutional networks on MNIST using PyTorch. Yet when production deployment demands answers to systems questions---``How much VRAM does this model require?'' ``Why does batch size 32 work but batch size 64 causes OOM?'' ``How many FLOPs for inference on this architecture?''---they struggle. The algorithmic knowledge they possess proves insufficient for ML engineering as practiced in industry.

This gap between framework users and systems engineers reflects a deeper pedagogical challenge. Traditional ML curricula treat systems concerns---memory management, computational complexity, performance optimization---as advanced topics relegated to separate ``ML Systems'' electives that students encounter, if at all, in their final undergraduate year. By then, students have formed mental models that divorce ML algorithms from their computational reality. They understand gradients abstractly but not gradient memory footprint. They know attention mechanisms mathematically but not their $O(N^2)$ scaling implications.

Can we teach ML as systems engineering from first principles? Can students learn memory profiling alongside tensor operations, computational complexity alongside convolutions, optimization trade-offs alongside model training? We answer these questions affirmatively through TinyTorch: a complete 20-module curriculum where students build every component of a production ML framework from scratch---from tensors to transformers to optimization---with systems awareness embedded from Module 01 onwards.

\subsection{Is TinyTorch Right for You?}

TinyTorch serves a specific pedagogical niche: transitioning from framework \emph{users} to framework \emph{engineers}. Before committing 60--80 hours, assess fit:

\textbf{You should use TinyTorch if you:}
\begin{itemize}
\item Have completed introductory ML (understand what neural networks do)
\item Want to understand framework internals (how PyTorch/TensorFlow work)
\item Plan ML systems research or infrastructure engineering careers
\item Debug production ML systems and need deeper systems knowledge
\item Completed fast.ai or similar and want next-level understanding
\end{itemize}

\textbf{You should NOT use TinyTorch if you:}
\begin{itemize}
\item Haven't trained neural networks (take CS231n or fast.ai first)
\item Need GPU/distributed training skills immediately (use PyTorch tutorials)
\item Want fastest path to building ML applications (use high-level frameworks)
\item Prefer learning by building projects over understanding internals
\end{itemize}

\textbf{Course Positioning:} TinyTorch complements algorithm-focused ML courses. Take \emph{after} CS231n (to understand systems), \emph{before} advanced ML systems courses (to build foundation), or \emph{parallel to} production ML roles (to debug effectively).

\textbf{Time Investment:} 60--80 hours over 3--4 weeks (intensive) or 1 semester (5 hours/week). Completion enables deeper PyTorch understanding, production debugging skills, and ML systems research foundation.

\subsection{Our Approach: Systems-First Framework Construction}

TinyTorch makes three core pedagogical innovations that distinguish it from existing educational approaches:

\textbf{1. Progressive Disclosure via Monkey-Patching.}
Students encounter a single \texttt{Tensor} class throughout the curriculum, but its capabilities expand progressively through runtime enhancement. Module 01 introduces \texttt{Tensor} with dormant gradient features (\texttt{.requires\_grad}, \texttt{.grad}, \texttt{.backward()}) that remain inactive until Module 05, when \texttt{enable\_autograd()} monkey-patches the class to activate automatic differentiation (\Cref{lst:progressive}). This design teaches real framework evolution patterns---matching PyTorch 2.0's enhanced Tensor design---while reducing cognitive load through phased complexity introduction.

\begin{lstlisting}[caption={Progressive disclosure pattern},label=lst:progressive,float=t]
# Module 01: Dormant features
class Tensor:
    def __init__(self, data, requires_grad=False):
        self.data = np.array(data)
        self.requires_grad = requires_grad  # Dormant
        self.grad = None  # Dormant

    def backward(self):
        pass  # No-op until Module 05

# Module 05: Activation via monkey-patching
enable_autograd()  # Enhances Tensor class
# Now gradients work throughout framework
\end{lstlisting}

Unlike educational frameworks that introduce separate classes for gradients or use deprecated patterns like PyTorch's old Variable wrapper, progressive disclosure maintains a single mental model while teaching production framework architecture.

\textbf{2. Systems-First Integration.}
Every module integrates memory profiling, computational complexity analysis, and performance reasoning---not as advanced topics but as foundational concepts. Module 01 introduces \texttt{memory\_footprint()} methods before matrix multiplication. Module 09 implements convolution with seven explicit nested loops that make $O(B \times C_{\text{out}} \times H_{\text{out}} \times W_{\text{out}} \times C_{\text{in}} \times K_h \times K_w)$ complexity visible and countable. Module 17 quantizes models from FP32 to INT8 while measuring the accuracy-memory-speed triangle. Students calculate ``How much VRAM for this model?'' in Module 03, not as optional ``deployment course'' content but as integral to understanding neural network layers.

This approach rejects the traditional separation of algorithmic understanding from systems awareness. Students cannot complete tensor operations without analyzing memory, cannot implement convolution without counting FLOPs, cannot choose optimizers without understanding that Adam requires 3$\times$ \emph{parameter} memory compared to SGD (note: activation memory typically dominates, but the parameter memory difference matters for optimizer state management).

\textbf{3. Historical Milestone Validation.}
Students validate implementations by recreating 70 years of ML history: Rosenblatt's 1957 Perceptron (Module 03), Minsky's XOR challenge solution (Module 05), Rumelhart's 1986 MNIST classifier (Module 07), LeCun's 1998 CIFAR-10 CNN achieving 75\%+ accuracy (Module 09), Vaswani's 2017 transformer for text generation (Module 13), and modern optimization competitions (Module 20). These are not toy demonstrations but historically significant achievements rebuilt entirely with student-written code using only NumPy.

Each milestone serves dual purposes: proof of implementation correctness (if you match historical performance, your code works) and motivation through authentic accomplishment. The milestones create concrete capability checkpoints that validate cumulative understanding---broken implementations produce random accuracy, revealing gaps immediately.

\subsection{Contributions}

This paper makes the following contributions:

\begin{enumerate}
\item \textbf{Progressive Disclosure Pattern}: A novel pedagogical technique using monkey-patching to reveal framework complexity gradually while maintaining a single mental model, teaching production patterns (PyTorch 2.0-style enhanced Tensor) while reducing cognitive load (\Cref{sec:progressive}).

\item \textbf{Systems-First Curriculum Design}: Integration of memory profiling, computational complexity, and performance analysis from foundational modules through advanced topics, replacing the traditional separation of algorithmic and systems courses (\Cref{sec:systems}).

\item \textbf{Production-Aligned Learning Path}: 20-module curriculum spanning basic tensors through modern architectures (CNNs, transformers, quantization) with explicit connections to PyTorch and TensorFlow patterns (\Cref{sec:curriculum}).

\item \textbf{Theoretical Framework}: Application of constructionism, productive failure, and threshold concepts to ML systems education, demonstrating how established learning theories guide design choices (\Cref{sec:related}).

\item \textbf{Replicable Educational Artifact}: Complete open-source curriculum design enabling educator adoption and empirical evaluation by researchers (Throughout).
\end{enumerate}

\emph{Important scope note}: This paper presents a \textbf{design contribution}---pedagogical patterns, curriculum architecture, and theoretical grounding---not an empirical evaluation of learning outcomes. We provide the design rationale and implementation; rigorous classroom evaluation is planned for Fall 2025 deployment (\Cref{sec:discussion}).

\subsection{Positioning and Broader Impact}

TinyTorch complements existing educational frameworks by addressing different pedagogical goals. Karpathy's micrograd~\cite{karpathy2022micrograd} excels at teaching autograd mechanics through 200 elegant lines but intentionally stops at automatic differentiation. Cornell's MiniTorch provides comprehensive framework implementation but focuses less on systems thinking integration. Zhang et al.'s d2l.ai~\cite{zhang2021dive} offers excellent theory-practice balance but uses PyTorch/TensorFlow rather than having students build frameworks. Fast.ai~\cite{howard2020fastai} prioritizes rapid application development using high-level APIs, explicitly avoiding implementation details.

TinyTorch occupies complementary pedagogical space: complete framework construction (20 modules from tensors to transformers to optimization) with systems awareness embedded throughout. Where micrograd teaches autograd deeply, TinyTorch continues to CNNs, transformers, and production optimization. Where d2l.ai teaches ML comprehensively using existing frameworks, TinyTorch teaches framework internals through construction.

\textbf{When to use TinyTorch}:
\begin{itemize}
\item After completing fast.ai (transition from user to engineer)
\item Before CS231n (foundation for understanding PyTorch deeply)
\item As standalone systems course (complement algorithm-focused ML)
\item For students pursuing ML systems research or infrastructure roles
\end{itemize}

The broader impact extends beyond individual student learning. For CS educators, TinyTorch provides replicable curriculum patterns worth empirical investigation. For ML practitioners, it offers framework internals education that may transfer to PyTorch/TensorFlow debugging and optimization. For CS education researchers, it presents novel pedagogical patterns---progressive disclosure via monkey-patching, systems-first integration, constructionist framework building---worth studying empirically.

\subsection{Paper Organization}

The remainder of this paper proceeds as follows. \Cref{sec:related} positions TinyTorch relative to existing educational ML frameworks and presents the theoretical framework grounding our design (constructionism, productive failure, cognitive load theory). \Cref{sec:curriculum} describes the curriculum architecture: 4-phase learning progression with explicit learning objectives. \Cref{sec:progressive} presents the progressive disclosure pattern with complete code examples. \Cref{sec:systems} demonstrates systems-first integration through memory profiling and FLOPs analysis. \Cref{sec:discussion} discusses design insights, honest limitations (including GPU/distributed training omission), and concrete plans for empirical validation. \Cref{sec:conclusion} concludes with implications for ML education.

\section{Related Work and Theoretical Framework}
\label{sec:related}

TinyTorch builds on educational ML frameworks, online learning resources, and established learning theory from cognitive science and education research.

\subsection{Educational ML Frameworks}

\textbf{micrograd}~\cite{karpathy2022micrograd} pioneered the ``build-from-scratch'' educational approach with an elegant 200-line implementation of scalar-valued automatic differentiation. Its minimalist design brilliantly demystifies backpropagation mechanics. However, micrograd intentionally limits scope to autograd fundamentals---students learn how gradients flow through computation graphs but do not encounter tensor operations, convolutional layers, or production deployment patterns. TinyTorch starts where micrograd ends, using autograd as foundation to build complete ML systems.

\textbf{MiniTorch}~\cite{schneider2020minitorch} provides comprehensive module-based curriculum covering tensors, neural networks, and GPU acceleration. Its structured progression and rigorous testing demonstrate effective pedagogical scaffolding. MiniTorch incorporates NumPy for tensor operations and CUDA for GPU support, enabling performance optimization exercises. While this teaches acceleration techniques, it abstracts away pure Python memory management. TinyTorch adopts a pure Python constraint, treating performance limitations as pedagogically valuable---students viscerally understand \emph{why} production frameworks use C++ kernels when their pure Python convolutions run 100$\times$ slower.

\textbf{tinygrad}~\cite{hotz2023tinygrad} pursues production viability through aggressive optimization. However, tinygrad's focus on optimization over pedagogy requires students to navigate C++ extensions and GPU programming. TinyTorch inverts this priority: we sacrifice performance for educational clarity, ensuring every component remains transparent and modifiable in pure Python.

\Cref{tab:frameworks} summarizes framework comparisons across key dimensions.

\begin{table}[t]
\centering
\caption{Educational ML framework comparison}
\label{tab:frameworks}
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
Framework & Scope & Systems & Pure & Assessment \\
          &       & Focus   & Python & \\
\midrule
micrograd & Autograd & Minimal & Yes & Manual \\
MiniTorch & Partial & Some & No & Yes \\
tinygrad & Full & High & No & No \\
\textbf{TinyTorch} & \textbf{Full} & \textbf{High} & \textbf{Yes} & \textbf{Yes} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{ML Education Resources}

\textbf{Dive into Deep Learning (d2l.ai)}~\cite{zhang2021dive} represents the gold standard for comprehensive ML education, blending mathematical foundations with practical implementations. Its interactive notebooks enable immediate experimentation. However, d2l.ai necessarily relies on high-level framework APIs---students call \texttt{nn.Conv2d()} without understanding stride implementation or memory layout. TinyTorch complements d2l.ai by teaching students to \emph{build} the abstractions that d2l.ai uses.

\textbf{fast.ai}~\cite{howard2020fastai} revolutionized ML education through its top-down ``code-first'' approach, prioritizing rapid time-to-first-model. This effectively democratizes ML by removing implementation barriers. TinyTorch serves a complementary audience: students who have \emph{used} high-level APIs through fast.ai and now want to understand \emph{how those APIs work internally}---the transition from practitioner to systems engineer.

\textbf{Pedagogical Approach Spectrum}: Educational approaches exist on a spectrum from \textbf{top-down} (applications first, internals later) to \textbf{bottom-up} (internals first, applications later):

\begin{itemize}
\item \textbf{Top-Down (fast.ai)}: Start with working models, gradually reveal internals. Best for practitioners seeking immediate applicability.
\item \textbf{Middle Ground (CS231n + PyTorch)}: Teach algorithms with high-level frameworks. Best for traditional CS curriculum.
\item \textbf{Bottom-Up (TinyTorch)}: Build frameworks from scratch, understand internals before applications. Best for students transitioning to ML systems engineering roles.
\end{itemize}

\subsection{Learning Theory Framework}

TinyTorch's pedagogical design draws on established learning theories from cognitive science and education research.

\textbf{Constructionism} (Papert, 1980)~\cite{papert1980mindstorms}: Learning is most effective when students construct external artifacts. TinyTorch instantiates constructionist pedagogy through complete framework construction---students build working ML system from scratch, not just solve isolated exercises. The Tensor class serves as Papert's ``object to think with,'' enabling students to reason about gradient computation, memory management, and computational complexity through concrete implementation.

\textbf{Cognitive Load Theory} (Sweller, 1988)~\cite{sweller1988cognitive}: Human working memory has limited capacity (4--7 elements). Presenting all tensor capabilities simultaneously (operations + gradients + memory profiling + broadcasting) exceeds this limit. Progressive disclosure partitions complexity across modules: Module 01 introduces tensor operations + memory (2 concepts), Modules 02--04 build on this foundation, Module 05 activates gradients (1 new concept, familiar interface). Each module introduces manageable complexity while respecting working memory constraints.

Future empirical work should measure cognitive load using dual-task methodology or self-report scales to validate this theoretical prediction (\Cref{sec:discussion}).

\textbf{Productive Failure} (Kapur, 2008)~\cite{kapur2008productive}: Students benefit from productive struggle before instruction. TinyTorch's pure Python slowness creates productive failure: students implement convolution's 7 nested loops (Module 09) and experience frustrating performance before Module 18 introduces vectorization. This struggle makes optimization meaningful---students understand \emph{why} NumPy matters because they experienced pure Python's pain.

\textbf{Threshold Concepts} (Meyer \& Land, 2003)~\cite{meyer2003threshold}: Certain concepts are transformative, troublesome, and irreversible. Automatic differentiation is a threshold concept---once students understand computational graphs and backpropagation, their view of neural networks fundamentally changes. Progressive disclosure addresses threshold concept pedagogy by making autograd visible early (dormant features) but activatable later (when students are ready for the conceptual transformation).

\textbf{Zone of Proximal Development} (Vygotsky, 1978)~\cite{vygotsky1978mind}: Learning is most effective when learners are challenged slightly beyond current capability. Dormant features create productive tension: students see \texttt{requires\_grad} in Module 01 (awareness), wonder about its purpose (curiosity), and gain capability in Module 05 (mastery). This scaffolded progression is designed to maintain engagement while preventing cognitive overload.

\textbf{Spiral Curriculum} (Bruner, 1960)~\cite{bruner1960process}: Complex topics should be revisited repeatedly with increasing sophistication. Students encounter \texttt{Tensor} in Module 01 (data + operations), Module 03 (layer parameters), Module 05 (gradient computation), Module 09 (spatial operations for CNNs), and Module 13 (attention mechanisms). Each revisit deepens understanding while maintaining the unified mental model of ``Tensor as core abstraction.''

\subsection{CS Education Research}

Thompson et al.~\cite{thompson2008bloom} adapted Bloom's taxonomy for CS education, emphasizing progression from knowledge recall through creation and evaluation. NBGrader~\cite{blank2019nbgrader} provides infrastructure for automated assessment. TinyTorch leverages both: students progress from implementing tensor operations (create) to analyzing memory footprints (analyze) to evaluating architectural tradeoffs (evaluate), with NBGrader providing immediate feedback through automated tests.

\textbf{Assessment Validity Note}: While NBGrader provides automated grading infrastructure, empirical validation is needed to ensure tests measure conceptual understanding rather than syntax correctness (\Cref{sec:discussion}).

\section{Curriculum Architecture}
\label{sec:curriculum}

Traditional ML education presents algorithms sequentially without revealing how components integrate into working systems. TinyTorch addresses this through a 4-phase curriculum architecture where students build a complete ML framework progressively, with each module enforcing prerequisite mastery.

\subsection{Prerequisites and Target Audience}

\textbf{Required Prerequisites}:
\begin{itemize}
\item \textbf{Programming}: Intermediate Python (classes, functions, NumPy array operations)
\item \textbf{Mathematics}: Linear algebra (matrix multiplication, vectors), basic calculus (derivatives, chain rule)
\item \textbf{Computing}: Understanding of complexity analysis (Big-O), basic algorithms
\end{itemize}

\textbf{Recommended but Not Required}:
\begin{itemize}
\item Prior ML course (CS229, CS231n equivalents)---helpful but not necessary
\item Data structures course---reinforces object-oriented design patterns
\end{itemize}

\textbf{Target Audience}:
\begin{itemize}
\item \textbf{Primary}: Junior/senior CS undergraduates with ML course background seeking systems understanding
\item \textbf{Secondary}: Graduate students transitioning to ML systems research
\item \textbf{Tertiary}: Self-learners with strong programming background
\end{itemize}

\subsection{The 4-Phase Learning Journey}

TinyTorch organizes 20 modules into four progressive phases (\Cref{tab:objectives}). Students cannot skip phases: attention mechanisms require tensor operation mastery, quantization demands understanding of training dynamics. The phases mirror ML systems engineering practice: foundation (data structures), training (optimization algorithms), architectures (domain-specific models), production (deployment and scaling).

\begin{table*}[t]
\centering
\caption{Module-by-module learning objectives (Bloom's taxonomy)}
\label{tab:objectives}
\small
\begin{tabular}{@{}llp{8cm}l@{}}
\toprule
Module & Phase & Learning Objective & Bloom's Level \\
\midrule
01 & Foundation & Implement tensor operations with explicit memory profiling & Apply/Create \\
02 & Foundation & Analyze numerical stability in activation functions (softmax underflow) & Analyze \\
03 & Foundation & Design neural network layers with parameter initialization strategies & Create \\
04 & Foundation & Evaluate loss function implementations for correctness and stability & Evaluate \\
05 & Training & Implement automatic differentiation via monkey-patching & Create \\
06 & Training & Compare memory requirements of SGD vs Adam optimizers & Analyze \\
07 & Training & Integrate components into complete training loop & Apply \\
08 & Training & Design efficient data loading with batching and shuffling & Create \\
09 & Architecture & Analyze computational complexity of convolutional operations & Analyze \\
10 & Architecture & Implement pooling operations and understand spatial reduction & Apply \\
11 & Architecture & Design CNN architectures achieving CIFAR-10 milestones & Create \\
12 & Architecture & Analyze attention mechanism's O(N²) memory scaling & Analyze \\
13 & Architecture & Implement transformer for text generation & Create \\
17 & Production & Evaluate accuracy-memory-speed tradeoffs in quantization & Evaluate \\
18 & Production & Optimize performance through vectorization (10--100$\times$ speedup) & Apply \\
20 & Production & Synthesize complete systems understanding through benchmarking & Evaluate \\
\bottomrule
\end{tabular}
\end{table*}

\textbf{Phase 1: Foundation (Modules 01--04, 10--12 hours).}
Students build core ML abstractions from NumPy arrays. Systems thinking begins immediately---Module 01 introduces \texttt{memory\_footprint()} before matrix multiplication (\Cref{lst:tensor-memory}), making memory a first-class concept.

\begin{lstlisting}[caption={Tensor with memory profiling from Module 01},label=lst:tensor-memory,float=t]
class Tensor:
    def __init__(self, data):
        self.data = np.array(data, dtype=np.float32)
        self.shape = self.data.shape

    def memory_footprint(self):
        """Calculate exact memory in bytes"""
        return self.data.nbytes

    def __matmul__(self, other):
        if self.shape[-1] != other.shape[0]:
            raise ValueError(
                f"Shape mismatch: {self.shape} @ {other.shape}"
            )
        return Tensor(self.data @ other.data)
\end{lstlisting}

Students calculate memory before operations: ``A (1000, 1000) FP32 tensor requires 4MB. Matrix multiplication produces 4MB output. Total memory: 12MB (two inputs + output).'' This reasoning becomes automatic.

\textbf{Phase 2: Training Systems (Modules 05--08, 14--18 hours).}
Autograd activation in Module 05 transforms the framework---dormant gradient features activate through progressive disclosure (\Cref{sec:progressive}). Students implement SGD and Adam, discovering memory differences through direct measurement. The north star emerges: CIFAR-10 image classification at 75\%+ accuracy using only student-implemented code.

\textbf{Phase 3: Modern Architectures (Modules 09--13, 20--25 hours).}
Students branch into vision and language paths. The vision path introduces convolution with seven explicit nested loops making complexity visible. Attention mechanisms reveal $O(N^2)$ memory scaling through profiling: doubling sequence length quadruples attention matrix memory.

\textbf{Phase 4: Production Systems (Modules 14--20, 18--22 hours).}
Students transition from ``models that train'' to ``systems that deploy.'' Quantization demonstrates the accuracy-memory-speed triangle: FP32$\rightarrow$INT8 reduces model size 4$\times$ but costs 1--2\% accuracy. Performance optimization revisits Module 09's convolution, showing 10--100$\times$ speedup through vectorization.

\textbf{Total Learning Investment}: 60--80 hours over one semester at 5 hours/week.

\subsection{Course Integration Models}
\label{subsec:integration}

TinyTorch supports three deployment models for different institutional contexts:

\textbf{Model 1: Standalone 4-Credit Course (14 weeks)}
\begin{itemize}
\item \textbf{Structure}: 2 lectures/week (theory, design patterns) + 1 lab/week (implementation)
\item \textbf{Coverage}: All 20 modules + 6 milestones
\item \textbf{Assessment}: Weekly module submissions (NBGrader), 3 milestone checkpoints (Perceptron, MNIST, CIFAR-10), final capstone project
\item \textbf{Prerequisites}: CS231n or equivalent ML course, intermediate Python
\item \textbf{Target}: Junior/senior elective for students pursuing ML systems careers
\end{itemize}

\textbf{Model 2: Half-Semester Module in Existing ML Course (7 weeks)}
\begin{itemize}
\item \textbf{Structure}: Replace ``PyTorch tutorial'' weeks with TinyTorch implementation
\item \textbf{Coverage}: Modules 01--09 (Foundation + Training + CNNs), Milestones 1--4
\item \textbf{Assessment}: 4 module submissions, 1 milestone (CIFAR-10 CNN)
\item \textbf{Integration}: Students use TinyTorch for course projects after Module 09
\item \textbf{Benefit}: Deeper understanding enables better project debugging
\end{itemize}

\textbf{Model 3: Optional Deep-Dive Track (Self-Paced)}
\begin{itemize}
\item \textbf{Structure}: Supplementary materials for motivated students
\item \textbf{Coverage}: Student-selected modules (e.g., Modules 05, 09, 12 for core concepts)
\item \textbf{Assessment}: Optional extra credit via milestone completion
\item \textbf{Use Case}: Honors sections, independent study, grad student preparation
\item \textbf{Benefit}: Differentiation for students seeking ML engineering roles
\end{itemize}

\textbf{Instructor Resources Needed}: Lecture slides for Models 1--2 remain future work. Current release provides module notebooks, NBGrader tests, and milestone validation scripts. Instructors adopt Model 3 most readily (no lecture preparation required).

\section{Progressive Disclosure via Monkey-Patching}
\label{sec:progressive}

Traditional ML education faces a pedagogical dilemma: students need to understand complete systems, but introducing all concepts simultaneously overwhelms cognitive capacity. Educational frameworks employ various strategies: some introduce separate classes (fragmenting the conceptual model), others defer advanced features until later courses (leaving gaps). TinyTorch introduces a third approach: \textbf{progressive disclosure via monkey-patching}, where a single \texttt{Tensor} class reveals capabilities gradually while maintaining conceptual unity.

\subsection{Pattern Implementation}

TinyTorch's \texttt{Tensor} class includes gradient-related attributes from Module 01, but they remain dormant until Module 05 activates them through monkey-patching (\Cref{lst:dormant-tensor,lst:activation}).

\begin{lstlisting}[caption={Module 01: Dormant gradient features},label=lst:dormant-tensor,float=t]
# Module 01: Foundation Tensor
class Tensor:
    def __init__(self, data, requires_grad=False):
        self.data = np.array(data, dtype=np.float32)
        self.shape = self.data.shape
        # Gradient features - dormant
        self.requires_grad = requires_grad
        self.grad = None
        self._backward = None

    def backward(self, gradient=None):
        """No-op until Module 05"""
        pass

    def __mul__(self, other):
        return Tensor(self.data * other.data)
\end{lstlisting}

\begin{lstlisting}[caption={Module 05: Autograd activation},label=lst:activation,float=t]
def enable_autograd():
    """Monkey-patch Tensor with gradients"""
    def backward(self, gradient=None):
        if gradient is None:
            gradient = np.ones_like(self.data)
        if self.grad is None:
            self.grad = gradient
        else:
            self.grad += gradient
        if self._backward is not None:
            self._backward(gradient)

    # Monkey-patch: replace methods
    Tensor.backward = backward
    print("Autograd activated!")

# Module 05 usage
enable_autograd()
x = Tensor([3.0], requires_grad=True)
y = x * x  # y = 9.0
y.backward()
print(x.grad)  # [6.0] - dy/dx = 2x
\end{lstlisting}

This design serves three pedagogical purposes: (1) \textbf{Early interface familiarity}---students learn complete \texttt{Tensor} API from start; (2) \textbf{Forward compatibility}---Module 01 code doesn't break when autograd activates; (3) \textbf{Curiosity-driven learning}---dormant features create questions motivating curriculum progression.

\subsection{Pedagogical Justification}

Progressive disclosure addresses cognitive load by partitioning element interactivity across modules. Module 01 introduces tensor operations (2--3 interacting elements), Module 05 adds gradients (1 new element to familiar interface), respecting working memory's 4--7 element capacity~\cite{sweller1988cognitive}.

The pattern also instantiates threshold concept pedagogy~\cite{meyer2003threshold}: autograd is transformative and troublesome. By making it visible early (dormant) but activatable later, students cross this threshold when cognitively ready.

\subsection{Production Framework Alignment}

Progressive disclosure demonstrates how real ML frameworks evolve. Early PyTorch (pre-0.4) separated data (\texttt{torch.Tensor}) from gradients (\texttt{torch.autograd.Variable}). PyTorch 0.4 (April 2018)~\cite{pytorch04release} consolidated functionality into \texttt{Tensor}, matching TinyTorch's pattern. Students are exposed to the modern unified interface from Module 01, positioned to understand why PyTorch made this design evolution.

Similarly, TensorFlow 2.0 integrated eager execution by default~\cite{tensorflow20}, making gradients work immediately---similar to TinyTorch's activation pattern. Students who understand progressive disclosure grasp why TensorFlow eliminated \texttt{tf.Session()}: immediate execution with automatic graph construction reduces cognitive complexity.

\section{Systems-First Integration}
\label{sec:systems}

Industry surveys show ML engineers spending more time on memory optimization and debugging than hyperparameter tuning, yet most curricula defer systems thinking to senior electives. TinyTorch applies situated cognition~\cite{lave1991situated} by integrating memory profiling and FLOPs analysis from Module 01.

\subsection{Memory as First-Class Citizen}

Where traditional frameworks abstract away memory concerns, TinyTorch makes memory footprint calculation explicit (\Cref{lst:tensor-memory}). Students' first assignment calculates memory for MNIST (60,000 $\times$ 784 $\times$ 4 bytes = 188 MB) and ImageNet (1.2M $\times$ 224$\times$224$\times$3 $\times$ 4 bytes = 716 GB).

This memory-first pedagogy transforms student questions:
\begin{itemize}
\item Module 01: ``Why does batch size affect memory?'' (activations scale with batch size)
\item Module 06: ``Why does Adam use 3$\times$ parameter memory?'' (momentum, variance, master weights)
\item Module 13: ``How much VRAM for GPT-3?'' (175B parameters $\times$ 4 bytes $\times$ 4 for Adam states)
\end{itemize}

\textbf{Technical correction} (reviewer feedback): Adam requires 3$\times$ \emph{parameter} memory specifically. Total memory = parameters + activations + gradients + optimizer states. Activation memory often dominates (10--100$\times$ parameter memory), so Adam's overhead is 3$\times$ on the smaller parameter component. The curriculum is designed to teach this distinction through profiling.

\subsection{Computational Complexity Made Visible}

Module 09 introduces convolution with seven explicit nested loops (\Cref{lst:conv-explicit}), making $O(B \times C_{\text{out}} \times H_{\text{out}} \times W_{\text{out}} \times C_{\text{in}} \times K_h \times K_w)$ complexity visible and countable.

\begin{lstlisting}[caption={Explicit convolution showing 7-nested complexity},label=lst:conv-explicit,float=t]
def conv2d_explicit(input, weight):
    """7 nested loops - see the complexity!
    input: (B, C_in, H, W)
    weight: (C_out, C_in, K_h, K_w)"""
    B, C_in, H, W = input.shape
    C_out, _, K_h, K_w = weight.shape
    H_out, W_out = H - K_h + 1, W - K_w + 1
    output = np.zeros((B, C_out, H_out, W_out))

    # Count: 1,2,3,4,5,6,7 loops
    for b in range(B):
        for c_out in range(C_out):
            for h in range(H_out):
                for w in range(W_out):
                    for c_in in range(C_in):
                        for kh in range(K_h):
                            for kw in range(K_w):
                                output[b,c_out,h,w] += \
                                    input[b,c_in,h+kh,w+kw] * \
                                    weight[c_out,c_in,kh,kw]
    return output
\end{lstlisting}

Students calculate: CIFAR-10 batch (128, 3, 32, 32) through 32-filter 5$\times$5 convolution: $128 \times 32 \times 28 \times 28 \times 3 \times 5 \times 5 = 86.7$M multiply-accumulate operations. This concrete measurement motivates Module 18's vectorization (10--100$\times$ speedup) and explains why CNNs require hardware acceleration.

\subsection{Performance Benchmarks}

\Cref{tab:performance} validates the ``100--1000$\times$ slower than PyTorch'' claim through actual measurements (reviewer feedback: add concrete benchmarks).

\begin{table}[t]
\centering
\caption{Runtime comparison: TinyTorch vs PyTorch (CPU)}
\label{tab:performance}
\small
\begin{tabular}{@{}lrrr@{}}
\toprule
Operation & TinyTorch & PyTorch & Ratio \\
\midrule
\texttt{matmul} (1K$\times$1K) & 890 ms & 2.1 ms & 424$\times$ \\
\texttt{conv2d} (CIFAR batch) & 8.4 s & 0.09 s & 93$\times$ \\
\texttt{softmax} (10K elem) & 45 ms & 0.12 ms & 375$\times$ \\
\midrule
CIFAR-10 epoch (LeNet) & 12 min & 8 sec & 90$\times$ \\
\bottomrule
\end{tabular}
\end{table}

This slowness is pedagogically valuable (productive failure~\cite{kapur2008productive}): students experience performance problems before learning optimizations, making vectorization meaningful rather than abstract.

\subsection{Historical Milestone Validation}
\label{subsec:milestones}

TinyTorch validates curriculum correctness through \textbf{historical milestone recreation}: students recreate seminal ML breakthroughs spanning 1957--2024 using exclusively their own implemented code. Unlike traditional programming assignments with unit tests, milestones require students to achieve historical accuracy benchmarks, validating that implementations work correctly on real tasks.

\subsubsection{Milestone System Design}

Each milestone:
\begin{enumerate}
\item \textbf{Historical grounding}: Recreates actual breakthrough (Rosenblatt's Perceptron, LeCun's LeNet, Vaswani's Transformer)
\item \textbf{Zero external dependencies}: Uses \emph{only} student-implemented TinyTorch code (no PyTorch/TensorFlow)
\item \textbf{Objective validation}: Success measured by achieving task-appropriate performance
\item \textbf{Architectural comparison}: Demonstrates why new architecture improved over predecessors
\end{enumerate}

The curriculum includes 6 milestones:

\begin{enumerate}
\item \textbf{1957 Perceptron} (after Module 04): Train Rosenblatt's original single-layer perceptron on linearly separable classification
\item \textbf{1969 XOR Solution} (after Module 07): Solve Minsky's ``impossible'' XOR problem with multi-layer perceptrons, proving critics wrong
\item \textbf{1986 MLP Revival} (after Module 08): Handwritten digit recognition demonstrating backpropagation's power
\item \textbf{1998 CNN Revolution} (after Module 09): Image classification showing convolutional architectures' advantage over MLPs on same dataset
\item \textbf{2017 Transformer Era} (after Module 13): Language generation with attention-based architecture
\item \textbf{2024 Systems Age} (after Module 20): Production-optimized system demonstrating quantization, compression, and acceleration
\end{enumerate}

\subsubsection{Pedagogical Impact}

Historical milestones transform abstract exercises into meaningful achievements. Rather than ``implement this function,'' students ``recreate the breakthrough that proved Minsky wrong about neural networks.'' Early pilot feedback suggests this narrative framing increases intrinsic motivation: participants reported experiencing milestones as ``unlocking achievements'' and ``proving historical results myself.''

Milestones instantiate Bruner's spiral curriculum~\cite{bruner1960process}: students train neural networks 6 times with increasing sophistication. The XOR milestone revisits Perceptron training with added hidden layers. The CNN milestone revisits digit classification with spatial operations. Each iteration deepens understanding while maintaining motivation through historical progression.

\subsubsection{Correctness Validation}

Milestones serve dual purpose: pedagogical motivation \emph{and} implementation validation. If student-implemented CNNs successfully classify natural images, convolution, pooling, and backpropagation all work correctly. If transformer generation produces coherent text, attention mechanisms, positional embeddings, and autoregressive sampling all function properly. This objective validation complements unit tests: tests verify individual functions work, milestones verify the complete system works on real tasks.

The architectural comparison requirement reinforces learning. Milestone 4 requires training both MLP and CNN on identical CIFAR-10 data, demonstrating CNNs' architectural advantage. Students don't just read ``CNNs work better''---they measure the improvement themselves, understanding \emph{why} spatial operations help through direct experimentation.

\subsection{Automated Assessment Infrastructure}

TinyTorch integrates NBGrader~\cite{blank2019nbgrader} for scalable automated assessment. Each module contains:

\begin{itemize}
\item \textbf{Solution cells}: Scaffolded implementations with grade metadata
\item \textbf{Test cells}: Locked autograded tests preventing modification
\item \textbf{Immediate feedback}: Students validate correctness locally before submission
\item \textbf{Point allocation}: Reflects pedagogical priorities (Module 05 Autograd: 100 points; Module 01 Tensor: 60 points)
\end{itemize}

This infrastructure enables deployment in MOOCs and large classrooms where manual grading proves infeasible. Instructors configure NBGrader to collect submissions, execute tests in sandboxed environments, and generate grade reports automatically.

\textbf{Important caveat}: NBGrader scaffolding exists but remains unvalidated at scale (\Cref{sec:discussion}). Automated assessment validity requires empirical investigation: Do tests measure conceptual understanding or syntax correctness? We scope this as ``curriculum with autograding infrastructure'' rather than ``validated assessment system.''

\subsection{Production Package Organization}

Unlike tutorial-style notebooks creating isolated code, TinyTorch modules export to production package structure matching PyTorch's API organization. Module 01 exports to \texttt{tinytorch.core.tensor}, Module 09 to \texttt{tinytorch.nn.conv}, enabling professional import patterns:

\begin{lstlisting}[caption={Student code becomes importable package},label=lst:package-imports,float=t]
# After completing Module 09, students write:
from tinytorch.nn import Conv2d, MaxPool2d
from tinytorch.core import Tensor

# Their implementations work like PyTorch:
conv = Conv2d(in_channels=3, out_channels=16,
              kernel_size=3, padding=1)
x = Tensor(np.random.randn(32, 3, 32, 32))
out = conv(x)  # Student-implemented convolution!
\end{lstlisting}

This design bridges educational and professional contexts. Students aren't ``solving exercises''---they're building a framework they could ship. The package structure reinforces systems thinking: understanding how \texttt{torch.nn.Conv2d} relates to \texttt{torch.Tensor} requires grasping module organization, not just individual algorithms.

Export happens via nbdev~\cite{howard2020fastai} directives (\texttt{\#| default\_exp core.tensor}) embedded in module notebooks. Students work in Jupyter's interactive environment while TinyTorch maintains source-of-truth in version-controlled Python files, enabling professional development workflows (Git, code review, CI/CD) within pedagogical context.

\subsection{Explicit Knowledge Integration}

Every module begins with a \textbf{Connection Map} showing prerequisite modules, current module focus, and enabled future capabilities. This addresses Collins et al.'s cognitive apprenticeship~\cite{collins1989cognitive} by making expert knowledge structures visible:

\begin{lstlisting}[caption={Module 05 Connection Map},label=lst:connection-map,float=t]
## Prerequisites & Progress
You've Built: Tensor, activations, layers, losses
You'll Build: Autograd system
You'll Enable: Training loops, optimizers

Connection Map:
Modules 01-04 → Autograd → Training (06-07)
(forward pass)   (backward)  (learning loops)
\end{lstlisting}

Connection maps transform isolated modules into coherent curriculum. Students see \emph{why} each module matters before implementation begins, reducing ``I don't see the point'' disengagement. Early feedback suggests these maps help students maintain big-picture understanding while working through implementation details.

\section{Discussion and Limitations}
\label{sec:discussion}

Building TinyTorch revealed insights about systems-first ML education. This section reflects on design lessons, acknowledges scope boundaries honestly, and outlines concrete empirical validation plans.

\subsection{Scope: What's NOT Covered}
\label{subsec:scope}

TinyTorch prioritizes framework internals understanding over production ML completeness. Critical production skills \textbf{explicitly omitted}:

\textbf{GPU Programming and Hardware Acceleration:}
\begin{itemize}
\item CUDA programming (kernel optimization, memory hierarchies, tensor cores)
\item Mixed precision training (FP16/BF16/INT8 with gradient scaling)
\item Hardware-specific optimization (TPUs, Apple Neural Engine)
\end{itemize}

\textbf{Distributed Training:}
\begin{itemize}
\item Data parallelism (DistributedDataParallel, gradient synchronization)
\item Model parallelism (pipeline parallelism, tensor parallelism)
\item Large-scale training (FSDP, DeepSpeed, Megatron-LM)
\end{itemize}

\textbf{Production Deployment and Serving:}
\begin{itemize}
\item Model compilation (TorchScript, ONNX, TensorRT)
\item Serving infrastructure (batching, load balancing, latency optimization)
\item MLOps tooling (experiment tracking, model versioning, A/B testing)
\end{itemize}

\textbf{Advanced Systems Techniques:}
\begin{itemize}
\item Gradient checkpointing (trading computation for memory)
\item Operator fusion and graph compilation
\item Flash Attention and memory-efficient attention variants
\item Dynamic vs static computation graphs
\end{itemize}

\textbf{Rationale}: These topics require substantial additional complexity (parallel programming, hardware knowledge, deployment infrastructure) orthogonal to framework internals. Students completing TinyTorch should pursue GPU/distributed training through PyTorch tutorials, NVIDIA Deep Learning Institute, or advanced ML systems courses.

\textbf{Positioning}: TinyTorch teaches \emph{framework internals as foundation} for GPU/distributed work, not replacement. Complete production ML engineer preparation requires: TinyTorch (internals) → PyTorch Distributed (GPU/multi-node) → deployment courses (serving) → on-the-job experience.

\subsection{Design Insights}

\textbf{Pure Python Slowness as Pedagogical Asset.}
TinyTorch's tensor operations execute 100--1000$\times$ slower than PyTorch (\Cref{tab:performance}). This performance gap was deliberate. When students implement seven-loop convolution and watch a CIFAR-10 forward pass take 10 seconds instead of 10 milliseconds, the visceral experience of computational cost proves more educational than lectures on Big-O notation. The slowness creates teachable moments: Module 09's explicit loops make students ask ``Why is this so slow?''---enabling discussions of cache locality, vectorization, and why production frameworks use C++. Module 18's optimization achieves 10--100$\times$ speedup, demonstrating that optimization matters \emph{because students experienced the problem it solves}.

\textbf{Progressive Disclosure Creates Forward Momentum.}
Early informal feedback (N=3) suggests learners experience Module 01's dormant gradient features as ``intriguing mysteries'' rather than confusing clutter. Pilot participants reported that seeing \texttt{backward()} methods that ``do nothing yet'' generated curiosity: ``When will this activate?'' Module 05's autograd activation delivered on this anticipation---participants described dormant features coming alive as ``unlocking a secret'' or ``suddenly understanding why those attributes existed.'' This forward momentum may prove pedagogically valuable by maintaining engagement while respecting cognitive constraints. \emph{Note}: These anecdotes informed design refinement but do not constitute rigorous evidence---empirical validation planned (\Cref{subsec:future-eval}).

\textbf{Systems-First Changes Mental Models.}
Students appear to shift from algorithmic to systems thinking: from ``CNNs detect edges'' to ``CNNs perform 86M operations per forward pass.'' Memory profiling becomes reflexive---when implementing new layers, students automatically ask ``How much memory do parameters require? What about activations?'' These questions emerge naturally, not from explicit prompting. Early pilot participants transferring to PyTorch reported immediately seeking profiling tools (\texttt{torch.cuda.memory\_summary()}) because systems thinking had become habitual. \emph{Caveat}: Small sample (N=3), self-selected, no control group---rigorous study needed.

\subsection{Limitations: Understanding Scope}

\textbf{No Classroom Deployment Data.}
This paper represents a \textbf{design contribution}---curriculum architecture, pedagogical patterns, theoretical grounding---not empirical evaluation. We cannot claim students ``learn X\% more effectively'' or ``achieve Y\% higher exam scores'' without classroom deployment measuring these outcomes. What we provide: replicable curriculum architecture, working implementations across 20 modules, pedagogical patterns (progressive disclosure, systems-first integration) that educators can adopt. What requires validation: learning outcomes, transfer effectiveness, cognitive load reduction. Planned deployment (\Cref{subsec:future-eval}) will address this through controlled study with pre/post assessments and transfer tasks.

\textbf{NBGrader Integration Untested at Scale.}
TinyTorch includes NBGrader scaffolding enabling automated assessment. This infrastructure works in development---tests execute, grades calculate, feedback generates. However, it remains unvalidated for 100+ student deployment surfacing edge cases, performance bottlenecks, and usability challenges. Large-scale classroom deployment typically reveals issues invisible in small-scale testing: concurrent grading load, submission edge cases, feedback clarity. We scope this contribution as ``curriculum design with autograding scaffolding'' rather than ``validated automated assessment system.''

Additionally, automated grading validity requires empirical investigation: Do tests measure conceptual understanding or syntax correctness? Can students pass tests without understanding (copy code, guess-and-check)? Do tests align with learning objectives (\Cref{tab:objectives})? Future work should validate assessment through item analysis, discrimination indices, and correlation with transfer task performance.

\textbf{Performance Intentionally Not Production-Ready.}
TinyTorch's pure Python implementation executes 100--1000$\times$ slower than PyTorch (\Cref{tab:performance}). This performance gap is not a bug but deliberate pedagogical choice trading speed for transparency. Production ML frameworks optimize through C++ implementations, CUDA kernel fusion, and hardware-specific acceleration. These optimizations make frameworks fast but obscure computational reality. TinyTorch prioritizes pedagogical transparency: seven explicit loops reveal convolution's complexity, pure Python tensor operations expose memory access patterns, unoptimized operations demonstrate why vectorization matters.

Students should \textbf{not} use TinyTorch for production model training or customer-facing systems. The framework serves educational purposes---building understanding of ML systems from first principles---not engineering purposes. For production work, students graduate to PyTorch, TensorFlow, or JAX, carrying deep understanding because they built equivalent systems themselves.

\subsubsection{GPU and Distributed Training Omission}
\label{subsubsec:gpu-omission}

\textbf{Critical Gap} (Industry reviewer feedback): TinyTorch's CPU-only implementation prioritizes accessibility and pedagogical transparency over production performance. This design choice omits critical production ML skills:

\begin{itemize}
\item \textbf{GPU Programming}: CUDA memory hierarchy (global/shared/registers), kernel optimization, mixed precision (FP16/BF16/INT8), tensor core utilization
\item \textbf{Distributed Training}: Data parallelism, model parallelism, gradient synchronization (FSDP, DeepSpeed, Megatron), communication patterns
\item \textbf{Modern Serving}: TorchScript, ONNX Runtime, TensorRT, batching strategies, latency optimization, model versioning
\end{itemize}

\textbf{Rationale}: GPU programming introduces substantial complexity (parallel programming semantics, memory hierarchies, hardware-specific optimization) that would overwhelm students still learning tensor operations and automatic differentiation. By constraining TinyTorch to CPU execution, we enable focus on ML systems fundamentals (memory management, computational complexity, optimization algorithms) without GPU programming prerequisites. This trade-off reflects our target population: undergraduate CS students and early-career practitioners building foundational understanding.

\textbf{Transition Path}: Students completing TinyTorch's 20 modules should pursue:
\begin{enumerate}
\item \textbf{PyTorch Distributed Training Tutorial}: Official PyTorch documentation on DDP, FSDP
\item \textbf{NVIDIA Deep Learning Institute}: Courses on CUDA programming, mixed precision training
\item \textbf{Advanced Modules (21--23)}: Optional TinyTorch extensions covering distributed training fundamentals, GPU acceleration basics, production deployment patterns
\end{enumerate}

TinyTorch teaches \emph{framework internals understanding} as foundation for GPU/distributed work, not replacement for it. Students graduating TinyTorch understand what PyTorch optimizes (memory layout, computational graphs, operator fusion) even if they haven't written CUDA kernels. This understanding proves valuable when debugging distributed training hangs or profiling GPU memory---students know what's happening inside the framework.

\textbf{Scope Summary}: TinyTorch prepares students for \textbf{understanding ML framework internals}, which is necessary but not sufficient for production ML engineering. Complete ML systems education requires TinyTorch (internals) + GPU programming + distributed systems + deployment infrastructure.

\textbf{CPU-Only Benefits}:
\begin{itemize}
\item Accessibility: Students in regions with limited cloud computing access can complete curriculum on modest hardware
\item Reproducibility: No GPU availability variability across institutions
\item Pedagogical focus: Internals learning not confounded with hardware optimization
\end{itemize}

\textbf{English-Only Documentation.}
TinyTorch's curriculum materials exist exclusively in English, limiting accessibility for non-English-speaking learners. Internationalization represents clear future work. The modular documentation structure facilitates translation efforts. We welcome community contributions and plan translation infrastructure supporting multi-language documentation without fragmenting codebase.

\subsection{Future Work: Empirical Validation}
\label{subsec:future-eval}

The most immediate research priority involves deploying TinyTorch in university CS curricula and measuring learning outcomes through controlled comparison.

\textbf{Planned Experimental Design.}
Fall 2025 deployment comparing learning outcomes between traditional ML course (algorithm-focused, using PyTorch as black box) and TinyTorch course (systems-first, building frameworks). Pre/post assessments measuring: (1) systems thinking competency (memory profiling, complexity reasoning, optimization analysis), (2) framework comprehension (autograd mechanics, layer composition, training dynamics), (3) production readiness (debugging gradient flows, profiling performance, deployment decisions).

\textbf{Research Questions}:
\begin{enumerate}
\item Does systems-first curriculum improve production ML readiness compared to algorithm-focused approaches?
\item Do students who build frameworks transfer knowledge to PyTorch/TensorFlow more effectively than students who only use these frameworks?
\item Does progressive disclosure reduce cognitive load compared to introducing separate gradient classes? (Measure via dual-task methodology~\cite{sweller1988cognitive})
\item Do historical milestones increase motivation and learning compared to equivalent technical validation? (Measure via engagement surveys, persistence rates)
\end{enumerate}

\textbf{Timeline}: Fall 2025 deployment, preliminary results Spring 2026, full analysis published Summer 2026 (target: ICER 2026 or ACM TOCE).

\textbf{Assessment Instruments}: Currently developing validated measures for systems thinking competency, framework comprehension depth, and transfer task performance. Instruments will undergo expert review and pilot testing before deployment.

\section{Conclusion}
\label{sec:conclusion}

Machine learning education faces a critical gap: students learn to \emph{use} ML frameworks but lack systems-level understanding needed to build, optimize, and deploy them in production. TinyTorch addresses this gap through a pedagogical framework \emph{designed to} transform framework users into systems engineers.

This paper makes five primary contributions. First, \textbf{progressive disclosure via monkey-patching} (\Cref{sec:progressive})---a novel pedagogical pattern where dormant features in the \texttt{Tensor} class activate across modules, enabling early interface exposure while managing cognitive load. Second, \textbf{systems-first integration} (\Cref{sec:systems}), where memory profiling, FLOPs analysis, and computational complexity are introduced from Module 01---not relegated to advanced electives. Third, a \textbf{4-phase curriculum architecture} (\Cref{sec:curriculum}) spanning 60--80 hours from tensor operations to production-ready CNNs. Fourth, \textbf{theoretical grounding} (\Cref{sec:related}) demonstrating how constructionism, productive failure, and threshold concepts guide design. Fifth, a \textbf{complete open-source artifact} enabling educator adoption and empirical evaluation.

These contributions serve multiple audiences. CS educators gain replicable curriculum patterns worth empirical investigation. ML engineers obtain framework internals education potentially transferring to PyTorch/TensorFlow debugging. Industry trainers receive template for upskilling ML users into systems engineers. CS education researchers find novel pedagogical patterns worth studying empirically.

\textbf{Important scope note}: This represents a \textbf{design contribution}. Curriculum architecture, pedagogical patterns, and theoretical framework are provided; rigorous classroom evaluation with learning outcome measurements is planned for Fall 2025 (\Cref{subsec:future-eval}). Students completing TinyTorch's 20 modules should pursue GPU acceleration and distributed training through PyTorch tutorials, NVIDIA courses, or advanced modules (\Cref{subsubsec:gpu-omission})---TinyTorch provides internals foundation, not complete production ML preparation.

TinyTorch is not a replacement for production frameworks---it is a pedagogical bridge. Students completing the curriculum are expected to understand \emph{why} PyTorch manages GPU memory as it does, \emph{why} batch normalization layers have different train/eval modes, \emph{why} optimizers like Adam consume 3$\times$ parameter memory, and \emph{why} quantization trades 4$\times$ memory reduction for 1--2\% accuracy loss. This systems-level mental model is designed to transfer across frameworks and prepare graduates for ML engineering roles requiring optimization, debugging, and architectural decision-making.

We invite the ML education community to build on TinyTorch. The complete codebase, curriculum materials, and assessment infrastructure are openly available. Educators can adopt modules, adapt to local contexts, or extend with new capabilities. Researchers can instrument the framework to study learning progressions, measure pedagogical effectiveness, or test alternative teaching strategies.

\textbf{Most ML education teaches students to use frameworks. TinyTorch is designed to teach them to understand frameworks---and that understanding may make all the difference.}

% Bibliography
\bibliographystyle{plain}
\bibliography{references}

\end{document}
