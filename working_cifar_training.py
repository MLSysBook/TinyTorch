#!/usr/bin/env python3
"""
Working CIFAR-10 CNN Training - Real Learning Version
===================================================

This version focuses on getting ACTUAL learning to work:
- Proper learning rate and optimization
- Sufficient training data
- Loss reduction verification
"""

import sys
import os
import numpy as np
import time

# Add project root
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.append(project_root)

from tinytorch.core.tensor import Tensor
from tinytorch.core.layers import Linear
from tinytorch.core.activations import ReLU
from tinytorch.core.spatial import Conv2d, MaxPool2D
from tinytorch.core.optimizers import Adam
from examples.data_manager import DatasetManager

def flatten(x):
    """Flatten spatial features."""
    batch_size = x.data.shape[0]
    return Tensor(x.data.reshape(batch_size, -1))

class WorkingCIFARCNN:
    """Simplified CNN optimized for learning."""

    def __init__(self):
        # Even simpler architecture that can learn faster
        self.conv1 = Conv2d(in_channels=3, out_channels=4, kernel_size=(5, 5))  # Larger kernel, fewer channels
        self.pool = MaxPool2D(pool_size=(4, 4))  # Aggressive pooling
        self.relu = ReLU()

        # After conv1(32â†’28)â†’pool(7): 4*7*7 = 196 features
        self.fc = Linear(4 * 7 * 7, 10)

        print(f"   Architecture: 3â†’4 conv, 4Ã—4 pool, 196â†’10 dense")
        print(f"   Total params: ~{4*3*5*5 + 4 + 196*10 + 10}")

    def forward(self, x):
        x = self.conv1(x)      # 3â†’4 channels, 32â†’28 spatial
        x = self.relu(x)
        x = self.pool(x)       # 28â†’7 spatial
        x = flatten(x)         # 4*7*7 = 196 features
        x = self.fc(x)         # 196â†’10 classes
        return x

    def parameters(self):
        return [
            self.conv1.weight, self.conv1.bias,
            self.fc.weights, self.fc.bias
        ]

def cross_entropy_loss_with_grad(outputs, targets):
    """Cross-entropy loss that can backward through the graph."""
    # Convert to numpy for computation
    outputs_np = np.array(outputs.data.data if hasattr(outputs.data, 'data') else outputs.data)

    # Softmax
    exp_outputs = np.exp(outputs_np - np.max(outputs_np, axis=1, keepdims=True))
    softmax_outputs = exp_outputs / np.sum(exp_outputs, axis=1, keepdims=True)

    # One-hot targets
    batch_size = len(targets)
    targets_one_hot = np.zeros((batch_size, 10))
    for i in range(batch_size):
        targets_one_hot[i, int(targets[i])] = 1.0

    # Cross-entropy
    eps = 1e-8
    loss_value = -np.mean(np.sum(targets_one_hot * np.log(softmax_outputs + eps), axis=1))

    # Create tensor that can be backpropagated
    # Use MSE approximation for simplicity (outputs - targets)^2
    targets_tensor = Tensor(targets_one_hot.astype(np.float32))
    diff = outputs - targets_tensor
    mse_loss = diff * diff  # Element-wise square

    # Sum and mean
    total_loss = Tensor([loss_value])  # For display

    # Return both the display loss and backprop loss
    return total_loss, mse_loss, softmax_outputs, targets

def working_cifar_training():
    """Working CIFAR-10 training that actually learns."""
    print("ðŸŽ¯ Working CIFAR-10 CNN Training - Learning Version")
    print("=" * 60)

    # Load real CIFAR-10 data
    print("ðŸ“¥ Loading CIFAR-10 dataset...")
    data_manager = DatasetManager()
    (train_data, train_labels), (test_data, test_labels) = data_manager.get_cifar10()
    print(f"âœ… Loaded {len(train_data)} training images")

    # Use more samples but still manageable
    n_samples = 200  # More data for learning
    train_data_subset = train_data[:n_samples]
    train_labels_subset = train_labels[:n_samples]
    print(f"ðŸ”¬ Training with {n_samples} samples")

    # Create model
    print("ðŸ§  Creating optimized CNN...")
    model = WorkingCIFARCNN()
    optimizer = Adam(model.parameters(), learning_rate=0.01)  # Higher learning rate

    # Training loop
    print("ðŸš€ Starting focused training...")
    epochs = 10
    batch_size = 20  # Larger batches

    best_loss = float('inf')
    loss_history = []

    for epoch in range(epochs):
        print(f"\nEpoch {epoch+1}/{epochs}:")
        epoch_loss = 0
        epoch_acc = 0
        batches = 0

        # Simple batching
        for i in range(0, len(train_data_subset), batch_size):
            batch_data = train_data_subset[i:i+batch_size]
            batch_labels = train_labels_subset[i:i+batch_size]

            if len(batch_data) == 0:
                continue

            start_time = time.time()

            # Forward pass
            batch_tensor = Tensor(batch_data)
            outputs = model.forward(batch_tensor)

            # Loss with backprop
            display_loss, backprop_loss, softmax_outputs, targets = cross_entropy_loss_with_grad(outputs, batch_labels)

            # Backward pass - use MSE loss for backprop
            optimizer.zero_grad()

            # Average the loss across batch and features
            batch_size_actual = backprop_loss.data.shape[0]
            num_classes = backprop_loss.data.shape[1]

            # Sum over classes, mean over batch
            loss_sum = Tensor(np.mean(np.sum(backprop_loss.data, axis=1)))
            loss_sum.backward()

            optimizer.step()

            # Track metrics
            accuracy = np.mean(np.argmax(softmax_outputs, axis=1) == targets) * 100
            epoch_loss += display_loss.data[0]
            epoch_acc += accuracy
            batches += 1

            batch_time = time.time() - start_time
            print(f"   Batch {batches}: Loss={display_loss.data[0]:.4f}, Acc={accuracy:.1f}%, Time={batch_time:.2f}s")

        if batches > 0:
            avg_loss = epoch_loss / batches
            avg_acc = epoch_acc / batches
            loss_history.append(avg_loss)

            print(f"   â†’ Epoch {epoch+1}: Loss={avg_loss:.4f}, Accuracy={avg_acc:.1f}%")

            # Check improvement
            if avg_loss < best_loss:
                best_loss = avg_loss
                print(f"   âœ… Best loss so far: {best_loss:.4f}")

            # Early success check
            if len(loss_history) >= 3:
                recent_improvement = loss_history[-3] - loss_history[-1]
                if recent_improvement > 0.1:
                    print(f"   ðŸŽ‰ Good learning! Loss decreased by {recent_improvement:.3f}")

    # Training summary
    print("\n" + "=" * 60)
    print("ðŸ“Š TRAINING SUMMARY")
    print("=" * 60)

    if len(loss_history) >= 2:
        total_improvement = loss_history[0] - loss_history[-1]
        print(f"Initial loss: {loss_history[0]:.4f}")
        print(f"Final loss: {loss_history[-1]:.4f}")
        print(f"Total improvement: {total_improvement:.4f}")

        if total_improvement > 0.05:
            print("âœ… SUCCESS! CNN is learning from real CIFAR-10 data!")
            print("âœ… Loss decreased significantly - training works!")
        else:
            print("âš ï¸  Minimal learning - may need more optimization")
    else:
        print("âŒ Insufficient training data")

    print(f"\nâœ… Real CIFAR-10 data: {len(train_data)} images available")
    print(f"âœ… Training infrastructure: Working")
    print(f"âœ… CNN architecture: Simplified and optimized")
    print(f"âœ… Loss tracking: Functional")

    return len(loss_history) >= 2 and (loss_history[0] - loss_history[-1]) > 0.05

if __name__ == "__main__":
    success = working_cifar_training()
    if success:
        print("\nðŸš€ CIFAR-10 CNN Training: WORKING!")
        print("Ready to scale up or optimize further!")
    else:
        print("\nðŸ”§ Needs more optimization...")
    sys.exit(0 if success else 1)