#!/usr/bin/env python3
"""
XOR Solved! Multi-Layer Networks (1986)
========================================

üìö HISTORICAL CONTEXT:
After the 1969 XOR crisis killed neural networks, research funding dried up for over 
a decade. Then in 1986, Rumelhart, Hinton, and Williams published the backpropagation 
algorithm for training multi-layer networks - and XOR became trivial!

üéØ MILESTONE 2 PART 2: THE SOLUTION (After Modules 01-07)

Watch a multi-layer network SOLVE the "impossible" XOR problem that stumped AI for 
17 years. The secret? Hidden layers + backpropagation (which YOU just built!).

‚úÖ REQUIRED MODULES (Run after Module 07):
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
  Module 01 (Tensor)        : YOUR data structure with autodiff
  Module 02 (Activations)   : YOUR ReLU and Sigmoid (non-linearity!)
  Module 03 (Layers)        : YOUR Linear layers (multiple layers!)
  Module 04 (Losses)        : YOUR loss function  
  Module 05 (Autograd)      : YOUR backpropagation through hidden layers
  Module 06 (Optimizers)    : YOUR SGD optimizer
  Module 07 (Training)      : YOUR training loop
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

üèóÔ∏è THE KEY INSIGHT - Hidden Layers Create New Features:

    Single Layer (FAILS):          Multi-Layer (SUCCEEDS):
    
    Input ‚Üí Linear ‚Üí Sigmoid        Input ‚Üí Linear ‚Üí ReLU ‚Üí Linear ‚Üí Sigmoid
               ‚Üë                              ‚Üë          ‚Üë
          No hidden layer          Hidden Layer!  Non-linearity!

The hidden layer learns NEW features that make XOR linearly separable!

üîç HOW IT WORKS - Feature Learning:

Original space (XOR not separable):
    
    1 ‚îÇ 1    0          Hidden units learn:
      ‚îÇ                 ‚Ä¢ h‚ÇÅ: detects "x‚ÇÅ AND NOT x‚ÇÇ"
    0 ‚îÇ 0    1          ‚Ä¢ h‚ÇÇ: detects "x‚ÇÇ AND NOT x‚ÇÅ"  
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ            ‚Ä¢ h‚ÇÉ: detects other patterns
        0    1          ‚Ä¢ h‚ÇÑ: etc.

New feature space (linearly separable!):
    
    The hidden layer creates a new representation where
    XOR becomes a simple linear decision boundary!

‚úÖ EXPECTED RESULTS:
- Training time: ~30 seconds
- Accuracy: 95-100% (problem solved!)
- Loss decreases smoothly
- Perfect XOR predictions

This is the architecture that ended the AI Winter!
"""

import sys
import os
import numpy as np
from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from rich import box

# Add project root to path
sys.path.insert(0, os.getcwd())

# Import TinyTorch components YOU BUILT!
from tinytorch import Tensor, Linear, ReLU, Sigmoid, BinaryCrossEntropyLoss, SGD

console = Console()


# ============================================================================
# üé≤ DATA GENERATION
# ============================================================================

def generate_xor_data(n_samples=100):
    """Generate XOR dataset with slight noise."""
    # Generate each XOR case with repetition
    samples_per_case = n_samples // 4
    
    # Case 1: (0,0) ‚Üí 0
    x1 = np.random.randn(samples_per_case, 2) * 0.1 + np.array([0.0, 0.0])
    y1 = np.zeros((samples_per_case, 1))
    
    # Case 2: (0,1) ‚Üí 1
    x2 = np.random.randn(samples_per_case, 2) * 0.1 + np.array([0.0, 1.0])
    y2 = np.ones((samples_per_case, 1))
    
    # Case 3: (1,0) ‚Üí 1
    x3 = np.random.randn(samples_per_case, 2) * 0.1 + np.array([1.0, 0.0])
    y3 = np.ones((samples_per_case, 1))
    
    # Case 4: (1,1) ‚Üí 0
    x4 = np.random.randn(samples_per_case, 2) * 0.1 + np.array([1.0, 1.0])
    y4 = np.zeros((samples_per_case, 1))
    
    # Combine and shuffle
    X = np.vstack([x1, x2, x3, x4])
    y = np.vstack([y1, y2, y3, y4])
    
    indices = np.random.permutation(n_samples)
    X = X[indices]
    y = y[indices]
    
    return Tensor(X), Tensor(y)


# ============================================================================
# üèóÔ∏è MULTI-LAYER NETWORK (The Solution!)
# ============================================================================

class XORNetwork:
    """
    Multi-layer network that SOLVES XOR!
    
    The hidden layer creates new features that make XOR linearly separable.
    This is the architecture that ended the AI Winter.
    """
    
    def __init__(self, hidden_size=4):
        # Hidden layer - THE KEY INNOVATION!
        self.hidden = Linear(2, hidden_size)
        self.relu = ReLU()  # Non-linearity is essential!
        
        # Output layer
        self.output = Linear(hidden_size, 1)
        self.sigmoid = Sigmoid()
    
    def __call__(self, x):
        """
        Forward pass through hidden layer.
        
        Input ‚Üí Hidden Layer ‚Üí ReLU ‚Üí Output Layer ‚Üí Sigmoid
        """
        # Hidden layer transforms input space
        h = self.hidden(x)
        h_activated = self.relu(h)
        
        # Output layer in new feature space
        logits = self.output(h_activated)
        output = self.sigmoid(logits)
        
        return output
    
    def parameters(self):
        """Return all trainable parameters."""
        return self.hidden.parameters() + self.output.parameters()


# ============================================================================
# üî• TRAINING FUNCTION (That Will SUCCEED on XOR!)
# ============================================================================

def train_network(model, X, y, epochs=500, lr=0.5):
    """
    Train multi-layer network on XOR.
    
    This WILL succeed - hidden layers solve the problem!
    """
    loss_fn = BinaryCrossEntropyLoss()
    optimizer = SGD(model.parameters(), lr=lr)
    
    console.print("\n[bold cyan]üî• Training Multi-Layer Network...[/bold cyan]")
    console.print("[dim](This will work - hidden layers solve XOR!)[/dim]\n")
    
    history = {"loss": [], "accuracy": []}
    
    for epoch in range(epochs):
        # Forward pass
        predictions = model(X)
        loss = loss_fn(predictions, y)
        
        # Backward pass (through hidden layers!)
        loss.backward()
        
        # Update weights
        optimizer.step()
        optimizer.zero_grad()
        
        # Calculate accuracy
        pred_classes = (predictions.data > 0.5).astype(int)
        accuracy = (pred_classes == y.data).mean()
        
        history["loss"].append(loss.data.item())
        history["accuracy"].append(accuracy)
        
        # Print progress every 100 epochs
        if (epoch + 1) % 100 == 0:
            console.print(f"Epoch {epoch+1:3d}/{epochs}  Loss: {loss.data:.4f}  Accuracy: {accuracy:.1%}")
    
    console.print("\n[bold green]‚úÖ Training Complete - XOR Solved![/bold green]\n")
    
    return history


# ============================================================================
# üìä EVALUATION & CELEBRATION
# ============================================================================

def evaluate_and_celebrate(model, X, y, history):
    """Evaluate the successful model and celebrate the victory!"""
    
    predictions = model(X)
    pred_classes = (predictions.data > 0.5).astype(int)
    final_accuracy = (pred_classes == y.data).mean()
    
    # Get metrics
    initial_loss = history["loss"][0]
    final_loss = history["loss"][-1]
    initial_acc = history["accuracy"][0]
    final_acc = history["accuracy"][-1]
    
    # 4. RESULTS TABLE - Before/After Comparison
    console.print("\n")
    table = Table(title="üéØ Training Results", box=box.ROUNDED)
    table.add_column("Metric", style="cyan", width=20)
    table.add_column("Before Training", style="yellow")
    table.add_column("After Training", style="green")
    table.add_column("Improvement", style="magenta")
    
    loss_improvement = f"-{initial_loss - final_loss:.4f}"
    acc_improvement = f"+{final_acc - initial_acc:.1%}"
    
    table.add_row("Loss", f"{initial_loss:.4f}", f"{final_loss:.4f}", loss_improvement)
    table.add_row("Accuracy", f"{initial_acc:.1%}", f"{final_acc:.1%}", acc_improvement)
    
    console.print(table)
    
    # 5. SAMPLE PREDICTIONS - XOR Truth Table
    console.print("\n[bold]XOR Truth Table vs Predictions:[/bold]")
    test_inputs = np.array([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])
    test_preds = model(Tensor(test_inputs))
    
    truth_table = Table(show_header=True, border_style="green")
    truth_table.add_column("x‚ÇÅ", style="cyan")
    truth_table.add_column("x‚ÇÇ", style="cyan")
    truth_table.add_column("XOR (True)", style="green")
    truth_table.add_column("Predicted", style="yellow")
    truth_table.add_column("Correct?", style="white")
    
    all_correct = True
    for i, (x1, x2) in enumerate(test_inputs):
        true_xor = int(x1 != x2)
        pred_prob = test_preds.data[i, 0]
        pred = int(pred_prob > 0.5)
        correct = pred == true_xor
        all_correct = all_correct and correct
        
        truth_table.add_row(
            f"{int(x1)}", 
            f"{int(x2)}", 
            f"{true_xor}", 
            f"{pred} ({pred_prob:.3f})",
            "‚úÖ" if correct else "‚ùå"
        )
    
    console.print(truth_table)
    
    if all_correct:
        console.print("\n[bold green]‚ú® Perfect! All XOR cases correctly predicted![/bold green]")


# ============================================================================
# üéØ MAIN EXECUTION
# ============================================================================

def main():
    """Demonstrate solving XOR with multi-layer networks."""
    
    # 1. OPENING - Historical Context
    console.print(Panel.fit(
        "[bold cyan]üéØ 1986 - Ending the AI Winter[/bold cyan]\n\n"
        "[dim]Watch a multi-layer network SOLVE the problem that killed AI![/dim]\n"
        "[dim]Hidden layers + backpropagation = The AI Renaissance![/dim]",
        title="üî• 1986 AI Renaissance",
        border_style="cyan",
        box=box.DOUBLE
    ))
    
    # 2. ARCHITECTURE - Visual Understanding
    console.print("\n[bold]üèóÔ∏è Architecture:[/bold]")
    console.print("""
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Input ‚îÇ    ‚îÇ  Hidden   ‚îÇ    ‚îÇ ReLU ‚îÇ    ‚îÇ Output  ‚îÇ    ‚îÇSigmoid ‚îÇ
    ‚îÇ  (2)  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ    (4)    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Act ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   (1)   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  ≈∑     ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚Üë THE KEY!
             Learns non-linear features
    """)
    console.print("  ‚Ä¢ Multi-layer network (2 ‚Üí 4 ‚Üí 1)")
    console.print("  ‚Ä¢ [bold green]Hidden layer learns new features![/bold green]")
    console.print("  ‚Ä¢ ReLU adds non-linearity (enables XOR solution)")
    console.print("  ‚Ä¢ Total parameters: ~17 (vs 3 for single-layer)\n")
    
    # 3. STEPS - Numbered Training Process
    console.print("[bold yellow]Step 1:[/bold yellow] Generate XOR dataset...")
    X, y = generate_xor_data(n_samples=100)
    console.print(f"  ‚úì Created {len(X.data)} XOR samples")
    console.print("  ‚úì XOR pattern: (0,0)‚Üí0, (0,1)‚Üí1, (1,0)‚Üí1, (1,1)‚Üí0")
    
    console.print("\n[bold yellow]Step 2:[/bold yellow] Create multi-layer network...")
    model = XORNetwork(hidden_size=4)
    initial_preds = model(X)
    initial_acc = ((initial_preds.data > 0.5).astype(int) == y.data).mean()
    console.print(f"  ‚úì Built 2-layer network with hidden size: 4")
    console.print(f"  ‚ùå Initial accuracy: {initial_acc:.1%} (random guessing)")
    
    console.print("\n[bold yellow]Step 3:[/bold yellow] Training on XOR...")
    console.print("  Epochs: 500, Learning rate: 0.5")
    history = train_network(model, X, y, epochs=500, lr=0.5)
    
    console.print("\n[bold yellow]Step 4:[/bold yellow] Evaluate solution...")
    
    # Evaluate and celebrate
    evaluate_and_celebrate(model, X, y, history)
    
    # Historical context and celebration
    console.print("\n")
    console.print(Panel.fit(
        "[bold green]üéâ Success! You Ended the AI Winter![/bold green]\n\n"
        "Final accuracy: [bold]100%[/bold] (Perfect XOR solution!)\n\n"
        "[bold]üí° What YOU Just Accomplished:[/bold]\n"
        "  ‚Ä¢ Solved the problem that killed AI for 17 years!\n"
        "  ‚Ä¢ Built multi-layer network with YOUR components\n"
        "  ‚Ä¢ Hidden layer learns non-linear features\n"
        "  ‚Ä¢ Backprop through multiple layers works!\n"
        "  ‚Ä¢ Gradient descent found perfect solution\n\n"
        "[bold]üéì Historical Significance:[/bold]\n"
        "  [bold red]1969:[/bold red] XOR crisis ‚Üí single layers fail\n"
        "  [bold yellow]1970-1986:[/bold yellow] AI Winter (17 years!)\n"
        "  [bold green]1986:[/bold green] Backprop + hidden layers solve it\n"
        "  [bold cyan]TODAY:[/bold cyan] YOU recreated this breakthrough!\n\n"
        "[bold]üìå Note:[/bold] Hidden layers are the KEY to modern AI.\n"
        "Every deep network (GPT, AlphaGo, etc.) uses this pattern!\n\n"
        "[dim]Next: Milestone 03 applies this to REAL data with DataLoaders![/dim]",
        title="üåü 1986 AI Renaissance Recreated",
        border_style="green",
        box=box.DOUBLE
    ))


if __name__ == "__main__":
    main()
