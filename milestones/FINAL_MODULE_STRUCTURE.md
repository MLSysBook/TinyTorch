# TinyTorch Final Module Structure

## ğŸ“š Core Learning Path (For All Milestones)

### **Phase 1: Building Blocks (Modules 01-04)**
```
01. Tensor           â†’ Data structure with autograd support
02. Activations      â†’ ReLU, Sigmoid, Softmax
03. Layers           â†’ Linear layers, parameter management
04. Losses           â†’ MSE, CrossEntropy, BinaryCrossEntropy
```

**Unlocks:** 
- ğŸ† Milestone 01: Perceptron (1957) - after Module 03

---

### **Phase 2: Learning Systems (Modules 05-06)**
```
05. Autograd         â†’ Backward passes, computational graph
06. Optimizers       â†’ SGD, Adam parameter updates
```

**Unlocks:**
- ğŸ† Milestone 02: XOR (1969) - after Module 06
- ğŸ† Milestone 03: MNIST MLP (1986) - after Module 06

**Training Pattern:** Manual loops
```python
for epoch in range(epochs):
    for batch in batches:
        outputs = model(inputs)
        loss = loss_fn(outputs, targets)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
```

---

### **Phase 3: Vision & Data (Modules 08-09)**
```
08. DataLoader       â†’ Batching, shuffling, Dataset abstraction
09. Spatial          â†’ Conv2d, MaxPool2D for vision
```

**Note:** Module 07 is intentionally skipped in core progression (see Advanced Topics)

**Unlocks:**
- ğŸ† Milestone 04: CNN (1998) - after Module 09

---

### **Phase 4: Language & Attention (Modules 10-13)**
```
10. Tokenization     â†’ Character & BPE tokenizers
11. Embeddings       â†’ Token + positional embeddings
12. Attention        â†’ Multi-head self-attention
13. Transformers     â†’ LayerNorm, TransformerBlock
```

**Unlocks:**
- ğŸ† Milestone 05: Transformer (2017) - after Module 13

---

## ğŸ“ Advanced Topics (Optional - After Completing All Milestones)

### **Module 07: Training Abstractions â­**
```
âš ï¸ OPTIONAL - Skip in your first pass!

This module teaches abstraction patterns like:
- PyTorch Lightning-style Trainer classes
- Learning rate scheduling
- Gradient clipping
- Checkpointing systems

Come back to this AFTER you've written manual training 
loops many times and understand the fundamentals.
```

**Why it's optional:**
- Manual training loops teach fundamentals better
- All milestones use manual loops (matching PyTorch patterns)
- Abstractions make sense AFTER you understand what they're abstracting
- PyTorch itself doesn't have a Trainer class - you write manual loops

---

### **Advanced Systems (Modules 14-20)**
```
14. KV-Caching       â†’ Efficient transformer inference
15. Profiling        â†’ Performance measurement
16. Acceleration     â†’ Speed optimizations
17. Quantization     â†’ INT8 quantization
18. Compression      â†’ Pruning techniques
19. Benchmarking     â†’ Performance comparison
20. Capstone         â†’ Complete TinyGPT system
```

---

## ğŸ¯ Module â†’ Milestone Mapping

| Milestone | Name | Required Modules | Training Style |
|-----------|------|------------------|----------------|
| 01 | Perceptron (1957) | 01-03 | Simple gradient step |
| 02 | XOR (1969) | 01-06 | Manual loop |
| 03 | MNIST MLP (1986) | 01-06 | Manual loop + batching |
| 04 | CNN (1998) | 01-06, 08-09 | Manual loop + DataLoader |
| 05 | Transformer (2017) | 01-06, 08-13 | Manual loop + Attention |

---

## ğŸ“– Table of Contents Structure

The course book now follows this progression:

```
ğŸ§± Building Blocks
  â”œâ”€ 01. Tensor
  â”œâ”€ 02. Activations
  â”œâ”€ 03. Layers
  â””â”€ 04. Losses

ğŸ§  Learning Systems
  â”œâ”€ 05. Autograd
  â””â”€ 06. Optimizers
      â””â”€ ğŸ† Milestones 01-03 unlock here

ğŸ–¼ï¸ Vision & Data
  â”œâ”€ 08. DataLoader       [Comes right after 06!]
  â””â”€ 09. Spatial
      â””â”€ ğŸ† Milestone 04 unlocks here

ğŸ—£ï¸ Language & Attention
  â”œâ”€ 10. Tokenization
  â”œâ”€ 11. Embeddings
  â”œâ”€ 12. Attention
  â””â”€ 13. Transformers
      â””â”€ ğŸ† Milestone 05 unlocks here

ğŸ“ Advanced Topics
  â”œâ”€ 07. Training Abstractions â­ Optional
  â”œâ”€ 14. KV Caching
  â”œâ”€ 15. Profiling
  â”œâ”€ 16. Acceleration
  â”œâ”€ 17. Quantization
  â”œâ”€ 18. Compression
  â””â”€ 19. Benchmarking

ğŸ… Competition
  â””â”€ 20. AI Olympics (Capstone)
```

---

## ğŸ’¡ Design Rationale

### Why Skip Module 07 in Core Path?

1. **PyTorch doesn't have it**: PyTorch has no built-in `Trainer` class. Research code uses manual loops.

2. **Pedagogical superiority**: Writing manual loops 5 times teaches you more than using an abstraction once.

3. **Real-world relevance**: Production code needs custom training loops for:
   - Mixed precision training
   - Gradient accumulation
   - Custom loss functions
   - Complex data pipelines
   - Debugging and monitoring

4. **Learn fundamentals first**: Understand what you're abstracting before learning abstractions.

### Why Keep the Number 07?

1. **Zero breaking changes**: Students mid-course aren't affected
2. **Git history stays clean**: No massive refactoring needed
3. **Semantic meaning**: The gap documents an intentional design decision
4. **Version precedent**: Software versions often have gaps (Python 2.7 â†’ 3.0)
5. **Future value**: Advanced students can still learn abstraction patterns

---

## ğŸš€ Student Learning Journey

```
Week 1-2: Modules 01-03
   â””â”€ Run Milestone 01 (Perceptron)

Week 3-4: Modules 04-06
   â””â”€ Run Milestones 02-03 (XOR, MNIST)
   â””â”€ Master manual training loops!

Week 5-6: Modules 08-09
   â””â”€ Run Milestone 04 (CNN)
   â””â”€ Learn DataLoader + spatial ops

Week 7-9: Modules 10-13
   â””â”€ Run Milestone 05 (Transformer)
   â””â”€ Understand attention mechanisms

Week 10+: Advanced Topics
   â””â”€ Optional: Module 07 (Training Abstractions)
   â””â”€ Modules 14-20 (Systems optimization)
   â””â”€ Capstone project
```

---

## âœ… What Changed?

### From Previous Structure:
- âŒ Module 07 was in core path but unused by any milestone
- âŒ Students confused about whether to use Trainer or manual loops
- âŒ TOC showed modules 07, 08, 09 in sequence

### To Current Structure:
- âœ… Module 07 moved to Advanced Topics (still available)
- âœ… Core path 01-06 â†’ 08-13 (clear progression)
- âœ… All milestones use manual loops (consistent pedagogy)
- âœ… TOC clearly shows the learning path
- âœ… Zero breaking changes (numbers preserved)

---

## ğŸ¯ Key Takeaway

**Manual training loops are not a limitation - they're a feature.**

Every ML engineer should be comfortable writing:
```python
for epoch in range(epochs):
    for batch in dataloader:
        outputs = model(batch)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
```

This is how PyTorch works. This is how research works. This is what you'll do in production.

Learn this pattern deeply before learning when to abstract it.

---

**Module 07 is there when you're ready for it - but fundamentals come first. ğŸ“**

