# ğŸ¤– Milestone 05: Transformer Era (2017) - TinyGPT

**After completing Modules 10-13**, you can build complete transformer language models!

## ğŸ¯ What You'll Build

A character-level transformer trained on Shakespeare's works - the classic "hello world" of language modeling!

### Shakespeare Text Generation
**File**: `vaswani_shakespeare.py`  
**Goal**: Build a transformer that generates Shakespeare-style text

```bash
python vaswani_shakespeare.py
```

**What it does**:
- Downloads Tiny Shakespeare dataset
- Trains character-level transformer (YOUR implementation!)
- Generates coherent Shakespeare-style text

**Demo**:
```
Prompt: 'To be or not to be,'
Output: 'To be or not to be, that is the question
         Whether tis nobler in the mind to suffer...'
```

---

## ğŸš€ Quick Start

### Prerequisites
Complete these TinyTorch modules:
- âœ… Module 10: Tokenization
- âœ… Module 11: Embeddings
- âœ… Module 12: Attention
- âœ… Module 13: Transformers

### Run the Example

```bash
# Train transformer on Shakespeare (15-20 min)
python vaswani_shakespeare.py
```

---

## ğŸ“ Learning Outcomes

After completing this milestone, you'll understand:

### Technical Mastery
- âœ… How tokenization bridges text and numbers
- âœ… How embeddings capture semantic meaning
- âœ… How attention enables context-aware processing
- âœ… How transformers generate sequences autoregressively

### Systems Insights
- âœ… Memory scaling: O(nÂ²) attention complexity
- âœ… Compute trade-offs: model size vs inference speed
- âœ… Vocabulary design: characters vs subwords vs words
- âœ… Generation strategies: greedy vs sampling

### Real-World Connection
- âœ… **GitHub Copilot** = transformer on code
- âœ… **ChatGPT** = scaled-up version of your TinyGPT
- âœ… **GPT-4** = same architecture, 1000Ã— more parameters
- âœ… YOU understand the math that powers modern AI!

---

## ğŸ—ï¸ Architecture You Built

```
Input Tokens
    â†“
Token Embeddings (Module 11)
    â†“
Positional Encoding (Module 11)
    â†“
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   Transformer Block Ã— N      â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â•‘
â•‘  â”‚ Multi-Head Attentionâ”‚ â†â”€â”€ Module 12
â•‘  â”‚         â†“           â”‚     â•‘
â•‘  â”‚    Layer Norm       â”‚ â†â”€â”€ Module 13
â•‘  â”‚         â†“           â”‚     â•‘
â•‘  â”‚  Feed Forward Net   â”‚ â†â”€â”€ Module 13
â•‘  â”‚         â†“           â”‚     â•‘
â•‘  â”‚    Layer Norm       â”‚ â†â”€â”€ Module 13
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    â†“
Output Projection
    â†“
Generated Text
```

---

## ğŸ”¬ Systems Analysis

### Memory Requirements
```python
TinyCoder (100K params):
  â€¢ Model weights: ~400KB
  â€¢ Activation memory: ~2MB per batch
  â€¢ Total: <10MB RAM

ChatGPT (175B params):
  â€¢ Model weights: ~350GB
  â€¢ Activation memory: ~100GB per batch
  â€¢ Total: ~500GB+ GPU RAM
```

### Computational Complexity
```python
For sequence length n:
  â€¢ Attention: O(nÂ²) operations
  â€¢ Feed-forward: O(n) operations
  â€¢ Total: O(nÂ²) dominated by attention

Why this matters:
  â€¢ 10 tokens: ~100 ops
  â€¢ 100 tokens: ~10,000 ops
  â€¢ 1000 tokens: ~1,000,000 ops
  
Quadratic scaling is why context length is expensive!
```

---

## ğŸ’¡ Production Differences

### Your TinyGPT vs Production GPT

| Feature | Your TinyGPT | Production GPT-4 |
|---------|--------------|------------------|
| **Parameters** | ~100K | ~1.8 Trillion |
| **Layers** | 4 | ~120 |
| **Training Data** | ~50K tokens | ~13 Trillion tokens |
| **Training Time** | 2 minutes | Months on supercomputers |
| **Inference** | CPU, seconds | GPU clusters, <100ms |
| **Memory** | <10MB | ~500GB |
| **Architecture** | âœ… IDENTICAL | âœ… IDENTICAL |

**Key insight**: You built the SAME architecture. Production is just bigger & optimized!

---

## ğŸš§ Troubleshooting

### Import Errors
```bash
# Make sure modules are exported
cd modules/source/10_tokenization && tito export
cd ../11_embeddings && tito export
cd ../12_attention && tito export
cd ../13_transformers && tito export

# Rebuild package
cd ../../.. && tito nbdev build
```

### Slow Training
```python
# Reduce model size
model = TinyGPT(
    vocab_size=vocab_size,
    embed_dim=64,      # Smaller (was 128)
    num_heads=4,       # Fewer (was 8)
    num_layers=2,      # Fewer (was 4)
    max_length=64      # Shorter (was 128)
)
```

### Poor Generation Quality
- âœ… Train longer (more steps)
- âœ… Increase model size
- âœ… Use more training data
- âœ… Adjust temperature (0.5-1.0 for code, 0.7-1.2 for text)

---

## ğŸ‰ Success Criteria

You've succeeded when:

âœ… Model trains without errors  
âœ… Loss decreases over training epochs  
âœ… Generated Shakespeare text is coherent (even if not perfect)  
âœ… You can generate text with custom prompts  

**Don't expect perfection!** Production models train for months on massive data. Your demo proves you understand the architecture!

---

## ğŸ“š What's Next?

After mastering transformers, you can:

1. **Experiment**: Try different model sizes, hyperparameters
2. **Extend**: Add more sophisticated generation (beam search, top-k sampling)
3. **Scale**: Train on larger datasets for better quality
4. **Optimize**: Add KV caching (Module 14) for faster inference
5. **Benchmark**: Profile memory and compute (Module 15)
6. **Quantize**: Reduce model size (Module 17)

---

## ğŸ† Achievement Unlocked

**You built the foundation of modern AI!**

The transformer architecture you implemented powers:
- ChatGPT, GPT-4 (OpenAI)
- Claude (Anthropic)
- LLaMA (Meta)
- PaLM (Google)
- GitHub Copilot
- And virtually every modern LLM!

**The only difference**: Scale. The architecture is what YOU built! ğŸ‰

---

**Ready to generate some text?** Run `python vaswani_shakespeare.py`!