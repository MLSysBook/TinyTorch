#!/usr/bin/env python3
"""
Shakespeare Text Generation (2017) - Transformer Era
===================================================

ğŸ“š HISTORICAL CONTEXT:
In 2017, Vaswani et al. published "Attention Is All You Need", showing that
attention mechanisms alone (no RNNs!) could achieve state-of-the-art results
on sequence tasks. This breakthrough launched the era of GPT, BERT, and modern LLMs.

ğŸ¯ WHAT YOU'RE BUILDING:
Using YOUR TinyTorch implementations, you'll build a character-level language model  
that generates Shakespeare-style text - proving YOUR attention mechanism works!

âœ… REQUIRED MODULES (Run after Module 13):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  Module 02 (Tensor)       : YOUR data structure with autograd
  Module 03 (Activations)  : YOUR ReLU in feed-forward networks
  Module 04 (Layers)       : YOUR Linear layers
  Module 08 (Optimizers)   : YOUR Adam optimizer
  Module 11 (Embeddings)   : YOUR token & positional embeddings
  Module 12 (Attention)    : YOUR multi-head self-attention
  Module 13 (Transformers) : YOUR LayerNorm + TransformerBlock
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ—ï¸ ARCHITECTURE (Character-Level Language Model):
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                               Output Predictions                             â”‚
    â”‚                         Character Probabilities (vocab_size)                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                            â–²
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                            Output Projection                                 â”‚
    â”‚                       Module 04: vectors â†’ vocabulary                        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                            â–²
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                              Layer Norm                                      â”‚
    â”‚                        Module 13: Final normalization                        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                            â–²
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                      Transformer Block Ã— N (Repeat)                          â•‘
    â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘
    â•‘  â”‚                       Feed Forward Network                             â”‚  â•‘
    â•‘  â”‚              Module 04: Linear â†’ ReLU â†’ Linear                         â”‚  â•‘
    â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘
    â•‘                                  â–²                                            â•‘
    â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘
    â•‘  â”‚                    Multi-Head Self-Attention                           â”‚  â•‘
    â•‘  â”‚           Module 12: QueryÂ·Key^TÂ·Value across all positions           â”‚  â•‘
    â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                            â–²
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                          Positional Encoding                                 â”‚
    â”‚                   Module 11: Add position information                        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                            â–²
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                         Character Embeddings                                 â”‚
    â”‚                    Module 11: chars â†’ embed_dim vectors                      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                            â–²
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                            Input Characters                                  â”‚
    â”‚                    "To be or not to be, that is..."                          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“Š EXPECTED PERFORMANCE:
- Dataset: ~1MB Shakespeare corpus (40,000 lines)
- Training time: 5-10 minutes (demonstration mode)
- Vocabulary: ~65 unique characters
- Expected: Coherent (if not perfect) Shakespeare-style text
- Parameters: ~500K (small by modern standards!)
"""

import sys
import os
import numpy as np
import argparse
import time

# Add project root to path
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

# Import TinyTorch components YOU BUILT!
from tinytorch.core.tensor import Tensor                    # Module 02: YOU built this!
from tinytorch.core.layers import Linear                    # Module 04: YOU built this!
from tinytorch.core.activations import ReLU, Softmax        # Module 03: YOU built this!
from tinytorch.core.optimizers import Adam                  # Module 08: YOU built this!
from tinytorch.core.attention import MultiHeadAttention     # Module 12: YOU built this!
from tinytorch.models.transformer import LayerNorm, TransformerBlock  # Module 13: YOU built this!
from tinytorch.text.embeddings import Embedding, PositionalEncoding   # Module 11: YOU built this!
from tinytorch.data.loader import DataLoader, Dataset   # Module 08: YOU built this!

# Import dataset manager
try:
    from data_manager import DatasetManager
except ImportError:
    sys.path.append(os.path.join(project_root, 'milestones'))
    from data_manager import DatasetManager


class ShakespeareDataset(Dataset):
    """
    Character-level Shakespeare dataset using YOUR Dataset interface!
    
    Tokenizes text into characters and creates sequences for language modeling.
    """
    
    def __init__(self, text, seq_length=64):
        """
        Initialize dataset with text and sequence length.
        
        Args:
            text: Raw Shakespeare text
            seq_length: Length of input sequences
        """
        # Build character vocabulary
        self.chars = sorted(list(set(text)))
        self.vocab_size = len(self.chars)
        self.char_to_idx = {ch: i for i, ch in enumerate(self.chars)}
        self.idx_to_char = {i: ch for i, ch in enumerate(self.chars)}
        
        # Convert text to indices
        self.data = [self.char_to_idx[ch] for ch in text]
        self.seq_length = seq_length
        
        # Calculate number of sequences
        self.num_sequences = len(self.data) - seq_length
        
    def __getitem__(self, idx):
        """Get a single training sequence - YOUR Dataset interface!"""
        # Input: characters at positions [idx, idx+seq_length)
        # Target: characters at positions [idx+1, idx+seq_length+1)
        input_seq = self.data[idx:idx + self.seq_length]
        target_seq = self.data[idx + 1:idx + self.seq_length + 1]
        
        return Tensor(np.array(input_seq, dtype=np.int32)), Tensor(np.array(target_seq, dtype=np.int32))
    
    def __len__(self):
        """Return dataset size - YOUR Dataset interface!"""
        return self.num_sequences
    
    def decode(self, indices):
        """Convert indices back to text."""
        return ''.join([self.idx_to_char[int(idx)] for idx in indices])


class TinyGPT:
    """
    Character-level Transformer Language Model using YOUR TinyTorch!
    
    This architecture is what powers GPT, ChatGPT, and modern LLMs.
    """
    
    def __init__(self, vocab_size, embed_dim, max_length, num_heads, num_layers):
        print("ğŸ§  Building TinyGPT with YOUR TinyTorch modules...")
        
        # Token representation
        self.embedding = Embedding(vocab_size, embed_dim)           # Module 11!
        self.pos_encoding = PositionalEncoding(max_length, embed_dim)  # Module 11!

        # Transformer stack
        self.layers = []
        hidden_dim = embed_dim * 4  # Standard 4x expansion in FFN
        for _ in range(num_layers):
            block = TransformerBlock(embed_dim, num_heads, hidden_dim)  # Module 13!
            self.layers.append(block)

        # Output head
        self.layer_norm = LayerNorm(embed_dim)          # Module 13!
        self.output_proj = Linear(embed_dim, vocab_size)  # Module 04!
        
        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        
        # Calculate parameters
        self.total_params = self._count_parameters()
        
        print(f"   Architecture: {num_layers} layers, {num_heads} heads, {embed_dim}-dim embeddings")
        print(f"   Vocabulary: {vocab_size} characters")
        print(f"   Total parameters: {self.total_params:,} (YOUR components!)")
    
    def _count_parameters(self):
        """Count total parameters in model."""
        count = 0
        for param in self.parameters():
            count += param.data.size
        return count

    def parameters(self):
        """Get all trainable parameters from YOUR model."""
        params = []
        # Embedding parameters
        params.extend([self.embedding.weight])
        # Transformer block parameters
        for layer in self.layers:
            if hasattr(layer, 'parameters'):
                if callable(layer.parameters):
                    params.extend(layer.parameters())
                else:
                    params.extend(layer.parameters)
        # Output projection parameters
        params.extend([self.layer_norm.gamma, self.layer_norm.beta])
        params.extend([self.output_proj.weight, self.output_proj.bias])
        return params

    def forward(self, x):
        """Forward pass through YOUR transformer stack."""
        # Convert tokens to contextual vectors
        x = self.embedding.forward(x)        # Module 11: char â†’ vectors
        x = self.pos_encoding.forward(x)     # Module 11: add position info
        
        # Process through transformer layers
        for layer in self.layers:
            x = layer.forward(x)  # Module 13: Attention â†’ FFN
        
        # Generate predictions
        x = self.layer_norm.forward(x)       # Module 13: final norm

        # Reshape for Linear layer
        x_np = np.array(x.data.data if hasattr(x.data, 'data') else x.data)
        batch_size, seq_len, embed_dim = x_np.shape
        x_2d_np = x_np.reshape(batch_size * seq_len, embed_dim)
        x_2d = Tensor(x_2d_np)

        # Apply output projection
        logits_2d = self.output_proj(x_2d)   # Module 04: vocab predictions

        # Reshape back
        logits_2d_np = np.array(logits_2d.data.data if hasattr(logits_2d.data, 'data') else logits_2d.data)
        logits_np = logits_2d_np.reshape(batch_size, seq_len, self.vocab_size)
        logits = Tensor(logits_np)
        
        return logits


def visualize_transformer():
    """Show how transformers process text sequences."""
    print("\n" + "="*70)
    print("ğŸ¤– VISUALIZING TRANSFORMER TEXT GENERATION:")
    print("="*70)
    
    print("""
    How YOUR Transformer Sees Text:      What It Learns:
    
    Input: "To be or not to be"          Layer 1 (Attention):
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â€¢ Each word attends to others
    â”‚ T o   b e   o r ... â”‚              â€¢ "be" looks at "To", "or", etc.
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â€¢ Captures dependencies
            â†“                            
    Character Embeddings                 Layer 2-4 (Deep Attention):
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â€¢ Builds complex patterns
    â”‚ 128-dim vectors     â”‚              â€¢ Grammar, style, meaning
    â”‚ for each character  â”‚              â€¢ Shakespeare-specific patterns
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              
            â†“                            Output Prediction:
    Position Encoding                    "To be or not to be, that is the"
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                         â†“
    â”‚ Add positional info â”‚              Next char probabilities:
    â”‚ (order matters!)    â”‚              't' â†’ 0.85  (highest!)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              'n' â†’ 0.03
            â†“                            'a' â†’ 0.02
    Transformer Layers Ã—4                ...
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Self-Attention      â”‚              Key Transformer Insight:
    â”‚ Feed-Forward        â”‚              Unlike RNNs, attention lets each
    â”‚ Layer Norm          â”‚              position look at ALL others
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              simultaneously - capturing long-range
            â†“                            dependencies in O(1) operations!
    Character Predictions
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Probability for     â”‚
    â”‚ each next character â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    print("="*70)


def train_shakespeare_gpt(model, train_loader, dataset, epochs=5, learning_rate=0.001):
    """Train TinyGPT using YOUR complete training system with DataLoader!"""
    print("\nğŸš€ Training Shakespeare TinyGPT with YOUR TinyTorch!")
    print(f"   Dataset: {len(train_loader.dataset):,} character sequences")
    print(f"   Batch size: {train_loader.batch_size}")
    print(f"   YOUR DataLoader (Module 08) handles batching!")
    print(f"   YOUR Adam optimizer (Module 08)")
    
    # YOUR optimizer
    optimizer = Adam(model.parameters(), learning_rate=learning_rate)
    
    for epoch in range(epochs):
        print(f"\n   Epoch {epoch+1}/{epochs}:")
        epoch_loss = 0
        batch_count = 0
        
        # Use YOUR DataLoader to iterate through batches!
        for batch_idx, (batch_input, batch_target) in enumerate(train_loader):
            if batch_idx >= 100:  # Demo mode - limit batches
                break
            
            # Forward pass with YOUR Transformer
            logits = model.forward(batch_input)  # YOUR attention mechanism!
            
            # Language modeling loss
            logits_np = np.array(logits.data.data if hasattr(logits.data, 'data') else logits.data)
            targets_np = np.array(batch_target.data.data if hasattr(batch_target.data, 'data') else batch_target.data)
            
            batch_size, seq_length = targets_np.shape
            vocab_size = logits_np.shape[-1]
            
            # Cross-entropy loss
            targets_one_hot = np.zeros((batch_size, seq_length, vocab_size))
            for b in range(batch_size):
                for s in range(seq_length):
                    targets_one_hot[b, s, int(targets_np[b, s])] = 1.0

            # Softmax + cross entropy
            exp_logits = np.exp(logits_np - np.max(logits_np, axis=2, keepdims=True))
            softmax = exp_logits / np.sum(exp_logits, axis=2, keepdims=True)
            loss_value = -np.mean(np.sum(targets_one_hot * np.log(softmax + 1e-8), axis=2))
            loss = Tensor([loss_value])
            
            # Backward pass with YOUR autograd
            optimizer.zero_grad()  # Module 08!
            loss.backward()        # Module 05: YOUR autodiff!
            optimizer.step()       # Module 08!
            
            epoch_loss += loss_value
            batch_count += 1
            
            # Progress
            if (batch_idx + 1) % 20 == 0:
                print(f"   Batch {batch_idx+1}: Loss = {loss_value:.4f}")
        
        # Epoch summary
        avg_loss = epoch_loss / max(1, batch_count)
        print(f"   â†’ Epoch Complete: Avg Loss = {avg_loss:.4f} (YOUR Transformer learning!)")
    
    return model


def generate_text(model, dataset, prompt="To be or not", max_length=200, temperature=0.8):
    """
    Generate text from a prompt - THE WOW MOMENT!
    
    This is autoregressive generation: predict next char, add it, repeat.
    """
    print("\nâœ¨ TEXT GENERATION DEMO - THE PAYOFF!")
    print("="*70)
    
    # Convert prompt to indices
    prompt_indices = [dataset.char_to_idx[ch] for ch in prompt if ch in dataset.char_to_idx]
    generated = prompt_indices.copy()
    
    print(f"ğŸ“ Prompt: \"{prompt}\"")
    print(f"ğŸ¯ Generating {max_length} characters...\n")
    
    # Generate character by character
    for _ in range(max_length):
        # Take last seq_length characters as input
        input_seq = generated[-dataset.seq_length:] if len(generated) >= dataset.seq_length else generated
        
        # Pad if necessary
        if len(input_seq) < dataset.seq_length:
            input_seq = [0] * (dataset.seq_length - len(input_seq)) + input_seq
        
        # Forward pass
        input_tensor = Tensor(np.array([input_seq], dtype=np.int32))
        logits = model.forward(input_tensor)
        
        # Get logits for last position
        logits_np = np.array(logits.data.data if hasattr(logits.data, 'data') else logits.data)
        next_logits = logits_np[0, -1, :]  # Last position predictions
        
        # Apply temperature and sample
        next_logits = next_logits / temperature
        exp_logits = np.exp(next_logits - np.max(next_logits))
        probs = exp_logits / np.sum(exp_logits)
        
        # Sample from distribution
        next_idx = np.random.choice(len(probs), p=probs)
        generated.append(next_idx)
    
    # Decode to text
    generated_text = dataset.decode(generated)
    
    print("ğŸ“– Generated Text:")
    print("â”€" * 70)
    print(generated_text)
    print("â”€" * 70)
    
    return generated_text


def analyze_transformer_systems(model):
    """Analyze YOUR Transformer from an ML systems perspective."""
    print("\nğŸ”¬ SYSTEMS ANALYSIS of YOUR Transformer Implementation:")
    
    print(f"\n   Model Architecture:")
    print(f"   â€¢ Parameters: {model.total_params:,} weights")
    print(f"   â€¢ Embedding dim: {model.embed_dim}")
    print(f"   â€¢ Vocabulary: {model.vocab_size} characters")
    
    print(f"\n   Computational Complexity:")
    print(f"   â€¢ Attention: O(nÂ²Â·d) where n=sequence, d=dimension")
    print(f"   â€¢ Self-attention allows parallel processing (vs RNN sequential)")
    print(f"   â€¢ YOUR implementation: Pure Python + NumPy")
    
    print(f"\n   Memory Requirements:")
    print(f"   â€¢ Parameters: {model.total_params * 4 / 1024:.1f} KB")
    print(f"   â€¢ Attention matrices: O(nÂ²) per layer")
    print(f"   â€¢ YOUR TinyTorch tracks gradients automatically")
    
    print(f"\n   ğŸ›ï¸ Transformer Evolution:")
    print(f"   â€¢ 2017: Vaswani et al. 'Attention Is All You Need'")
    print(f"   â€¢ 2018: BERT (bidirectional), GPT (autoregressive)")
    print(f"   â€¢ 2020: GPT-3 (175B params, same architecture!)")
    print(f"   â€¢ 2022: ChatGPT (YOUR architecture at massive scale)")
    print(f"   â€¢ YOUR TinyGPT: Core principles that power them all!")
    
    print(f"\n   ğŸ’¡ Why Transformers Dominate:")
    print(f"   â€¢ Parallelizable (vs sequential RNNs)")
    print(f"   â€¢ Long-range dependencies (attention sees everything)")
    print(f"   â€¢ Scalable (architecture works from 1M to 175B params)")
    print(f"   â€¢ YOUR implementation demonstrates all of these!")


def main():
    """Demonstrate Shakespeare text generation using YOUR TinyTorch!"""
    
    parser = argparse.ArgumentParser(description='Shakespeare Transformer 2017')
    parser.add_argument('--test-only', action='store_true',
                       help='Test architecture only')
    parser.add_argument('--epochs', type=int, default=5,
                       help='Training epochs (demo mode)')
    parser.add_argument('--batch-size', type=int, default=32,
                       help='Batch size')
    parser.add_argument('--seq-length', type=int, default=64,
                       help='Sequence length')
    parser.add_argument('--embed-dim', type=int, default=128,
                       help='Embedding dimension')
    parser.add_argument('--num-layers', type=int, default=4,
                       help='Number of transformer layers')
    parser.add_argument('--num-heads', type=int, default=4,
                       help='Number of attention heads')
    parser.add_argument('--visualize', action='store_true', default=True,
                       help='Show transformer visualization')
    parser.add_argument('--quick-test', action='store_true',
                       help='Use small subset for testing')
    args = parser.parse_args()
    
    print("ğŸ¯ Shakespeare Transformer - Text Generation with YOUR Attention!")
    print("   Historical significance: Attention revolutionized sequence modeling")
    print("   YOUR achievement: Generate Shakespeare-style text")
    print("   Components used: YOUR complete transformer system (Modules 2-13)")
    
    # Visualization
    if args.visualize:
        visualize_transformer()
    
    # Step 1: Load Shakespeare dataset
    print("\nğŸ“¥ Loading Shakespeare corpus...")
    data_manager = DatasetManager()
    
    try:
        text = data_manager.get_shakespeare()
        
        if args.quick_test:
            text = text[:10000]  # Use small subset for testing
            print("   (Using subset for quick testing)")
            
    except Exception as e:
        print(f"âš ï¸  Shakespeare download failed: {e}")
        print("   Using synthetic text for demonstration...")
        text = "To be or not to be, that is the question. " * 100
    
    # Step 2: Create Dataset and DataLoader using YOUR Module 08!
    print(f"\nğŸ“¦ Creating YOUR Dataset and DataLoader (Module 08)...")
    dataset = ShakespeareDataset(text, seq_length=args.seq_length)
    
    # YOUR DataLoader handles batching and shuffling!
    train_loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)
    
    print(f"   Vocabulary: {dataset.vocab_size} unique characters")
    print(f"   Characters: '{dataset.decode(list(range(min(20, dataset.vocab_size))))}...'")
    print(f"   DataLoader: {len(dataset):,} sequences, batch_size={args.batch_size}")
    
    # Step 3: Build Transformer
    model = TinyGPT(
        vocab_size=dataset.vocab_size,
        embed_dim=args.embed_dim,
        max_length=args.seq_length,
        num_heads=args.num_heads,
        num_layers=args.num_layers
    )
    
    if args.test_only:
        print("\nğŸ§ª ARCHITECTURE TEST MODE")
        # Test with minimal data
        test_input = Tensor(np.random.randint(0, dataset.vocab_size, (1, args.seq_length), dtype=np.int32))
        test_output = model.forward(test_input)
        print(f"âœ… Forward pass successful! Output shape: {test_output.data.shape}")
        print("âœ… YOUR Transformer + DataLoader work together!")
        return
    
    # Step 4: Train using YOUR DataLoader
    start_time = time.time()
    model = train_shakespeare_gpt(model, train_loader, dataset, epochs=args.epochs)
    train_time = time.time() - start_time
    
    # Step 5: Generate text!
    generated = generate_text(model, dataset, prompt="To be or not", max_length=200)
    
    # Additional generation examples
    print("\nğŸ­ More Generation Examples:")
    print("â”€" * 70)
    
    prompts = ["ROMEO:", "The king", "What is"]
    for prompt in prompts:
        if all(ch in dataset.char_to_idx for ch in prompt):
            print(f"\nPrompt: \"{prompt}\"")
            gen = generate_text(model, dataset, prompt=prompt, max_length=100, temperature=0.8)
    
    # Step 6: Systems Analysis
    analyze_transformer_systems(model)
    
    print(f"\nâ±ï¸  Training time: {train_time:.1f} seconds")
    print(f"   Sequences/sec: {len(dataset) * args.epochs / train_time:.0f}")
    
    print("\nâœ… SUCCESS! Shakespeare Transformer Milestone Complete!")
    print("\nğŸ“ What YOU Accomplished:")
    print("   â€¢ YOUR attention mechanism processes sequences in parallel")
    print("   â€¢ YOUR transformer captures long-range text dependencies")
    print("   â€¢ YOUR DataLoader efficiently batches character sequences")
    print("   â€¢ YOUR TinyGPT generates coherent text!")
    print("   â€¢ YOUR complete language modeling system works!")
    
    print("\nğŸš€ Next Steps:")
    print("   â€¢ Continue to Module 14 (KV-Caching) for 3x faster inference")
    print("   â€¢ YOUR transformer architecture scales to GPT-scale models")
    print("   â€¢ This is the foundation of ChatGPT, GPT-4, and all modern LLMs!")

if __name__ == "__main__":
    main()
