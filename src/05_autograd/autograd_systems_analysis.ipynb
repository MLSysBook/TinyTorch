{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3f2511",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Autograd Systems Analysis - Memory & Performance Profiling\n",
    "\n",
    "This file contains the P0 critical additions for Module 05 autograd:\n",
    "- Memory profiling with tracemalloc\n",
    "- Performance benchmarking\n",
    "- Computational complexity analysis\n",
    "\n",
    "These functions should be inserted after test_module() and before the module summary.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdc2afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tracemalloc\n",
    "import time\n",
    "from tinytorch.core.tensor import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05201c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_autograd_memory():\n",
    "    \"\"\"\n",
    "    Profile memory usage of autograd operations.\n",
    "\n",
    "    This function demonstrates the memory cost of gradient tracking\n",
    "    by comparing requires_grad=True vs. requires_grad=False.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìä Autograd Memory Profiling\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Test 1: Memory without gradients\n",
    "    print(\"\\nüî¨ Test 1: Memory without gradient tracking...\")\n",
    "    tracemalloc.start()\n",
    "    x_no_grad = Tensor(np.random.randn(1000, 1000), requires_grad=False)\n",
    "    y_no_grad = x_no_grad.matmul(x_no_grad)\n",
    "    mem_no_grad = tracemalloc.get_traced_memory()[1] / (1024 * 1024)  # MB\n",
    "    tracemalloc.stop()\n",
    "\n",
    "    # Test 2: Memory with gradients\n",
    "    print(\"üî¨ Test 2: Memory with gradient tracking...\")\n",
    "    tracemalloc.start()\n",
    "    x_with_grad = Tensor(np.random.randn(1000, 1000), requires_grad=True)\n",
    "    y_with_grad = x_with_grad.matmul(x_with_grad)\n",
    "    mem_with_grad = tracemalloc.get_traced_memory()[1] / (1024 * 1024)  # MB\n",
    "    tracemalloc.stop()\n",
    "\n",
    "    # Test 3: Memory after backward\n",
    "    print(\"üî¨ Test 3: Memory after backward pass...\")\n",
    "    tracemalloc.start()\n",
    "    x_backward = Tensor(np.random.randn(1000, 1000), requires_grad=True)\n",
    "    y_backward = x_backward.matmul(x_backward)\n",
    "    loss = y_backward.sum()\n",
    "    loss.backward()\n",
    "    mem_after_backward = tracemalloc.get_traced_memory()[1] / (1024 * 1024)  # MB\n",
    "    tracemalloc.stop()\n",
    "\n",
    "    print(f\"\\nüìä Memory Usage (1000√ó1000 matrix):\")\n",
    "    print(f\"  ‚Ä¢ No gradients:      {mem_no_grad:.2f} MB\")\n",
    "    print(f\"  ‚Ä¢ With gradients:    {mem_with_grad:.2f} MB ({mem_with_grad/mem_no_grad:.2f}√ó overhead)\")\n",
    "    print(f\"  ‚Ä¢ After backward:    {mem_after_backward:.2f} MB\")\n",
    "\n",
    "    graph_overhead = mem_with_grad - mem_no_grad\n",
    "    gradient_storage = mem_after_backward - mem_with_grad\n",
    "\n",
    "    print(f\"  ‚Ä¢ Graph overhead:    {graph_overhead:.2f} MB\")\n",
    "    print(f\"  ‚Ä¢ Gradient storage:  {gradient_storage:.2f} MB\")\n",
    "\n",
    "    print(\"\\nüí° Key Insight: Autograd adds ~2-3√ó memory overhead\")\n",
    "    print(\"   (1√ó for gradients + 1-2√ó for computation graph)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05835f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_backward_pass():\n",
    "    \"\"\"\n",
    "    Benchmark forward vs. backward pass timing.\n",
    "\n",
    "    Demonstrates that backward pass is typically 2-3√ó slower than forward\n",
    "    due to additional matmul operations for gradient computation.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚ö° Backward Pass Performance Benchmarking\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    sizes = [100, 500, 1000]\n",
    "\n",
    "    for size in sizes:\n",
    "        # Forward pass timing (no gradients)\n",
    "        x = Tensor(np.random.randn(size, size), requires_grad=False)\n",
    "        W = Tensor(np.random.randn(size, size), requires_grad=False)\n",
    "\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(10):\n",
    "            y = x.matmul(W)\n",
    "        forward_time = (time.perf_counter() - start) / 10\n",
    "\n",
    "        # Forward + backward timing\n",
    "        x = Tensor(np.random.randn(size, size), requires_grad=True)\n",
    "        W = Tensor(np.random.randn(size, size), requires_grad=True)\n",
    "\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(10):\n",
    "            x.zero_grad()\n",
    "            W.zero_grad()\n",
    "            y = x.matmul(W)\n",
    "            loss = y.sum()\n",
    "            loss.backward()\n",
    "        total_time = (time.perf_counter() - start) / 10\n",
    "\n",
    "        backward_time = total_time - forward_time\n",
    "\n",
    "        print(f\"\\nüìê Matrix size: {size}√ó{size}\")\n",
    "        print(f\"  ‚Ä¢ Forward pass:  {forward_time*1000:.2f} ms\")\n",
    "        print(f\"  ‚Ä¢ Backward pass: {backward_time*1000:.2f} ms ({backward_time/forward_time:.2f}√ó forward)\")\n",
    "        print(f\"  ‚Ä¢ Total:         {total_time*1000:.2f} ms\")\n",
    "\n",
    "    print(\"\\nüí° Key Insight: Backward pass ‚âà 2-3√ó forward pass time\")\n",
    "    print(\"   (grad_x = grad @ W.T + W.T @ grad = 2 matmuls vs. 1 in forward)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d9e3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_complexity():\n",
    "    \"\"\"\n",
    "    Display computational complexity analysis for autograd operations.\n",
    "\n",
    "    Shows time and space complexity for common operations.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìä Computational Complexity Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\n### Time Complexity\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Operation':<20} {'Forward':<15} {'Backward':<15} {'Total':<15}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Add':<20} {'O(n)':<15} {'O(n)':<15} {'O(n)':<15}\")\n",
    "    print(f\"{'Mul':<20} {'O(n)':<15} {'O(n)':<15} {'O(n)':<15}\")\n",
    "    print(f\"{'Matmul (n√ón)':<20} {'O(n¬≥)':<15} {'O(n¬≥) √ó 2':<15} {'O(n¬≥)':<15}\")\n",
    "    print(f\"{'Sum':<20} {'O(n)':<15} {'O(n)':<15} {'O(n)':<15}\")\n",
    "    print(f\"{'ReLU':<20} {'O(n)':<15} {'O(n)':<15} {'O(n)':<15}\")\n",
    "    print(f\"{'Softmax':<20} {'O(n)':<15} {'O(n)':<15} {'O(n)':<15}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    print(\"\\nüí° Key Insight: Matrix operations dominate training time\")\n",
    "    print(\"   For Matmul with (m√ók) @ (k√ón):\")\n",
    "    print(\"   - Forward: O(m√ók√ón)\")\n",
    "    print(\"   - Backward grad_A: O(m√ón√ók)  [grad_Z @ B.T]\")\n",
    "    print(\"   - Backward grad_B: O(k√óm√ón)  [A.T @ grad_Z]\")\n",
    "    print(\"   - Total: ~3√ó forward pass cost\")\n",
    "\n",
    "    print(\"\\n### Space Complexity\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Component':<25} {'Memory Usage':<35}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Parameters':<25} {'P (baseline)':<35}\")\n",
    "    print(f\"{'Activations':<25} {'~P (for N layers ‚âà P/N per layer)':<35}\")\n",
    "    print(f\"{'Gradients':<25} {'P (1:1 with parameters)':<35}\")\n",
    "    print(f\"{'Computation Graph':<25} {'0.2-0.5P (Function objects)':<35}\")\n",
    "    print(f\"{'Total Training':<25} {'~2.5-3P':<35}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    print(\"\\nüí° Key Insight: Training requires ~3√ó parameter memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390ccc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution block with all profiling\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üî¨ AUTOGRAD SYSTEMS ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    profile_autograd_memory()\n",
    "    benchmark_backward_pass()\n",
    "    analyze_complexity()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ Systems analysis complete!\")\n",
    "    print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
