# TinyğŸ”¥Torch 

**Build your own ML framework. Start small. Go deep.**

![Work in Progress](https://img.shields.io/badge/status-work--in--progress-yellow)
![Educational Project](https://img.shields.io/badge/purpose-educational-informational)
[![GitHub](https://img.shields.io/badge/github-mlsysbook/TinyTorch-blue.svg)](https://github.com/MLSysBook/TinyTorch)
[![Python](https://img.shields.io/badge/python-3.8+-green.svg)](https://python.org)
[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/MLSysBook/TinyTorch/main)
[![Jupyter Book](https://img.shields.io/badge/docs-Jupyter_Book-orange.svg)](https://mlsysbook.github.io/TinyTorch/)

ğŸ“š **[Read the Interactive Course â†’](https://mlsysbook.github.io/TinyTorch/)**

---

## ğŸ—ï¸ **The Big Picture: Why Build from Scratch?**

**Most ML education teaches you to _use_ frameworks.** TinyTorch teaches you to _build_ them.

```python
Traditional ML Course:          TinyTorch Approach:
â”œâ”€â”€ import torch               â”œâ”€â”€ class Tensor:
â”œâ”€â”€ model = nn.Linear(10, 1)   â”‚     def __add__(self, other): ...
â”œâ”€â”€ loss = nn.MSELoss()        â”‚     def backward(self): ...
â””â”€â”€ optimizer.step()           â”œâ”€â”€ class Linear:
                               â”‚     def forward(self, x):
                               â”‚       return x @ self.weight + self.bias
                               â”œâ”€â”€ def mse_loss(pred, target):
                               â”‚     return ((pred - target) ** 2).mean()
                               â”œâ”€â”€ class SGD:
                               â”‚     def step(self):
                               â””â”€â”€     param.data -= lr * param.grad

Go from "How does this work?" ğŸ¤· to "I implemented every line!" ğŸ’ª
```

**Result:** You become the person others come to when they need to understand "how PyTorch actually works under the hood."

---

## ğŸŒŸ **What Makes TinyTorch Different**

### **ğŸ”¬ Build-First Philosophy**
- **No black boxes**: Implement every component from scratch
- **Immediate ownership**: Use YOUR code in real neural networks
- **Deep understanding**: Know exactly how each piece works

### **ğŸš€ Real Production Skills**
- **Professional workflow**: Development with `tito` CLI, automated testing
- **Real datasets**: Train on CIFAR-10, not toy data
- **Production patterns**: MLOps, monitoring, optimization from day one

### **ğŸ¯ Progressive Mastery** 
- **Start simple**: Implement `hello_world()` function
- **Build systematically**: Each module enables the next
- **End powerful**: Deploy production ML systems with monitoring

### **âš¡ Instant Feedback**
- **Code works immediately**: No waiting to see results
- **Visual progress**: Success indicators and system integration
- **"Aha moments"**: Watch your `ReLU` power real neural networks

### **ğŸ“Š NEW: Visual System Architecture**
- **Interactive dependency graphs**: See how all 17 modules connect
- **Learning roadmap visualization**: Optimal path through the system
- **Architecture diagrams**: Complete framework overview
- **Automated analysis**: Live system statistics and component mapping

---

## ğŸ¯ What You'll Build

* **One Complete ML Framework** â€” Not 17 separate exercises, but integrated components building into your own PyTorch-style toolkit
* **Fully Functional System** â€” Every piece connects: your tensors power your layers, your autograd enables your optimizers, your framework trains real networks
* **Real Applications** â€” Train neural networks on CIFAR-10 using 100% your own code, no PyTorch imports
* **Production-Ready Skills** â€” Complete ML lifecycle: data loading, training, optimization, deployment, monitoring
* **Deep Systems Understanding** â€” Know exactly how every component works and integrates because you built it all

---

## ğŸš€ Quick Start (2 minutes)

### ğŸ“Š **First Time? Start with the System Overview**

```bash
git clone https://github.com/mlsysbook/TinyTorch.git
cd TinyTorch
pip install -r requirements.txt           # Install all dependencies (numpy, jupyter, pytest, etc.)
pip install -e .                          # Install TinyTorch package in editable mode
tito system doctor                         # Verify your setup

# ğŸ¯ NEW: Interactive System Architecture Overview
cd modules/source/00_introduction
jupyter lab introduction_dev.py           # Explore the complete TinyTorch system visually!
```

### ğŸ§‘â€ğŸ“ **Ready to Build? Start Here**

```bash
# After exploring the system overview, start building:
cd modules/source/01_setup
jupyter lab setup_dev.py                  # Launch your first implementation module
```

### ğŸ‘©â€ğŸ« **Instructors**

```bash
# System check
tito system info
tito system doctor

# Module workflow
tito export 01_setup
tito test 01_setup
tito nbdev build                          # Update package
```

---

## ğŸ“ **Repository Structure**

```
TinyTorch/
â”œâ”€â”€ modules/source/           # 17 educational modules
â”‚   â”œâ”€â”€ 00_introduction/     # ğŸ¯ NEW: Visual system overview & architecture
â”‚   â”‚   â”œâ”€â”€ module.yaml      # Module metadata  
â”‚   â”‚   â”œâ”€â”€ README.md        # Getting started guide
â”‚   â”‚   â””â”€â”€ introduction_dev.py  # Interactive visualizations & dependency analysis
â”‚   â”œâ”€â”€ 01_setup/            # Development environment setup
â”‚   â”‚   â”œâ”€â”€ module.yaml      # Module metadata
â”‚   â”‚   â”œâ”€â”€ README.md        # Learning objectives and guide
â”‚   â”‚   â””â”€â”€ setup_dev.py     # Implementation file
â”‚   â”œâ”€â”€ 02_tensor/           # N-dimensional arrays
â”‚   â”‚   â”œâ”€â”€ module.yaml
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â””â”€â”€ tensor_dev.py
â”‚   â”œâ”€â”€ 03_activations/      # Neural network activation functions
â”‚   â”œâ”€â”€ 04_layers/           # Dense layers and transformations
â”‚   â”œâ”€â”€ 05_dense/            # Sequential networks and MLPs
â”‚   â”œâ”€â”€ 06_spatial/          # Convolutional neural networks
â”‚   â”œâ”€â”€ 07_attention/        # Self-attention and transformer components
â”‚   â”œâ”€â”€ 08_dataloader/       # Data loading and preprocessing
â”‚   â”œâ”€â”€ 09_autograd/         # Automatic differentiation
â”‚   â”œâ”€â”€ 10_optimizers/       # SGD, Adam, learning rate scheduling
â”‚   â”œâ”€â”€ 11_training/         # Training loops and validation
â”‚   â”œâ”€â”€ 12_compression/      # Model optimization and compression
â”‚   â”œâ”€â”€ 13_kernels/          # High-performance operations
â”‚   â”œâ”€â”€ 14_benchmarking/     # Performance analysis and profiling
â”‚   â”œâ”€â”€ 15_mlops/            # Production monitoring and deployment
â”‚   â””â”€â”€ 16_capstone/         # Systems engineering capstone project
â”œâ”€â”€ tinytorch/               # Your built framework package
â”‚   â”œâ”€â”€ core/                # Core implementations (exported from modules)
â”‚   â”‚   â”œâ”€â”€ tensor.py        # Generated from 02_tensor
â”‚   â”‚   â”œâ”€â”€ activations.py   # Generated from 03_activations
â”‚   â”‚   â”œâ”€â”€ layers.py        # Generated from 04_layers
â”‚   â”‚   â”œâ”€â”€ dense.py         # Generated from 05_dense
â”‚   â”‚   â”œâ”€â”€ spatial.py       # Generated from 06_spatial
â”‚   â”‚   â”œâ”€â”€ attention.py     # Generated from 07_attention
â”‚   â”‚   â””â”€â”€ ...              # All your implementations
â”‚   â””â”€â”€ utils/               # Shared utilities and tools
â”œâ”€â”€ book/                    # Interactive course website
â”‚   â”œâ”€â”€ _config.yml          # Jupyter Book configuration
â”‚   â”œâ”€â”€ intro.md             # Course introduction
â”‚   â””â”€â”€ chapters/            # Generated from module READMEs
â”œâ”€â”€ tito/                    # CLI tool for development workflow
â”‚   â”œâ”€â”€ commands/            # Student and instructor commands
â”‚   â””â”€â”€ tools/               # Testing and build automation
â””â”€â”€ tests/                   # Integration tests
```

**How It Works:**
0. **ğŸ¯ Start with Overview** - Explore `00_introduction` for visual system architecture and dependencies
1. **Develop in `modules/source/`** - Each module has a `*_dev.py` file where you implement components
2. **Export to `tinytorch/`** - Use `tito export` to build your implementations into a real Python package
3. **Use your framework** - Import and use your own code: `from tinytorch.core.tensor import Tensor`
4. **Test everything** - Run `tito test` to verify your implementations work correctly
5. **Build iteratively** - Each module builds on previous ones, creating a complete ML framework

---

## ğŸ“š Complete Course: 17 Modules

**Difficulty Progression:** ğŸ“Š Overview â†’ â­ Beginner â†’ â­â­ Intermediate â†’ â­â­â­ Advanced â†’ â­â­â­â­ Expert â†’ â­â­â­â­â­ğŸ¥· Capstone

### **ğŸ“Š System Overview** (Module 00)
* **ğŸ¯ 00_introduction**: Interactive system architecture, dependency visualization, and learning roadmap

### **ğŸ—ï¸ Foundations** (Modules 01-05)
* **01_setup**: Development environment and CLI tools
* **02_tensor**: N-dimensional arrays and tensor operations  
* **03_activations**: ReLU, Sigmoid, Tanh, Softmax functions
* **04_layers**: Dense layers and matrix operations
* **05_dense**: Sequential networks and MLPs

### **ğŸ§  Deep Learning** (Modules 06-10)
* **06_spatial**: Convolutional neural networks and image processing
* **07_attention**: Self-attention and transformer components
* **08_dataloader**: Data loading, batching, and preprocessing
* **09_autograd**: Automatic differentiation and backpropagation  
* **10_optimizers**: SGD, Adam, and learning rate scheduling

### **âš¡ Systems & Production** (Modules 11-15)
* **11_training**: Training loops, metrics, and validation
* **12_compression**: Model pruning, quantization, and distillation
* **13_kernels**: Performance optimization and custom operations
* **14_benchmarking**: Profiling, testing, and performance analysis
* **15_mlops**: Monitoring, deployment, and production systems

### **ğŸ“ Capstone Project** (Module 16)
* **16_capstone**: Advanced framework engineering specialization tracks

**Status**: All 17 modules complete with inline tests and educational content

---

## ğŸ”— **Complete System Integration**

**This isn't 17 isolated assignments.** Every component you build integrates into one cohesive, fully functional ML framework:

**ğŸ¯ NEW: Explore the full system architecture visually in Module 00 before diving into implementation!**

```mermaid
flowchart TD
    Z[00_introduction<br/>ğŸ¯ System Overview & Architecture] --> A[01_setup<br/>Setup & Environment] 
    A --> B[02_tensor<br/>Core Tensor Operations]
    B --> C[03_activations<br/>ReLU, Sigmoid, Tanh]
    C --> D[04_layers<br/>Dense Layers]
    D --> E[05_dense<br/>Sequential Networks]
    
    E --> F[06_spatial<br/>Convolutional Networks]
    E --> G[07_attention<br/>Self-Attention]
    F --> H[08_dataloader<br/>Data Loading]
    B --> I[09_autograd<br/>Automatic Differentiation]
    I --> J[10_optimizers<br/>SGD & Adam]
    
    H --> K[11_training<br/>Training Loops]
    G --> K
    J --> K
    
    K --> L[12_compression<br/>Model Optimization]
    K --> M[13_kernels<br/>High-Performance Ops]
    K --> N[14_benchmarking<br/>Performance Analysis]
    K --> O[15_mlops<br/>Production Monitoring]
    
    L --> P[16_capstone<br/>Framework Engineering]
    M --> P
    N --> P
    O --> P
```

### **ğŸ¯ How It All Connects**

**Foundation (01-05):** Build your core data structures and basic operations  
**Deep Learning (06-10):** Add neural networks and automatic differentiation  
**Production (11-15):** Scale to real applications with training and production systems  
**Mastery (16):** Optimize and extend your complete framework

**The Result:** A complete, working ML framework built entirely by you, capable of:
- âœ… Training CNNs on CIFAR-10 with 90%+ accuracy
- âœ… Implementing modern optimizers (Adam, learning rate scheduling)  
- âœ… Deploying compressed models with 75% size reduction
- âœ… Production monitoring with comprehensive metrics

### **ğŸš€ Capstone: Optimize Your Framework**

After completing the 15 core modules, you have a **complete ML framework**. The final challenge: make it better through systems engineering.

**Choose Your Focus:**
- âš¡ **Performance Engineering**: GPU kernels, vectorization, memory-efficient operations
- ğŸ§  **Algorithm Extensions**: Transformer layers, BatchNorm, Dropout, advanced optimizers  
- ğŸ”§ **Systems Optimization**: Multi-GPU training, distributed computing, memory profiling
- ğŸ“Š **Benchmarking Analysis**: Compare your framework to PyTorch, identify bottlenecks
- ğŸ› ï¸ **Developer Tools**: Better debugging, visualization, error messages, testing

**The Constraint:** No `import torch` allowed. Build on **your TinyTorch implementation**. This demonstrates true mastery of ML systems engineering and optimization.

---

## ğŸ§  Pedagogical Framework: Build â†’ Use â†’ Reflect

### **Example: How You'll Master Activation Functions**

**ğŸ”§ Build:** Implement ReLU from scratch
```python
def relu(x):
    # YOU implement this function
    return ???  # What should this be?
```

**ğŸš€ Use:** Immediately use your own code
```python
from tinytorch.core.activations import ReLU  # YOUR implementation!
layer = ReLU()
output = layer.forward(input_tensor)  # Your code working!
```

**ğŸ’¡ Reflect:** See it working in real networks
```python
# Your ReLU is now part of a real neural network
model = Sequential([
    Dense(784, 128),
    ReLU(),           # <-- Your implementation
    Dense(128, 10)
])
```

**This pattern repeats for every component** â€” you build it, use it immediately, then see how it fits into larger systems.

---

## ğŸ“ Teaching Philosophy

### **No Black Boxes**
* Build every component from scratch
* Understand performance trade-offs  
* See how engineering decisions impact ML outcomes

### **Production-Ready Thinking**
* Use real datasets (CIFAR-10, MNIST)
* Implement proper testing and benchmarking
* Learn MLOps and system design principles

### **Iterative Mastery**
* Each module builds on previous work
* Immediate feedback through inline testing
* Progressive complexity with solid foundations

---

## ğŸ“– Documentation

### **Interactive Jupyter Book**
- **Live Site**: https://mlsysbook.github.io/TinyTorch/
- **Auto-updated** from source code on every release
- **Complete course content** with executable examples
- **Real implementation details** with solution code

### **Development Workflow**
- **`dev` branch**: Active development and experiments  
- **`main` branch**: Stable releases that trigger documentation deployment
- **Inline testing**: Tests embedded directly in source modules
- **Continuous integration**: Automatic building and deployment

---

## ğŸ› ï¸ Development Workflow

### **Module Development**
```bash
# Work on dev branch
git checkout dev

# Edit source modules  
cd modules/source/02_tensor
jupyter lab tensor_dev.py

# Export to package
tito export 02_tensor

# Test your implementation
tito test 02_tensor

# Build complete package
tito nbdev build
```

### **Release Process**
```bash
# Ready for release
git checkout main
git merge dev
git push origin main        # Triggers documentation deployment
```

---

## ğŸ“ Project Structure

```
TinyTorch/
â”œâ”€â”€ modules/source/XX/               # 16 source modules with inline tests
â”œâ”€â”€ tinytorch/core/                  # Your exported ML framework
â”œâ”€â”€ tito/                           # CLI and course management tools
â”œâ”€â”€ book/                           # Jupyter Book source and config
â”œâ”€â”€ tests/                          # Integration tests
â””â”€â”€ docs/                           # Development guides and workflows
```

---

## ğŸ§ª Tech Stack

* **Python 3.8+** â€” Modern Python with type hints
* **NumPy** â€” Numerical foundations  
* **Jupyter Lab** â€” Interactive development
* **Rich** â€” Beautiful CLI output
* **NBDev** â€” Literate programming and packaging
* **Jupyter Book** â€” Interactive documentation
* **GitHub Actions** â€” Continuous integration and deployment

---

## âœ… Verified Learning Outcomes

Students who complete TinyTorch can:

âœ… **Build complete neural networks** from tensors to training loops  
âœ… **Implement modern ML algorithms** (Adam, dropout, batch norm)  
âœ… **Optimize performance** with profiling and custom kernels  
âœ… **Deploy production systems** with monitoring and MLOps  
âœ… **Debug and test** ML systems with proper engineering practices  
âœ… **Understand trade-offs** between accuracy, speed, and resources  

---

## ğŸƒâ€â™€ï¸ Getting Started

### **Option 1: Interactive Course**
ğŸ‘‰ **[Start Learning Now](https://mlsysbook.github.io/TinyTorch/)** â€” Complete course in your browser

### **Option 2: Local Development**
```bash
git clone https://github.com/mlsysbook/TinyTorch.git
cd TinyTorch
pip install -r requirements.txt           # Install all dependencies (numpy, jupyter, pytest, etc.)
pip install -e .                          # Install TinyTorch package in editable mode  
tito system doctor
cd modules/source/01_setup
jupyter lab setup_dev.py
```

### **Option 3: Instructor Setup**
```bash
# Clone and verify system
git clone https://github.com/mlsysbook/TinyTorch.git
cd TinyTorch
tito system info

# Test module workflow
tito export 01_setup && tito test 01_setup
```

---

**ğŸ”¥ Ready to build your ML framework? Start with TinyTorch and understand every layer. _Start Small. Go Deep._**

---

## â“ **Frequently Asked Questions**

<details>
<summary><strong>ğŸš€ "Why not just use PyTorch/TensorFlow? This seems like reinventing the wheel."</strong></summary>

<br>

> **You're right - for production, use PyTorch!** But consider:
>
> **ğŸ¤” Deep Understanding Questions:**
> - **Do you understand what `loss.backward()` actually does?** Most engineers don't.
> - **Can you debug when gradients vanish?** You'll know why and how to fix it.  
> - **Could you optimize a custom operation?** You'll have built the primitives.
>
> **ğŸ’¡ The Learning Analogy:**  
> Think of it like this: Pilots learn in small planes before flying 747s. You're learning the fundamentals that make you a **better PyTorch engineer**.

---
</details>

<details>
<summary><strong>âš¡ "How is this different from online tutorials that build neural networks?"</strong></summary>

<br>

> **Most tutorials focus on isolated components** - a Colab here, a notebook there. TinyTorch builds a **fully integrated system**.
>
> **ğŸ—ï¸ Systems Engineering Analogy:**  
> Think of building a **compiler** or **operating system**. You don't just implement a lexer or a scheduler - you build how **every component works together**. Each piece must integrate seamlessly with the whole.
>
> **ğŸ“Š Component vs. System Approach:**
> ```python
> Component Approach:          Systems Approach (TinyTorch):
> â”œâ”€â”€ Build a neural network   â”œâ”€â”€ Build a complete ML framework
> â”œâ”€â”€ Jupyter notebook demos   â”œâ”€â”€ Full Python package with CLI
> â”œâ”€â”€ Isolated examples        â”œâ”€â”€ Integrated: tensors â†’ layers â†’ training
> â””â”€â”€ "Here's how ReLU works"  â”œâ”€â”€ Production patterns: testing, profiling
>                             â””â”€â”€ "Here's how EVERYTHING connects"
> ```
>
> **ğŸ¯ Key Insight:**  
> You learn **systems engineering**, not just individual algorithms. Like understanding how every part of a compiler interacts to turn code into executable programs.

---
</details>

<details>
<summary><strong>ğŸ’¡ "Can't I just read papers/books instead of implementing?"</strong></summary>

<br>

> **ğŸ“š Reading vs. ğŸ”§ Building:**
> ```
> Reading about neural networks:     Building neural networks:
> â”œâ”€â”€ "I understand the theory"      â”œâ”€â”€ "Why are my gradients exploding?"
> â”œâ”€â”€ "Backprop makes sense"         â”œâ”€â”€ "Oh, that's why we need gradient clipping"
> â”œâ”€â”€ "Adam is better than SGD"      â”œâ”€â”€ "Now I see when each optimizer works"
> â””â”€â”€ Theoretical knowledge          â””â”€â”€ Deep intuitive understanding
> ```
>
> **ğŸŒŸ The Reality Check:**  
> Implementation **forces you to confront reality** - edge cases, numerical stability, memory management, performance trade-offs that papers gloss over.

---
</details>

<details>
<summary><strong>ğŸ¤” "Isn't everything a Transformer now? Why learn old architectures?"</strong></summary>

<br>

> **Great question!** Transformers are indeed dominant, but they're built on the same foundations you'll implement:
>
> **ğŸ—ï¸ Transformer Building Blocks You'll Build:**
> - **Attention is just matrix operations** - which you'll build from tensors
> - **LayerNorm uses your activations and layers** 
> - **Adam optimizer powers Transformer training** - you'll implement it
> - **Multi-head attention = your Linear layers + reshaping** 
>
> **ğŸ¯ The Strategic Reality:**  
> Understanding foundations makes you the engineer who can **optimize Transformers**, not just use them. Plus, CNNs still power computer vision, RNNs drive real-time systems, and new architectures emerge constantly.

---
</details>

<details>
<summary><strong>ğŸ“ "I'm already good at ML. Is this too basic for me?"</strong></summary>

<br>

> **ğŸ§ª Challenge Test - Can You:**
> - **Implement Adam optimizer from the paper?** (Not just use `torch.optim.Adam`)
> - **Explain why ReLU causes dying neurons** and how to fix it?
> - **Debug a 50% accuracy drop** after model deployment?
>
> **ğŸ’ª Why Advanced Engineers Love TinyTorch:**  
> It fills the **"implementation gap"** that most ML education skips. You'll go from understanding concepts to implementing production systems.

---
</details>

<details>
<summary><strong>ğŸ§ª "Is this academic or practical?"</strong></summary>

<br>

> **Both!** TinyTorch bridges academic understanding with engineering reality:
>
> **ğŸ“ Academic Rigor:**
> - Mathematical foundations implemented correctly
> - Proper testing and validation methodologies
> - Research-quality implementations you can trust
>
> **âš™ï¸ Engineering Practicality:**
> - Production-style code organization and CLI tools
> - Performance considerations and optimization techniques
> - Real datasets, realistic scale, professional development workflow

---
</details>

<details>
<summary><strong>â° "How much time does this take?"</strong></summary>

<br>

> **ğŸ“Š Time Investment:** ~40-60 hours for complete framework
>
> **ğŸ¯ Flexible Learning Paths:**
> - **Quick exploration:** 1-2 modules to understand the approach
> - **Focused learning:** Core modules (01-10) for solid foundations  
> - **Complete mastery:** All 16 modules for full framework expertise
>
> **âœ¨ Self-Paced Design:**  
> Each module is self-contained, so you can stop and start as needed.

---
</details>

<details>
<summary><strong>ğŸ”„ "What if I get stuck or confused?"</strong></summary>

<br>

> **ğŸ›¡ï¸ Built-in Support System:**
> - **Progressive scaffolding:** Each step builds on the previous, with guided implementations
> - **Comprehensive testing:** 200+ tests ensure your code works correctly
> - **Rich documentation:** Visual explanations, real-world context, debugging tips
> - **Professional error messages:** Helpful feedback when things go wrong
> - **Modular design:** Skip ahead or go back without breaking your progress
>
> **ğŸ’¡ Learning Philosophy:**  
> The course is designed to **guide you through complexity**, not leave you struggling alone.

---
</details>

<details>
<summary><strong>ğŸš€ "What can I build after completing TinyTorch?"</strong></summary>

<br>

> **ğŸ—ï¸ Your Framework Becomes the Foundation For:**
> - **Research projects:** Implement cutting-edge papers on solid foundations
> - **Specialized systems:** Computer vision, NLP, robotics applications
> - **Performance engineering:** GPU kernels, distributed training, quantization
> - **Custom architectures:** New layer types, novel optimizers, experimental designs
>
> **ğŸ¯ Ultimate Skill Unlock:**  
> You'll have the implementation skills to **turn any ML paper into working code**.

---
</details>
