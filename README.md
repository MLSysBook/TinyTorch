# TinyğŸ”¥Torch: Build a Machine Learning System from Scratch

TinyTorch is a pedagogical project designed to accompany the [*Machine Learning Systems*](https://mlsysbook.ai) textbook. Inspired by OS and compiler courses where students build entire systems from first principles, TinyTorch guides you through building a complete ML training and inference runtime â€” from autograd to data pipelines, optimizers to profilers â€” **entirely from scratch**.

This is not a PyTorch tutorial. In TinyTorch, you'll **write the components that frameworks like PyTorch are built on.**

---

## ğŸ¯ What You'll Build

By the end of this project, you'll have implemented a fully functional ML system capable of:

- **Training neural networks** (MLPs, CNNs) on real datasets 10)
- **Automatic differentiation** with a custom autograd engine
- **Memory-efficient data loading** with custom DataLoader implementations
- **Multiple optimization algorithms** (SGD, Adam, RMSprop)
- **Performance profiling** and bottleneck identification
- **Model compression** through pruning and quantization
- **Custom compute kernels** for matrix operations
- **Production monitoring** with MLOps infrastructure
- **Reproducible experiments** with checkpointing and logging

**End Goal**: Train a CNN on CIFAR-10 achieving >85% accuracy using only your implementation.

---

## ğŸ§  Project Goals & Learning Objectives

### Core Learning Objectives
- **Systems Understanding**: Learn how modern ML systems are constructed, not just how to use them
- **Full-Stack ML Infrastructure**: Build core components from tensor operations to training orchestration
- **Performance Engineering**: Understand computational and memory bottlenecks in ML workloads
- **Software Architecture**: Design modular, extensible systems with clean abstractions
- **Infrastructure Thinking**: Make design decisions that impact performance, reproducibility, and maintainability

### Technical Skills Gained
- **Low-level ML Implementation**: Tensor operations, gradient computation, optimization algorithms
- **Memory Management**: Efficient data structures, gradient accumulation, batch processing
- **Performance Optimization**: Profiling, kernel optimization, memory access patterns
- **System Design**: Modular architecture, clean APIs, extensible frameworks
- **Testing & Validation**: Numerical stability, gradient checking, performance regression testing

---

## ğŸ—ï¸ System Architecture

### Core Components Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TinyTorch System                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  CLI Interface (bin/tito.py)                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Training Orchestration (trainer.py)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Model Definition     â”‚  Data Pipeline    â”‚  Optimization   â”‚
â”‚  (modules.py)         â”‚  (dataloader.py)  â”‚  (optimizer.py) â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Automatic Differentiation Engine (autograd)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Tensor Operations & Storage (tensor.py)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Profiling & MLOps (profiler.py, mlops.py)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Design Principles
1. **Modularity**: Each component has a single responsibility and clean interfaces
2. **Composability**: Components can be mixed and matched for different use cases
3. **Performance**: Designed for efficiency without sacrificing readability
4. **Extensibility**: Easy to add new layers, optimizers, and functionality
5. **Debuggability**: Built-in profiling and logging for understanding system behavior

---

## ğŸ“š Curriculum Integration & Roadmap

TinyTorch aligns with **Chapters 1â€“13** of the [*Machine Learning Systems*](https://mlsysbook.ai) textbook. Each project builds progressively toward a complete ML infrastructure.

### ğŸ“š Course Details & Learning Objectives

Each project is inspired by key themes from the [*Machine Learning Systems*](https://mlsysbook.ai) textbook:

### ğŸ“– Part I: The Essentials (Chapters 1-4)
*Core principles, components, and architectures*

| Project | Chapter | Core Learning | Key Deliverable |
|---------|---------|---------------|-----------------|
| Setup | 1 (Introduction) | Environment setup, tool familiarity | Working dev environment + CLI |
| Tensor | 2 (ML Systems) | Basic tensor operations, NumPy-style | Simple Tensor class with math ops |
| MLP | 3 (DL Primer) | Forward pass, manual backprop | Train MLP on MNIST (manual gradients) |
| CNN | 4 (DNN Architectures) | Convolution concepts, architectures | Basic conv implementation |

### ğŸ—ï¸ Part II: Engineering Principles (Chapters 5-13)
*Workflows, data engineering, optimization strategies, and operational challenges*

| Project | Chapter | Core Learning | Key Deliverable |
|---------|---------|---------------|-----------------|
| Data | 6 (Data Engineering) | Efficient data loading, batching | Custom DataLoader with transformations |
| Training | 8 (AI Training) | **Autograd engine**, optimization algorithms | Complete training system with autodiff |
| Profiling | 9 (Efficient AI) | Performance measurement, debugging | Memory/compute profiler with visualizations |
| Compression | 10 (Model Optimizations) | Pruning, quantization techniques | Compress model while maintaining accuracy |
| Kernels | 11 (AI Acceleration) | Low-level optimization, vectorization | Optimized matrix multiplication kernels |
| Benchmarking | 12 (Benchmarking AI) | Performance testing, comparison | Comprehensive benchmarking suite |
| MLOps | 13 (ML Operations) | Production monitoring, deployment | MLOps pipeline with drift detection |

**Note**: Chapters 5 (AI Workflow) and 7 (AI Frameworks) provide conceptual frameworks that inform the systems projects. Part III (AI Best Practice, Chapters 14-18) and Part IV (Closing Perspectives, Chapters 19-20) focus on deployment considerations and emerging trends covered through readings and discussions.

### Milestone Targets

- **Week 1**: Environment setup (`setup`) and basic command familiarity
- **Week 3**: Basic tensor operations (`tensor`) working
- **Week 5**: Train MLP on MNIST (`mlp`) with manual backprop achieving >90% accuracy  
- **Week 7**: Train CNN on CIFAR-10 (`cnn`) basic implementation achieving >70% accuracy
- **Week 9**: Data pipeline (`data`) operational with efficient loading
- **Week 11**: Complete autograd engine and training framework (`training`) working
- **Week 13**: Optimized system with profiling tools (`profiling`)
- **Final**: Complete production system with MLOps monitoring (`mlops`)

---

## ğŸ“¦ Course Repository Structure

```
TinyTorch/
â”œâ”€â”€ bin/                           # Command-line interfaces
â”‚   â””â”€â”€ tito.py                    # Main TinyTorch CLI (tito)
â”œâ”€â”€ tinytorch/                     # Core ML system package
â”‚   â”œâ”€â”€ core/                      # Core ML components
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ tensor.py              # Tensor class with autograd support
â”‚   â”‚   â”œâ”€â”€ autograd.py            # Automatic differentiation engine
â”‚   â”‚   â”œâ”€â”€ modules.py             # Neural network layers and models
â”‚   â”‚   â”œâ”€â”€ functional.py          # Core operations (conv2d, relu, etc.)
â”‚   â”‚   â”œâ”€â”€ dataloader.py          # Data loading and preprocessing
â”‚   â”‚   â”œâ”€â”€ optimizer.py           # Optimization algorithms
â”‚   â”‚   â”œâ”€â”€ trainer.py             # Training loop orchestration
â”‚   â”‚   â”œâ”€â”€ profiler.py            # Performance measurement tools
â”‚   â”‚   â”œâ”€â”€ benchmark.py           # Benchmarking and evaluation
â”‚   â”‚   â”œâ”€â”€ mlops.py               # MLOps and production monitoring
â”‚   â”‚   â””â”€â”€ utils.py               # Utility functions
â”‚   â”œâ”€â”€ configs/                   # Configuration files
â”‚   â”‚   â”œâ”€â”€ default.yaml           # Default training configuration
â”‚   â”‚   â”œâ”€â”€ models/                # Model-specific configs
â”‚   â”‚   â””â”€â”€ datasets/              # Dataset-specific configs
â”‚   â””â”€â”€ datasets/                  # Dataset implementations
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ mnist.py
â”‚       â”œâ”€â”€ cifar10.py
â”‚       â””â”€â”€ transforms.py
â”œâ”€â”€ modules/                      # ğŸ§© System Modules
â”‚   â”œâ”€â”€ 01_setup/                # Environment setup & onboarding
â”‚   â”œâ”€â”€ 02_tensor/               # Basic tensor implementation
â”‚   â”œâ”€â”€ 03_mlp/                  # Multi-layer perceptron (manual backprop)
â”‚   â”œâ”€â”€ 04_cnn/                  # Convolutional neural networks (basic)
â”‚   â”œâ”€â”€ 05_data/                 # Data pipeline & loading
â”‚   â”œâ”€â”€ 06_training/             # Autograd engine & training optimization
â”‚   â”œâ”€â”€ 07_profiling/            # Performance profiling tools
â”‚   â”œâ”€â”€ 08_compression/          # Model compression techniques
â”‚   â”œâ”€â”€ 09_kernels/              # Custom compute kernels
â”‚   â”œâ”€â”€ 10_benchmarking/         # Performance benchmarking
â”‚   â””â”€â”€ 11_mlops/                # MLOps & production monitoring
â”œâ”€â”€ docs/                         # Course documentation
â”‚   â”œâ”€â”€ tutorials/                # Step-by-step tutorials
â”‚   â”œâ”€â”€ api/                      # API documentation
â”‚   â””â”€â”€ lectures/                 # Lecture materials
â”œâ”€â”€ notebooks/                    # Jupyter tutorials and demos
â”œâ”€â”€ examples/                     # Working examples and demos
â”‚   â”œâ”€â”€ train_mnist_mlp.py
â”‚   â”œâ”€â”€ train_cifar_cnn.py
â”‚   â””â”€â”€ benchmark_ops.py
â”œâ”€â”€ tests/                        # Comprehensive test suite
â”‚   â”œâ”€â”€ test_tensor.py
â”‚   â”œâ”€â”€ test_autograd.py
â”‚   â”œâ”€â”€ test_modules.py
â”‚   â””â”€â”€ test_training.py
â”œâ”€â”€ grading/                      # Course grading materials
â”‚   â”œâ”€â”€ rubrics/                  # Assignment rubrics
â”‚   â”œâ”€â”€ autograders/              # Automated grading scripts
â”‚   â””â”€â”€ solutions/               # Reference solutions
â”œâ”€â”€ resources/                    # Course resources
â”‚   â”œâ”€â”€ datasets/                 # Course datasets
â”‚   â”œâ”€â”€ pretrained/              # Pre-trained models
â”‚   â””â”€â”€ references/              # Reference materials
â”œâ”€â”€ logs/                         # Training logs and artifacts
â”‚   â””â”€â”€ runs/
â”œâ”€â”€ checkpoints/                  # Model checkpoints
â”œâ”€â”€ requirements.txt              # Python dependencies
â””â”€â”€ README.md                     # This file
```

---

## ğŸ¯ Course Navigation & Getting Started

**New to TinyTorch?** Start here: [`modules/01_setup/README.md`](modules/01_setup/README.md)

### ğŸ“‹ Module Sequence
Each module builds on the previous ones. Click the links to jump to specific instructions:

| Order | Module | Status | Description | Instructions |
|-------|--------|--------|-------------|--------------|
| 0 | **Setup** | ğŸš€ **START HERE** | Environment & CLI setup | [`modules/01_setup/README.md`](modules/01_setup/README.md) |
| 1 | **Tensor** | â³ Coming Next | Basic tensor operations | [`modules/02_tensor/README.md`](modules/02_tensor/README.md) |
| 2 | **MLP** | â³ Future | Multi-layer perceptron (manual backprop) | [`modules/03_mlp/README.md`](modules/03_mlp/README.md) |
| 3 | **CNN** | â³ Future | Convolutional networks (basic) | [`modules/04_cnn/README.md`](modules/04_cnn/README.md) |
| 4 | **Data** | â³ Future | Data loading pipeline | [`modules/05_data/README.md`](modules/05_data/README.md) |
| 5 | **Training** | â³ Future | Autograd engine & optimization | [`modules/06_training/README.md`](modules/06_training/README.md) |
| 6 | **Profiling** | â³ Future | Performance profiling | [`modules/07_profiling/README.md`](modules/07_profiling/README.md) |
| 7 | **Compression** | â³ Future | Model compression | [`modules/08_compression/README.md`](modules/08_compression/README.md) |
| 8 | **Kernels** | â³ Future | Custom compute kernels | [`modules/09_kernels/README.md`](modules/09_kernels/README.md) |
| 9 | **Benchmarking** | â³ Future | Performance benchmarking | [`modules/10_benchmarking/README.md`](modules/10_benchmarking/README.md) |
| 10 | **MLOps** | â³ Future | Production monitoring | [`modules/11_mlops/README.md`](modules/11_mlops/README.md) |

### ğŸš€ Quick Start Guide
**First time?** Follow this exact sequence:

1. **ğŸ“– Read the overview** (you're here!)
2. **ğŸ¯ Detailed guidance**: [`COURSE_GUIDE.md`](COURSE_GUIDE.md) (comprehensive walkthrough)
3. **ğŸš€ One-command setup**: `python3 bin/tito.py setup` (sets up everything!)
4. **âœ… Run activation**: Follow the command it gives you (usually `bin/activate-tinytorch.sh`)
5. **ğŸ“– Start building**: [`projects/setup/README.md`](projects/setup/README.md)

### Prerequisites
- **Python 3.8+** (type hints and modern features required)
- **NumPy** (numerical computations)
- **Optional**: Numba (JIT compilation for performance)
- **Development**: pytest, black, mypy (for testing and code quality)

### Environment Setup
```bash
# Clone and setup environment
git clone <repository-url>
cd TinyTorch

# ONE SMART COMMAND - shows clear commands to copy
python3 bin/tito.py setup
# â†³ First time: Creates environment, installs dependencies, shows activation command
# â†³ Already exists: Shows activation command to copy
# â†³ Already active: Can show deactivation command

# Copy and run the highlighted command, then:
tito info  # Check system status
```

### For Instructors
```bash
# Set up the complete course environment
pip install -r requirements.txt

# Generate course materials
python3 bin/tito.py generate-projects
python3 bin/tito.py setup-autograders

# View course progress
python3 bin/tito.py course-status
```

### For Students
```bash
# ONE SMART COMMAND - handles everything (IMPORTANT!)
python3 bin/tito.py setup
# â†³ Shows clear commands to copy and paste!

# Start with Project 0: Setup  
cd projects/setup/
cat README.md  # Read instructions
python3 -m pytest test_setup.py -v  # Run tests

# Then move through the sequence (Part I: The Essentials)
cd ../tensor/           # Project 1: Basic tensors
cd ../mlp/              # Project 2: Multi-layer perceptron (manual backprop)
cd ../cnn/              # Project 3: Convolutional networks (basic)

# Part II: Engineering Principles  
cd ../data/             # Project 4: Data pipeline
cd ../training/         # Project 5: Autograd engine & training

# Always run tests before submitting
python3 -m pytest projects/tensor/test_tensor.py -v
python3 bin/tito.py submit --project tensor

python3 -m pytest projects/mlp/test_mlp.py -v
python3 bin/tito.py submit --project mlp
```

---

## ğŸ¯ Implementation Guidelines

### Code Quality Standards
- **Type Hints**: All public APIs must have complete type annotations
- **Documentation**: Docstrings for all classes and public methods
- **Testing**: >90% code coverage with unit and integration tests
- **Performance**: Profile-guided optimization with benchmarking
- **Style**: Black code formatting, consistent naming conventions

### API Design Principles
- **Familiar Interface**: Similar to PyTorch where it makes sense (for learning transfer)
- **Explicit Over Implicit**: Clear parameter names and behavior
- **Composable**: Small, focused components that work together
- **Debuggable**: Rich error messages and debugging hooks

### Performance Targets
- **MNIST MLP**: <5 seconds per epoch on modern laptop
- **CIFAR-10 CNN**: <30 seconds per epoch on modern laptop
- **Memory Usage**: <2GB RAM for standard training runs
- **Numerical Stability**: Gradient checking passes for all operations

---

## ğŸ§ª Testing & Validation Strategy

### Test Categories
1. **Unit Tests**: Individual component functionality
2. **Integration Tests**: Component interaction and data flow
3. **Numerical Tests**: Gradient checking and mathematical correctness
4. **Performance Tests**: Regression testing for speed and memory
5. **End-to-End Tests**: Complete training runs with known results

### Validation Methodology
- **Gradient Checking**: Numerical verification of all autodiff operations
- **Reference Comparisons**: Output validation against NumPy/PyTorch (where applicable)
- **Convergence Testing**: Training curves must match expected behavior
- **Ablation Studies**: Systematic testing of individual components

---

## ğŸ’¡ Educational Philosophy

> "You don't really understand a system until you've built it."

### Learning Through Building
TinyTorch emphasizes **active construction** over passive consumption. Students don't just learn about autogradâ€”they implement it. They don't just use optimizersâ€”they write them from scratch.

### Systems Thinking
By building a complete system, students understand:
- **Abstraction Boundaries**: What belongs where in the system hierarchy
- **Performance Trade-offs**: How design decisions impact speed and memory
- **Debugging Strategies**: How to trace problems through complex systems
- **Integration Challenges**: How components interact and depend on each other

### Real-World Relevance
Every component in TinyTorch has a direct analog in production ML systems. The skills learned here transfer directly to understanding and contributing to frameworks like PyTorch, TensorFlow, and JAX.

---

## ğŸ”§ Advanced Features & Extensions

### Chapter 13: MLOps Deep Dive

**Core MLOps Components** (Chapter 13 will implement):
- **Data Drift Detection**: Statistical tests for distribution shifts in input features
- **Model Performance Monitoring**: Track accuracy, latency, and throughput in production  
- **Automatic Retraining Triggers**: When to retrain based on performance degradation
- **A/B Testing Framework**: Compare model versions safely in production
- **Model Registry**: Version control and metadata tracking for deployed models
- **Alert Systems**: Notifications for model failures or performance drops
- **Rollback Mechanisms**: Safe deployment and quick rollback strategies

**Production Integration**:
- **REST API Serving**: Deploy models as web services
- **Batch Inference Pipelines**: Large-scale offline predictions
- **Feature Store Integration**: Consistent feature engineering across training/serving
- **Monitoring Dashboards**: Real-time system health visualization

### Optional Advanced Components
- **Mixed Precision Training**: FP16/FP32 mixed precision implementation
- **Distributed Training**: Multi-GPU and multi-node training support
- **Dynamic Graphs**: Support for variable computation graphs
- **Custom Operators**: Framework for implementing new operations
- **JIT Compilation**: Integration with Numba or custom compilation

### Research Extensions
- **Novel Optimizers**: Implement cutting-edge optimization algorithms
- **Architecture Search**: Automated neural architecture search
- **Compression Techniques**: Advanced pruning and quantization methods
- **Hardware Acceleration**: GPU kernels and specialized hardware support

---

## ğŸ“Š Success Metrics

### Technical Milestones
- [ ] Train MLP on MNIST achieving >95% accuracy
- [ ] Implement working autograd engine with gradient checking
- [ ] Train CNN on CIFAR-10 achieving >85% accuracy
- [ ] Profile and optimize for 2x performance improvement
- [ ] Complete all core project implementations

### Learning Outcomes Assessment
- **Code Reviews**: Peer and instructor evaluation of implementations
- **Design Document**: Architecture decisions and trade-off analysis
- **Performance Analysis**: Profiling report and optimization strategy
- **Presentation**: Explain system design and key insights learned

---

## ğŸ¤ Course Management

### For Instructors

**Project Management**:
- Each chapter has structured projects in `projects/XX-name/`
- Rubrics and grading criteria in `grading/rubrics/`
- Automated testing and grading in `grading/autograders/`
- Reference solutions in `grading/solutions/`

**Progress Tracking**:
- Student progress dashboards
- Automated testing and feedback
- Performance benchmarking
- Code quality metrics

### For Students

**Development Process**:
1. **Start Each Project**: Read project description in `projects/XX-name/README.md`
2. **Implement Features**: Follow step-by-step guided implementation
3. **Run Tests**: Use automated tests to validate implementation
4. **Submit Work**: Automated submission and grading system
5. **Get Feedback**: Detailed feedback on implementation and performance

### Getting Help
- **Documentation**: Comprehensive docs in `docs/`
- **Tutorials**: Step-by-step tutorials in `notebooks/`
- **Office Hours**: Regular sessions for questions and debugging
- **Peer Discussion**: Collaborative learning encouraged
- **Issue Tracking**: GitHub issues for bugs and feature requests

---

## ğŸ“¬ License and Attribution

TinyTorch is part of the *Machine Learning Systems* course and textbook by Vijay Janapa Reddi et al. Inspired by systems-style pedagogical projects like xv6 (OS), PintOS (OS), and cs231n assignments (ML).

**License**: MIT  
**Citation**: Please cite the Machine Learning Systems textbook when using this educational material.

---

## ğŸ”— Additional Resources

- **Textbook**: [*Machine Learning Systems*](https://mlsysbook.ai) (Chapters 1-13) | [PDF](https://mlsysbook.ai/Machine-Learning-Systems.pdf)
- **Course Website**: Coming soon
- **Video Lectures**: Coming soon
- **External Reading**: Coming soon
- **Community Forum**: [GitHub Discussions](../../discussions)
- **Office Hours**: Coming soon
