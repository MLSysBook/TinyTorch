# TinyğŸ”¥Torch 

**Build your own ML framework. Start small. Go deep.**

![Work in Progress](https://img.shields.io/badge/status-work--in--progress-yellow)
![Educational Project](https://img.shields.io/badge/purpose-educational-informational)
[![GitHub](https://img.shields.io/badge/github-mlsysbook/TinyTorch-blue.svg)](https://github.com/MLSysBook/TinyTorch)
[![Python](https://img.shields.io/badge/python-3.8+-green.svg)](https://python.org)
[![Jupyter Book](https://img.shields.io/badge/docs-Jupyter_Book-orange.svg)](https://mlsysbook.github.io/TinyTorch/)

ğŸ“š **[Read the Interactive Course â†’](https://mlsysbook.github.io/TinyTorch/)**

---

## ğŸ—ï¸ **The Big Picture: Why Build from Scratch?**

**Most ML education teaches you to _use_ frameworks.** TinyTorch teaches you to _build_ them.

```python
Traditional ML Course:          TinyTorch Approach:
â”œâ”€â”€ import torch               â”œâ”€â”€ class Tensor:
â”œâ”€â”€ model = nn.Linear(10, 1)   â”‚     def __add__(self, other): ...
â”œâ”€â”€ loss = nn.MSELoss()        â”‚     def backward(self): ...
â””â”€â”€ optimizer.step()           â”œâ”€â”€ class Linear:
                               â”‚     def forward(self, x):
                               â”‚       return x @ self.weight + self.bias
                               â”œâ”€â”€ def mse_loss(pred, target):
                               â”‚     return ((pred - target) ** 2).mean()
                               â”œâ”€â”€ class SGD:
                               â”‚     def step(self):
                               â””â”€â”€     param.data -= lr * param.grad

Go from "How does this work?" ğŸ¤· to "I implemented every line!" ğŸ’ª
```

**Result:** You become the person others come to when they need to understand "how PyTorch actually works under the hood."

---

## ğŸŒŸ **What Makes TinyTorch Different**

### **ğŸ”¬ Build-First Philosophy**
- **No black boxes**: Implement every component from scratch
- **Immediate ownership**: Use YOUR code in real neural networks
- **Deep understanding**: Know exactly how each piece works

### **ğŸš€ Real Production Skills**
- **Professional workflow**: Development with `tito` CLI, automated testing
- **Real datasets**: Train on CIFAR-10, not toy data
- **Production patterns**: MLOps, monitoring, optimization from day one

### **ğŸ¯ Progressive Mastery** 
- **Start simple**: Implement `hello_world()` function
- **Build systematically**: Each module enables the next
- **End powerful**: Deploy production ML systems with monitoring

### **âš¡ Instant Feedback**
- **Code works immediately**: No waiting to see results
- **Visual progress**: Success indicators and system integration
- **"Aha moments"**: Watch your `ReLU` power real neural networks

---

## ğŸ¯ What You'll Build

* **One Complete ML Framework** â€” Not 14 separate exercises, but integrated components building into your own PyTorch-style toolkit
* **Fully Functional System** â€” Every piece connects: your tensors power your layers, your autograd enables your optimizers, your framework trains real networks
* **Real Applications** â€” Train neural networks on CIFAR-10 using 100% your own code, no PyTorch imports
* **Production-Ready Skills** â€” Complete ML lifecycle: data loading, training, optimization, deployment, monitoring
* **Deep Systems Understanding** â€” Know exactly how every component works and integrates because you built it all

---

## ğŸš€ Quick Start (2 minutes)

### ğŸ§‘â€ğŸ“ **Students**

```bash
git clone https://github.com/mlsysbook/TinyTorch.git
cd TinyTorch
pip install -e .
tito system doctor                         # Verify your setup
cd modules/source/01_setup
jupyter lab setup_dev.py                  # Launch your first module
```

### ğŸ‘©â€ğŸ« **Instructors**

```bash
# System check
tito system info
tito system doctor

# Module workflow
tito export 01_setup
tito test 01_setup
tito nbdev build                          # Update package
```

---

## ğŸ“š Complete Course: 14 Modules

### **ğŸ—ï¸ Foundations** (Modules 01-05)
* **01_setup**: Development environment and CLI tools
* **02_tensor**: N-dimensional arrays and tensor operations  
* **03_activations**: ReLU, Sigmoid, Tanh, Softmax functions
* **04_layers**: Dense layers and matrix operations
* **05_networks**: Sequential networks and MLPs

### **ğŸ§  Deep Learning** (Modules 06-09)
* **06_cnn**: Convolutional neural networks and image processing
* **07_dataloader**: Data loading, batching, and preprocessing
* **08_autograd**: Automatic differentiation and backpropagation  
* **09_optimizers**: SGD, Adam, and learning rate scheduling

### **âš¡ Systems & Production** (Modules 10-14)
* **10_training**: Training loops, metrics, and validation
* **11_compression**: Model pruning, quantization, and distillation
* **12_kernels**: Performance optimization and custom operations
* **13_benchmarking**: Profiling, testing, and performance analysis
* **14_mlops**: Monitoring, deployment, and production systems

**Status**: All 14 modules complete with inline tests and educational content

---

## ğŸ”— **Complete System Integration**

**This isn't 14 isolated assignments.** Every component you build integrates into one cohesive, fully functional ML framework:

```
Module 02: Tensor operations  â†’  Module 03: Activation functions  â†’  Module 04: Dense layers
     â†“                               â†“                                â†“
Module 08: Autograd system    â†’  Module 09: SGD/Adam optimizers  â†’  Module 10: Training loops
     â†“                               â†“                                â†“  
Module 11: Model compression  â†’  Module 13: Benchmarking tools   â†’  Module 14: MLOps monitoring
```

**The Result:** A complete, working ML framework built entirely by you, capable of:
- âœ… Training CNNs on CIFAR-10 with 90%+ accuracy
- âœ… Implementing modern optimizers (Adam, learning rate scheduling)  
- âœ… Deploying compressed models with 75% size reduction
- âœ… Production monitoring with comprehensive metrics

### **ğŸš€ Capstone: Your Framework, Your Project**

After completing the 14 core modules, you have a **complete ML framework**. The final challenge: build something real using **only your TinyTorch implementation**.

**Choose Your Application:**
- ğŸ–¼ï¸ **Computer Vision**: Object detection, image segmentation, style transfer
- ğŸ“ **Natural Language**: Sentiment analysis, text generation, translation  
- ğŸµ **Audio/Speech**: Voice recognition, music generation, audio classification
- ğŸ§¬ **Scientific ML**: Molecular modeling, climate prediction, medical imaging
- ğŸ® **Creative AI**: Game AI, generative art, interactive systems

**The Constraint:** No `import torch` allowed. Use your own tensors, your own layers, your own optimizers. This proves your framework works and demonstrates true mastery of ML systems engineering.

---

## ğŸ§  Pedagogical Framework: Build â†’ Use â†’ Reflect

### **Example: How You'll Master Activation Functions**

**ğŸ”§ Build:** Implement ReLU from scratch
```python
def relu(x):
    # YOU implement this function
    return ???  # What should this be?
```

**ğŸš€ Use:** Immediately use your own code
```python
from tinytorch.core.activations import ReLU  # YOUR implementation!
layer = ReLU()
output = layer.forward(input_tensor)  # Your code working!
```

**ğŸ’¡ Reflect:** See it working in real networks
```python
# Your ReLU is now part of a real neural network
model = Sequential([
    Dense(784, 128),
    ReLU(),           # <-- Your implementation
    Dense(128, 10)
])
```

**This pattern repeats for every component** â€” you build it, use it immediately, then see how it fits into larger systems.

---

## ğŸ“ Teaching Philosophy

### **No Black Boxes**
* Build every component from scratch
* Understand performance trade-offs  
* See how engineering decisions impact ML outcomes

### **Production-Ready Thinking**
* Use real datasets (CIFAR-10, MNIST)
* Implement proper testing and benchmarking
* Learn MLOps and system design principles

### **Iterative Mastery**
* Each module builds on previous work
* Immediate feedback through inline testing
* Progressive complexity with solid foundations

---

## ğŸ“– Documentation

### **Interactive Jupyter Book**
- **Live Site**: https://mlsysbook.github.io/TinyTorch/
- **Auto-updated** from source code on every release
- **Complete course content** with executable examples
- **Real implementation details** with solution code

### **Development Workflow**
- **`dev` branch**: Active development and experiments  
- **`main` branch**: Stable releases that trigger documentation deployment
- **Inline testing**: Tests embedded directly in source modules
- **Continuous integration**: Automatic building and deployment

---

## ğŸ› ï¸ Development Workflow

### **Module Development**
```bash
# Work on dev branch
git checkout dev

# Edit source modules  
cd modules/source/02_tensor
jupyter lab tensor_dev.py

# Export to package
tito export 02_tensor

# Test your implementation
tito test 02_tensor

# Build complete package
tito nbdev build
```

### **Release Process**
```bash
# Ready for release
git checkout main
git merge dev
git push origin main        # Triggers documentation deployment
```

---

## ğŸ“ Project Structure

```
TinyTorch/
â”œâ”€â”€ modules/source/XX/               # 14 source modules with inline tests
â”œâ”€â”€ tinytorch/core/                  # Your exported ML framework
â”œâ”€â”€ tito/                           # CLI and course management tools
â”œâ”€â”€ book/                           # Jupyter Book source and config
â”œâ”€â”€ tests/                          # Integration tests
â””â”€â”€ docs/                           # Development guides and workflows
```

---

## ğŸ§ª Tech Stack

* **Python 3.8+** â€” Modern Python with type hints
* **NumPy** â€” Numerical foundations  
* **Jupyter Lab** â€” Interactive development
* **Rich** â€” Beautiful CLI output
* **NBDev** â€” Literate programming and packaging
* **Jupyter Book** â€” Interactive documentation
* **GitHub Actions** â€” Continuous integration and deployment

---

## âœ… Verified Learning Outcomes

Students who complete TinyTorch can:

âœ… **Build complete neural networks** from tensors to training loops  
âœ… **Implement modern ML algorithms** (Adam, dropout, batch norm)  
âœ… **Optimize performance** with profiling and custom kernels  
âœ… **Deploy production systems** with monitoring and MLOps  
âœ… **Debug and test** ML systems with proper engineering practices  
âœ… **Understand trade-offs** between accuracy, speed, and resources  

---

## ğŸƒâ€â™€ï¸ Getting Started

### **Option 1: Interactive Course**
ğŸ‘‰ **[Start Learning Now](https://mlsysbook.github.io/TinyTorch/)** â€” Complete course in your browser

### **Option 2: Local Development**
```bash
git clone https://github.com/mlsysbook/TinyTorch.git
cd TinyTorch
pip install -e .
tito system doctor
cd modules/source/01_setup
jupyter lab setup_dev.py
```

### **Option 3: Instructor Setup**
```bash
# Clone and verify system
git clone https://github.com/mlsysbook/TinyTorch.git
cd TinyTorch
tito system info

# Test module workflow
tito export 01_setup && tito test 01_setup
```

---

**ğŸ”¥ Ready to build your ML framework? Start with TinyTorch and understand every layer. _Start Small. Go Deep._**
