# TinyTorch: Build a Machine Learning System from Scratch

TinyTorch is a pedagogical project designed to accompany the *Machine Learning Systems* textbook. Inspired by OS and compiler courses where students build entire systems from first principles, TinyTorch guides you through building a complete ML training and inference runtime â€” from autograd to data pipelines, optimizers to profilers â€” **entirely from scratch**.

This is not a PyTorch tutorial. In TinyTorch, youâ€™ll **write the components that frameworks like PyTorch are built on.**

---

## ğŸ§  Project Goals

- **Systems Understanding**: Learn how modern ML systems are constructed, not just how to use them.
- **Full-Stack ML Infra**: Build core components such as tensor classes, autograd engine, data loaders, optimizers, profilers, and benchmarking tools.
- **Modularity**: Understand abstraction boundaries in ML systems â€” what belongs in a model vs. a trainer vs. a runtime.
- **Infrastructure Thinking**: Learn how design decisions impact performance, reproducibility, deployment, and maintainability.

---

## ğŸ“š Curriculum Integration

TinyTorch aligns with **Chapters 1â€“13** of the *Machine Learning Systems* textbook. Each chapter is paired with a corresponding system component youâ€™ll implement. This forms a progressively richer ML infrastructure, culminating in a working system that can train and evaluate models on real data.

| Chapter | Component |
|--------|-----------|
| 1â€“2 | CLI + system architecture scaffold |
| 3 | Forward/backward engine for MLP |
| 4 | CNN layers: Conv2D, MaxPool |
| 5 | Configs + artifact logging |
| 6 | DataLoader and Dataset |
| 7 | Autograd engine |
| 8 | Training loop and optimizers |
| 9 | Profiling tools |
| 10 | Pruning and quantization |
| 11 | Custom matmul kernels |
| 12 | Benchmarking harness |
| 13 | Checkpointing + experiment reproducibility |

---

## ğŸ“¦ Repository Structure (early preview)

tinytorch/
â”œâ”€â”€ core/
â”‚ â”œâ”€â”€ tensor.py # Autograd and data storage
â”‚ â”œâ”€â”€ modules.py # Layers and model composition
â”‚ â”œâ”€â”€ dataloader.py # Data loading and batching
â”‚ â”œâ”€â”€ optimizer.py # SGD, Adam, etc.
â”‚ â”œâ”€â”€ trainer.py # Training loop
â”‚ â”œâ”€â”€ profiler.py # Runtime measurement
â”‚ â””â”€â”€ benchmark.py # Latency, throughput, energy est.
â”œâ”€â”€ configs/
â”‚ â””â”€â”€ default.yaml
â”œâ”€â”€ logs/
â”‚ â””â”€â”€ run_YYYY_MM_DD/
â”œâ”€â”€ main.py # CLI entrypoint
â””â”€â”€ README.md

---

## ğŸš€ Getting Started

Each chapter will introduce a new concept and a programming assignment that builds toward the full TinyTorch runtime.

**To begin:**
1. Clone this repository.
2. Follow the instructions in `chapters/01-intro/README.md` to implement the CLI.
3. Proceed incrementally â€” each module builds on the last.

---

## ğŸ’¡ Philosophy

> â€œYou donâ€™t really understand a system until youâ€™ve built it.â€

TinyTorch aims to give students that understanding â€” by reconstructing the guts of machine learning infrastructure in a clean, minimal, yet fully functional form.

By the end, youâ€™ll not only have built a system capable of training neural networks â€” youâ€™ll understand every line that makes it work.

---

## ğŸ§° Requirements

- Python 3.8+
- NumPy
- (Optional: Numba for kernel acceleration)

No PyTorch. No TensorFlow. No black boxes.

---

## ğŸ“¬ License and Attribution

TinyTorch is part of the *Machine Learning Systems* course and textbook by Vijay Janapa Reddi et al. Inspired by systems-style projects like xv6, PintOS, and cs231n assignments.

License: MIT
